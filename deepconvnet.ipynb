{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IS_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dpostolovski/eeg_is/blob/train_compare_full_data/deepconvnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBqW8_NNlAgz",
        "colab_type": "text"
      },
      "source": [
        "<h1>\n",
        "  <img alt=\"FINKI **LOGO**\" height=\"30px\" src=\"https://www.finki.ukim.mk/Content/dataImages/downloads/logo-large-500x500_2.png\" hspace=\"10px\" vspace=\"0px\">\n",
        "  Интелигентни системи - Лабораториска вежба 2 (Претпроцесирање)\n",
        "</h1>\n",
        "<center><h3><i>Група 5<i><h3></center>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTx3XQgUOhCX",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "996f6323-607e-4e90-eb5b-d2c5fb9aa658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "#@title Монтирање на Google Drive податочниот систем\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLrrPj8clKjP",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "4c663284-df4a-493f-ef16-6e6461741abb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "#@title Инсталирање и вчитување на потребните библиотеки\n",
        "\n",
        "# Библиотека за истражување, визуелизација и анализирање на човечки \n",
        "# неврофизиолошки податоци (EEG, sEEG и др)\n",
        "!pip install mne \n",
        "!pip install termcolor\n",
        "\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, date, time\n",
        "import pandas as pd\n",
        "from termcolor import colored\n",
        "import mne\n",
        "from sklearn.decomposition import PCA, FastICA\n",
        "import mne"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mne\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/1a/8fd2d3b0065597f4aafe9d90eb62db0d40547f818f128349c6878b657302/mne-0.20.3-py3-none-any.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from mne) (1.18.4)\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from mne) (1.4.1)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-0.20.3\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (1.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NeWVBhf1VxlH",
        "outputId": "db96f216-215a-4e06-b831-7a31075eb648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "%tensorflow_version 1.12.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.12.0`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBEiowYKtnPJ",
        "colab_type": "code",
        "outputId": "69800a4d-4801-490f-fa24-138e491edc96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "!wget \"https://raw.githubusercontent.com/vlawhern/arl-eegmodels/master/EEGModels.py\"\n",
        "!mkdir saved_models"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-10 23:56:01--  https://raw.githubusercontent.com/vlawhern/arl-eegmodels/master/EEGModels.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18283 (18K) [text/plain]\n",
            "Saving to: ‘EEGModels.py’\n",
            "\n",
            "\rEEGModels.py          0%[                    ]       0  --.-KB/s               \rEEGModels.py        100%[===================>]  17.85K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2020-05-10 23:56:02 (2.35 MB/s) - ‘EEGModels.py’ saved [18283/18283]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWGg4jBTlCR9",
        "colab_type": "code",
        "outputId": "b5bececa-af20-40e9-af5e-e2b56bbdedf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!readlink -f ~/.keras/keras.json"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.keras/keras.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c8w9tgTiV53",
        "colab_type": "code",
        "outputId": "b253a195-d490-47f2-822e-ce2e097c00d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "#@title Fourier analysis\n",
        "# Вчитување на податоците\n",
        "\n",
        "from sklearn import metrics \n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint \n",
        "from sklearn.model_selection import train_test_split\n",
        "from EEGModels import EEGNet,ShallowConvNet\n",
        "import scipy.io as sio\n",
        "from scipy.fft import fft\n",
        "from scipy import signal\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import os\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow import keras\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "print(open(\"/root/.keras/keras.json\").read())\n",
        "\n",
        "# Вчитување на податоците\n",
        "data = loadmat('drive/My Drive/Интелигентни Системи/Data/SBJ01/S01-Train/trainData.mat')['trainData'] \n",
        "for i in range(1, 16):\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  for j in range(1, 4):\n",
        "    if i == 1 and j == 1: continue\n",
        "    file_train_set = 'S' + format(j, '02') + '-Train'\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/trainData.mat\"\n",
        "    temp = loadmat(full_path)['trainData'] \n",
        "    data = np.concatenate((data, temp), axis=2)\n",
        "\n",
        "print(data.size)\n",
        "print(data.shape)\n",
        "\n",
        "# Вчитување на label-ите\n",
        "labels_arr = np.empty(0)\n",
        "for i in range(1, 16):\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  for j in range(1, 4):\n",
        "    file_train_set = 'S' + format(j, '02') + '-Train'\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/trainLabels.txt\"\n",
        "    with open(full_path, \"r\") as file_labels:\n",
        "      temp = file_labels.read().splitlines()\n",
        "      temp = np.repeat(temp, 10)\n",
        "      labels_arr = np.concatenate((labels_arr, temp))\n",
        "\n",
        "print(labels_arr)\n",
        "print(len(labels_arr))\n",
        "\n",
        "# Вчитување на редоследот на светкање\n",
        "events_arr = np.empty(0)\n",
        "for i in range(1, 16):\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  for j in range(1, 4):\n",
        "    file_train_set = 'S' + format(j, '02') + '-Train'\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      events_arr = np.concatenate((events_arr, temp))\n",
        "\n",
        "# Вчитување на редоследот на објекти кои се target\n",
        "targets_arr = np.empty(0)\n",
        "for i in range(1, 16):\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  for j in range(1, 4):\n",
        "    file_train_set = 'S' + format(j, '02') + '-Train'\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/trainTargets.txt\"\n",
        "    with open(full_path, \"r\") as file_targets:\n",
        "      temp = file_targets.read().splitlines()\n",
        "      targets_arr = np.concatenate((targets_arr, temp))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"epsilon\": 1e-07, \n",
            "    \"floatx\": \"float32\", \n",
            "    \"image_data_format\": \"channels_last\", \n",
            "    \"backend\": \"tensorflow\"\n",
            "}\n",
            "201600000\n",
            "(8, 350, 72000)\n",
            "['6' '6' '6' ... '1' '1' '1']\n",
            "9000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUn8ZQxz7JLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#targes only\n",
        "\n",
        "indecis = np.where( targets_arr == '1' )\n",
        "targets_data = data\n",
        "targets_data = np.swapaxes(data, 0, 2)\n",
        "targets_data = np.swapaxes(targets_data, 1, 2)\n",
        "targets_data = targets_data[indecis]\n",
        "events_arr = events_arr[indecis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrcvST3JzJNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "target_events_data_scaled =[]\n",
        "for i in range(8):\n",
        "  channel_scaled= preprocessing.scale(targets_data[i], axis=1)\n",
        "  target_events_data_scaled.append(channel_scaled)\n",
        "\n",
        "target_events_data_scaled = np.array(target_events_data_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_6RGx7K4xkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data = target_events_data_scaled\n",
        "#mne_array = np.swapaxes(targets_data, 0, 2) # (епохa, канал, настан). \n",
        "#mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "mne_array = targets_data.reshape(9000,1, 8,350)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZpAD8zt4zrd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c287652f-8a6a-435e-e577-4e4c1793ca72"
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from EEGModels import EEGNet,ShallowConvNet,DeepConvNet\n",
        "events_arr = events_arr.astype(np.int)\n",
        "X_train, X_test, y_train, y_test = train_test_split(mne_array, events_arr-1, test_size=0.33, random_state=42)\n",
        "\n",
        "\n",
        "model = DeepConvNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "model.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = 'adam')\n",
        "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "num_batch_size=100\n",
        "num_epochs=50\n",
        "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "          validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(score)\n",
        "\n",
        "#model = Sequential()\n",
        "#model.add(Dense(2800, input_dim=2800), activation='relu')\n",
        "#model.add(Dense(350, activation='sigmoid'))\n",
        "#model.add(Dense(350, activation='sigmoid'))\n",
        "#model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#clf = svm.SVC()\n",
        "#clf = RandomForestClassifier(random_state=0, n_estimators=350)\n",
        "#clf = LinearDiscriminantAnalysis()\n",
        "#clf.fit(X_train, y_train)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6030 samples, validate on 2970 samples\n",
            "Epoch 1/50\n",
            "5900/6030 [============================>.] - ETA: 0s - loss: 2.2896 - acc: 0.1398\n",
            "Epoch 00001: val_loss improved from inf to 2.10894, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "6030/6030 [==============================] - 8s 1ms/sample - loss: 2.2917 - acc: 0.1395 - val_loss: 2.1089 - val_acc: 0.1556\n",
            "Epoch 2/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 2.1974 - acc: 0.1575\n",
            "Epoch 00002: val_loss improved from 2.10894 to 2.02503, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "6030/6030 [==============================] - 1s 122us/sample - loss: 2.1949 - acc: 0.1575 - val_loss: 2.0250 - val_acc: 0.1923\n",
            "Epoch 3/50\n",
            "6000/6030 [============================>.] - ETA: 0s - loss: 2.1503 - acc: 0.1715\n",
            "Epoch 00003: val_loss did not improve from 2.02503\n",
            "6030/6030 [==============================] - 1s 115us/sample - loss: 2.1504 - acc: 0.1710 - val_loss: 2.0839 - val_acc: 0.1852\n",
            "Epoch 4/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 2.0865 - acc: 0.1947\n",
            "Epoch 00004: val_loss did not improve from 2.02503\n",
            "6030/6030 [==============================] - 1s 115us/sample - loss: 2.0858 - acc: 0.1957 - val_loss: 2.0595 - val_acc: 0.2071\n",
            "Epoch 5/50\n",
            "6000/6030 [============================>.] - ETA: 0s - loss: 2.0617 - acc: 0.2090\n",
            "Epoch 00005: val_loss did not improve from 2.02503\n",
            "6030/6030 [==============================] - 1s 114us/sample - loss: 2.0613 - acc: 0.2093 - val_loss: 2.0684 - val_acc: 0.2152\n",
            "Epoch 6/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 2.0382 - acc: 0.2151\n",
            "Epoch 00006: val_loss improved from 2.02503 to 2.01758, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "6030/6030 [==============================] - 1s 122us/sample - loss: 2.0424 - acc: 0.2131 - val_loss: 2.0176 - val_acc: 0.2269\n",
            "Epoch 7/50\n",
            "6000/6030 [============================>.] - ETA: 0s - loss: 2.0161 - acc: 0.2213\n",
            "Epoch 00007: val_loss improved from 2.01758 to 1.95426, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "6030/6030 [==============================] - 1s 125us/sample - loss: 2.0166 - acc: 0.2209 - val_loss: 1.9543 - val_acc: 0.2586\n",
            "Epoch 8/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.9897 - acc: 0.2333\n",
            "Epoch 00008: val_loss did not improve from 1.95426\n",
            "6030/6030 [==============================] - 1s 116us/sample - loss: 1.9877 - acc: 0.2348 - val_loss: 2.0114 - val_acc: 0.2576\n",
            "Epoch 9/50\n",
            "6000/6030 [============================>.] - ETA: 0s - loss: 1.9790 - acc: 0.2375\n",
            "Epoch 00009: val_loss did not improve from 1.95426\n",
            "6030/6030 [==============================] - 1s 118us/sample - loss: 1.9795 - acc: 0.2370 - val_loss: 2.0246 - val_acc: 0.2172\n",
            "Epoch 10/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.9702 - acc: 0.2422\n",
            "Epoch 00010: val_loss did not improve from 1.95426\n",
            "6030/6030 [==============================] - 1s 115us/sample - loss: 1.9732 - acc: 0.2405 - val_loss: 2.0093 - val_acc: 0.2566\n",
            "Epoch 11/50\n",
            "6000/6030 [============================>.] - ETA: 0s - loss: 1.9643 - acc: 0.2412\n",
            "Epoch 00011: val_loss did not improve from 1.95426\n",
            "6030/6030 [==============================] - 1s 116us/sample - loss: 1.9639 - acc: 0.2408 - val_loss: 2.0268 - val_acc: 0.2337\n",
            "Epoch 12/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.9524 - acc: 0.2458\n",
            "Epoch 00012: val_loss improved from 1.95426 to 1.93789, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "6030/6030 [==============================] - 1s 123us/sample - loss: 1.9530 - acc: 0.2449 - val_loss: 1.9379 - val_acc: 0.2539\n",
            "Epoch 13/50\n",
            "6000/6030 [============================>.] - ETA: 0s - loss: 1.9390 - acc: 0.2608\n",
            "Epoch 00013: val_loss did not improve from 1.93789\n",
            "6030/6030 [==============================] - 1s 116us/sample - loss: 1.9384 - acc: 0.2610 - val_loss: 1.9630 - val_acc: 0.2438\n",
            "Epoch 14/50\n",
            "5900/6030 [============================>.] - ETA: 0s - loss: 1.9249 - acc: 0.2605\n",
            "Epoch 00014: val_loss did not improve from 1.93789\n",
            "6030/6030 [==============================] - 1s 120us/sample - loss: 1.9266 - acc: 0.2604 - val_loss: 1.9867 - val_acc: 0.2303\n",
            "Epoch 15/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.9095 - acc: 0.2736\n",
            "Epoch 00015: val_loss did not improve from 1.93789\n",
            "6030/6030 [==============================] - 1s 117us/sample - loss: 1.9123 - acc: 0.2733 - val_loss: 1.9494 - val_acc: 0.2434\n",
            "Epoch 16/50\n",
            "6000/6030 [============================>.] - ETA: 0s - loss: 1.9144 - acc: 0.2663\n",
            "Epoch 00016: val_loss did not improve from 1.93789\n",
            "6030/6030 [==============================] - 1s 118us/sample - loss: 1.9140 - acc: 0.2665 - val_loss: 1.9561 - val_acc: 0.2694\n",
            "Epoch 17/50\n",
            "6000/6030 [============================>.] - ETA: 0s - loss: 1.9016 - acc: 0.2733\n",
            "Epoch 00017: val_loss did not improve from 1.93789\n",
            "6030/6030 [==============================] - 1s 117us/sample - loss: 1.9029 - acc: 0.2726 - val_loss: 1.9392 - val_acc: 0.2663\n",
            "Epoch 18/50\n",
            "5900/6030 [============================>.] - ETA: 0s - loss: 1.8975 - acc: 0.2686\n",
            "Epoch 00018: val_loss did not improve from 1.93789\n",
            "6030/6030 [==============================] - 1s 120us/sample - loss: 1.8969 - acc: 0.2690 - val_loss: 1.9406 - val_acc: 0.2593\n",
            "Epoch 19/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.8812 - acc: 0.2851\n",
            "Epoch 00019: val_loss improved from 1.93789 to 1.93243, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "6030/6030 [==============================] - 1s 129us/sample - loss: 1.8892 - acc: 0.2814 - val_loss: 1.9324 - val_acc: 0.2721\n",
            "Epoch 20/50\n",
            "6000/6030 [============================>.] - ETA: 0s - loss: 1.8814 - acc: 0.2780\n",
            "Epoch 00020: val_loss improved from 1.93243 to 1.91130, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "6030/6030 [==============================] - 1s 127us/sample - loss: 1.8814 - acc: 0.2784 - val_loss: 1.9113 - val_acc: 0.2825\n",
            "Epoch 21/50\n",
            "5700/6030 [===========================>..] - ETA: 0s - loss: 1.8611 - acc: 0.2967\n",
            "Epoch 00021: val_loss did not improve from 1.91130\n",
            "6030/6030 [==============================] - 1s 121us/sample - loss: 1.8645 - acc: 0.2955 - val_loss: 1.9362 - val_acc: 0.2710\n",
            "Epoch 22/50\n",
            "5900/6030 [============================>.] - ETA: 0s - loss: 1.8525 - acc: 0.2968\n",
            "Epoch 00022: val_loss improved from 1.91130 to 1.89829, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "6030/6030 [==============================] - 1s 127us/sample - loss: 1.8546 - acc: 0.2972 - val_loss: 1.8983 - val_acc: 0.2582\n",
            "Epoch 23/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.8455 - acc: 0.2973\n",
            "Epoch 00023: val_loss improved from 1.89829 to 1.87871, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "6030/6030 [==============================] - 1s 125us/sample - loss: 1.8467 - acc: 0.2980 - val_loss: 1.8787 - val_acc: 0.2791\n",
            "Epoch 24/50\n",
            "5900/6030 [============================>.] - ETA: 0s - loss: 1.8362 - acc: 0.3010\n",
            "Epoch 00024: val_loss did not improve from 1.87871\n",
            "6030/6030 [==============================] - 1s 118us/sample - loss: 1.8346 - acc: 0.3012 - val_loss: 1.8808 - val_acc: 0.2848\n",
            "Epoch 25/50\n",
            "5900/6030 [============================>.] - ETA: 0s - loss: 1.8382 - acc: 0.3071\n",
            "Epoch 00025: val_loss did not improve from 1.87871\n",
            "6030/6030 [==============================] - 1s 120us/sample - loss: 1.8371 - acc: 0.3065 - val_loss: 1.9119 - val_acc: 0.2663\n",
            "Epoch 26/50\n",
            "6000/6030 [============================>.] - ETA: 0s - loss: 1.8235 - acc: 0.3140\n",
            "Epoch 00026: val_loss improved from 1.87871 to 1.85131, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "6030/6030 [==============================] - 1s 129us/sample - loss: 1.8249 - acc: 0.3133 - val_loss: 1.8513 - val_acc: 0.2976\n",
            "Epoch 27/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.8232 - acc: 0.3209\n",
            "Epoch 00027: val_loss improved from 1.85131 to 1.83670, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "6030/6030 [==============================] - 1s 130us/sample - loss: 1.8220 - acc: 0.3216 - val_loss: 1.8367 - val_acc: 0.3114\n",
            "Epoch 28/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.8064 - acc: 0.3175\n",
            "Epoch 00028: val_loss did not improve from 1.83670\n",
            "6030/6030 [==============================] - 1s 115us/sample - loss: 1.8100 - acc: 0.3176 - val_loss: 1.8652 - val_acc: 0.2848\n",
            "Epoch 29/50\n",
            "5900/6030 [============================>.] - ETA: 0s - loss: 1.8005 - acc: 0.3125\n",
            "Epoch 00029: val_loss improved from 1.83670 to 1.81595, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "6030/6030 [==============================] - 1s 126us/sample - loss: 1.8015 - acc: 0.3126 - val_loss: 1.8159 - val_acc: 0.3195\n",
            "Epoch 30/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.7847 - acc: 0.3251\n",
            "Epoch 00030: val_loss did not improve from 1.81595\n",
            "6030/6030 [==============================] - 1s 115us/sample - loss: 1.7883 - acc: 0.3255 - val_loss: 1.8356 - val_acc: 0.3162\n",
            "Epoch 31/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.7977 - acc: 0.3173\n",
            "Epoch 00031: val_loss improved from 1.81595 to 1.81540, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "6030/6030 [==============================] - 1s 123us/sample - loss: 1.8003 - acc: 0.3169 - val_loss: 1.8154 - val_acc: 0.3128\n",
            "Epoch 32/50\n",
            "5900/6030 [============================>.] - ETA: 0s - loss: 1.7820 - acc: 0.3268\n",
            "Epoch 00032: val_loss did not improve from 1.81540\n",
            "6030/6030 [==============================] - 1s 115us/sample - loss: 1.7833 - acc: 0.3267 - val_loss: 1.8189 - val_acc: 0.2963\n",
            "Epoch 33/50\n",
            "6000/6030 [============================>.] - ETA: 0s - loss: 1.7689 - acc: 0.3283\n",
            "Epoch 00033: val_loss improved from 1.81540 to 1.80894, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "6030/6030 [==============================] - 1s 126us/sample - loss: 1.7679 - acc: 0.3295 - val_loss: 1.8089 - val_acc: 0.3135\n",
            "Epoch 34/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.7568 - acc: 0.3345\n",
            "Epoch 00034: val_loss did not improve from 1.80894\n",
            "6030/6030 [==============================] - 1s 112us/sample - loss: 1.7658 - acc: 0.3310 - val_loss: 1.8118 - val_acc: 0.3101\n",
            "Epoch 35/50\n",
            "6000/6030 [============================>.] - ETA: 0s - loss: 1.7702 - acc: 0.3332\n",
            "Epoch 00035: val_loss improved from 1.80894 to 1.78690, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "6030/6030 [==============================] - 1s 124us/sample - loss: 1.7709 - acc: 0.3333 - val_loss: 1.7869 - val_acc: 0.3263\n",
            "Epoch 36/50\n",
            "6000/6030 [============================>.] - ETA: 0s - loss: 1.7660 - acc: 0.3352\n",
            "Epoch 00036: val_loss improved from 1.78690 to 1.78211, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "6030/6030 [==============================] - 1s 124us/sample - loss: 1.7662 - acc: 0.3350 - val_loss: 1.7821 - val_acc: 0.3219\n",
            "Epoch 37/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.7568 - acc: 0.3316\n",
            "Epoch 00037: val_loss did not improve from 1.78211\n",
            "6030/6030 [==============================] - 1s 114us/sample - loss: 1.7618 - acc: 0.3294 - val_loss: 1.7837 - val_acc: 0.3290\n",
            "Epoch 38/50\n",
            "6000/6030 [============================>.] - ETA: 0s - loss: 1.7504 - acc: 0.3308\n",
            "Epoch 00038: val_loss did not improve from 1.78211\n",
            "6030/6030 [==============================] - 1s 115us/sample - loss: 1.7509 - acc: 0.3303 - val_loss: 1.8768 - val_acc: 0.2886\n",
            "Epoch 39/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.7484 - acc: 0.3398\n",
            "Epoch 00039: val_loss did not improve from 1.78211\n",
            "6030/6030 [==============================] - 1s 114us/sample - loss: 1.7511 - acc: 0.3375 - val_loss: 1.8288 - val_acc: 0.3000\n",
            "Epoch 40/50\n",
            "6000/6030 [============================>.] - ETA: 0s - loss: 1.7373 - acc: 0.3465\n",
            "Epoch 00040: val_loss improved from 1.78211 to 1.77643, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "6030/6030 [==============================] - 1s 125us/sample - loss: 1.7386 - acc: 0.3458 - val_loss: 1.7764 - val_acc: 0.3343\n",
            "Epoch 41/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.7226 - acc: 0.3504\n",
            "Epoch 00041: val_loss did not improve from 1.77643\n",
            "6030/6030 [==============================] - 1s 114us/sample - loss: 1.7252 - acc: 0.3498 - val_loss: 1.7847 - val_acc: 0.3263\n",
            "Epoch 42/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.7325 - acc: 0.3473\n",
            "Epoch 00042: val_loss improved from 1.77643 to 1.76429, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "6030/6030 [==============================] - 1s 122us/sample - loss: 1.7351 - acc: 0.3466 - val_loss: 1.7643 - val_acc: 0.3300\n",
            "Epoch 43/50\n",
            "6000/6030 [============================>.] - ETA: 0s - loss: 1.7214 - acc: 0.3503\n",
            "Epoch 00043: val_loss did not improve from 1.76429\n",
            "6030/6030 [==============================] - 1s 114us/sample - loss: 1.7204 - acc: 0.3511 - val_loss: 1.7662 - val_acc: 0.3327\n",
            "Epoch 44/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.7243 - acc: 0.3484\n",
            "Epoch 00044: val_loss did not improve from 1.76429\n",
            "6030/6030 [==============================] - 1s 112us/sample - loss: 1.7287 - acc: 0.3466 - val_loss: 1.8152 - val_acc: 0.3077\n",
            "Epoch 45/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.7238 - acc: 0.3449\n",
            "Epoch 00045: val_loss did not improve from 1.76429\n",
            "6030/6030 [==============================] - 1s 112us/sample - loss: 1.7271 - acc: 0.3418 - val_loss: 1.7729 - val_acc: 0.3205\n",
            "Epoch 46/50\n",
            "5900/6030 [============================>.] - ETA: 0s - loss: 1.7194 - acc: 0.3439\n",
            "Epoch 00046: val_loss did not improve from 1.76429\n",
            "6030/6030 [==============================] - 1s 116us/sample - loss: 1.7208 - acc: 0.3446 - val_loss: 1.7743 - val_acc: 0.3236\n",
            "Epoch 47/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.7052 - acc: 0.3589\n",
            "Epoch 00047: val_loss did not improve from 1.76429\n",
            "6030/6030 [==============================] - 1s 113us/sample - loss: 1.7100 - acc: 0.3582 - val_loss: 1.7743 - val_acc: 0.3253\n",
            "Epoch 48/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.7100 - acc: 0.3460\n",
            "Epoch 00048: val_loss improved from 1.76429 to 1.75927, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "6030/6030 [==============================] - 1s 122us/sample - loss: 1.7107 - acc: 0.3471 - val_loss: 1.7593 - val_acc: 0.3290\n",
            "Epoch 49/50\n",
            "6000/6030 [============================>.] - ETA: 0s - loss: 1.7018 - acc: 0.3625\n",
            "Epoch 00049: val_loss did not improve from 1.75927\n",
            "6030/6030 [==============================] - 1s 115us/sample - loss: 1.7033 - acc: 0.3624 - val_loss: 1.7653 - val_acc: 0.3391\n",
            "Epoch 50/50\n",
            "5500/6030 [==========================>...] - ETA: 0s - loss: 1.7110 - acc: 0.3496\n",
            "Epoch 00050: val_loss did not improve from 1.75927\n",
            "6030/6030 [==============================] - 1s 118us/sample - loss: 1.7125 - acc: 0.3466 - val_loss: 1.7840 - val_acc: 0.3242\n",
            "2970/2970 [==============================] - 0s 117us/sample - loss: 1.7840 - acc: 0.3242\n",
            "[1.7839998024481314, 0.3242424]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVrfvU9u-DHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(y_train);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmSE4LNT5Dhv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = clf.score(X_test, y_test)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-JYssfepBMvl",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "print(data.shape)\n",
        "#mne_array = data.reshape([target_events_data.shape[2],1,8, 350])\n",
        "print(mne_array.shape)\n",
        "#stft = mne.time_frequency.stft(mne_array[0], 24)\n",
        "\n",
        "# Number of sample points\n",
        "\n",
        "FS = 250\n",
        "N = 350\n",
        "# sample spacing\n",
        "T = 1.0 / FS\n",
        "x = np.linspace(0.0, N*T, N)\n",
        "num_labels= 8\n",
        "#plt.plot(mne_array[2][2], np.linspace(0.0,N,N))\n",
        "\n",
        "#fig, axs = plt.subplots(8,8, figsize=(4*8,4*8))\n",
        "#freq_data = list()\n",
        "#for i in range(8):\n",
        "#  for x in range(8):\n",
        "#    f, t, Sxx = signal.spectrogram(mne_array[x][i], fs=FS, nperseg=50, window=('hamming'), noverlap=35)\n",
        "#    freq_data.append(Sxx)\n",
        "#    axs[i,x].pcolormesh(t, f, Sxx)\n",
        "#plt.show()\n",
        "#print(open(\"~/keras/keras.json\").read())\n",
        "\n",
        "#events_arr = list(map(int, events_arr))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#y_train = np.array(y_train)\n",
        "#y_train = to_categorical(y_train)\n",
        "#y_test = np.array(y_test)\n",
        "#y_test = to_categorical(y_test)\n",
        "# Construct model \n",
        "\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "num_epochs = 50\n",
        "num_batch_size = 100\n",
        "\n",
        "clf = LinearDiscriminantAnalysis()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "score = clf.score(X_test, y_test)\n",
        "print(score)\n",
        "\n",
        "#preds = model.predict(X_test, verbose=0)\n",
        "#acc = accuracy_score(y_test, preds)\n",
        "\n",
        "print(\"accuracy_score: {f}\".format(f=score))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ovrlCYuqKIT4",
        "colab": {}
      },
      "source": [
        "#@title 1st objest, target vs non target\n",
        "# Вчитување на податоците\n",
        "data = loadmat('drive/My Drive/Интелигентни Системи/Data/SBJ01/S01-Train/trainData.mat')['trainData'] \n",
        "\n",
        "# Вчитување на label-ите\n",
        "labels_arr = []\n",
        "with open(\"drive/My Drive/Интелигентни Системи/Data/SBJ01/S01-Train/trainLabels.txt\", \"r\") as file_labels:\n",
        "    labels_arr = file_labels.read().splitlines()\n",
        "\n",
        "# Вчитување на редоследот на светкање\n",
        "events_arr = []\n",
        "with open(\"drive/My Drive/Интелигентни Системи/Data/SBJ01/S01-Train/trainEvents.txt\", \"r\") as file_events:\n",
        "    events_arr = file_events.read().splitlines()\n",
        "\n",
        "# Вчитување на редоследот на објекти кои се target\n",
        "targets_arr = []\n",
        "with open(\"drive/My Drive/Интелигентни Системи/Data/SBJ01/S01-Train/trainTargets.txt\", \"r\") as file_targets:\n",
        "    targets_arr = file_targets.read().splitlines()\n",
        "\n",
        "# Прилагодување на податоците за користење со mne библиотеката\n",
        "ch_names = [\"C3\", \"Cz\", \"C4\", \"CPz\", \"P3\", \"Pz\", \"P4\", \"POz\"]\n",
        "ch_types = ['eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg']\n",
        "mne_info = mne.create_info(ch_names=ch_names, sfreq=250, ch_types=ch_types)\n",
        "#mne_array = np.swapaxes(data, 0, 2) # (епохa, канал, настан). \n",
        "#mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "#raw_data = mne.epochs.EpochsArray(mne_array, mne_info)\n",
        "\n",
        "# Извлекување на настаните каде светнал првиот објект и бил target.\n",
        "first_object_events_target = [index for index, value in enumerate(events_arr) if value == '1']\n",
        "first_object_non_target = [index for index, value in enumerate(events_arr) if value == '1']\n",
        "\n",
        "for event_pos in first_object_events_target:\n",
        "  if targets_arr[event_pos] == 1:\n",
        "    first_object_non_target.remove(event_pos)\n",
        "    continue # Продолжи\n",
        "  else:\n",
        "    first_object_events_target.remove(event_pos) # Избриши -> Објектот не е target\n",
        "first_object_target_eeg_data = np.zeros((8,350, len(first_object_events_target)))\n",
        "first_object_non_target_eeg_data = np.zeros((8,350, len(first_object_non_target)))\n",
        "for channel in range(0, 8): # Секој канал\n",
        "  for epoch in range(0, 350): # Секоја епоха\n",
        "    i = 0\n",
        "    for event in first_object_events_target: # Настан\n",
        "      first_object_target_eeg_data[channel][epoch][i] = data[channel][epoch][event]\n",
        "      i = i+1\n",
        "    \n",
        "    k=0\n",
        "    for event in first_object_non_target: # Настан\n",
        "      first_object_non_target_eeg_data[channel][epoch][i] = data[channel][epoch][event]\n",
        "      k = k+1\n",
        "\n",
        "print(first_object_target_eeg_data.shape)\n",
        "print(first_object_non_target_eeg_data.shape)\n",
        "\n",
        "first_object_target_eeg_data = np.mean(first_object_target_eeg_data, axis=2)\n",
        "first_object_non_target_eeg_data = np.mean(first_object_non_target_eeg_data, axis=2)\n",
        "\n",
        "fig, axs = plt.subplots(8,2,figsize=(16,16*4))\n",
        "for i in range(8):\n",
        "  f, t, Sxx = signal.spectrogram(first_object_target_eeg_data[i], fs=FS, nperseg=50, window=('hamming'), noverlap=35)\n",
        "  fnon, tnon, Sxxnon = signal.spectrogram(first_object_non_target_eeg_data[i], fs=FS, nperseg=50, window=('hamming'), noverlap=35)\n",
        "\n",
        "  axs[i,0].pcolormesh(t, f, Sxx)\n",
        "  axs[i,1].pcolormesh(tnon, fnon, Sxxnon)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kitlqn__dEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Min, Max = round(mne_array_clean.min()),round(mne_array_clean.max())*1000000\n",
        "picks = mne.pick_types(raw_data_clean.info, meg=False, eeg=True, stim=False, eog=False)\n",
        "xd = mne.preprocessing.Xdawn(n_components=2, signal_cov=None)\n",
        "xd.fit(raw_data_clean)\n",
        "epochs_denoised = xd.apply(raw_data_clean)\n",
        "epochs_denoised.keys()\n",
        "mne.viz.plot_epochs_image(epochs_denoised['1'], picks='eeg', vmin=Min, vmax=Max)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nJEQxG5C3id",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(np.swapaxes(epochs_denoised['1'],0,0))\n",
        "standardizer = mne.decoding.Scaler(scalings='mean')\n",
        "standardizer.fit(np.swapaxes(epochs_denoised['1'],0,0))\n",
        "standardized_data = standardizer.transform(np.swapaxes(epochs_denoised['1'],0,0))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6ILVN_zGHRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "standardized_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGywLqcAFgdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FS = 250\n",
        "N = 350\n",
        "# sample spacing\n",
        "T = 1.0 / FS\n",
        "x = np.linspace(0.0, N*T, N)\n",
        "num_labels= 8\n",
        "#plt.plot(mne_array[2][2], np.linspace(0.0,N,N))\n",
        "\n",
        "#fig, axs = plt.subplots(8,8, figsize=(4*8,4*8))\n",
        "#freq_data = list()\n",
        "#for i in range(8):\n",
        "#  for x in range(8):\n",
        "#    f, t, Sxx = signal.spectrogram(mne_array[x][i], fs=FS, nperseg=50, window=('hamming'), noverlap=35)\n",
        "#    freq_data.append(Sxx)\n",
        "#    axs[i,x].pcolormesh(t, f, Sxx)\n",
        "#plt.show()\n",
        "#print(open(\"~/keras/keras.json\").read())\n",
        "\n",
        "#events_arr = list(map(int, events_arr))\n",
        "standardized_data_eeg = np.swapaxes(standardized_data,0,2)\n",
        "standardized_data_eeg=standardized_data_eeg.reshape([1600,1,8, 350])\n",
        "X_train, X_test, y_train, y_test = train_test_split(standardized_data_eeg, events_arr, test_size=0.33, random_state=42)\n",
        "#y_train = np.array(y_train)\n",
        "y_train = to_categorical(y_train)\n",
        "#y_test = np.array(y_test)\n",
        "y_test = to_categorical(y_test)\n",
        "# Construct model \n",
        "\n",
        "\n",
        "num_epochs = 50\n",
        "num_batch_size = 10\n",
        "\n",
        "model = EEGNet(nb_classes = 9, Chans = 8, Samples = 350)\n",
        "model.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = 'adam')\n",
        "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "start = datetime.now()\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "          validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "#preds = model.predict(X_test, verbose=0)\n",
        "#acc = accuracy_score(y_test, preds)\n",
        "\n",
        "print(\"accuracy_score: {f}\".format(f=score))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}