{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Faza4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dpostolovski/eeg_is/blob/train_compare_full_data/FINALNOOOOOOO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UDle2I8OMUa",
        "colab_type": "code",
        "outputId": "ab70afb9-12d9-4fff-cf6a-681d877107b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qm05RkaOpZW",
        "colab_type": "code",
        "outputId": "ed5bb63a-1754-45c6-8e40-902496b3d409",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "#@title Инсталирање и вчитување на потребните библиотеки\n",
        "!pip install mne \n",
        "!pip install termcolor\n",
        "\n",
        "%tensorflow_version 1.12.0\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import mne\n",
        "\n",
        "from sklearn import metrics \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint \n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from scipy.io import loadmat\n",
        "from collections import Counter\n",
        "\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "import sys\n",
        "sys.path.append('drive/My Drive/Интелигентни Системи')\n",
        "from EEGModels import DeepConvNet, EEGNet\n",
        "\n",
        "!mkdir saved_models"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mne\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/d8/62dd4099860614185e6d2730567d62266a0726ed2b5424052f100b385b40/mne-0.20.6-py3-none-any.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from mne) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from mne) (1.18.5)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-0.20.6\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.12.0`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2CpCSd15Hvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reshape(data, labels, events, targets):\n",
        "  mne_array = np.swapaxes(data, 0, 2) # (епохa, канал, настан). \n",
        "  mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "  mne_array = mne_array.reshape(mne_array.shape[0], 1, 8, 350)\n",
        "\n",
        "  labels_arr = labels.astype(np.int)\n",
        "  events_arr = events.astype(np.int)\n",
        "  return mne_array, labels_arr-1, events_arr-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wIxcjNywN0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reshape_data_to_mne_format(data):\n",
        "  mne_array = np.swapaxes(data, 0, 2) # (епохa, канал, настан). \n",
        "  mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "  mne_array = mne_array.reshape(mne_array.shape[0], 1, 8, 350)\n",
        "\n",
        "  return mne_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU0YDj0OO4jD",
        "colab_type": "code",
        "outputId": "d5755770-a13e-47a1-8ed6-d0ce6f83107d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Иницијализација на променливите каде ќе бидат вчитани податоците\n",
        "train_data = []\n",
        "train_labels = []\n",
        "train_events = []\n",
        "train_targets = []\n",
        "models = []\n",
        "\n",
        "start_i = 7\n",
        "for i in range(7, 9): # Итерација низ секој испитен примерок\n",
        "  print(\"Вчитување тест податоци од испитниот примерок \" + str(i) + \"...\")\n",
        "\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_train_data = np.empty(0)\n",
        "  temp_train_labels = np.empty(0)\n",
        "  temp_train_events = np.empty(0)\n",
        "  temp_train_targets = np.empty(0)\n",
        "\n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') + '/Train'\n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/trainData.mat\"\n",
        "    temp = loadmat(full_path)['trainData']\n",
        "    if temp_train_data.size != 0:\n",
        "      temp_train_data = np.concatenate((temp_train_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_train_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на label-ите\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/trainLabels.txt\"\n",
        "    with open(full_path, \"r\") as file_labels:\n",
        "      temp = file_labels.read().splitlines()\n",
        "      temp = np.repeat(temp, 8*10)\n",
        "      if temp_train_labels.size != 0:\n",
        "        temp_train_labels = np.concatenate((temp_train_labels, temp))\n",
        "      else:\n",
        "        temp_train_labels = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_train_events.size != 0:\n",
        "        temp_train_events = np.append(temp_train_events, temp)\n",
        "      else:\n",
        "        temp_train_events = np.array(temp)\n",
        "      \n",
        "\n",
        "    # Вчитување на редоследот на објекти кои се target\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/trainTargets.txt\"\n",
        "    with open(full_path, \"r\") as file_targets:\n",
        "      temp = file_targets.read().splitlines()\n",
        "      if temp_train_targets.size != 0:\n",
        "        temp_train_targets = np.concatenate((temp_train_targets, temp))\n",
        "      else:\n",
        "        temp_train_targets = np.array(temp)\n",
        "\n",
        "    print(\"\\t - Податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "  # Зачувај ги податоците вчитани од испитниот примерок во низа\n",
        "  train_data.append(temp_train_data)\n",
        "  train_labels.append(temp_train_labels)\n",
        "  train_events.append(temp_train_events)\n",
        "  train_targets.append(temp_train_targets)\n",
        "  print(\"Податоците од испитниот примерок \" + str(i) + \" се вчитани.\\n\")\n",
        "\n",
        "  array_i = i - start_i\n",
        "      \n",
        "\n",
        "  data, labels, events = reshape(train_data[array_i], train_labels[array_i], train_events[array_i], train_targets[array_i])\n",
        "  X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "  model = DeepConvNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "  model.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = Adam(0.0009))\n",
        "  checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                                verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  num_batch_size=100\n",
        "  num_epochs=150\n",
        "  model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "            validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "  score = model.evaluate(X_test, y_test, verbose=1)\n",
        "  print(score)\n",
        "\n",
        "  models.append(model) # Ke imame 15 modeli neli, pa vo niza se staveni\n",
        "                       # pristap do niv model[i]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Вчитување тест податоци од испитниот примерок 7...\n",
            "\t - Податоците од сесија 1 се вчитани.\n",
            "\t - Податоците од сесија 2 се вчитани.\n",
            "\t - Податоците од сесија 3 се вчитани.\n",
            "Податоците од испитниот примерок 7 се вчитани.\n",
            "\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Train on 3600 samples, validate on 1200 samples\n",
            "Epoch 1/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.3713 - acc: 0.1500\n",
            "Epoch 00001: val_loss improved from inf to 2.22734, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 10s 3ms/sample - loss: 2.3646 - acc: 0.1519 - val_loss: 2.2273 - val_acc: 0.1425\n",
            "Epoch 2/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.2658 - acc: 0.1674\n",
            "Epoch 00002: val_loss did not improve from 2.22734\n",
            "3600/3600 [==============================] - 1s 223us/sample - loss: 2.2707 - acc: 0.1664 - val_loss: 2.5608 - val_acc: 0.1042\n",
            "Epoch 3/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.2267 - acc: 0.1835\n",
            "Epoch 00003: val_loss did not improve from 2.22734\n",
            "3600/3600 [==============================] - 1s 228us/sample - loss: 2.2302 - acc: 0.1817 - val_loss: 2.2970 - val_acc: 0.1508\n",
            "Epoch 4/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.0810 - acc: 0.2065\n",
            "Epoch 00004: val_loss did not improve from 2.22734\n",
            "3600/3600 [==============================] - 1s 224us/sample - loss: 2.0795 - acc: 0.2092 - val_loss: 2.2511 - val_acc: 0.2258\n",
            "Epoch 5/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.9782 - acc: 0.2562\n",
            "Epoch 00005: val_loss improved from 2.22734 to 2.08767, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 245us/sample - loss: 1.9744 - acc: 0.2572 - val_loss: 2.0877 - val_acc: 0.2592\n",
            "Epoch 6/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.8718 - acc: 0.2865\n",
            "Epoch 00006: val_loss improved from 2.08767 to 2.07959, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 246us/sample - loss: 1.8696 - acc: 0.2858 - val_loss: 2.0796 - val_acc: 0.2542\n",
            "Epoch 7/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.8013 - acc: 0.3156\n",
            "Epoch 00007: val_loss did not improve from 2.07959\n",
            "3600/3600 [==============================] - 1s 224us/sample - loss: 1.7936 - acc: 0.3194 - val_loss: 2.1612 - val_acc: 0.2483\n",
            "Epoch 8/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.7279 - acc: 0.3406\n",
            "Epoch 00008: val_loss improved from 2.07959 to 1.94344, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 240us/sample - loss: 1.7282 - acc: 0.3400 - val_loss: 1.9434 - val_acc: 0.2458\n",
            "Epoch 9/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.6547 - acc: 0.3659\n",
            "Epoch 00009: val_loss did not improve from 1.94344\n",
            "3600/3600 [==============================] - 1s 226us/sample - loss: 1.6575 - acc: 0.3642 - val_loss: 2.0119 - val_acc: 0.2892\n",
            "Epoch 10/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.6222 - acc: 0.3788\n",
            "Epoch 00010: val_loss improved from 1.94344 to 1.73446, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 247us/sample - loss: 1.6165 - acc: 0.3764 - val_loss: 1.7345 - val_acc: 0.3217\n",
            "Epoch 11/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.5476 - acc: 0.4103\n",
            "Epoch 00011: val_loss did not improve from 1.73446\n",
            "3600/3600 [==============================] - 1s 229us/sample - loss: 1.5521 - acc: 0.4081 - val_loss: 2.1323 - val_acc: 0.2458\n",
            "Epoch 12/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4862 - acc: 0.4315\n",
            "Epoch 00012: val_loss did not improve from 1.73446\n",
            "3600/3600 [==============================] - 1s 227us/sample - loss: 1.4895 - acc: 0.4306 - val_loss: 1.7414 - val_acc: 0.3483\n",
            "Epoch 13/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4305 - acc: 0.4659\n",
            "Epoch 00013: val_loss improved from 1.73446 to 1.73253, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 243us/sample - loss: 1.4321 - acc: 0.4656 - val_loss: 1.7325 - val_acc: 0.3308\n",
            "Epoch 14/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.3696 - acc: 0.4771\n",
            "Epoch 00014: val_loss improved from 1.73253 to 1.68245, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 245us/sample - loss: 1.3667 - acc: 0.4778 - val_loss: 1.6824 - val_acc: 0.3750\n",
            "Epoch 15/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.3022 - acc: 0.4982\n",
            "Epoch 00015: val_loss did not improve from 1.68245\n",
            "3600/3600 [==============================] - 1s 226us/sample - loss: 1.2964 - acc: 0.5008 - val_loss: 1.7848 - val_acc: 0.3508\n",
            "Epoch 16/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2802 - acc: 0.5253\n",
            "Epoch 00016: val_loss did not improve from 1.68245\n",
            "3600/3600 [==============================] - 1s 227us/sample - loss: 1.2901 - acc: 0.5222 - val_loss: 1.9204 - val_acc: 0.3142\n",
            "Epoch 17/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2669 - acc: 0.5209\n",
            "Epoch 00017: val_loss improved from 1.68245 to 1.50396, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 241us/sample - loss: 1.2634 - acc: 0.5236 - val_loss: 1.5040 - val_acc: 0.4425\n",
            "Epoch 18/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2079 - acc: 0.5418\n",
            "Epoch 00018: val_loss did not improve from 1.50396\n",
            "3600/3600 [==============================] - 1s 227us/sample - loss: 1.2088 - acc: 0.5400 - val_loss: 1.9830 - val_acc: 0.3267\n",
            "Epoch 19/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.1929 - acc: 0.5615\n",
            "Epoch 00019: val_loss did not improve from 1.50396\n",
            "3600/3600 [==============================] - 1s 226us/sample - loss: 1.1935 - acc: 0.5589 - val_loss: 2.0742 - val_acc: 0.3025\n",
            "Epoch 20/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.1341 - acc: 0.5718\n",
            "Epoch 00020: val_loss improved from 1.50396 to 1.49561, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 245us/sample - loss: 1.1454 - acc: 0.5689 - val_loss: 1.4956 - val_acc: 0.4483\n",
            "Epoch 21/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0967 - acc: 0.5900\n",
            "Epoch 00021: val_loss improved from 1.49561 to 1.28948, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 246us/sample - loss: 1.0954 - acc: 0.5903 - val_loss: 1.2895 - val_acc: 0.5300\n",
            "Epoch 22/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0610 - acc: 0.6085\n",
            "Epoch 00022: val_loss did not improve from 1.28948\n",
            "3600/3600 [==============================] - 1s 229us/sample - loss: 1.0593 - acc: 0.6111 - val_loss: 1.2905 - val_acc: 0.5367\n",
            "Epoch 23/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0315 - acc: 0.6044\n",
            "Epoch 00023: val_loss improved from 1.28948 to 1.16138, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 247us/sample - loss: 1.0257 - acc: 0.6069 - val_loss: 1.1614 - val_acc: 0.5542\n",
            "Epoch 24/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9971 - acc: 0.6306\n",
            "Epoch 00024: val_loss did not improve from 1.16138\n",
            "3600/3600 [==============================] - 1s 227us/sample - loss: 0.9945 - acc: 0.6306 - val_loss: 1.2961 - val_acc: 0.5367\n",
            "Epoch 25/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9474 - acc: 0.6491\n",
            "Epoch 00025: val_loss improved from 1.16138 to 1.11146, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 246us/sample - loss: 0.9480 - acc: 0.6483 - val_loss: 1.1115 - val_acc: 0.6042\n",
            "Epoch 26/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9203 - acc: 0.6629\n",
            "Epoch 00026: val_loss improved from 1.11146 to 1.09977, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 244us/sample - loss: 0.9167 - acc: 0.6639 - val_loss: 1.0998 - val_acc: 0.5917\n",
            "Epoch 27/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8587 - acc: 0.6832\n",
            "Epoch 00027: val_loss did not improve from 1.09977\n",
            "3600/3600 [==============================] - 1s 228us/sample - loss: 0.8626 - acc: 0.6814 - val_loss: 1.1163 - val_acc: 0.5992\n",
            "Epoch 28/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8437 - acc: 0.6976\n",
            "Epoch 00028: val_loss did not improve from 1.09977\n",
            "3600/3600 [==============================] - 1s 228us/sample - loss: 0.8411 - acc: 0.6989 - val_loss: 1.1915 - val_acc: 0.5500\n",
            "Epoch 29/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8449 - acc: 0.6859\n",
            "Epoch 00029: val_loss improved from 1.09977 to 0.97899, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 242us/sample - loss: 0.8491 - acc: 0.6847 - val_loss: 0.9790 - val_acc: 0.6325\n",
            "Epoch 30/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8129 - acc: 0.6982\n",
            "Epoch 00030: val_loss did not improve from 0.97899\n",
            "3600/3600 [==============================] - 1s 228us/sample - loss: 0.8207 - acc: 0.6969 - val_loss: 1.1386 - val_acc: 0.5867\n",
            "Epoch 31/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8092 - acc: 0.7050\n",
            "Epoch 00031: val_loss did not improve from 0.97899\n",
            "3600/3600 [==============================] - 1s 229us/sample - loss: 0.8095 - acc: 0.7031 - val_loss: 1.0132 - val_acc: 0.6200\n",
            "Epoch 32/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7794 - acc: 0.7226\n",
            "Epoch 00032: val_loss did not improve from 0.97899\n",
            "3600/3600 [==============================] - 1s 226us/sample - loss: 0.7809 - acc: 0.7183 - val_loss: 1.0340 - val_acc: 0.6008\n",
            "Epoch 33/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7655 - acc: 0.7232\n",
            "Epoch 00033: val_loss improved from 0.97899 to 0.94687, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 248us/sample - loss: 0.7666 - acc: 0.7233 - val_loss: 0.9469 - val_acc: 0.6442\n",
            "Epoch 34/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7024 - acc: 0.7526\n",
            "Epoch 00034: val_loss improved from 0.94687 to 0.88488, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 244us/sample - loss: 0.7071 - acc: 0.7486 - val_loss: 0.8849 - val_acc: 0.6650\n",
            "Epoch 35/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7100 - acc: 0.7509\n",
            "Epoch 00035: val_loss improved from 0.88488 to 0.87395, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 246us/sample - loss: 0.7064 - acc: 0.7519 - val_loss: 0.8740 - val_acc: 0.6850\n",
            "Epoch 36/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6926 - acc: 0.7553\n",
            "Epoch 00036: val_loss did not improve from 0.87395\n",
            "3600/3600 [==============================] - 1s 231us/sample - loss: 0.6882 - acc: 0.7575 - val_loss: 0.9877 - val_acc: 0.6242\n",
            "Epoch 37/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6760 - acc: 0.7600\n",
            "Epoch 00037: val_loss did not improve from 0.87395\n",
            "3600/3600 [==============================] - 1s 227us/sample - loss: 0.6781 - acc: 0.7575 - val_loss: 0.9479 - val_acc: 0.6583\n",
            "Epoch 38/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6481 - acc: 0.7688\n",
            "Epoch 00038: val_loss improved from 0.87395 to 0.74976, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 246us/sample - loss: 0.6458 - acc: 0.7703 - val_loss: 0.7498 - val_acc: 0.7258\n",
            "Epoch 39/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6443 - acc: 0.7632\n",
            "Epoch 00039: val_loss improved from 0.74976 to 0.74247, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 250us/sample - loss: 0.6496 - acc: 0.7622 - val_loss: 0.7425 - val_acc: 0.7325\n",
            "Epoch 40/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6089 - acc: 0.7915\n",
            "Epoch 00040: val_loss improved from 0.74247 to 0.72628, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 243us/sample - loss: 0.6123 - acc: 0.7894 - val_loss: 0.7263 - val_acc: 0.7300\n",
            "Epoch 41/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5965 - acc: 0.7841\n",
            "Epoch 00041: val_loss improved from 0.72628 to 0.72169, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 244us/sample - loss: 0.5995 - acc: 0.7828 - val_loss: 0.7217 - val_acc: 0.7242\n",
            "Epoch 42/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5866 - acc: 0.7938\n",
            "Epoch 00042: val_loss did not improve from 0.72169\n",
            "3600/3600 [==============================] - 1s 227us/sample - loss: 0.5917 - acc: 0.7936 - val_loss: 0.7598 - val_acc: 0.7267\n",
            "Epoch 43/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5679 - acc: 0.7982\n",
            "Epoch 00043: val_loss improved from 0.72169 to 0.67970, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 246us/sample - loss: 0.5704 - acc: 0.7975 - val_loss: 0.6797 - val_acc: 0.7492\n",
            "Epoch 44/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5504 - acc: 0.8050\n",
            "Epoch 00044: val_loss improved from 0.67970 to 0.64882, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 242us/sample - loss: 0.5445 - acc: 0.8081 - val_loss: 0.6488 - val_acc: 0.7767\n",
            "Epoch 45/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5383 - acc: 0.8159\n",
            "Epoch 00045: val_loss did not improve from 0.64882\n",
            "3600/3600 [==============================] - 1s 229us/sample - loss: 0.5409 - acc: 0.8139 - val_loss: 0.6674 - val_acc: 0.7650\n",
            "Epoch 46/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4959 - acc: 0.8359\n",
            "Epoch 00046: val_loss did not improve from 0.64882\n",
            "3600/3600 [==============================] - 1s 231us/sample - loss: 0.4983 - acc: 0.8333 - val_loss: 0.6705 - val_acc: 0.7475\n",
            "Epoch 47/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4938 - acc: 0.8335\n",
            "Epoch 00047: val_loss improved from 0.64882 to 0.62809, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 246us/sample - loss: 0.4970 - acc: 0.8325 - val_loss: 0.6281 - val_acc: 0.7817\n",
            "Epoch 48/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4957 - acc: 0.8285\n",
            "Epoch 00048: val_loss improved from 0.62809 to 0.61231, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 245us/sample - loss: 0.4943 - acc: 0.8300 - val_loss: 0.6123 - val_acc: 0.7842\n",
            "Epoch 49/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4816 - acc: 0.8382\n",
            "Epoch 00049: val_loss improved from 0.61231 to 0.55257, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 248us/sample - loss: 0.4856 - acc: 0.8369 - val_loss: 0.5526 - val_acc: 0.8008\n",
            "Epoch 50/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4528 - acc: 0.8465\n",
            "Epoch 00050: val_loss did not improve from 0.55257\n",
            "3600/3600 [==============================] - 1s 228us/sample - loss: 0.4535 - acc: 0.8450 - val_loss: 0.5876 - val_acc: 0.8000\n",
            "Epoch 51/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4659 - acc: 0.8388\n",
            "Epoch 00051: val_loss improved from 0.55257 to 0.55176, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 246us/sample - loss: 0.4627 - acc: 0.8406 - val_loss: 0.5518 - val_acc: 0.8108\n",
            "Epoch 52/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4332 - acc: 0.8524\n",
            "Epoch 00052: val_loss improved from 0.55176 to 0.53305, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 247us/sample - loss: 0.4319 - acc: 0.8525 - val_loss: 0.5330 - val_acc: 0.8208\n",
            "Epoch 53/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4240 - acc: 0.8621\n",
            "Epoch 00053: val_loss did not improve from 0.53305\n",
            "3600/3600 [==============================] - 1s 230us/sample - loss: 0.4291 - acc: 0.8592 - val_loss: 0.6519 - val_acc: 0.7625\n",
            "Epoch 54/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4013 - acc: 0.8712\n",
            "Epoch 00054: val_loss did not improve from 0.53305\n",
            "3600/3600 [==============================] - 1s 231us/sample - loss: 0.4030 - acc: 0.8706 - val_loss: 0.5728 - val_acc: 0.7967\n",
            "Epoch 55/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3807 - acc: 0.8791\n",
            "Epoch 00055: val_loss improved from 0.53305 to 0.50354, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 246us/sample - loss: 0.3842 - acc: 0.8772 - val_loss: 0.5035 - val_acc: 0.8267\n",
            "Epoch 56/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3922 - acc: 0.8726\n",
            "Epoch 00056: val_loss improved from 0.50354 to 0.49652, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 245us/sample - loss: 0.3935 - acc: 0.8717 - val_loss: 0.4965 - val_acc: 0.8292\n",
            "Epoch 57/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3793 - acc: 0.8732\n",
            "Epoch 00057: val_loss improved from 0.49652 to 0.47687, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 247us/sample - loss: 0.3798 - acc: 0.8725 - val_loss: 0.4769 - val_acc: 0.8408\n",
            "Epoch 58/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3724 - acc: 0.8794\n",
            "Epoch 00058: val_loss did not improve from 0.47687\n",
            "3600/3600 [==============================] - 1s 229us/sample - loss: 0.3810 - acc: 0.8767 - val_loss: 0.5179 - val_acc: 0.8183\n",
            "Epoch 59/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3589 - acc: 0.8847\n",
            "Epoch 00059: val_loss did not improve from 0.47687\n",
            "3600/3600 [==============================] - 1s 232us/sample - loss: 0.3626 - acc: 0.8833 - val_loss: 0.4919 - val_acc: 0.8292\n",
            "Epoch 60/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3570 - acc: 0.8862\n",
            "Epoch 00060: val_loss improved from 0.47687 to 0.44078, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 250us/sample - loss: 0.3545 - acc: 0.8869 - val_loss: 0.4408 - val_acc: 0.8492\n",
            "Epoch 61/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3354 - acc: 0.8929\n",
            "Epoch 00061: val_loss did not improve from 0.44078\n",
            "3600/3600 [==============================] - 1s 228us/sample - loss: 0.3400 - acc: 0.8914 - val_loss: 0.4490 - val_acc: 0.8508\n",
            "Epoch 62/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3272 - acc: 0.9003\n",
            "Epoch 00062: val_loss improved from 0.44078 to 0.38643, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 251us/sample - loss: 0.3279 - acc: 0.8997 - val_loss: 0.3864 - val_acc: 0.8800\n",
            "Epoch 63/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3086 - acc: 0.9041\n",
            "Epoch 00063: val_loss improved from 0.38643 to 0.36295, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 245us/sample - loss: 0.3139 - acc: 0.9022 - val_loss: 0.3629 - val_acc: 0.8875\n",
            "Epoch 64/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3106 - acc: 0.9041\n",
            "Epoch 00064: val_loss did not improve from 0.36295\n",
            "3600/3600 [==============================] - 1s 228us/sample - loss: 0.3149 - acc: 0.9014 - val_loss: 0.4680 - val_acc: 0.8350\n",
            "Epoch 65/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3030 - acc: 0.9053\n",
            "Epoch 00065: val_loss did not improve from 0.36295\n",
            "3600/3600 [==============================] - 1s 230us/sample - loss: 0.3028 - acc: 0.9056 - val_loss: 0.4884 - val_acc: 0.8367\n",
            "Epoch 66/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2914 - acc: 0.9068\n",
            "Epoch 00066: val_loss did not improve from 0.36295\n",
            "3600/3600 [==============================] - 1s 229us/sample - loss: 0.2931 - acc: 0.9067 - val_loss: 0.3958 - val_acc: 0.8692\n",
            "Epoch 67/150\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3143 - acc: 0.8935\n",
            "Epoch 00067: val_loss did not improve from 0.36295\n",
            "3600/3600 [==============================] - 1s 229us/sample - loss: 0.3129 - acc: 0.8953 - val_loss: 0.4670 - val_acc: 0.8375\n",
            "Epoch 68/150\n",
            "2800/3600 [======================>.......] - ETA: 0s - loss: 0.3123 - acc: 0.9014"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a926db098ea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m   \u001b[0mnum_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxW8fPu6wlY-",
        "colab_type": "code",
        "outputId": "4608c346-db6f-4372-93b8-53644c467bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "# Иницијализација на променливите каде ќе бидат вчитани тест податоците\n",
        "test_data = []\n",
        "test_events = []\n",
        "test_runs_per_block = [[i for i in range(3)] for j in range(15)] # Covek, Sesija\n",
        "\n",
        "start_i = 7\n",
        "for i in range(7, 9): # Итерација низ секој испитен примерок\n",
        "  print(f\"====================== Примерок ({i}) ======================\")\n",
        "  print(\"Вчитување тест податоци од испитниот примерок \" + str(i) + \"...\")\n",
        "  \n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_test_data = np.empty(0)\n",
        "  temp_test_events = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_test_set = 'S' + format(j, '02') + '/Test'\n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testData.mat\"\n",
        "    temp = loadmat(full_path)['testData']\n",
        "    if temp_test_data.size != 0:\n",
        "      temp_test_data = np.concatenate((temp_test_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_test_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_test_events.size != 0:\n",
        "        temp_test_events = np.append(temp_test_events, temp)\n",
        "      else:\n",
        "        temp_test_events = np.array(temp)\n",
        "\n",
        "    # Вчитување на бројот на runs \n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/runs_per_block.txt\"\n",
        "    with open(full_path, \"r\") as runs_per_block:\n",
        "      test_runs_per_block[i-1][j-1] = int(runs_per_block.read())\n",
        "\n",
        "    print(\"\\t - Тест податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "  # Зачувај ги тест податоците вчитани од испитниот примерок во низа\n",
        "  test_data.append(temp_test_data)\n",
        "  test_events.append(temp_test_events)\n",
        "  print(\"Тест податоците од испитниот примерок \" + str(i) + \" се вчитани.\\n\")\n",
        "\n",
        "  array_i = i - start_i\n",
        "      \n",
        "  \n",
        "  # =========================================================================\n",
        "\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_data: \" + str(test_data[array_i].shape)) # test_data to predict\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_events: \" + str(len(test_events[array_i]))) # test_events\n",
        "  for j in range (1,4):\n",
        "    print(\"SBJ\" + str(format(i-1, '02')) + \" / S\" + str(format(j-1, '02')) + \"| Runs per block: \" + str(test_runs_per_block[i-1][j-1])) # runs per block in SJB01, SJ00 \n",
        "\n",
        "  to_predict_data = reshape_data_to_mne_format(test_data[array_i])\n",
        "  predictions = models[array_i].predict(to_predict_data)\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Predictions: \" + str(len(predictions)))\n",
        "  # np.savetxt(\"predictions.csv\", predictions, delimiter=\",\")\n",
        "\n",
        "\n",
        "  # ========= FALI USTE DA SE ISPARSIRA PREDICTIONOT... NE E SREDEN OVOJ KOD DOLE =======\n",
        "\n",
        "  int_pred = np.argmax(predictions, axis=1)\n",
        "  int_ytest = np.argmax(y_test, axis=1)\n",
        "\n",
        "  session_start = 0\n",
        "  start_prediction_index = 0\n",
        "  end_prediction_index = 0\n",
        "  for session in range(0, 3):\n",
        "    print(f\"============== Сесија ({session}) ==============\")\n",
        "    for block in range(0, 50):    \n",
        "      events_per_block = test_runs_per_block[i-1][session]\n",
        "\n",
        "      start_prediction_index = session_start + (block*events_per_block)*8\n",
        "      end_prediction_index = session_start + ((block+1)*events_per_block)*8\n",
        "\n",
        "      block_prediction = int_pred[start_prediction_index:end_prediction_index]\n",
        "      prediction = np.bincount(block_prediction).argmax()\n",
        "\n",
        "      # UNCOMMENT ZA PODOBAR PRIKAZ :)\n",
        "      # print(f\"Session {session} | Block: {block} | Prediction: {prediction} | Address: {end_prediction_index}\")\n",
        "\n",
        "      print(str(prediction) + \",\", end=\"\")\n",
        "    session_start = end_prediction_index\n",
        "    print(\"\")\n",
        "  print(\"Stigna li do kraj: \" + str(session_start == len(predictions)))\n",
        "  print(f\"====================== Примерок ({i}) ======================\\n\\n\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Примерок (5) ======================\n",
            "Вчитување тест податоци од испитниот примерок 5...\n",
            "\t - Тест податоците од сесија 1 се вчитани.\n",
            "\t - Тест податоците од сесија 2 се вчитани.\n",
            "\t - Тест податоците од сесија 3 се вчитани.\n",
            "Тест податоците од испитниот примерок 5 се вчитани.\n",
            "\n",
            "SBJ04| Test_data: (8, 350, 8800)\n",
            "SBJ04| Test_events: 8800\n",
            "SBJ04 / S00| Runs per block: 8\n",
            "SBJ04 / S01| Runs per block: 7\n",
            "SBJ04 / S02| Runs per block: 7\n",
            "SBJ04| Predictions: 8800\n",
            "============== Сесија (0) ==============\n",
            "4,1,1,3,5,3,4,1,1,2,6,2,2,1,6,4,5,1,0,1,0,0,2,0,2,5,3,1,1,5,1,3,5,1,1,1,5,2,0,1,0,2,6,0,6,3,0,0,6,4,\n",
            "============== Сесија (1) ==============\n",
            "1,5,6,4,1,5,6,6,5,6,5,3,5,5,5,5,5,5,3,5,4,5,5,5,5,5,5,1,5,3,5,0,0,5,4,5,3,5,0,5,3,1,5,0,5,5,6,5,5,5,\n",
            "============== Сесија (2) ==============\n",
            "4,6,6,5,4,6,1,3,5,1,1,5,5,5,6,1,7,4,1,4,4,1,4,1,6,5,1,6,3,3,4,4,1,1,1,5,0,6,7,5,5,6,6,1,1,6,4,1,3,1,\n",
            "Stigna li do kraj: True\n",
            "====================== Примерок (5) ======================\n",
            "\n",
            "\n",
            "====================== Примерок (6) ======================\n",
            "Вчитување тест податоци од испитниот примерок 6...\n",
            "\t - Тест податоците од сесија 1 се вчитани.\n",
            "\t - Тест податоците од сесија 2 се вчитани.\n",
            "\t - Тест податоците од сесија 3 се вчитани.\n",
            "Тест податоците од испитниот примерок 6 се вчитани.\n",
            "\n",
            "SBJ05| Test_data: (8, 350, 8000)\n",
            "SBJ05| Test_events: 8000\n",
            "SBJ05 / S00| Runs per block: 4\n",
            "SBJ05 / S01| Runs per block: 7\n",
            "SBJ05 / S02| Runs per block: 9\n",
            "SBJ05| Predictions: 8000\n",
            "============== Сесија (0) ==============\n",
            "0,0,0,0,2,2,0,6,6,1,0,0,0,2,7,1,0,7,7,6,0,0,6,2,0,7,0,0,0,3,0,7,2,0,0,0,3,6,0,3,2,3,6,6,0,0,3,0,6,7,\n",
            "============== Сесија (1) ==============\n",
            "0,0,0,7,0,0,0,0,7,0,3,0,0,0,2,7,0,0,3,3,3,3,3,0,7,3,1,5,0,1,1,0,3,7,7,3,3,0,0,3,1,0,3,7,4,4,4,7,6,4,\n",
            "============== Сесија (2) ==============\n",
            "0,2,0,0,0,0,0,0,0,2,0,0,0,0,2,2,2,2,0,0,0,0,2,0,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,7,0,0,0,0,0,0,0,0,\n",
            "Stigna li do kraj: True\n",
            "====================== Примерок (6) ======================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_854cF8A9NHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_data = reshape_data_to_mne_format(test_data)\n",
        "# print(test_data.shape)\n",
        "# predictions = model.predict(X_test)\n",
        "# np.savetxt(\"predictions.csv\", predictions, delimiter=\",\")\n",
        "# np.savetxt(\"y_test.csv\", y_test, delimiter=\",\")\n",
        "\n",
        "\n",
        "# predictions = model.predict(X_test)\n",
        "# np.savetxt(\"predictions.csv\", predictions, delimiter=\",\")\n",
        "# np.savetxt(\"y_test.csv\", y_test, delimiter=\",\")\n",
        "\n",
        "# from collections import Counter\n",
        "# int_pred = np.argmax(predictions, axis=1)\n",
        "# int_ytest = np.argmax(y_test, axis=1)\n",
        "\n",
        "# correct = 0\n",
        "# for i in range(20):\n",
        "#   block_y = int_ytest[i*80:(i+1)*80]\n",
        "#   block_y_pred = int_pred[i*80:(i+1)*80]\n",
        "\n",
        "#   class_y = np.bincount(block_y).argmax()\n",
        "#   class_y_pred = np.bincount(block_y_pred).argmax()\n",
        "\n",
        "#   print(f\"Class Y: {class_y}, prediciton: {class_y_pred}\")\n",
        "#   if class_y == class_y_pred:\n",
        "#     correct = correct+1\n",
        "\n",
        "# correct"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}