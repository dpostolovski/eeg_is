{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IS_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dpostolovski/eeg_is/blob/train_compare_full_data/IS_project_train_target_only.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBqW8_NNlAgz",
        "colab_type": "text"
      },
      "source": [
        "<h1>\n",
        "  <img alt=\"FINKI **LOGO**\" height=\"30px\" src=\"https://www.finki.ukim.mk/Content/dataImages/downloads/logo-large-500x500_2.png\" hspace=\"10px\" vspace=\"0px\">\n",
        "  Интелигентни системи - Лабораториска вежба 2 (Претпроцесирање)\n",
        "</h1>\n",
        "<center><h3><i>Група 5<i><h3></center>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTx3XQgUOhCX",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "b9c6d75a-8880-43ce-f477-21f03a383ad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@title Монтирање на Google Drive податочниот систем\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLrrPj8clKjP",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "f80b06ed-2eb7-42e4-faae-eb9088aeb0f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "#@title Инсталирање и вчитување на потребните библиотеки\n",
        "\n",
        "# Библиотека за истражување, визуелизација и анализирање на човечки \n",
        "# неврофизиолошки податоци (EEG, sEEG и др)\n",
        "!pip install mne \n",
        "!pip install termcolor\n",
        "\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, date, time\n",
        "import pandas as pd\n",
        "from termcolor import colored\n",
        "import mne\n",
        "from sklearn.decomposition import PCA, FastICA\n",
        "import mne"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mne in /usr/local/lib/python3.6/dist-packages (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from mne) (1.18.4)\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from mne) (1.4.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (1.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NeWVBhf1VxlH",
        "outputId": "44c8e6f4-c5df-445f-d689-68cd18a7ef55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "%tensorflow_version 1.12.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.12.0`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBEiowYKtnPJ",
        "colab_type": "code",
        "outputId": "f0c7ddc4-cf6d-46ad-bc99-645ee05332cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "!wget \"https://raw.githubusercontent.com/vlawhern/arl-eegmodels/master/EEGModels.py\"\n",
        "!mkdir saved_models"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-09 23:55:09--  https://raw.githubusercontent.com/vlawhern/arl-eegmodels/master/EEGModels.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18283 (18K) [text/plain]\n",
            "Saving to: ‘EEGModels.py.3’\n",
            "\n",
            "\rEEGModels.py.3        0%[                    ]       0  --.-KB/s               \rEEGModels.py.3      100%[===================>]  17.85K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2020-05-09 23:55:09 (5.00 MB/s) - ‘EEGModels.py.3’ saved [18283/18283]\n",
            "\n",
            "mkdir: cannot create directory ‘saved_models’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "916f2510-f71a-4313-d8a5-3785ebead491",
        "id": "-JYssfepBMvl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        }
      },
      "source": [
        "#@title Fourier analysis\n",
        "# Вчитување на податоците\n",
        "\n",
        "from sklearn import metrics \n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint \n",
        "from sklearn.model_selection import train_test_split\n",
        "from EEGModels import EEGNet,ShallowConvNet\n",
        "import scipy.io as sio\n",
        "from scipy.fft import fft\n",
        "from scipy import signal\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import os\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow import keras\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "print(open(\"/root/.keras/keras.json\").read())\n",
        "\n",
        "# Вчитување на податоците\n",
        "data = loadmat('drive/My Drive/Интелигентни Системи/Data/SBJ01/S01-Train/trainData.mat')['trainData'] \n",
        "for i in range(1, 16):\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  for j in range(1, 4):\n",
        "    if i == 1: continue\n",
        "\n",
        "    file_train_set = 'S' + format(j, '02') + '-Train'\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/trainData.mat\"\n",
        "    temp = loadmat(full_path)['trainData'] \n",
        "    data = np.concatenate((data, temp), axis=2)\n",
        "\n",
        "print(data.size)\n",
        "print(data.shape)\n",
        "\n",
        "# Вчитување на label-ите\n",
        "labels_arr = np.empty(0)\n",
        "with open('drive/My Drive/Интелигентни Системи/Data/SBJ01/S01-Train/trainLabels.txt', \"r\") as file_labels:\n",
        "  labels_arr = file_labels.read().splitlines()\n",
        "for i in range(1, 16):\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  for j in range(1, 4):\n",
        "    if i == 1: continue\n",
        "\n",
        "    file_train_set = 'S' + format(j, '02') + '-Train'\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/trainLabels.txt\"\n",
        "    with open(full_path, \"r\") as file_labels:\n",
        "      temp = file_labels.read().splitlines()\n",
        "      labels_arr = np.concatenate((labels_arr, temp))\n",
        "\n",
        "# Вчитување на редоследот на светкање\n",
        "events_arr = np.empty(0)\n",
        "with open('drive/My Drive/Интелигентни Системи/Data/SBJ01/S01-Train/trainEvents.txt', \"r\") as file_events:\n",
        "  events_arr = file_events.read().splitlines()\n",
        "for i in range(1, 16):\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  for j in range(1, 4):\n",
        "    if i == 1: continue\n",
        "\n",
        "    file_train_set = 'S' + format(j, '02') + '-Train'\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      events_arr = np.concatenate((events_arr, temp))\n",
        "\n",
        "# Вчитување на редоследот на објекти кои се target\n",
        "targets_arr = np.empty(0)\n",
        "with open('drive/My Drive/Интелигентни Системи/Data/SBJ01/S01-Train/trainTargets.txt', \"r\") as file_targets:\n",
        "  targets_arr = file_targets.read().splitlines()\n",
        "for i in range(1, 16):\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  for j in range(1, 4):\n",
        "    if i == 1: continue\n",
        "\n",
        "    file_train_set = 'S' + format(j, '02') + '-Train'\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_targets:\n",
        "      temp = file_targets.read().splitlines()\n",
        "      targets_arr = np.concatenate((targets_arr, temp))\n",
        "\n",
        "\n",
        "indecis = np.where( targets_arr == '1' )\n",
        "data = np.swapaxes(data, 0, 2)\n",
        "data = np.swapaxes(data, 1, 2)\n",
        "data = data[indecis]\n",
        "#data = data[indecis]\n",
        "print(data.shape)\n",
        "mne_array = data.reshape([data.shape[0],1,8, 350])\n",
        "print(mne_array.shape)\n",
        "\n",
        "#stft = mne.time_frequency.stft(mne_array[0], 24)\n",
        "\n",
        "# Number of sample points\n",
        "\n",
        "FS = 250\n",
        "N = 350\n",
        "# sample spacing\n",
        "T = 1.0 / FS\n",
        "x = np.linspace(0.0, N*T, N)\n",
        "num_labels= 8\n",
        "#plt.plot(mne_array[2][2], np.linspace(0.0,N,N))\n",
        "\n",
        "#fig, axs = plt.subplots(8,8, figsize=(4*8,4*8))\n",
        "#freq_data = list()\n",
        "#for i in range(8):\n",
        "#  for x in range(8):\n",
        "#    f, t, Sxx = signal.spectrogram(mne_array[x][i], fs=FS, nperseg=50, window=('hamming'), noverlap=35)\n",
        "#    freq_data.append(Sxx)\n",
        "#    axs[i,x].pcolormesh(t, f, Sxx)\n",
        "#plt.show()\n",
        "#print(open(\"~/keras/keras.json\").read())\n",
        "\n",
        "#events_arr = list(map(int, events_arr))\n",
        "\n",
        "\n",
        "\n",
        "#standardized_data_eeg=standardized_data_eeg.reshape([data.shape[0],1,8, 350])\n",
        "events_arr = events_arr[indecis]\n",
        "events_arr = events_arr.astype(np.int)\n",
        "X_train, X_test, y_train, y_test = train_test_split(mne_array, events_arr-1, test_size=0.33, random_state=42)\n",
        "\n",
        "\n",
        "#y_train = np.array(y_train)\n",
        "y_train = to_categorical(y_train)\n",
        "#y_test = np.array(y_test)\n",
        "y_test = to_categorical(y_test)\n",
        "# Construct model \n",
        "\n",
        "\n",
        "num_epochs = 50\n",
        "num_batch_size = 100\n",
        "\n",
        "model = EEGNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "model.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer =keras.optimizers.Adam(learning_rate=1e-2))\n",
        "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "start = datetime.now()\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "          validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "#preds = model.predict(X_test, verbose=0)\n",
        "#acc = accuracy_score(y_test, preds)\n",
        "\n",
        "print(\"accuracy_score: {f}\".format(f=score))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"epsilon\": 1e-07, \n",
            "    \"floatx\": \"float32\", \n",
            "    \"image_data_format\": \"channels_last\", \n",
            "    \"backend\": \"tensorflow\"\n",
            "}\n",
            "192640000\n",
            "(8, 350, 68800)\n",
            "(8600, 8, 350)\n",
            "(8600, 1, 8, 350)\n",
            "Train on 5762 samples, validate on 2838 samples\n",
            "Epoch 1/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1c3258a1dcc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3471\u001b[0m         \u001b[0mfeed_symbols\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_symbols\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3472\u001b[0m         session != self._session):\n\u001b[0;32m-> 3473\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   3408\u001b[0m       \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3409\u001b[0m     \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3410\u001b[0;31m     \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3411\u001b[0m     \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3412\u001b[0m     \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1503\u001b[0m     \"\"\"\n\u001b[1;32m   1504\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1458\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m         self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[0;32m-> 1460\u001b[0;31m             session._session, options_ptr)\n\u001b[0m\u001b[1;32m   1461\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Default AvgPoolingOp only supports NHWC on device type CPU\n\t [[{{node average_pooling2d_2/AvgPool}}]]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWGg4jBTlCR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!readlink -f ~/.keras/keras.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c8w9tgTiV53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ovrlCYuqKIT4",
        "colab": {}
      },
      "source": [
        "#@title 1st objest, target vs non target\n",
        "# Вчитување на податоците\n",
        "data = loadmat('drive/My Drive/Интелигентни Системи/Data/SBJ01/S01-Train/trainData.mat')['trainData'] \n",
        "\n",
        "# Вчитување на label-ите\n",
        "labels_arr = []\n",
        "with open(\"drive/My Drive/Интелигентни Системи/Data/SBJ01/S01-Train/trainLabels.txt\", \"r\") as file_labels:\n",
        "    labels_arr = file_labels.read().splitlines()\n",
        "\n",
        "# Вчитување на редоследот на светкање\n",
        "events_arr = []\n",
        "with open(\"drive/My Drive/Интелигентни Системи/Data/SBJ01/S01-Train/trainEvents.txt\", \"r\") as file_events:\n",
        "    events_arr = file_events.read().splitlines()\n",
        "\n",
        "# Вчитување на редоследот на објекти кои се target\n",
        "targets_arr = []\n",
        "with open(\"drive/My Drive/Интелигентни Системи/Data/SBJ01/S01-Train/trainTargets.txt\", \"r\") as file_targets:\n",
        "    targets_arr = file_targets.read().splitlines()\n",
        "\n",
        "# Прилагодување на податоците за користење со mne библиотеката\n",
        "ch_names = [\"C3\", \"Cz\", \"C4\", \"CPz\", \"P3\", \"Pz\", \"P4\", \"POz\"]\n",
        "ch_types = ['eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg']\n",
        "mne_info = mne.create_info(ch_names=ch_names, sfreq=250, ch_types=ch_types)\n",
        "#mne_array = np.swapaxes(data, 0, 2) # (епохa, канал, настан). \n",
        "#mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "#raw_data = mne.epochs.EpochsArray(mne_array, mne_info)\n",
        "\n",
        "# Извлекување на настаните каде светнал првиот објект и бил target.\n",
        "first_object_events_target = [index for index, value in enumerate(events_arr) if value == '1']\n",
        "first_object_non_target = [index for index, value in enumerate(events_arr) if value == '1']\n",
        "\n",
        "for event_pos in first_object_events_target:\n",
        "  if targets_arr[event_pos] == 1:\n",
        "    first_object_non_target.remove(event_pos)\n",
        "    continue # Продолжи\n",
        "  else:\n",
        "    first_object_events_target.remove(event_pos) # Избриши -> Објектот не е target\n",
        "first_object_target_eeg_data = np.zeros((8,350, len(first_object_events_target)))\n",
        "first_object_non_target_eeg_data = np.zeros((8,350, len(first_object_non_target)))\n",
        "for channel in range(0, 8): # Секој канал\n",
        "  for epoch in range(0, 350): # Секоја епоха\n",
        "    i = 0\n",
        "    for event in first_object_events_target: # Настан\n",
        "      first_object_target_eeg_data[channel][epoch][i] = data[channel][epoch][event]\n",
        "      i = i+1\n",
        "    \n",
        "    k=0\n",
        "    for event in first_object_non_target: # Настан\n",
        "      first_object_non_target_eeg_data[channel][epoch][i] = data[channel][epoch][event]\n",
        "      k = k+1\n",
        "\n",
        "print(first_object_target_eeg_data.shape)\n",
        "print(first_object_non_target_eeg_data.shape)\n",
        "\n",
        "first_object_target_eeg_data = np.mean(first_object_target_eeg_data, axis=2)\n",
        "first_object_non_target_eeg_data = np.mean(first_object_non_target_eeg_data, axis=2)\n",
        "\n",
        "fig, axs = plt.subplots(8,2,figsize=(16,16*4))\n",
        "for i in range(8):\n",
        "  f, t, Sxx = signal.spectrogram(first_object_target_eeg_data[i], fs=FS, nperseg=50, window=('hamming'), noverlap=35)\n",
        "  fnon, tnon, Sxxnon = signal.spectrogram(first_object_non_target_eeg_data[i], fs=FS, nperseg=50, window=('hamming'), noverlap=35)\n",
        "\n",
        "  axs[i,0].pcolormesh(t, f, Sxx)\n",
        "  axs[i,1].pcolormesh(tnon, fnon, Sxxnon)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kitlqn__dEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Min, Max = round(mne_array_clean.min()),round(mne_array_clean.max())*1000000\n",
        "picks = mne.pick_types(raw_data_clean.info, meg=False, eeg=True, stim=False, eog=False)\n",
        "xd = mne.preprocessing.Xdawn(n_components=2, signal_cov=None)\n",
        "xd.fit(raw_data_clean)\n",
        "epochs_denoised = xd.apply(raw_data_clean)\n",
        "epochs_denoised.keys()\n",
        "mne.viz.plot_epochs_image(epochs_denoised['1'], picks='eeg', vmin=Min, vmax=Max)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nJEQxG5C3id",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(np.swapaxes(epochs_denoised['1'],0,0))\n",
        "standardizer = mne.decoding.Scaler(scalings='mean')\n",
        "standardizer.fit(np.swapaxes(epochs_denoised['1'],0,0))\n",
        "standardized_data = standardizer.transform(np.swapaxes(epochs_denoised['1'],0,0))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6ILVN_zGHRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "standardized_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGywLqcAFgdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FS = 250\n",
        "N = 350\n",
        "# sample spacing\n",
        "T = 1.0 / FS\n",
        "x = np.linspace(0.0, N*T, N)\n",
        "num_labels= 8\n",
        "#plt.plot(mne_array[2][2], np.linspace(0.0,N,N))\n",
        "\n",
        "#fig, axs = plt.subplots(8,8, figsize=(4*8,4*8))\n",
        "#freq_data = list()\n",
        "#for i in range(8):\n",
        "#  for x in range(8):\n",
        "#    f, t, Sxx = signal.spectrogram(mne_array[x][i], fs=FS, nperseg=50, window=('hamming'), noverlap=35)\n",
        "#    freq_data.append(Sxx)\n",
        "#    axs[i,x].pcolormesh(t, f, Sxx)\n",
        "#plt.show()\n",
        "#print(open(\"~/keras/keras.json\").read())\n",
        "\n",
        "#events_arr = list(map(int, events_arr))\n",
        "standardized_data_eeg = np.swapaxes(standardized_data,0,2)\n",
        "standardized_data_eeg=standardized_data_eeg.reshape([1600,1,8, 350])\n",
        "X_train, X_test, y_train, y_test = train_test_split(standardized_data_eeg, events_arr, test_size=0.33, random_state=42)\n",
        "#y_train = np.array(y_train)\n",
        "y_train = to_categorical(y_train)\n",
        "#y_test = np.array(y_test)\n",
        "y_test = to_categorical(y_test)\n",
        "# Construct model \n",
        "\n",
        "\n",
        "num_epochs = 50\n",
        "num_batch_size = 10\n",
        "\n",
        "model = EEGNet(nb_classes = 9, Chans = 8, Samples = 350)\n",
        "model.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = 'adam')\n",
        "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "start = datetime.now()\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "          validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "#preds = model.predict(X_test, verbose=0)\n",
        "#acc = accuracy_score(y_test, preds)\n",
        "\n",
        "print(\"accuracy_score: {f}\".format(f=score))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}