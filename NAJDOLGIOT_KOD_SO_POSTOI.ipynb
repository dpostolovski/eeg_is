{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Faza4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dpostolovski/eeg_is/blob/train_compare_full_data/NAJDOLGIOT_KOD_SO_POSTOI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UDle2I8OMUa",
        "colab_type": "code",
        "outputId": "46fe8438-543f-4a4a-9739-ca2abe6056a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qm05RkaOpZW",
        "colab_type": "code",
        "outputId": "fe4fed24-4a4e-4048-dede-97936c524972",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "#@title Инсталирање и вчитување на потребните библиотеки\n",
        "\n",
        "# Библиотека за истражување, визуелизација и анализирање на човечки \n",
        "# неврофизиолошки податоци (EEG, sEEG и др)\n",
        "!pip install mne \n",
        "!pip install termcolor\n",
        "!mkdir saved_models\n",
        "\n",
        "%tensorflow_version 1.12.0"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mne in /usr/local/lib/python3.6/dist-packages (0.20.6)\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from mne) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from mne) (1.18.5)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "mkdir: cannot create directory ‘saved_models’: File exists\n",
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.12.0`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myyif7ke9kXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, date, time\n",
        "import pandas as pd\n",
        "from termcolor import colored\n",
        "import mne\n",
        "from sklearn.decomposition import PCA, FastICA\n",
        "import mne\n",
        "from sklearn import metrics \n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint \n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.io as sio\n",
        "from scipy.fft import fft\n",
        "from scipy import signal\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import os\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow import keras\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas.util.testing as tm\n",
        "\n",
        "import sys\n",
        "sys.path.append('drive/My Drive/Интелигентни Системи')\n",
        "from EEGModels import DeepConvNet, EEGNet\n",
        "\n",
        "K.set_image_data_format('channels_first')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS7SYnJE7DaE",
        "colab_type": "code",
        "outputId": "74a95dac-2616-424e-bc14-8fbb87593f0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df=pd.read_csv('drive/My Drive/Интелигентни Системи/output_format.csv')\n",
        "#df.iat[0,0] = 5\n",
        "df"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUBJECT</th>\n",
              "      <th>SESSION</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>Unnamed: 52</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    SUBJECT  SESSION  1  2  3  4  5  6  7  ... 43 44 45 46 47 48 49 50 Unnamed: 52\n",
              "0         1        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "1         1        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "2         1        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "3         2        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "4         2        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "5         2        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "6         3        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "7         3        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "8         3        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "9         4        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "10        4        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "11        4        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "12        5        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "13        5        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "14        5        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "15        6        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "16        6        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "17        6        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "18        7        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "19        7        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "20        7        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "21        8        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "22        8        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "23        8        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "24        9        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "25        9        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "26        9        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "27       10        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "28       10        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "29       10        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "30       11        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "31       11        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "32       11        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "33       12        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "34       12        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "35       12        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "36       13        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "37       13        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "38       13        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "39       14        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "40       14        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "41       14        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "42       15        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "43       15        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "44       15        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "\n",
              "[45 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU0YDj0OO4jD",
        "colab_type": "code",
        "outputId": "6391b93a-4ab3-4736-9df7-c05bf82a70c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Иницијализација на променливите каде ќе бидат вчитани податоците\n",
        "data = []\n",
        "labels = []\n",
        "events = []\n",
        "targets = []\n",
        "\n",
        "for i in range(1, 2): # Итерација низ секој испитен примерок\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_data = np.empty(0)\n",
        "  temp_labels = np.empty(0)\n",
        "  temp_events = np.empty(0)\n",
        "  temp_targets = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainData.mat\"\n",
        "    temp = loadmat(full_path)['trainData']\n",
        "    if temp_data.size != 0:\n",
        "      temp_data = np.concatenate((temp_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на label-ите\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainLabels.txt\"\n",
        "    with open(full_path, \"r\") as file_labels:\n",
        "      temp = file_labels.read().splitlines()\n",
        "      temp = np.repeat(temp, 8*10)\n",
        "      if temp_labels.size != 0:\n",
        "        temp_labels = np.concatenate((temp_labels, temp))\n",
        "      else:\n",
        "        temp_labels = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_events.size != 0:\n",
        "        temp_events = np.append(temp_events, temp)\n",
        "      else:\n",
        "        temp_events = np.array(temp)\n",
        "      \n",
        "\n",
        "    # Вчитување на редоследот на објекти кои се target\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainTargets.txt\"\n",
        "    with open(full_path, \"r\") as file_targets:\n",
        "      temp = file_targets.read().splitlines()\n",
        "      if temp_targets.size != 0:\n",
        "        temp_targets = np.concatenate((temp_targets, temp))\n",
        "      else:\n",
        "        temp_targets = np.array(temp)\n",
        "    print(\"\\t - Податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "\n",
        "  for j in range(4, 8): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "      \n",
        "  # Зачувај ги податоците вчитани од испитниот примерок во низа\n",
        "  data.append(temp_data)\n",
        "  labels.append(temp_labels)\n",
        "  events.append(temp_events)\n",
        "  targets.append(temp_targets)\n",
        "\n",
        "  \n",
        "  print(\"Податоците од испитниот примерок \" + str(i) + \" се вчитани.\")\n",
        "\n",
        "\n",
        "  #data = target_events_data_scaled\n",
        "  mne_array = np.swapaxes(data[i-1], 0, 2) # (епохa, канал, настан). \n",
        "  mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "  mne_array = mne_array.reshape(mne_array.shape[0],1, 8,350)\n",
        "  print(mne_array.shape)\n",
        "\n",
        "  events_arr = events[i-1].astype(np.int)\n",
        "  labels_arr = labels[i-1].astype(np.int)\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(mne_array, labels_arr-1, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "  model1 = DeepConvNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "  model1.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = Adam(0.0009))\n",
        "  checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                                verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "  #clf = RandomForestClassifier(max_depth=5)\n",
        "  #clf.fit(X_train, y_train)\n",
        "  #score = clf.score(X_test, y_test)\n",
        "  # print(score)\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  num_batch_size=100\n",
        "  num_epochs=400\n",
        "  model1.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "            validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "  score = model1.evaluate(X_test, y_test, verbose=1)\n",
        "  print(score)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t - Податоците од сесија 1 се вчитани.\n",
            "\t - Податоците од сесија 2 се вчитани.\n",
            "\t - Податоците од сесија 3 се вчитани.\n",
            "Податоците од испитниот примерок 1 се вчитани.\n",
            "(4800, 1, 8, 350)\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Train on 3600 samples, validate on 1200 samples\n",
            "Epoch 1/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.3168 - acc: 0.1600\n",
            "Epoch 00001: val_loss improved from inf to 2.17509, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 3s 892us/sample - loss: 2.3041 - acc: 0.1622 - val_loss: 2.1751 - val_acc: 0.1942\n",
            "Epoch 2/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.1270 - acc: 0.1854\n",
            "Epoch 00002: val_loss did not improve from 2.17509\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 2.1265 - acc: 0.1844 - val_loss: 2.2666 - val_acc: 0.2025\n",
            "Epoch 3/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.1223 - acc: 0.2009\n",
            "Epoch 00003: val_loss did not improve from 2.17509\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 2.1229 - acc: 0.1994 - val_loss: 2.2239 - val_acc: 0.1333\n",
            "Epoch 4/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.0457 - acc: 0.2053\n",
            "Epoch 00004: val_loss did not improve from 2.17509\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 2.0509 - acc: 0.2056 - val_loss: 2.2924 - val_acc: 0.2092\n",
            "Epoch 5/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.0543 - acc: 0.2158\n",
            "Epoch 00005: val_loss improved from 2.17509 to 2.15801, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 2.0424 - acc: 0.2156 - val_loss: 2.1580 - val_acc: 0.1950\n",
            "Epoch 6/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.9753 - acc: 0.2361\n",
            "Epoch 00006: val_loss improved from 2.15801 to 1.98564, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 1.9676 - acc: 0.2367 - val_loss: 1.9856 - val_acc: 0.2425\n",
            "Epoch 7/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.8647 - acc: 0.2626\n",
            "Epoch 00007: val_loss improved from 1.98564 to 1.97115, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 1.8609 - acc: 0.2647 - val_loss: 1.9712 - val_acc: 0.2542\n",
            "Epoch 8/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.8682 - acc: 0.2871\n",
            "Epoch 00008: val_loss did not improve from 1.97115\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.8691 - acc: 0.2878 - val_loss: 2.2469 - val_acc: 0.2575\n",
            "Epoch 9/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.8163 - acc: 0.2976\n",
            "Epoch 00009: val_loss improved from 1.97115 to 1.88416, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 1.8129 - acc: 0.3000 - val_loss: 1.8842 - val_acc: 0.2200\n",
            "Epoch 10/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.7433 - acc: 0.3197\n",
            "Epoch 00010: val_loss improved from 1.88416 to 1.86195, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 1.7438 - acc: 0.3208 - val_loss: 1.8619 - val_acc: 0.3183\n",
            "Epoch 11/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.7050 - acc: 0.3348\n",
            "Epoch 00011: val_loss improved from 1.86195 to 1.84214, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 1.6921 - acc: 0.3383 - val_loss: 1.8421 - val_acc: 0.3042\n",
            "Epoch 12/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.6328 - acc: 0.3741\n",
            "Epoch 00012: val_loss did not improve from 1.84214\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.6259 - acc: 0.3781 - val_loss: 1.9556 - val_acc: 0.3367\n",
            "Epoch 13/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.5893 - acc: 0.3926\n",
            "Epoch 00013: val_loss did not improve from 1.84214\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.5853 - acc: 0.3919 - val_loss: 1.8443 - val_acc: 0.3358\n",
            "Epoch 14/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.5450 - acc: 0.4012\n",
            "Epoch 00014: val_loss improved from 1.84214 to 1.54132, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 1.5437 - acc: 0.4008 - val_loss: 1.5413 - val_acc: 0.3767\n",
            "Epoch 15/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.4786 - acc: 0.4294\n",
            "Epoch 00015: val_loss did not improve from 1.54132\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 1.4881 - acc: 0.4250 - val_loss: 1.8951 - val_acc: 0.3650\n",
            "Epoch 16/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.4749 - acc: 0.4317\n",
            "Epoch 00016: val_loss did not improve from 1.54132\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 1.4787 - acc: 0.4300 - val_loss: 1.5809 - val_acc: 0.4067\n",
            "Epoch 17/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.4729 - acc: 0.4297\n",
            "Epoch 00017: val_loss did not improve from 1.54132\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 1.4638 - acc: 0.4361 - val_loss: 1.6865 - val_acc: 0.3700\n",
            "Epoch 18/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.3838 - acc: 0.4600\n",
            "Epoch 00018: val_loss improved from 1.54132 to 1.48732, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 1.3877 - acc: 0.4583 - val_loss: 1.4873 - val_acc: 0.4150\n",
            "Epoch 19/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.3870 - acc: 0.4575\n",
            "Epoch 00019: val_loss improved from 1.48732 to 1.39557, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 1.3844 - acc: 0.4581 - val_loss: 1.3956 - val_acc: 0.4283\n",
            "Epoch 20/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.3087 - acc: 0.4941\n",
            "Epoch 00020: val_loss improved from 1.39557 to 1.36470, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 1.3061 - acc: 0.4972 - val_loss: 1.3647 - val_acc: 0.4683\n",
            "Epoch 21/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.2682 - acc: 0.5029\n",
            "Epoch 00021: val_loss improved from 1.36470 to 1.29796, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 1.2713 - acc: 0.5011 - val_loss: 1.2980 - val_acc: 0.4825\n",
            "Epoch 22/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.2659 - acc: 0.5077\n",
            "Epoch 00022: val_loss did not improve from 1.29796\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 1.2676 - acc: 0.5078 - val_loss: 1.3844 - val_acc: 0.4675\n",
            "Epoch 23/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.1862 - acc: 0.5379\n",
            "Epoch 00023: val_loss did not improve from 1.29796\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.1923 - acc: 0.5331 - val_loss: 1.4521 - val_acc: 0.4450\n",
            "Epoch 24/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.1472 - acc: 0.5472\n",
            "Epoch 00024: val_loss did not improve from 1.29796\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 1.1533 - acc: 0.5419 - val_loss: 1.3489 - val_acc: 0.4850\n",
            "Epoch 25/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.1194 - acc: 0.5794\n",
            "Epoch 00025: val_loss improved from 1.29796 to 1.17393, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 1.1230 - acc: 0.5769 - val_loss: 1.1739 - val_acc: 0.5342\n",
            "Epoch 26/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0827 - acc: 0.5788\n",
            "Epoch 00026: val_loss did not improve from 1.17393\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.0811 - acc: 0.5808 - val_loss: 1.2174 - val_acc: 0.5408\n",
            "Epoch 27/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.0685 - acc: 0.5851\n",
            "Epoch 00027: val_loss did not improve from 1.17393\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 1.0639 - acc: 0.5864 - val_loss: 1.1876 - val_acc: 0.5308\n",
            "Epoch 28/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0051 - acc: 0.6138\n",
            "Epoch 00028: val_loss improved from 1.17393 to 1.10753, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 1.0079 - acc: 0.6144 - val_loss: 1.1075 - val_acc: 0.5508\n",
            "Epoch 29/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0187 - acc: 0.6088\n",
            "Epoch 00029: val_loss did not improve from 1.10753\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 1.0192 - acc: 0.6067 - val_loss: 1.1186 - val_acc: 0.5633\n",
            "Epoch 30/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9678 - acc: 0.6353\n",
            "Epoch 00030: val_loss did not improve from 1.10753\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.9760 - acc: 0.6314 - val_loss: 1.1545 - val_acc: 0.5558\n",
            "Epoch 31/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9415 - acc: 0.6382\n",
            "Epoch 00031: val_loss improved from 1.10753 to 1.05250, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.9365 - acc: 0.6403 - val_loss: 1.0525 - val_acc: 0.5742\n",
            "Epoch 32/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9463 - acc: 0.6347\n",
            "Epoch 00032: val_loss improved from 1.05250 to 0.97360, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.9455 - acc: 0.6353 - val_loss: 0.9736 - val_acc: 0.6125\n",
            "Epoch 33/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8793 - acc: 0.6668\n",
            "Epoch 00033: val_loss did not improve from 0.97360\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.8821 - acc: 0.6650 - val_loss: 1.0251 - val_acc: 0.5992\n",
            "Epoch 34/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8579 - acc: 0.6761\n",
            "Epoch 00034: val_loss did not improve from 0.97360\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.8539 - acc: 0.6775 - val_loss: 0.9944 - val_acc: 0.6033\n",
            "Epoch 35/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8245 - acc: 0.6974\n",
            "Epoch 00035: val_loss improved from 0.97360 to 0.88453, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.8234 - acc: 0.6978 - val_loss: 0.8845 - val_acc: 0.6675\n",
            "Epoch 36/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7961 - acc: 0.7026\n",
            "Epoch 00036: val_loss improved from 0.88453 to 0.85549, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.7986 - acc: 0.7022 - val_loss: 0.8555 - val_acc: 0.6875\n",
            "Epoch 37/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.8005 - acc: 0.6991\n",
            "Epoch 00037: val_loss did not improve from 0.85549\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.7969 - acc: 0.7011 - val_loss: 0.9021 - val_acc: 0.6667\n",
            "Epoch 38/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7588 - acc: 0.7159\n",
            "Epoch 00038: val_loss improved from 0.85549 to 0.85027, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.7547 - acc: 0.7172 - val_loss: 0.8503 - val_acc: 0.6833\n",
            "Epoch 39/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7077 - acc: 0.7406\n",
            "Epoch 00039: val_loss improved from 0.85027 to 0.78571, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.7122 - acc: 0.7392 - val_loss: 0.7857 - val_acc: 0.6992\n",
            "Epoch 40/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7424 - acc: 0.7242\n",
            "Epoch 00040: val_loss did not improve from 0.78571\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.7378 - acc: 0.7286 - val_loss: 0.8164 - val_acc: 0.7033\n",
            "Epoch 41/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.6748 - acc: 0.7591\n",
            "Epoch 00041: val_loss did not improve from 0.78571\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.6877 - acc: 0.7514 - val_loss: 0.8616 - val_acc: 0.6800\n",
            "Epoch 42/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.6709 - acc: 0.7527\n",
            "Epoch 00042: val_loss improved from 0.78571 to 0.71207, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.6680 - acc: 0.7539 - val_loss: 0.7121 - val_acc: 0.7300\n",
            "Epoch 43/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6779 - acc: 0.7511\n",
            "Epoch 00043: val_loss did not improve from 0.71207\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.6752 - acc: 0.7503 - val_loss: 0.7237 - val_acc: 0.7158\n",
            "Epoch 44/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.6187 - acc: 0.7812\n",
            "Epoch 00044: val_loss improved from 0.71207 to 0.67490, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.6189 - acc: 0.7806 - val_loss: 0.6749 - val_acc: 0.7408\n",
            "Epoch 45/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5991 - acc: 0.7882\n",
            "Epoch 00045: val_loss did not improve from 0.67490\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.6027 - acc: 0.7872 - val_loss: 0.7433 - val_acc: 0.7233\n",
            "Epoch 46/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5905 - acc: 0.7968\n",
            "Epoch 00046: val_loss improved from 0.67490 to 0.62630, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.5925 - acc: 0.7942 - val_loss: 0.6263 - val_acc: 0.7650\n",
            "Epoch 47/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5596 - acc: 0.7961\n",
            "Epoch 00047: val_loss did not improve from 0.62630\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.5680 - acc: 0.7922 - val_loss: 0.6816 - val_acc: 0.7458\n",
            "Epoch 48/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5265 - acc: 0.8162\n",
            "Epoch 00048: val_loss improved from 0.62630 to 0.58101, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.5319 - acc: 0.8128 - val_loss: 0.5810 - val_acc: 0.8017\n",
            "Epoch 49/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5311 - acc: 0.8200\n",
            "Epoch 00049: val_loss did not improve from 0.58101\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.5285 - acc: 0.8211 - val_loss: 0.5906 - val_acc: 0.7942\n",
            "Epoch 50/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5096 - acc: 0.8273\n",
            "Epoch 00050: val_loss did not improve from 0.58101\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.5116 - acc: 0.8258 - val_loss: 0.6798 - val_acc: 0.7692\n",
            "Epoch 51/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5083 - acc: 0.8265\n",
            "Epoch 00051: val_loss improved from 0.58101 to 0.57766, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.5090 - acc: 0.8264 - val_loss: 0.5777 - val_acc: 0.8008\n",
            "Epoch 52/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.4941 - acc: 0.8275\n",
            "Epoch 00052: val_loss improved from 0.57766 to 0.56987, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.4952 - acc: 0.8272 - val_loss: 0.5699 - val_acc: 0.7942\n",
            "Epoch 53/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4582 - acc: 0.8400\n",
            "Epoch 00053: val_loss did not improve from 0.56987\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.4665 - acc: 0.8378 - val_loss: 0.5965 - val_acc: 0.7892\n",
            "Epoch 54/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.4501 - acc: 0.8516\n",
            "Epoch 00054: val_loss did not improve from 0.56987\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.4506 - acc: 0.8514 - val_loss: 0.5914 - val_acc: 0.7817\n",
            "Epoch 55/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4530 - acc: 0.8449\n",
            "Epoch 00055: val_loss improved from 0.56987 to 0.54566, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.4555 - acc: 0.8433 - val_loss: 0.5457 - val_acc: 0.8217\n",
            "Epoch 56/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4502 - acc: 0.8509\n",
            "Epoch 00056: val_loss did not improve from 0.54566\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.4495 - acc: 0.8511 - val_loss: 0.5625 - val_acc: 0.7950\n",
            "Epoch 57/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4181 - acc: 0.8579\n",
            "Epoch 00057: val_loss improved from 0.54566 to 0.48371, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.4159 - acc: 0.8606 - val_loss: 0.4837 - val_acc: 0.8433\n",
            "Epoch 58/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4068 - acc: 0.8652\n",
            "Epoch 00058: val_loss did not improve from 0.48371\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.4109 - acc: 0.8633 - val_loss: 0.4956 - val_acc: 0.8242\n",
            "Epoch 59/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.4042 - acc: 0.8628\n",
            "Epoch 00059: val_loss improved from 0.48371 to 0.42868, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.4082 - acc: 0.8622 - val_loss: 0.4287 - val_acc: 0.8558\n",
            "Epoch 60/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3576 - acc: 0.8833\n",
            "Epoch 00060: val_loss did not improve from 0.42868\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.3615 - acc: 0.8817 - val_loss: 0.4824 - val_acc: 0.8300\n",
            "Epoch 61/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.4123 - acc: 0.8675\n",
            "Epoch 00061: val_loss did not improve from 0.42868\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.4089 - acc: 0.8708 - val_loss: 0.4420 - val_acc: 0.8483\n",
            "Epoch 62/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3878 - acc: 0.8691\n",
            "Epoch 00062: val_loss did not improve from 0.42868\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.3887 - acc: 0.8692 - val_loss: 0.4407 - val_acc: 0.8633\n",
            "Epoch 63/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3537 - acc: 0.8914\n",
            "Epoch 00063: val_loss improved from 0.42868 to 0.38388, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.3556 - acc: 0.8900 - val_loss: 0.3839 - val_acc: 0.8850\n",
            "Epoch 64/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3459 - acc: 0.8946\n",
            "Epoch 00064: val_loss did not improve from 0.38388\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.3457 - acc: 0.8939 - val_loss: 0.4262 - val_acc: 0.8708\n",
            "Epoch 65/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3168 - acc: 0.9021\n",
            "Epoch 00065: val_loss did not improve from 0.38388\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.3158 - acc: 0.9022 - val_loss: 0.4292 - val_acc: 0.8625\n",
            "Epoch 66/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3268 - acc: 0.8956\n",
            "Epoch 00066: val_loss improved from 0.38388 to 0.36691, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.3276 - acc: 0.8953 - val_loss: 0.3669 - val_acc: 0.8942\n",
            "Epoch 67/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3253 - acc: 0.9019\n",
            "Epoch 00067: val_loss did not improve from 0.36691\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.3272 - acc: 0.9006 - val_loss: 0.3695 - val_acc: 0.8808\n",
            "Epoch 68/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3231 - acc: 0.8977\n",
            "Epoch 00068: val_loss did not improve from 0.36691\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.3226 - acc: 0.8981 - val_loss: 0.3908 - val_acc: 0.8833\n",
            "Epoch 69/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2997 - acc: 0.9111\n",
            "Epoch 00069: val_loss did not improve from 0.36691\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.3007 - acc: 0.9111 - val_loss: 0.3694 - val_acc: 0.8800\n",
            "Epoch 70/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3133 - acc: 0.8997\n",
            "Epoch 00070: val_loss improved from 0.36691 to 0.33441, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.3148 - acc: 0.9006 - val_loss: 0.3344 - val_acc: 0.8983\n",
            "Epoch 71/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2987 - acc: 0.9085\n",
            "Epoch 00071: val_loss improved from 0.33441 to 0.32844, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.3014 - acc: 0.9083 - val_loss: 0.3284 - val_acc: 0.9108\n",
            "Epoch 72/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3226 - acc: 0.8953\n",
            "Epoch 00072: val_loss did not improve from 0.32844\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.3216 - acc: 0.8964 - val_loss: 0.3609 - val_acc: 0.8850\n",
            "Epoch 73/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2847 - acc: 0.9109\n",
            "Epoch 00073: val_loss improved from 0.32844 to 0.32569, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.2907 - acc: 0.9092 - val_loss: 0.3257 - val_acc: 0.9017\n",
            "Epoch 74/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2840 - acc: 0.9149\n",
            "Epoch 00074: val_loss improved from 0.32569 to 0.28485, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.2850 - acc: 0.9147 - val_loss: 0.2849 - val_acc: 0.9208\n",
            "Epoch 75/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2686 - acc: 0.9151\n",
            "Epoch 00075: val_loss did not improve from 0.28485\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2705 - acc: 0.9142 - val_loss: 0.3522 - val_acc: 0.8825\n",
            "Epoch 76/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2772 - acc: 0.9147\n",
            "Epoch 00076: val_loss did not improve from 0.28485\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2732 - acc: 0.9169 - val_loss: 0.3404 - val_acc: 0.8950\n",
            "Epoch 77/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2587 - acc: 0.9252\n",
            "Epoch 00077: val_loss did not improve from 0.28485\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2585 - acc: 0.9256 - val_loss: 0.3223 - val_acc: 0.9067\n",
            "Epoch 78/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2648 - acc: 0.9200\n",
            "Epoch 00078: val_loss did not improve from 0.28485\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2652 - acc: 0.9200 - val_loss: 0.3148 - val_acc: 0.8983\n",
            "Epoch 79/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2543 - acc: 0.9237\n",
            "Epoch 00079: val_loss did not improve from 0.28485\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2545 - acc: 0.9239 - val_loss: 0.2893 - val_acc: 0.9083\n",
            "Epoch 80/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2352 - acc: 0.9309\n",
            "Epoch 00080: val_loss did not improve from 0.28485\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2339 - acc: 0.9314 - val_loss: 0.2881 - val_acc: 0.9117\n",
            "Epoch 81/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.9260\n",
            "Epoch 00081: val_loss improved from 0.28485 to 0.25988, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.2456 - acc: 0.9258 - val_loss: 0.2599 - val_acc: 0.9292\n",
            "Epoch 82/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2470 - acc: 0.9280\n",
            "Epoch 00082: val_loss did not improve from 0.25988\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2474 - acc: 0.9275 - val_loss: 0.3886 - val_acc: 0.8742\n",
            "Epoch 83/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2386 - acc: 0.9285\n",
            "Epoch 00083: val_loss did not improve from 0.25988\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2374 - acc: 0.9281 - val_loss: 0.2864 - val_acc: 0.9100\n",
            "Epoch 84/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2492 - acc: 0.9234\n",
            "Epoch 00084: val_loss improved from 0.25988 to 0.25536, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.2492 - acc: 0.9242 - val_loss: 0.2554 - val_acc: 0.9200\n",
            "Epoch 85/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2390 - acc: 0.9281\n",
            "Epoch 00085: val_loss improved from 0.25536 to 0.24716, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.2385 - acc: 0.9272 - val_loss: 0.2472 - val_acc: 0.9308\n",
            "Epoch 86/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2320 - acc: 0.9344\n",
            "Epoch 00086: val_loss did not improve from 0.24716\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.2343 - acc: 0.9336 - val_loss: 0.2746 - val_acc: 0.9183\n",
            "Epoch 87/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2241 - acc: 0.9309\n",
            "Epoch 00087: val_loss did not improve from 0.24716\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2255 - acc: 0.9303 - val_loss: 0.2782 - val_acc: 0.9167\n",
            "Epoch 88/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2374 - acc: 0.9276\n",
            "Epoch 00088: val_loss improved from 0.24716 to 0.22602, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.2394 - acc: 0.9256 - val_loss: 0.2260 - val_acc: 0.9375\n",
            "Epoch 89/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2215 - acc: 0.9303\n",
            "Epoch 00089: val_loss did not improve from 0.22602\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2272 - acc: 0.9283 - val_loss: 0.2529 - val_acc: 0.9342\n",
            "Epoch 90/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2041 - acc: 0.9409\n",
            "Epoch 00090: val_loss improved from 0.22602 to 0.20586, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.2042 - acc: 0.9408 - val_loss: 0.2059 - val_acc: 0.9492\n",
            "Epoch 91/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1882 - acc: 0.9509\n",
            "Epoch 00091: val_loss did not improve from 0.20586\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1865 - acc: 0.9519 - val_loss: 0.2411 - val_acc: 0.9308\n",
            "Epoch 92/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2091 - acc: 0.9374\n",
            "Epoch 00092: val_loss did not improve from 0.20586\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.2071 - acc: 0.9389 - val_loss: 0.2161 - val_acc: 0.9417\n",
            "Epoch 93/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2046 - acc: 0.9424\n",
            "Epoch 00093: val_loss did not improve from 0.20586\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2071 - acc: 0.9403 - val_loss: 0.2606 - val_acc: 0.9267\n",
            "Epoch 94/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1989 - acc: 0.9412\n",
            "Epoch 00094: val_loss did not improve from 0.20586\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2022 - acc: 0.9394 - val_loss: 0.2079 - val_acc: 0.9483\n",
            "Epoch 95/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2054 - acc: 0.9429\n",
            "Epoch 00095: val_loss did not improve from 0.20586\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2051 - acc: 0.9431 - val_loss: 0.2593 - val_acc: 0.9258\n",
            "Epoch 96/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1859 - acc: 0.9471\n",
            "Epoch 00096: val_loss did not improve from 0.20586\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1890 - acc: 0.9447 - val_loss: 0.2345 - val_acc: 0.9417\n",
            "Epoch 97/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1809 - acc: 0.9476\n",
            "Epoch 00097: val_loss did not improve from 0.20586\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1788 - acc: 0.9486 - val_loss: 0.2152 - val_acc: 0.9433\n",
            "Epoch 98/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1767 - acc: 0.9506\n",
            "Epoch 00098: val_loss improved from 0.20586 to 0.17748, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1759 - acc: 0.9506 - val_loss: 0.1775 - val_acc: 0.9592\n",
            "Epoch 99/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1744 - acc: 0.9543\n",
            "Epoch 00099: val_loss did not improve from 0.17748\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1747 - acc: 0.9542 - val_loss: 0.1951 - val_acc: 0.9508\n",
            "Epoch 100/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1776 - acc: 0.9488\n",
            "Epoch 00100: val_loss did not improve from 0.17748\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1808 - acc: 0.9469 - val_loss: 0.2243 - val_acc: 0.9367\n",
            "Epoch 101/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1944 - acc: 0.9466\n",
            "Epoch 00101: val_loss did not improve from 0.17748\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1965 - acc: 0.9447 - val_loss: 0.1890 - val_acc: 0.9533\n",
            "Epoch 102/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1910 - acc: 0.9429\n",
            "Epoch 00102: val_loss did not improve from 0.17748\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1913 - acc: 0.9422 - val_loss: 0.1963 - val_acc: 0.9517\n",
            "Epoch 103/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9463\n",
            "Epoch 00103: val_loss did not improve from 0.17748\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1833 - acc: 0.9456 - val_loss: 0.1991 - val_acc: 0.9483\n",
            "Epoch 104/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1696 - acc: 0.9528\n",
            "Epoch 00104: val_loss did not improve from 0.17748\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1710 - acc: 0.9519 - val_loss: 0.2033 - val_acc: 0.9492\n",
            "Epoch 105/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1634 - acc: 0.9518\n",
            "Epoch 00105: val_loss did not improve from 0.17748\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1658 - acc: 0.9517 - val_loss: 0.1879 - val_acc: 0.9408\n",
            "Epoch 106/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9514\n",
            "Epoch 00106: val_loss did not improve from 0.17748\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1801 - acc: 0.9511 - val_loss: 0.2541 - val_acc: 0.9192\n",
            "Epoch 107/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1742 - acc: 0.9503\n",
            "Epoch 00107: val_loss did not improve from 0.17748\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1757 - acc: 0.9497 - val_loss: 0.2361 - val_acc: 0.9150\n",
            "Epoch 108/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1684 - acc: 0.9544\n",
            "Epoch 00108: val_loss did not improve from 0.17748\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1702 - acc: 0.9531 - val_loss: 0.2093 - val_acc: 0.9392\n",
            "Epoch 109/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1838 - acc: 0.9435\n",
            "Epoch 00109: val_loss did not improve from 0.17748\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1832 - acc: 0.9450 - val_loss: 0.1986 - val_acc: 0.9450\n",
            "Epoch 110/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1525 - acc: 0.9591\n",
            "Epoch 00110: val_loss did not improve from 0.17748\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1530 - acc: 0.9606 - val_loss: 0.2234 - val_acc: 0.9325\n",
            "Epoch 111/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1685 - acc: 0.9524\n",
            "Epoch 00111: val_loss did not improve from 0.17748\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1753 - acc: 0.9519 - val_loss: 0.2317 - val_acc: 0.9333\n",
            "Epoch 112/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9443\n",
            "Epoch 00112: val_loss did not improve from 0.17748\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1799 - acc: 0.9447 - val_loss: 0.1789 - val_acc: 0.9567\n",
            "Epoch 113/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9454\n",
            "Epoch 00113: val_loss improved from 0.17748 to 0.17451, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1723 - acc: 0.9456 - val_loss: 0.1745 - val_acc: 0.9550\n",
            "Epoch 114/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1708 - acc: 0.9509\n",
            "Epoch 00114: val_loss improved from 0.17451 to 0.17213, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1695 - acc: 0.9506 - val_loss: 0.1721 - val_acc: 0.9550\n",
            "Epoch 115/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1676 - acc: 0.9491\n",
            "Epoch 00115: val_loss did not improve from 0.17213\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1676 - acc: 0.9494 - val_loss: 0.1945 - val_acc: 0.9475\n",
            "Epoch 116/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1644 - acc: 0.9509\n",
            "Epoch 00116: val_loss did not improve from 0.17213\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1610 - acc: 0.9528 - val_loss: 0.1905 - val_acc: 0.9483\n",
            "Epoch 117/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1482 - acc: 0.9597\n",
            "Epoch 00117: val_loss did not improve from 0.17213\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1510 - acc: 0.9586 - val_loss: 0.1890 - val_acc: 0.9450\n",
            "Epoch 118/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1694 - acc: 0.9482\n",
            "Epoch 00118: val_loss did not improve from 0.17213\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1703 - acc: 0.9478 - val_loss: 0.2025 - val_acc: 0.9317\n",
            "Epoch 119/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1536 - acc: 0.9547\n",
            "Epoch 00119: val_loss did not improve from 0.17213\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1571 - acc: 0.9533 - val_loss: 0.1790 - val_acc: 0.9475\n",
            "Epoch 120/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1659 - acc: 0.9486\n",
            "Epoch 00120: val_loss did not improve from 0.17213\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1676 - acc: 0.9478 - val_loss: 0.1810 - val_acc: 0.9417\n",
            "Epoch 121/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1776 - acc: 0.9418\n",
            "Epoch 00121: val_loss improved from 0.17213 to 0.15420, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.1807 - acc: 0.9419 - val_loss: 0.1542 - val_acc: 0.9575\n",
            "Epoch 122/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1635 - acc: 0.9515\n",
            "Epoch 00122: val_loss did not improve from 0.15420\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1662 - acc: 0.9511 - val_loss: 0.1770 - val_acc: 0.9475\n",
            "Epoch 123/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1641 - acc: 0.9509\n",
            "Epoch 00123: val_loss did not improve from 0.15420\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1661 - acc: 0.9492 - val_loss: 0.1635 - val_acc: 0.9558\n",
            "Epoch 124/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9540\n",
            "Epoch 00124: val_loss did not improve from 0.15420\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1592 - acc: 0.9528 - val_loss: 0.1695 - val_acc: 0.9450\n",
            "Epoch 125/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1568 - acc: 0.9556\n",
            "Epoch 00125: val_loss improved from 0.15420 to 0.15256, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1577 - acc: 0.9547 - val_loss: 0.1526 - val_acc: 0.9550\n",
            "Epoch 126/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.9540\n",
            "Epoch 00126: val_loss improved from 0.15256 to 0.14859, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1471 - acc: 0.9544 - val_loss: 0.1486 - val_acc: 0.9642\n",
            "Epoch 127/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.9557\n",
            "Epoch 00127: val_loss did not improve from 0.14859\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1504 - acc: 0.9553 - val_loss: 0.1886 - val_acc: 0.9450\n",
            "Epoch 128/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9500\n",
            "Epoch 00128: val_loss improved from 0.14859 to 0.14800, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.1557 - acc: 0.9494 - val_loss: 0.1480 - val_acc: 0.9633\n",
            "Epoch 129/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1478 - acc: 0.9579\n",
            "Epoch 00129: val_loss did not improve from 0.14800\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1465 - acc: 0.9583 - val_loss: 0.1543 - val_acc: 0.9592\n",
            "Epoch 130/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1403 - acc: 0.9594\n",
            "Epoch 00130: val_loss did not improve from 0.14800\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1428 - acc: 0.9589 - val_loss: 0.1628 - val_acc: 0.9450\n",
            "Epoch 131/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1242 - acc: 0.9671\n",
            "Epoch 00131: val_loss did not improve from 0.14800\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1251 - acc: 0.9667 - val_loss: 0.1529 - val_acc: 0.9608\n",
            "Epoch 132/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1311 - acc: 0.9639\n",
            "Epoch 00132: val_loss did not improve from 0.14800\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1326 - acc: 0.9633 - val_loss: 0.1541 - val_acc: 0.9625\n",
            "Epoch 133/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1347 - acc: 0.9597\n",
            "Epoch 00133: val_loss improved from 0.14800 to 0.13962, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1371 - acc: 0.9583 - val_loss: 0.1396 - val_acc: 0.9642\n",
            "Epoch 134/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1486 - acc: 0.9542\n",
            "Epoch 00134: val_loss did not improve from 0.13962\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1531 - acc: 0.9522 - val_loss: 0.1398 - val_acc: 0.9575\n",
            "Epoch 135/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1403 - acc: 0.9591\n",
            "Epoch 00135: val_loss did not improve from 0.13962\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1387 - acc: 0.9603 - val_loss: 0.1522 - val_acc: 0.9575\n",
            "Epoch 136/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1372 - acc: 0.9627\n",
            "Epoch 00136: val_loss did not improve from 0.13962\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1379 - acc: 0.9617 - val_loss: 0.1473 - val_acc: 0.9617\n",
            "Epoch 137/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9623\n",
            "Epoch 00137: val_loss did not improve from 0.13962\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1344 - acc: 0.9617 - val_loss: 0.1429 - val_acc: 0.9608\n",
            "Epoch 138/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1211 - acc: 0.9685\n",
            "Epoch 00138: val_loss did not improve from 0.13962\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1220 - acc: 0.9686 - val_loss: 0.1535 - val_acc: 0.9575\n",
            "Epoch 139/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1333 - acc: 0.9615\n",
            "Epoch 00139: val_loss did not improve from 0.13962\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1316 - acc: 0.9633 - val_loss: 0.1523 - val_acc: 0.9558\n",
            "Epoch 140/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1205 - acc: 0.9650\n",
            "Epoch 00140: val_loss did not improve from 0.13962\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1211 - acc: 0.9644 - val_loss: 0.1658 - val_acc: 0.9550\n",
            "Epoch 141/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.9626\n",
            "Epoch 00141: val_loss improved from 0.13962 to 0.13303, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1311 - acc: 0.9625 - val_loss: 0.1330 - val_acc: 0.9658\n",
            "Epoch 142/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9720\n",
            "Epoch 00142: val_loss did not improve from 0.13303\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1089 - acc: 0.9711 - val_loss: 0.1363 - val_acc: 0.9608\n",
            "Epoch 143/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1345 - acc: 0.9580\n",
            "Epoch 00143: val_loss did not improve from 0.13303\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1351 - acc: 0.9569 - val_loss: 0.1460 - val_acc: 0.9567\n",
            "Epoch 144/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1354 - acc: 0.9644\n",
            "Epoch 00144: val_loss did not improve from 0.13303\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1351 - acc: 0.9642 - val_loss: 0.1393 - val_acc: 0.9625\n",
            "Epoch 145/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9580\n",
            "Epoch 00145: val_loss did not improve from 0.13303\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1421 - acc: 0.9567 - val_loss: 0.1358 - val_acc: 0.9650\n",
            "Epoch 146/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1344 - acc: 0.9597\n",
            "Epoch 00146: val_loss did not improve from 0.13303\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1321 - acc: 0.9606 - val_loss: 0.1364 - val_acc: 0.9600\n",
            "Epoch 147/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1289 - acc: 0.9634\n",
            "Epoch 00147: val_loss did not improve from 0.13303\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1318 - acc: 0.9611 - val_loss: 0.1641 - val_acc: 0.9508\n",
            "Epoch 148/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9551\n",
            "Epoch 00148: val_loss did not improve from 0.13303\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1451 - acc: 0.9542 - val_loss: 0.1623 - val_acc: 0.9550\n",
            "Epoch 149/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9617\n",
            "Epoch 00149: val_loss did not improve from 0.13303\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1324 - acc: 0.9608 - val_loss: 0.1393 - val_acc: 0.9633\n",
            "Epoch 150/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1276 - acc: 0.9632\n",
            "Epoch 00150: val_loss did not improve from 0.13303\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1253 - acc: 0.9642 - val_loss: 0.1462 - val_acc: 0.9558\n",
            "Epoch 151/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1267 - acc: 0.9630\n",
            "Epoch 00151: val_loss did not improve from 0.13303\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1269 - acc: 0.9628 - val_loss: 0.1752 - val_acc: 0.9433\n",
            "Epoch 152/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9526\n",
            "Epoch 00152: val_loss improved from 0.13303 to 0.12942, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1521 - acc: 0.9531 - val_loss: 0.1294 - val_acc: 0.9733\n",
            "Epoch 153/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1359 - acc: 0.9609\n",
            "Epoch 00153: val_loss improved from 0.12942 to 0.12586, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1354 - acc: 0.9614 - val_loss: 0.1259 - val_acc: 0.9633\n",
            "Epoch 154/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1150 - acc: 0.9727\n",
            "Epoch 00154: val_loss improved from 0.12586 to 0.11936, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1147 - acc: 0.9722 - val_loss: 0.1194 - val_acc: 0.9725\n",
            "Epoch 155/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1202 - acc: 0.9648\n",
            "Epoch 00155: val_loss did not improve from 0.11936\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1238 - acc: 0.9642 - val_loss: 0.1399 - val_acc: 0.9567\n",
            "Epoch 156/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1107 - acc: 0.9676\n",
            "Epoch 00156: val_loss improved from 0.11936 to 0.11105, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1120 - acc: 0.9675 - val_loss: 0.1110 - val_acc: 0.9767\n",
            "Epoch 157/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1338 - acc: 0.9584\n",
            "Epoch 00157: val_loss did not improve from 0.11105\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1358 - acc: 0.9575 - val_loss: 0.1644 - val_acc: 0.9533\n",
            "Epoch 158/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1317 - acc: 0.9588\n",
            "Epoch 00158: val_loss did not improve from 0.11105\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1323 - acc: 0.9589 - val_loss: 0.1371 - val_acc: 0.9667\n",
            "Epoch 159/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9677\n",
            "Epoch 00159: val_loss did not improve from 0.11105\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1176 - acc: 0.9672 - val_loss: 0.1440 - val_acc: 0.9592\n",
            "Epoch 160/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1262 - acc: 0.9612\n",
            "Epoch 00160: val_loss did not improve from 0.11105\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1277 - acc: 0.9611 - val_loss: 0.1256 - val_acc: 0.9692\n",
            "Epoch 161/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1224 - acc: 0.9619\n",
            "Epoch 00161: val_loss improved from 0.11105 to 0.11041, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1212 - acc: 0.9633 - val_loss: 0.1104 - val_acc: 0.9717\n",
            "Epoch 162/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1168 - acc: 0.9626\n",
            "Epoch 00162: val_loss did not improve from 0.11041\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1159 - acc: 0.9633 - val_loss: 0.1156 - val_acc: 0.9692\n",
            "Epoch 163/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1207 - acc: 0.9671\n",
            "Epoch 00163: val_loss did not improve from 0.11041\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1219 - acc: 0.9664 - val_loss: 0.1584 - val_acc: 0.9517\n",
            "Epoch 164/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9660\n",
            "Epoch 00164: val_loss did not improve from 0.11041\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1243 - acc: 0.9647 - val_loss: 0.1472 - val_acc: 0.9592\n",
            "Epoch 165/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9683\n",
            "Epoch 00165: val_loss did not improve from 0.11041\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1094 - acc: 0.9675 - val_loss: 0.1564 - val_acc: 0.9558\n",
            "Epoch 166/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1213 - acc: 0.9636\n",
            "Epoch 00166: val_loss did not improve from 0.11041\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1182 - acc: 0.9656 - val_loss: 0.1478 - val_acc: 0.9558\n",
            "Epoch 167/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9694\n",
            "Epoch 00167: val_loss did not improve from 0.11041\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1053 - acc: 0.9697 - val_loss: 0.1456 - val_acc: 0.9525\n",
            "Epoch 168/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9646\n",
            "Epoch 00168: val_loss did not improve from 0.11041\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1141 - acc: 0.9647 - val_loss: 0.1305 - val_acc: 0.9608\n",
            "Epoch 169/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1157 - acc: 0.9673\n",
            "Epoch 00169: val_loss did not improve from 0.11041\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1167 - acc: 0.9667 - val_loss: 0.1531 - val_acc: 0.9558\n",
            "Epoch 170/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.9526\n",
            "Epoch 00170: val_loss did not improve from 0.11041\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1446 - acc: 0.9536 - val_loss: 0.1367 - val_acc: 0.9592\n",
            "Epoch 171/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1451 - acc: 0.9518\n",
            "Epoch 00171: val_loss did not improve from 0.11041\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1438 - acc: 0.9539 - val_loss: 0.1373 - val_acc: 0.9600\n",
            "Epoch 172/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9489\n",
            "Epoch 00172: val_loss did not improve from 0.11041\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1623 - acc: 0.9492 - val_loss: 0.1206 - val_acc: 0.9692\n",
            "Epoch 173/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1320 - acc: 0.9618\n",
            "Epoch 00173: val_loss did not improve from 0.11041\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1342 - acc: 0.9608 - val_loss: 0.1469 - val_acc: 0.9617\n",
            "Epoch 174/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1273 - acc: 0.9594\n",
            "Epoch 00174: val_loss did not improve from 0.11041\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1295 - acc: 0.9583 - val_loss: 0.1338 - val_acc: 0.9642\n",
            "Epoch 175/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1312 - acc: 0.9618\n",
            "Epoch 00175: val_loss did not improve from 0.11041\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1309 - acc: 0.9611 - val_loss: 0.1490 - val_acc: 0.9600\n",
            "Epoch 176/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1195 - acc: 0.9656\n",
            "Epoch 00176: val_loss did not improve from 0.11041\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1197 - acc: 0.9656 - val_loss: 0.1207 - val_acc: 0.9683\n",
            "Epoch 177/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1178 - acc: 0.9676\n",
            "Epoch 00177: val_loss did not improve from 0.11041\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1201 - acc: 0.9681 - val_loss: 0.1139 - val_acc: 0.9733\n",
            "Epoch 178/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1322 - acc: 0.9574\n",
            "Epoch 00178: val_loss improved from 0.11041 to 0.10951, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.1320 - acc: 0.9575 - val_loss: 0.1095 - val_acc: 0.9708\n",
            "Epoch 179/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9634\n",
            "Epoch 00179: val_loss did not improve from 0.10951\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1196 - acc: 0.9633 - val_loss: 0.1289 - val_acc: 0.9683\n",
            "Epoch 180/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1177 - acc: 0.9642\n",
            "Epoch 00180: val_loss did not improve from 0.10951\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1223 - acc: 0.9625 - val_loss: 0.1281 - val_acc: 0.9650\n",
            "Epoch 181/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1219 - acc: 0.9641\n",
            "Epoch 00181: val_loss did not improve from 0.10951\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1202 - acc: 0.9647 - val_loss: 0.1149 - val_acc: 0.9717\n",
            "Epoch 182/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1158 - acc: 0.9659\n",
            "Epoch 00182: val_loss did not improve from 0.10951\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1168 - acc: 0.9653 - val_loss: 0.1156 - val_acc: 0.9683\n",
            "Epoch 183/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9717\n",
            "Epoch 00183: val_loss did not improve from 0.10951\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1060 - acc: 0.9708 - val_loss: 0.1300 - val_acc: 0.9633\n",
            "Epoch 184/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9689\n",
            "Epoch 00184: val_loss did not improve from 0.10951\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1004 - acc: 0.9683 - val_loss: 0.1172 - val_acc: 0.9725\n",
            "Epoch 185/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.9663\n",
            "Epoch 00185: val_loss did not improve from 0.10951\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1092 - acc: 0.9658 - val_loss: 0.1232 - val_acc: 0.9633\n",
            "Epoch 186/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9694\n",
            "Epoch 00186: val_loss improved from 0.10951 to 0.10918, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.1037 - acc: 0.9700 - val_loss: 0.1092 - val_acc: 0.9675\n",
            "Epoch 187/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9657\n",
            "Epoch 00187: val_loss did not improve from 0.10918\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1110 - acc: 0.9661 - val_loss: 0.1286 - val_acc: 0.9633\n",
            "Epoch 188/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9606\n",
            "Epoch 00188: val_loss did not improve from 0.10918\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1180 - acc: 0.9603 - val_loss: 0.1386 - val_acc: 0.9625\n",
            "Epoch 189/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1112 - acc: 0.9666\n",
            "Epoch 00189: val_loss did not improve from 0.10918\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1113 - acc: 0.9672 - val_loss: 0.1136 - val_acc: 0.9717\n",
            "Epoch 190/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1435 - acc: 0.9537\n",
            "Epoch 00190: val_loss did not improve from 0.10918\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1437 - acc: 0.9539 - val_loss: 0.1126 - val_acc: 0.9758\n",
            "Epoch 191/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9600\n",
            "Epoch 00191: val_loss did not improve from 0.10918\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1280 - acc: 0.9600 - val_loss: 0.1459 - val_acc: 0.9508\n",
            "Epoch 192/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9623\n",
            "Epoch 00192: val_loss improved from 0.10918 to 0.10824, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1172 - acc: 0.9614 - val_loss: 0.1082 - val_acc: 0.9700\n",
            "Epoch 193/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1230 - acc: 0.9579\n",
            "Epoch 00193: val_loss did not improve from 0.10824\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1208 - acc: 0.9594 - val_loss: 0.1237 - val_acc: 0.9625\n",
            "Epoch 194/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9689\n",
            "Epoch 00194: val_loss improved from 0.10824 to 0.09931, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1071 - acc: 0.9694 - val_loss: 0.0993 - val_acc: 0.9708\n",
            "Epoch 195/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1025 - acc: 0.9718\n",
            "Epoch 00195: val_loss did not improve from 0.09931\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1026 - acc: 0.9719 - val_loss: 0.1093 - val_acc: 0.9675\n",
            "Epoch 196/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1021 - acc: 0.9733\n",
            "Epoch 00196: val_loss did not improve from 0.09931\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1029 - acc: 0.9739 - val_loss: 0.1077 - val_acc: 0.9675\n",
            "Epoch 197/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9620\n",
            "Epoch 00197: val_loss did not improve from 0.09931\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1275 - acc: 0.9619 - val_loss: 0.1268 - val_acc: 0.9575\n",
            "Epoch 198/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1219 - acc: 0.9667\n",
            "Epoch 00198: val_loss improved from 0.09931 to 0.09161, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1240 - acc: 0.9661 - val_loss: 0.0916 - val_acc: 0.9767\n",
            "Epoch 199/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1196 - acc: 0.9591\n",
            "Epoch 00199: val_loss did not improve from 0.09161\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1187 - acc: 0.9594 - val_loss: 0.1075 - val_acc: 0.9733\n",
            "Epoch 200/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9634\n",
            "Epoch 00200: val_loss did not improve from 0.09161\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1212 - acc: 0.9639 - val_loss: 0.1151 - val_acc: 0.9633\n",
            "Epoch 201/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9669\n",
            "Epoch 00201: val_loss did not improve from 0.09161\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1147 - acc: 0.9672 - val_loss: 0.1055 - val_acc: 0.9725\n",
            "Epoch 202/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0992 - acc: 0.9738\n",
            "Epoch 00202: val_loss did not improve from 0.09161\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0988 - acc: 0.9739 - val_loss: 0.1111 - val_acc: 0.9733\n",
            "Epoch 203/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1044 - acc: 0.9688\n",
            "Epoch 00203: val_loss did not improve from 0.09161\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1036 - acc: 0.9692 - val_loss: 0.0979 - val_acc: 0.9750\n",
            "Epoch 204/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0952 - acc: 0.9756\n",
            "Epoch 00204: val_loss did not improve from 0.09161\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0938 - acc: 0.9753 - val_loss: 0.0941 - val_acc: 0.9733\n",
            "Epoch 205/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1004 - acc: 0.9724\n",
            "Epoch 00205: val_loss did not improve from 0.09161\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1015 - acc: 0.9717 - val_loss: 0.1063 - val_acc: 0.9708\n",
            "Epoch 206/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9669\n",
            "Epoch 00206: val_loss did not improve from 0.09161\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1114 - acc: 0.9658 - val_loss: 0.0961 - val_acc: 0.9733\n",
            "Epoch 207/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0972 - acc: 0.9730\n",
            "Epoch 00207: val_loss improved from 0.09161 to 0.08862, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.0969 - acc: 0.9731 - val_loss: 0.0886 - val_acc: 0.9733\n",
            "Epoch 208/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1077 - acc: 0.9670\n",
            "Epoch 00208: val_loss did not improve from 0.08862\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1101 - acc: 0.9661 - val_loss: 0.1205 - val_acc: 0.9692\n",
            "Epoch 209/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1134 - acc: 0.9691\n",
            "Epoch 00209: val_loss did not improve from 0.08862\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1107 - acc: 0.9700 - val_loss: 0.1036 - val_acc: 0.9708\n",
            "Epoch 210/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9654\n",
            "Epoch 00210: val_loss did not improve from 0.08862\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1122 - acc: 0.9658 - val_loss: 0.0926 - val_acc: 0.9775\n",
            "Epoch 211/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1120 - acc: 0.9645\n",
            "Epoch 00211: val_loss did not improve from 0.08862\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1099 - acc: 0.9661 - val_loss: 0.1021 - val_acc: 0.9733\n",
            "Epoch 212/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1147 - acc: 0.9662\n",
            "Epoch 00212: val_loss did not improve from 0.08862\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1132 - acc: 0.9672 - val_loss: 0.1097 - val_acc: 0.9683\n",
            "Epoch 213/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9634\n",
            "Epoch 00213: val_loss did not improve from 0.08862\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1206 - acc: 0.9636 - val_loss: 0.1336 - val_acc: 0.9633\n",
            "Epoch 214/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1144 - acc: 0.9635\n",
            "Epoch 00214: val_loss did not improve from 0.08862\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1140 - acc: 0.9639 - val_loss: 0.1066 - val_acc: 0.9725\n",
            "Epoch 215/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9634\n",
            "Epoch 00215: val_loss did not improve from 0.08862\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1072 - acc: 0.9631 - val_loss: 0.1197 - val_acc: 0.9658\n",
            "Epoch 216/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1163 - acc: 0.9653\n",
            "Epoch 00216: val_loss did not improve from 0.08862\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1148 - acc: 0.9658 - val_loss: 0.1127 - val_acc: 0.9700\n",
            "Epoch 217/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1105 - acc: 0.9682\n",
            "Epoch 00217: val_loss did not improve from 0.08862\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1113 - acc: 0.9675 - val_loss: 0.1273 - val_acc: 0.9633\n",
            "Epoch 218/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9674\n",
            "Epoch 00218: val_loss did not improve from 0.08862\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1154 - acc: 0.9681 - val_loss: 0.1078 - val_acc: 0.9717\n",
            "Epoch 219/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1012 - acc: 0.9718\n",
            "Epoch 00219: val_loss did not improve from 0.08862\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1019 - acc: 0.9717 - val_loss: 0.0979 - val_acc: 0.9708\n",
            "Epoch 220/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0938 - acc: 0.9756\n",
            "Epoch 00220: val_loss did not improve from 0.08862\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0963 - acc: 0.9750 - val_loss: 0.1004 - val_acc: 0.9750\n",
            "Epoch 221/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1031 - acc: 0.9679\n",
            "Epoch 00221: val_loss did not improve from 0.08862\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1015 - acc: 0.9686 - val_loss: 0.1584 - val_acc: 0.9483\n",
            "Epoch 222/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1300 - acc: 0.9569\n",
            "Epoch 00222: val_loss did not improve from 0.08862\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1296 - acc: 0.9581 - val_loss: 0.0941 - val_acc: 0.9767\n",
            "Epoch 223/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1019 - acc: 0.9727\n",
            "Epoch 00223: val_loss did not improve from 0.08862\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1008 - acc: 0.9731 - val_loss: 0.0940 - val_acc: 0.9775\n",
            "Epoch 224/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1003 - acc: 0.9694\n",
            "Epoch 00224: val_loss improved from 0.08862 to 0.08485, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1037 - acc: 0.9675 - val_loss: 0.0848 - val_acc: 0.9783\n",
            "Epoch 225/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0984 - acc: 0.9718\n",
            "Epoch 00225: val_loss did not improve from 0.08485\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1007 - acc: 0.9708 - val_loss: 0.1047 - val_acc: 0.9717\n",
            "Epoch 226/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0935 - acc: 0.9734\n",
            "Epoch 00226: val_loss did not improve from 0.08485\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0938 - acc: 0.9733 - val_loss: 0.0958 - val_acc: 0.9775\n",
            "Epoch 227/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9640\n",
            "Epoch 00227: val_loss did not improve from 0.08485\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1145 - acc: 0.9644 - val_loss: 0.1062 - val_acc: 0.9717\n",
            "Epoch 228/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1102 - acc: 0.9679\n",
            "Epoch 00228: val_loss did not improve from 0.08485\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1099 - acc: 0.9675 - val_loss: 0.1044 - val_acc: 0.9783\n",
            "Epoch 229/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0952 - acc: 0.9729\n",
            "Epoch 00229: val_loss did not improve from 0.08485\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0934 - acc: 0.9731 - val_loss: 0.0996 - val_acc: 0.9658\n",
            "Epoch 230/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0943 - acc: 0.9742\n",
            "Epoch 00230: val_loss did not improve from 0.08485\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0958 - acc: 0.9736 - val_loss: 0.1122 - val_acc: 0.9683\n",
            "Epoch 231/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0938 - acc: 0.9715\n",
            "Epoch 00231: val_loss did not improve from 0.08485\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0920 - acc: 0.9725 - val_loss: 0.1145 - val_acc: 0.9708\n",
            "Epoch 232/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1052 - acc: 0.9676\n",
            "Epoch 00232: val_loss did not improve from 0.08485\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1054 - acc: 0.9681 - val_loss: 0.1126 - val_acc: 0.9675\n",
            "Epoch 233/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1121 - acc: 0.9673\n",
            "Epoch 00233: val_loss did not improve from 0.08485\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1121 - acc: 0.9672 - val_loss: 0.0948 - val_acc: 0.9733\n",
            "Epoch 234/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1048 - acc: 0.9700\n",
            "Epoch 00234: val_loss did not improve from 0.08485\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1038 - acc: 0.9697 - val_loss: 0.0962 - val_acc: 0.9733\n",
            "Epoch 235/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9714\n",
            "Epoch 00235: val_loss did not improve from 0.08485\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1002 - acc: 0.9714 - val_loss: 0.1125 - val_acc: 0.9708\n",
            "Epoch 236/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1087 - acc: 0.9688\n",
            "Epoch 00236: val_loss improved from 0.08485 to 0.08410, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1080 - acc: 0.9689 - val_loss: 0.0841 - val_acc: 0.9775\n",
            "Epoch 237/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0842 - acc: 0.9759\n",
            "Epoch 00237: val_loss did not improve from 0.08410\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0857 - acc: 0.9761 - val_loss: 0.1085 - val_acc: 0.9717\n",
            "Epoch 238/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1075 - acc: 0.9709\n",
            "Epoch 00238: val_loss did not improve from 0.08410\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1065 - acc: 0.9714 - val_loss: 0.1000 - val_acc: 0.9742\n",
            "Epoch 239/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1126 - acc: 0.9621\n",
            "Epoch 00239: val_loss did not improve from 0.08410\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1133 - acc: 0.9619 - val_loss: 0.1098 - val_acc: 0.9675\n",
            "Epoch 240/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9686\n",
            "Epoch 00240: val_loss did not improve from 0.08410\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1046 - acc: 0.9689 - val_loss: 0.0985 - val_acc: 0.9750\n",
            "Epoch 241/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9700\n",
            "Epoch 00241: val_loss did not improve from 0.08410\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1028 - acc: 0.9694 - val_loss: 0.0876 - val_acc: 0.9775\n",
            "Epoch 242/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0945 - acc: 0.9727\n",
            "Epoch 00242: val_loss did not improve from 0.08410\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0961 - acc: 0.9725 - val_loss: 0.1088 - val_acc: 0.9725\n",
            "Epoch 243/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9786\n",
            "Epoch 00243: val_loss did not improve from 0.08410\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0861 - acc: 0.9792 - val_loss: 0.0865 - val_acc: 0.9775\n",
            "Epoch 244/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0859 - acc: 0.9742\n",
            "Epoch 00244: val_loss did not improve from 0.08410\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0857 - acc: 0.9744 - val_loss: 0.0854 - val_acc: 0.9808\n",
            "Epoch 245/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0898 - acc: 0.9764\n",
            "Epoch 00245: val_loss improved from 0.08410 to 0.07843, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.0916 - acc: 0.9750 - val_loss: 0.0784 - val_acc: 0.9825\n",
            "Epoch 246/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9777\n",
            "Epoch 00246: val_loss did not improve from 0.07843\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0814 - acc: 0.9775 - val_loss: 0.0811 - val_acc: 0.9783\n",
            "Epoch 247/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9726\n",
            "Epoch 00247: val_loss did not improve from 0.07843\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0959 - acc: 0.9725 - val_loss: 0.0939 - val_acc: 0.9742\n",
            "Epoch 248/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9726\n",
            "Epoch 00248: val_loss did not improve from 0.07843\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0918 - acc: 0.9728 - val_loss: 0.0798 - val_acc: 0.9783\n",
            "Epoch 249/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9703\n",
            "Epoch 00249: val_loss improved from 0.07843 to 0.07541, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.0948 - acc: 0.9697 - val_loss: 0.0754 - val_acc: 0.9800\n",
            "Epoch 250/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0883 - acc: 0.9735\n",
            "Epoch 00250: val_loss did not improve from 0.07541\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0875 - acc: 0.9739 - val_loss: 0.0829 - val_acc: 0.9792\n",
            "Epoch 251/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0990 - acc: 0.9691\n",
            "Epoch 00251: val_loss did not improve from 0.07541\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1006 - acc: 0.9683 - val_loss: 0.0857 - val_acc: 0.9725\n",
            "Epoch 252/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9683\n",
            "Epoch 00252: val_loss did not improve from 0.07541\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1052 - acc: 0.9686 - val_loss: 0.0847 - val_acc: 0.9775\n",
            "Epoch 253/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9740\n",
            "Epoch 00253: val_loss improved from 0.07541 to 0.07083, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.0963 - acc: 0.9731 - val_loss: 0.0708 - val_acc: 0.9892\n",
            "Epoch 254/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1139 - acc: 0.9688\n",
            "Epoch 00254: val_loss did not improve from 0.07083\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1128 - acc: 0.9694 - val_loss: 0.1357 - val_acc: 0.9608\n",
            "Epoch 255/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1159 - acc: 0.9650\n",
            "Epoch 00255: val_loss did not improve from 0.07083\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1194 - acc: 0.9636 - val_loss: 0.0903 - val_acc: 0.9750\n",
            "Epoch 256/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1118 - acc: 0.9676\n",
            "Epoch 00256: val_loss did not improve from 0.07083\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1117 - acc: 0.9675 - val_loss: 0.1000 - val_acc: 0.9717\n",
            "Epoch 257/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1003 - acc: 0.9688\n",
            "Epoch 00257: val_loss did not improve from 0.07083\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0981 - acc: 0.9697 - val_loss: 0.0778 - val_acc: 0.9792\n",
            "Epoch 258/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0892 - acc: 0.9768\n",
            "Epoch 00258: val_loss improved from 0.07083 to 0.06880, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.0909 - acc: 0.9764 - val_loss: 0.0688 - val_acc: 0.9817\n",
            "Epoch 259/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0901 - acc: 0.9747\n",
            "Epoch 00259: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0887 - acc: 0.9758 - val_loss: 0.0740 - val_acc: 0.9783\n",
            "Epoch 260/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0958 - acc: 0.9721\n",
            "Epoch 00260: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0942 - acc: 0.9731 - val_loss: 0.0811 - val_acc: 0.9775\n",
            "Epoch 261/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0992 - acc: 0.9697\n",
            "Epoch 00261: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0979 - acc: 0.9706 - val_loss: 0.0892 - val_acc: 0.9717\n",
            "Epoch 262/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1005 - acc: 0.9715\n",
            "Epoch 00262: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1029 - acc: 0.9703 - val_loss: 0.0817 - val_acc: 0.9775\n",
            "Epoch 263/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0977 - acc: 0.9712\n",
            "Epoch 00263: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0968 - acc: 0.9714 - val_loss: 0.0818 - val_acc: 0.9783\n",
            "Epoch 264/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0898 - acc: 0.9744\n",
            "Epoch 00264: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0889 - acc: 0.9744 - val_loss: 0.0824 - val_acc: 0.9775\n",
            "Epoch 265/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0813 - acc: 0.9767\n",
            "Epoch 00265: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0827 - acc: 0.9764 - val_loss: 0.0960 - val_acc: 0.9775\n",
            "Epoch 266/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9689\n",
            "Epoch 00266: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0916 - acc: 0.9692 - val_loss: 0.0963 - val_acc: 0.9700\n",
            "Epoch 267/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0923 - acc: 0.9716\n",
            "Epoch 00267: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0909 - acc: 0.9725 - val_loss: 0.0903 - val_acc: 0.9725\n",
            "Epoch 268/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0953 - acc: 0.9718\n",
            "Epoch 00268: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0953 - acc: 0.9711 - val_loss: 0.0915 - val_acc: 0.9725\n",
            "Epoch 269/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0932 - acc: 0.9739\n",
            "Epoch 00269: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0920 - acc: 0.9753 - val_loss: 0.1195 - val_acc: 0.9658\n",
            "Epoch 270/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1003 - acc: 0.9672\n",
            "Epoch 00270: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.1054 - acc: 0.9647 - val_loss: 0.0878 - val_acc: 0.9750\n",
            "Epoch 271/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0942 - acc: 0.9709\n",
            "Epoch 00271: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0930 - acc: 0.9711 - val_loss: 0.0714 - val_acc: 0.9867\n",
            "Epoch 272/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9723\n",
            "Epoch 00272: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0960 - acc: 0.9717 - val_loss: 0.0936 - val_acc: 0.9742\n",
            "Epoch 273/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0925 - acc: 0.9731\n",
            "Epoch 00273: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0937 - acc: 0.9731 - val_loss: 0.0921 - val_acc: 0.9700\n",
            "Epoch 274/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0948 - acc: 0.9700\n",
            "Epoch 00274: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0959 - acc: 0.9692 - val_loss: 0.0735 - val_acc: 0.9783\n",
            "Epoch 275/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9749\n",
            "Epoch 00275: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0922 - acc: 0.9753 - val_loss: 0.0888 - val_acc: 0.9775\n",
            "Epoch 276/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0915 - acc: 0.9745\n",
            "Epoch 00276: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0909 - acc: 0.9753 - val_loss: 0.0880 - val_acc: 0.9758\n",
            "Epoch 277/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9734\n",
            "Epoch 00277: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0885 - acc: 0.9736 - val_loss: 0.0835 - val_acc: 0.9742\n",
            "Epoch 278/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0864 - acc: 0.9735\n",
            "Epoch 00278: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0868 - acc: 0.9731 - val_loss: 0.0842 - val_acc: 0.9825\n",
            "Epoch 279/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0964 - acc: 0.9700\n",
            "Epoch 00279: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0996 - acc: 0.9689 - val_loss: 0.0830 - val_acc: 0.9758\n",
            "Epoch 280/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0997 - acc: 0.9691\n",
            "Epoch 00280: val_loss did not improve from 0.06880\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1005 - acc: 0.9689 - val_loss: 0.0748 - val_acc: 0.9833\n",
            "Epoch 281/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9706\n",
            "Epoch 00281: val_loss improved from 0.06880 to 0.06794, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1033 - acc: 0.9700 - val_loss: 0.0679 - val_acc: 0.9808\n",
            "Epoch 282/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1087 - acc: 0.9634\n",
            "Epoch 00282: val_loss did not improve from 0.06794\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1104 - acc: 0.9636 - val_loss: 0.0855 - val_acc: 0.9758\n",
            "Epoch 283/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0967 - acc: 0.9724\n",
            "Epoch 00283: val_loss did not improve from 0.06794\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0966 - acc: 0.9722 - val_loss: 0.0845 - val_acc: 0.9775\n",
            "Epoch 284/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9700\n",
            "Epoch 00284: val_loss did not improve from 0.06794\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1022 - acc: 0.9703 - val_loss: 0.0788 - val_acc: 0.9817\n",
            "Epoch 285/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1146 - acc: 0.9661\n",
            "Epoch 00285: val_loss did not improve from 0.06794\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1131 - acc: 0.9667 - val_loss: 0.0831 - val_acc: 0.9825\n",
            "Epoch 286/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0948 - acc: 0.9712\n",
            "Epoch 00286: val_loss did not improve from 0.06794\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0962 - acc: 0.9706 - val_loss: 0.0757 - val_acc: 0.9808\n",
            "Epoch 287/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0910 - acc: 0.9750\n",
            "Epoch 00287: val_loss did not improve from 0.06794\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0920 - acc: 0.9742 - val_loss: 0.0681 - val_acc: 0.9800\n",
            "Epoch 288/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0912 - acc: 0.9731\n",
            "Epoch 00288: val_loss did not improve from 0.06794\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0922 - acc: 0.9725 - val_loss: 0.0805 - val_acc: 0.9792\n",
            "Epoch 289/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9749\n",
            "Epoch 00289: val_loss did not improve from 0.06794\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0933 - acc: 0.9747 - val_loss: 0.0880 - val_acc: 0.9783\n",
            "Epoch 290/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0930 - acc: 0.9729\n",
            "Epoch 00290: val_loss did not improve from 0.06794\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0928 - acc: 0.9728 - val_loss: 0.1026 - val_acc: 0.9733\n",
            "Epoch 291/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9683\n",
            "Epoch 00291: val_loss improved from 0.06794 to 0.06346, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.0983 - acc: 0.9689 - val_loss: 0.0635 - val_acc: 0.9842\n",
            "Epoch 292/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9691\n",
            "Epoch 00292: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0986 - acc: 0.9689 - val_loss: 0.0817 - val_acc: 0.9742\n",
            "Epoch 293/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1142 - acc: 0.9621\n",
            "Epoch 00293: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1117 - acc: 0.9631 - val_loss: 0.0717 - val_acc: 0.9783\n",
            "Epoch 294/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0846 - acc: 0.9756\n",
            "Epoch 00294: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0864 - acc: 0.9747 - val_loss: 0.0691 - val_acc: 0.9808\n",
            "Epoch 295/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0814 - acc: 0.9759\n",
            "Epoch 00295: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0806 - acc: 0.9761 - val_loss: 0.0874 - val_acc: 0.9792\n",
            "Epoch 296/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0929 - acc: 0.9688\n",
            "Epoch 00296: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0925 - acc: 0.9697 - val_loss: 0.0969 - val_acc: 0.9717\n",
            "Epoch 297/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0904 - acc: 0.9700\n",
            "Epoch 00297: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.0939 - acc: 0.9681 - val_loss: 0.0844 - val_acc: 0.9783\n",
            "Epoch 298/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0827 - acc: 0.9736\n",
            "Epoch 00298: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0845 - acc: 0.9728 - val_loss: 0.0816 - val_acc: 0.9783\n",
            "Epoch 299/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0953 - acc: 0.9730\n",
            "Epoch 00299: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0978 - acc: 0.9725 - val_loss: 0.0641 - val_acc: 0.9875\n",
            "Epoch 300/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9723\n",
            "Epoch 00300: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1026 - acc: 0.9719 - val_loss: 0.0772 - val_acc: 0.9808\n",
            "Epoch 301/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1089 - acc: 0.9630\n",
            "Epoch 00301: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1095 - acc: 0.9633 - val_loss: 0.0801 - val_acc: 0.9750\n",
            "Epoch 302/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0939 - acc: 0.9718\n",
            "Epoch 00302: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0946 - acc: 0.9711 - val_loss: 0.0732 - val_acc: 0.9833\n",
            "Epoch 303/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9706\n",
            "Epoch 00303: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0922 - acc: 0.9706 - val_loss: 0.0798 - val_acc: 0.9742\n",
            "Epoch 304/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0990 - acc: 0.9727\n",
            "Epoch 00304: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0992 - acc: 0.9725 - val_loss: 0.0869 - val_acc: 0.9767\n",
            "Epoch 305/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0873 - acc: 0.9769\n",
            "Epoch 00305: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0880 - acc: 0.9761 - val_loss: 0.0660 - val_acc: 0.9842\n",
            "Epoch 306/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0908 - acc: 0.9752\n",
            "Epoch 00306: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.0881 - acc: 0.9756 - val_loss: 0.0674 - val_acc: 0.9825\n",
            "Epoch 307/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0868 - acc: 0.9734\n",
            "Epoch 00307: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0860 - acc: 0.9742 - val_loss: 0.0681 - val_acc: 0.9825\n",
            "Epoch 308/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9746\n",
            "Epoch 00308: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0908 - acc: 0.9739 - val_loss: 0.1009 - val_acc: 0.9683\n",
            "Epoch 309/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0936 - acc: 0.9728\n",
            "Epoch 00309: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.0943 - acc: 0.9728 - val_loss: 0.0637 - val_acc: 0.9808\n",
            "Epoch 310/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1092 - acc: 0.9647\n",
            "Epoch 00310: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1066 - acc: 0.9658 - val_loss: 0.0751 - val_acc: 0.9817\n",
            "Epoch 311/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0980 - acc: 0.9703\n",
            "Epoch 00311: val_loss did not improve from 0.06346\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.0984 - acc: 0.9706 - val_loss: 0.0678 - val_acc: 0.9833\n",
            "Epoch 312/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9760\n",
            "Epoch 00312: val_loss improved from 0.06346 to 0.05934, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.0837 - acc: 0.9761 - val_loss: 0.0593 - val_acc: 0.9833\n",
            "Epoch 313/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0970 - acc: 0.9732\n",
            "Epoch 00313: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0985 - acc: 0.9717 - val_loss: 0.0786 - val_acc: 0.9792\n",
            "Epoch 314/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0992 - acc: 0.9688\n",
            "Epoch 00314: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0969 - acc: 0.9703 - val_loss: 0.0722 - val_acc: 0.9808\n",
            "Epoch 315/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9777\n",
            "Epoch 00315: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0791 - acc: 0.9778 - val_loss: 0.0656 - val_acc: 0.9833\n",
            "Epoch 316/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9769\n",
            "Epoch 00316: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0779 - acc: 0.9775 - val_loss: 0.0633 - val_acc: 0.9825\n",
            "Epoch 317/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9709\n",
            "Epoch 00317: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0905 - acc: 0.9711 - val_loss: 0.0877 - val_acc: 0.9775\n",
            "Epoch 318/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0825 - acc: 0.9774\n",
            "Epoch 00318: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0827 - acc: 0.9772 - val_loss: 0.0899 - val_acc: 0.9725\n",
            "Epoch 319/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0785 - acc: 0.9803\n",
            "Epoch 00319: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0827 - acc: 0.9781 - val_loss: 0.0753 - val_acc: 0.9783\n",
            "Epoch 320/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0816 - acc: 0.9770\n",
            "Epoch 00320: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0824 - acc: 0.9772 - val_loss: 0.0819 - val_acc: 0.9783\n",
            "Epoch 321/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0714 - acc: 0.9788\n",
            "Epoch 00321: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0717 - acc: 0.9786 - val_loss: 0.0743 - val_acc: 0.9767\n",
            "Epoch 322/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0884 - acc: 0.9738\n",
            "Epoch 00322: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0913 - acc: 0.9731 - val_loss: 0.1000 - val_acc: 0.9717\n",
            "Epoch 323/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9746\n",
            "Epoch 00323: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0862 - acc: 0.9747 - val_loss: 0.0723 - val_acc: 0.9817\n",
            "Epoch 324/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9774\n",
            "Epoch 00324: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0775 - acc: 0.9778 - val_loss: 0.0659 - val_acc: 0.9825\n",
            "Epoch 325/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0688 - acc: 0.9816\n",
            "Epoch 00325: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0691 - acc: 0.9817 - val_loss: 0.0711 - val_acc: 0.9817\n",
            "Epoch 326/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9774\n",
            "Epoch 00326: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.0740 - acc: 0.9778 - val_loss: 0.0745 - val_acc: 0.9792\n",
            "Epoch 327/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0809 - acc: 0.9753\n",
            "Epoch 00327: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0819 - acc: 0.9747 - val_loss: 0.0603 - val_acc: 0.9817\n",
            "Epoch 328/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0876 - acc: 0.9730\n",
            "Epoch 00328: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0884 - acc: 0.9736 - val_loss: 0.0777 - val_acc: 0.9825\n",
            "Epoch 329/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0711 - acc: 0.9818\n",
            "Epoch 00329: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0708 - acc: 0.9822 - val_loss: 0.0742 - val_acc: 0.9792\n",
            "Epoch 330/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0712 - acc: 0.9791\n",
            "Epoch 00330: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0714 - acc: 0.9794 - val_loss: 0.0615 - val_acc: 0.9867\n",
            "Epoch 331/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0691 - acc: 0.9787\n",
            "Epoch 00331: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0739 - acc: 0.9764 - val_loss: 0.0768 - val_acc: 0.9800\n",
            "Epoch 332/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0793 - acc: 0.9770\n",
            "Epoch 00332: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0778 - acc: 0.9775 - val_loss: 0.0729 - val_acc: 0.9800\n",
            "Epoch 333/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9771\n",
            "Epoch 00333: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0770 - acc: 0.9772 - val_loss: 0.0667 - val_acc: 0.9833\n",
            "Epoch 334/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0821 - acc: 0.9759\n",
            "Epoch 00334: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0825 - acc: 0.9758 - val_loss: 0.0883 - val_acc: 0.9783\n",
            "Epoch 335/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0797 - acc: 0.9785\n",
            "Epoch 00335: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0797 - acc: 0.9778 - val_loss: 0.0746 - val_acc: 0.9808\n",
            "Epoch 336/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0837 - acc: 0.9732\n",
            "Epoch 00336: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0853 - acc: 0.9722 - val_loss: 0.0705 - val_acc: 0.9767\n",
            "Epoch 337/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9751\n",
            "Epoch 00337: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0736 - acc: 0.9758 - val_loss: 0.0624 - val_acc: 0.9833\n",
            "Epoch 338/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0869 - acc: 0.9748\n",
            "Epoch 00338: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0866 - acc: 0.9753 - val_loss: 0.0758 - val_acc: 0.9817\n",
            "Epoch 339/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0886 - acc: 0.9729\n",
            "Epoch 00339: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0876 - acc: 0.9736 - val_loss: 0.0792 - val_acc: 0.9808\n",
            "Epoch 340/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0866 - acc: 0.9756\n",
            "Epoch 00340: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0881 - acc: 0.9753 - val_loss: 0.0780 - val_acc: 0.9792\n",
            "Epoch 341/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0665 - acc: 0.9829\n",
            "Epoch 00341: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0687 - acc: 0.9822 - val_loss: 0.0744 - val_acc: 0.9808\n",
            "Epoch 342/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0839 - acc: 0.9737\n",
            "Epoch 00342: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0829 - acc: 0.9744 - val_loss: 0.0597 - val_acc: 0.9833\n",
            "Epoch 343/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0881 - acc: 0.9755\n",
            "Epoch 00343: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0878 - acc: 0.9758 - val_loss: 0.0663 - val_acc: 0.9817\n",
            "Epoch 344/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9697\n",
            "Epoch 00344: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0985 - acc: 0.9697 - val_loss: 0.0763 - val_acc: 0.9842\n",
            "Epoch 345/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0850 - acc: 0.9729\n",
            "Epoch 00345: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0878 - acc: 0.9717 - val_loss: 0.0718 - val_acc: 0.9808\n",
            "Epoch 346/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1010 - acc: 0.9653\n",
            "Epoch 00346: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1006 - acc: 0.9650 - val_loss: 0.0827 - val_acc: 0.9742\n",
            "Epoch 347/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1018 - acc: 0.9659\n",
            "Epoch 00347: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1047 - acc: 0.9639 - val_loss: 0.0793 - val_acc: 0.9783\n",
            "Epoch 348/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0762 - acc: 0.9774\n",
            "Epoch 00348: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0774 - acc: 0.9769 - val_loss: 0.0724 - val_acc: 0.9783\n",
            "Epoch 349/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0916 - acc: 0.9732\n",
            "Epoch 00349: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0901 - acc: 0.9731 - val_loss: 0.0690 - val_acc: 0.9808\n",
            "Epoch 350/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0869 - acc: 0.9750\n",
            "Epoch 00350: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.0859 - acc: 0.9758 - val_loss: 0.0655 - val_acc: 0.9808\n",
            "Epoch 351/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0802 - acc: 0.9765\n",
            "Epoch 00351: val_loss did not improve from 0.05934\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0817 - acc: 0.9756 - val_loss: 0.0661 - val_acc: 0.9825\n",
            "Epoch 352/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0971 - acc: 0.9694\n",
            "Epoch 00352: val_loss improved from 0.05934 to 0.05656, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.0960 - acc: 0.9694 - val_loss: 0.0566 - val_acc: 0.9858\n",
            "Epoch 353/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0890 - acc: 0.9755\n",
            "Epoch 00353: val_loss did not improve from 0.05656\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.0917 - acc: 0.9739 - val_loss: 0.0596 - val_acc: 0.9825\n",
            "Epoch 354/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0878 - acc: 0.9747\n",
            "Epoch 00354: val_loss did not improve from 0.05656\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0877 - acc: 0.9742 - val_loss: 0.0724 - val_acc: 0.9825\n",
            "Epoch 355/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1004 - acc: 0.9721\n",
            "Epoch 00355: val_loss did not improve from 0.05656\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0995 - acc: 0.9731 - val_loss: 0.0866 - val_acc: 0.9783\n",
            "Epoch 356/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1198 - acc: 0.9591\n",
            "Epoch 00356: val_loss did not improve from 0.05656\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1205 - acc: 0.9586 - val_loss: 0.1083 - val_acc: 0.9683\n",
            "Epoch 357/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1072 - acc: 0.9688\n",
            "Epoch 00357: val_loss did not improve from 0.05656\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1067 - acc: 0.9683 - val_loss: 0.0657 - val_acc: 0.9792\n",
            "Epoch 358/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0977 - acc: 0.9724\n",
            "Epoch 00358: val_loss did not improve from 0.05656\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0978 - acc: 0.9719 - val_loss: 0.0704 - val_acc: 0.9825\n",
            "Epoch 359/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9726\n",
            "Epoch 00359: val_loss did not improve from 0.05656\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0864 - acc: 0.9733 - val_loss: 0.0838 - val_acc: 0.9733\n",
            "Epoch 360/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0895 - acc: 0.9716\n",
            "Epoch 00360: val_loss did not improve from 0.05656\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0887 - acc: 0.9722 - val_loss: 0.0613 - val_acc: 0.9792\n",
            "Epoch 361/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9809\n",
            "Epoch 00361: val_loss did not improve from 0.05656\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0728 - acc: 0.9811 - val_loss: 0.0707 - val_acc: 0.9783\n",
            "Epoch 362/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0694 - acc: 0.9803\n",
            "Epoch 00362: val_loss did not improve from 0.05656\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0711 - acc: 0.9803 - val_loss: 0.1025 - val_acc: 0.9700\n",
            "Epoch 363/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0901 - acc: 0.9679\n",
            "Epoch 00363: val_loss did not improve from 0.05656\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0905 - acc: 0.9675 - val_loss: 0.0671 - val_acc: 0.9792\n",
            "Epoch 364/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9700\n",
            "Epoch 00364: val_loss did not improve from 0.05656\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0907 - acc: 0.9697 - val_loss: 0.0797 - val_acc: 0.9758\n",
            "Epoch 365/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0985 - acc: 0.9709\n",
            "Epoch 00365: val_loss did not improve from 0.05656\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0979 - acc: 0.9714 - val_loss: 0.0714 - val_acc: 0.9775\n",
            "Epoch 366/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9711\n",
            "Epoch 00366: val_loss did not improve from 0.05656\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1006 - acc: 0.9703 - val_loss: 0.0737 - val_acc: 0.9775\n",
            "Epoch 367/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0962 - acc: 0.9668\n",
            "Epoch 00367: val_loss did not improve from 0.05656\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0968 - acc: 0.9669 - val_loss: 0.0655 - val_acc: 0.9808\n",
            "Epoch 368/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0925 - acc: 0.9712\n",
            "Epoch 00368: val_loss did not improve from 0.05656\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0937 - acc: 0.9706 - val_loss: 0.1131 - val_acc: 0.9675\n",
            "Epoch 369/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0751 - acc: 0.9788\n",
            "Epoch 00369: val_loss did not improve from 0.05656\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0745 - acc: 0.9794 - val_loss: 0.0698 - val_acc: 0.9825\n",
            "Epoch 370/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9737\n",
            "Epoch 00370: val_loss improved from 0.05656 to 0.05576, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.0850 - acc: 0.9736 - val_loss: 0.0558 - val_acc: 0.9867\n",
            "Epoch 371/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0793 - acc: 0.9765\n",
            "Epoch 00371: val_loss did not improve from 0.05576\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0777 - acc: 0.9769 - val_loss: 0.0586 - val_acc: 0.9825\n",
            "Epoch 372/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9731\n",
            "Epoch 00372: val_loss did not improve from 0.05576\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0913 - acc: 0.9736 - val_loss: 0.0688 - val_acc: 0.9842\n",
            "Epoch 373/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0917 - acc: 0.9691\n",
            "Epoch 00373: val_loss did not improve from 0.05576\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0935 - acc: 0.9686 - val_loss: 0.0608 - val_acc: 0.9842\n",
            "Epoch 374/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9714\n",
            "Epoch 00374: val_loss did not improve from 0.05576\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1018 - acc: 0.9722 - val_loss: 0.0681 - val_acc: 0.9800\n",
            "Epoch 375/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9737\n",
            "Epoch 00375: val_loss did not improve from 0.05576\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0891 - acc: 0.9739 - val_loss: 0.0732 - val_acc: 0.9833\n",
            "Epoch 376/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0870 - acc: 0.9712\n",
            "Epoch 00376: val_loss did not improve from 0.05576\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0885 - acc: 0.9717 - val_loss: 0.0801 - val_acc: 0.9767\n",
            "Epoch 377/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0836 - acc: 0.9745\n",
            "Epoch 00377: val_loss did not improve from 0.05576\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0823 - acc: 0.9747 - val_loss: 0.0597 - val_acc: 0.9875\n",
            "Epoch 378/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0869 - acc: 0.9755\n",
            "Epoch 00378: val_loss did not improve from 0.05576\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0882 - acc: 0.9747 - val_loss: 0.0590 - val_acc: 0.9875\n",
            "Epoch 379/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0928 - acc: 0.9703\n",
            "Epoch 00379: val_loss did not improve from 0.05576\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0917 - acc: 0.9708 - val_loss: 0.0710 - val_acc: 0.9833\n",
            "Epoch 380/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0858 - acc: 0.9764\n",
            "Epoch 00380: val_loss did not improve from 0.05576\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0844 - acc: 0.9767 - val_loss: 0.0817 - val_acc: 0.9733\n",
            "Epoch 381/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0719 - acc: 0.9785\n",
            "Epoch 00381: val_loss did not improve from 0.05576\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0751 - acc: 0.9772 - val_loss: 0.0582 - val_acc: 0.9858\n",
            "Epoch 382/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0754 - acc: 0.9806\n",
            "Epoch 00382: val_loss did not improve from 0.05576\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.0750 - acc: 0.9811 - val_loss: 0.0584 - val_acc: 0.9867\n",
            "Epoch 383/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0719 - acc: 0.9785\n",
            "Epoch 00383: val_loss did not improve from 0.05576\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0721 - acc: 0.9789 - val_loss: 0.0597 - val_acc: 0.9825\n",
            "Epoch 384/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0898 - acc: 0.9716\n",
            "Epoch 00384: val_loss did not improve from 0.05576\n",
            "3600/3600 [==============================] - 1s 181us/sample - loss: 0.0881 - acc: 0.9722 - val_loss: 0.0818 - val_acc: 0.9742\n",
            "Epoch 385/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0834 - acc: 0.9742\n",
            "Epoch 00385: val_loss did not improve from 0.05576\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0840 - acc: 0.9739 - val_loss: 0.0626 - val_acc: 0.9792\n",
            "Epoch 386/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0716 - acc: 0.9791\n",
            "Epoch 00386: val_loss improved from 0.05576 to 0.05431, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.0714 - acc: 0.9786 - val_loss: 0.0543 - val_acc: 0.9883\n",
            "Epoch 387/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0676 - acc: 0.9845\n",
            "Epoch 00387: val_loss did not improve from 0.05431\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0690 - acc: 0.9825 - val_loss: 0.0693 - val_acc: 0.9875\n",
            "Epoch 388/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0778 - acc: 0.9753\n",
            "Epoch 00388: val_loss did not improve from 0.05431\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0767 - acc: 0.9764 - val_loss: 0.0648 - val_acc: 0.9817\n",
            "Epoch 389/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0737 - acc: 0.9803\n",
            "Epoch 00389: val_loss improved from 0.05431 to 0.04739, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.0786 - acc: 0.9789 - val_loss: 0.0474 - val_acc: 0.9883\n",
            "Epoch 390/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0748 - acc: 0.9776\n",
            "Epoch 00390: val_loss did not improve from 0.04739\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0741 - acc: 0.9783 - val_loss: 0.0642 - val_acc: 0.9833\n",
            "Epoch 391/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0686 - acc: 0.9791\n",
            "Epoch 00391: val_loss did not improve from 0.04739\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0672 - acc: 0.9806 - val_loss: 0.0576 - val_acc: 0.9875\n",
            "Epoch 392/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0610 - acc: 0.9826\n",
            "Epoch 00392: val_loss did not improve from 0.04739\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0611 - acc: 0.9822 - val_loss: 0.0559 - val_acc: 0.9875\n",
            "Epoch 393/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0922 - acc: 0.9688\n",
            "Epoch 00393: val_loss did not improve from 0.04739\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0930 - acc: 0.9683 - val_loss: 0.0767 - val_acc: 0.9792\n",
            "Epoch 394/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0776 - acc: 0.9782\n",
            "Epoch 00394: val_loss did not improve from 0.04739\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0789 - acc: 0.9775 - val_loss: 0.0716 - val_acc: 0.9817\n",
            "Epoch 395/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0803 - acc: 0.9762\n",
            "Epoch 00395: val_loss did not improve from 0.04739\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0788 - acc: 0.9767 - val_loss: 0.0558 - val_acc: 0.9883\n",
            "Epoch 396/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1009 - acc: 0.9712\n",
            "Epoch 00396: val_loss did not improve from 0.04739\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0988 - acc: 0.9719 - val_loss: 0.0606 - val_acc: 0.9833\n",
            "Epoch 397/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0681 - acc: 0.9812\n",
            "Epoch 00397: val_loss did not improve from 0.04739\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0693 - acc: 0.9803 - val_loss: 0.0570 - val_acc: 0.9850\n",
            "Epoch 398/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0823 - acc: 0.9762\n",
            "Epoch 00398: val_loss did not improve from 0.04739\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0821 - acc: 0.9764 - val_loss: 0.0626 - val_acc: 0.9817\n",
            "Epoch 399/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9823\n",
            "Epoch 00399: val_loss did not improve from 0.04739\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0753 - acc: 0.9825 - val_loss: 0.0663 - val_acc: 0.9850\n",
            "Epoch 400/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9762\n",
            "Epoch 00400: val_loss did not improve from 0.04739\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0758 - acc: 0.9769 - val_loss: 0.0668 - val_acc: 0.9808\n",
            "1200/1200 [==============================] - 0s 123us/sample - loss: 0.0668 - acc: 0.9808\n",
            "[0.06682938272754352, 0.98083335]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7dMkHvnyr03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reshape_data_to_mne_format(data):\n",
        "  mne_array = np.swapaxes(data, 0, 2) # (епохa, канал, настан). \n",
        "  mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "  mne_array = mne_array.reshape(mne_array.shape[0], 1, 8, 350)\n",
        "\n",
        "  return mne_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TESAwvOAnArb",
        "colab_type": "code",
        "outputId": "c251d877-ac5f-4c63-d584-7d5cd76578f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "# Иницијализација на променливите каде ќе бидат вчитани тест податоците\n",
        "test_data = []\n",
        "test_events = []\n",
        "test_runs_per_block = [[i for i in range(3)] for j in range(15)] # Covek, Sesija\n",
        "\n",
        "for i in range(1, 2): # Итерација низ секој испитен примерок\n",
        "  print(f\"====================== Примерок ({i}) ======================\")\n",
        "  print(\"Вчитување тест податоци од испитниот примерок \" + str(i) + \"...\")\n",
        "  \n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_test_data = np.empty(0)\n",
        "  temp_test_events = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_test_set = 'S' + format(j, '02') + '/Test'\n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testData.mat\"\n",
        "    temp = loadmat(full_path)['testData']\n",
        "    if temp_test_data.size != 0:\n",
        "      temp_test_data = np.concatenate((temp_test_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_test_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_test_events.size != 0:\n",
        "        temp_test_events = np.append(temp_test_events, temp)\n",
        "      else:\n",
        "        temp_test_events = np.array(temp)\n",
        "\n",
        "    # Вчитување на бројот на runs \n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/runs_per_block.txt\"\n",
        "    with open(full_path, \"r\") as runs_per_block:\n",
        "      test_runs_per_block[i-1][j-1] = int(runs_per_block.read())\n",
        "\n",
        "    print(\"\\t - Тест податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "  # Зачувај ги тест податоците вчитани од испитниот примерок во низа\n",
        "  test_data.append(temp_test_data)\n",
        "  test_events.append(temp_test_events)\n",
        "  print(\"Тест податоците од испитниот примерок \" + str(i) + \" се вчитани.\\n\")\n",
        "\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_data: \" + str(test_data[i-1].shape)) # test_data to predict\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_events: \" + str(len(test_events[i-1]))) # test_events\n",
        "  for j in range (1,4):\n",
        "    print(\"SBJ\" + str(format(i-1, '02')) + \" / S\" + str(format(j-1, '02')) + \"| Runs per block: \" + str(test_runs_per_block[i-1][j-1])) # runs per block in SJB01, SJ00 \n",
        "\n",
        "  to_predict_data = reshape_data_to_mne_format(test_data[i-1])\n",
        "  predictions = model1.predict(to_predict_data)\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Predictions: \" + str(len(predictions)))\n",
        "  # np.savetxt(\"predictions.csv\", predictions, delimiter=\",\")\n",
        "\n",
        "\n",
        "  # ========= FALI USTE DA SE ISPARSIRA PREDICTIONOT... NE E SREDEN OVOJ KOD DOLE =======\n",
        "\n",
        "  int_pred = np.argmax(predictions, axis=1)\n",
        "  int_ytest = np.argmax(y_test, axis=1)\n",
        "\n",
        "  session_start = 0\n",
        "  start_prediction_index = 0\n",
        "  end_prediction_index = 0\n",
        "  for session in range(0, 3):\n",
        "    print(f\"============== Сесија ({session}) ==============\")\n",
        "    for block in range(0, 50):    \n",
        "      events_per_block = test_runs_per_block[i-1][session]\n",
        "\n",
        "      start_prediction_index = session_start + (block*events_per_block)*8\n",
        "      end_prediction_index = session_start + ((block+1)*events_per_block)*8\n",
        "\n",
        "      block_prediction = int_pred[start_prediction_index:end_prediction_index]\n",
        "      prediction = np.bincount(block_prediction).argmax()\n",
        "      df.iat[session,block+2] = prediction +1\n",
        "      # UNCOMMENT ZA PODOBAR PRIKAZ :)\n",
        "      # print(f\"Session {session} | Block: {block} | Prediction: {prediction} | Address: {end_prediction_index}\")\n",
        "\n",
        "      print(str(prediction+1) + \",\", end=\"\")\n",
        "    session_start = end_prediction_index\n",
        "    print(\"\")\n",
        "  print(\"Stigna li do kraj: \" + str(session_start == len(predictions)))\n",
        "  print(f\"====================== Примерок ({i}) ======================\\n\\n\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Примерок (1) ======================\n",
            "Вчитување тест податоци од испитниот примерок 1...\n",
            "\t - Тест податоците од сесија 1 се вчитани.\n",
            "\t - Тест податоците од сесија 2 се вчитани.\n",
            "\t - Тест податоците од сесија 3 се вчитани.\n",
            "Тест податоците од испитниот примерок 1 се вчитани.\n",
            "\n",
            "SBJ00| Test_data: (8, 350, 8000)\n",
            "SBJ00| Test_events: 8000\n",
            "SBJ00 / S00| Runs per block: 6\n",
            "SBJ00 / S01| Runs per block: 7\n",
            "SBJ00 / S02| Runs per block: 7\n",
            "SBJ00| Predictions: 8000\n",
            "============== Сесија (0) ==============\n",
            "6,6,3,6,6,3,6,6,6,6,6,6,7,6,5,6,4,3,3,7,7,8,3,3,7,6,6,4,8,6,7,6,6,4,7,7,7,7,5,3,3,2,6,4,8,3,4,5,5,7,\n",
            "============== Сесија (1) ==============\n",
            "6,3,2,1,2,1,3,6,2,2,6,2,7,1,2,3,7,1,3,1,6,6,2,2,6,1,6,6,1,2,2,3,2,6,2,2,2,5,6,2,6,6,6,2,2,2,3,6,6,2,\n",
            "============== Сесија (2) ==============\n",
            "3,3,3,3,3,3,3,3,7,3,7,3,3,3,6,6,6,6,3,3,3,3,7,3,6,3,3,6,3,3,3,3,6,6,6,6,6,6,6,3,3,6,3,3,6,3,6,7,1,6,\n",
            "Stigna li do kraj: True\n",
            "====================== Примерок (1) ======================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEXmiytw7LWc",
        "colab_type": "code",
        "outputId": "56f262f1-27cb-4e85-cc34-5927051cddef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUBJECT</th>\n",
              "      <th>SESSION</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>Unnamed: 52</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    SUBJECT  SESSION  1  2  3  4  5  6  7  ... 43 44 45 46 47 48 49 50 Unnamed: 52\n",
              "0         1        1  6  6  3  6  6  3  6  ...  6  4  8  3  4  5  5  7         NaN\n",
              "1         1        2  6  3  2  1  2  1  3  ...  6  2  2  2  3  6  6  2         NaN\n",
              "2         1        3  3  3  3  3  3  3  3  ...  3  3  6  3  6  7  1  6         NaN\n",
              "3         2        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "4         2        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "5         2        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "6         3        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "7         3        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "8         3        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "9         4        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "10        4        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "11        4        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "12        5        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "13        5        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "14        5        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "15        6        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "16        6        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "17        6        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "18        7        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "19        7        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "20        7        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "21        8        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "22        8        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "23        8        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "24        9        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "25        9        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "26        9        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "27       10        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "28       10        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "29       10        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "30       11        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "31       11        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "32       11        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "33       12        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "34       12        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "35       12        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "36       13        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "37       13        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "38       13        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "39       14        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "40       14        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "41       14        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "42       15        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "43       15        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "44       15        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "\n",
              "[45 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlwSHVlAhy77",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a9ad2806-abe7-411c-e0a2-50a7256ac6f3"
      },
      "source": [
        "for i in range(2, 3): # Итерација низ секој испитен примерок\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_data = np.empty(0)\n",
        "  temp_labels = np.empty(0)\n",
        "  temp_events = np.empty(0)\n",
        "  temp_targets = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainData.mat\"\n",
        "    temp = loadmat(full_path)['trainData']\n",
        "    if temp_data.size != 0:\n",
        "      temp_data = np.concatenate((temp_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на label-ите\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainLabels.txt\"\n",
        "    with open(full_path, \"r\") as file_labels:\n",
        "      temp = file_labels.read().splitlines()\n",
        "      temp = np.repeat(temp, 8*10)\n",
        "      if temp_labels.size != 0:\n",
        "        temp_labels = np.concatenate((temp_labels, temp))\n",
        "      else:\n",
        "        temp_labels = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_events.size != 0:\n",
        "        temp_events = np.append(temp_events, temp)\n",
        "      else:\n",
        "        temp_events = np.array(temp)\n",
        "      \n",
        "\n",
        "    # Вчитување на редоследот на објекти кои се target\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainTargets.txt\"\n",
        "    with open(full_path, \"r\") as file_targets:\n",
        "      temp = file_targets.read().splitlines()\n",
        "      if temp_targets.size != 0:\n",
        "        temp_targets = np.concatenate((temp_targets, temp))\n",
        "      else:\n",
        "        temp_targets = np.array(temp)\n",
        "    print(\"\\t - Податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "\n",
        "  for j in range(4, 8): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "      \n",
        "  # Зачувај ги податоците вчитани од испитниот примерок во низа\n",
        "  data.append(temp_data)\n",
        "  labels.append(temp_labels)\n",
        "  events.append(temp_events)\n",
        "  targets.append(temp_targets)\n",
        "\n",
        "  \n",
        "  print(\"Податоците од испитниот примерок \" + str(i) + \" се вчитани.\")\n",
        "\n",
        "\n",
        "  #data = target_events_data_scaled\n",
        "  mne_array = np.swapaxes(data[i-1], 0, 2) # (епохa, канал, настан). \n",
        "  mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "  mne_array = mne_array.reshape(mne_array.shape[0],1, 8,350)\n",
        "  print(mne_array.shape)\n",
        "\n",
        "  events_arr = events[i-1].astype(np.int)\n",
        "  labels_arr = labels[i-1].astype(np.int)\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(mne_array, labels_arr-1, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "  model2 = DeepConvNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "  model2.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = Adam(0.0009))\n",
        "  checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                                verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "  #clf = RandomForestClassifier(max_depth=5)\n",
        "  #clf.fit(X_train, y_train)\n",
        "  #score = clf.score(X_test, y_test)\n",
        "  # print(score)\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  num_batch_size=100\n",
        "  num_epochs=400\n",
        "  model2.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "            validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "  score = model2.evaluate(X_test, y_test, verbose=1)\n",
        "  print(score)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t - Податоците од сесија 1 се вчитани.\n",
            "\t - Податоците од сесија 2 се вчитани.\n",
            "\t - Податоците од сесија 3 се вчитани.\n",
            "Податоците од испитниот примерок 2 се вчитани.\n",
            "(4800, 1, 8, 350)\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Train on 3600 samples, validate on 1200 samples\n",
            "Epoch 1/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.3099 - acc: 0.1527\n",
            "Epoch 00001: val_loss improved from inf to 2.33148, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 4s 1ms/sample - loss: 2.3074 - acc: 0.1542 - val_loss: 2.3315 - val_acc: 0.1558\n",
            "Epoch 2/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.1672 - acc: 0.1803\n",
            "Epoch 00002: val_loss improved from 2.33148 to 2.07297, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 2.1648 - acc: 0.1867 - val_loss: 2.0730 - val_acc: 0.2333\n",
            "Epoch 3/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.9924 - acc: 0.2409\n",
            "Epoch 00003: val_loss did not improve from 2.07297\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 1.9899 - acc: 0.2411 - val_loss: 2.3414 - val_acc: 0.2342\n",
            "Epoch 4/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.9111 - acc: 0.2536\n",
            "Epoch 00004: val_loss improved from 2.07297 to 2.02515, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 1.9036 - acc: 0.2583 - val_loss: 2.0252 - val_acc: 0.2633\n",
            "Epoch 5/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.8365 - acc: 0.2806\n",
            "Epoch 00005: val_loss improved from 2.02515 to 1.87717, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 1.8360 - acc: 0.2842 - val_loss: 1.8772 - val_acc: 0.2958\n",
            "Epoch 6/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.8010 - acc: 0.2931\n",
            "Epoch 00006: val_loss improved from 1.87717 to 1.78579, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 1.8014 - acc: 0.2939 - val_loss: 1.7858 - val_acc: 0.2817\n",
            "Epoch 7/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.7517 - acc: 0.2994\n",
            "Epoch 00007: val_loss did not improve from 1.78579\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.7519 - acc: 0.3017 - val_loss: 1.8393 - val_acc: 0.2350\n",
            "Epoch 8/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.7684 - acc: 0.3000\n",
            "Epoch 00008: val_loss improved from 1.78579 to 1.78553, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 1.7694 - acc: 0.2986 - val_loss: 1.7855 - val_acc: 0.2867\n",
            "Epoch 9/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.7319 - acc: 0.3155\n",
            "Epoch 00009: val_loss did not improve from 1.78553\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 1.7342 - acc: 0.3156 - val_loss: 1.8060 - val_acc: 0.2758\n",
            "Epoch 10/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.7206 - acc: 0.3232\n",
            "Epoch 00010: val_loss did not improve from 1.78553\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.7220 - acc: 0.3244 - val_loss: 1.7875 - val_acc: 0.3158\n",
            "Epoch 11/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.6951 - acc: 0.3300\n",
            "Epoch 00011: val_loss did not improve from 1.78553\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.6976 - acc: 0.3289 - val_loss: 1.9866 - val_acc: 0.2925\n",
            "Epoch 12/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.7153 - acc: 0.3297\n",
            "Epoch 00012: val_loss did not improve from 1.78553\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 1.7151 - acc: 0.3300 - val_loss: 1.8431 - val_acc: 0.2942\n",
            "Epoch 13/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.7115 - acc: 0.3300\n",
            "Epoch 00013: val_loss did not improve from 1.78553\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 1.7150 - acc: 0.3300 - val_loss: 1.8511 - val_acc: 0.2717\n",
            "Epoch 14/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.7009 - acc: 0.3347\n",
            "Epoch 00014: val_loss did not improve from 1.78553\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.6969 - acc: 0.3339 - val_loss: 2.0019 - val_acc: 0.2733\n",
            "Epoch 15/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.7006 - acc: 0.3286\n",
            "Epoch 00015: val_loss did not improve from 1.78553\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.7000 - acc: 0.3278 - val_loss: 2.0728 - val_acc: 0.2733\n",
            "Epoch 16/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.6405 - acc: 0.3560\n",
            "Epoch 00016: val_loss did not improve from 1.78553\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.6401 - acc: 0.3567 - val_loss: 1.9604 - val_acc: 0.2783\n",
            "Epoch 17/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.6250 - acc: 0.3636\n",
            "Epoch 00017: val_loss did not improve from 1.78553\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.6238 - acc: 0.3622 - val_loss: 1.9935 - val_acc: 0.3033\n",
            "Epoch 18/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.6538 - acc: 0.3424\n",
            "Epoch 00018: val_loss did not improve from 1.78553\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 1.6574 - acc: 0.3411 - val_loss: 1.8710 - val_acc: 0.2875\n",
            "Epoch 19/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.5897 - acc: 0.3665\n",
            "Epoch 00019: val_loss improved from 1.78553 to 1.75944, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 1.5910 - acc: 0.3656 - val_loss: 1.7594 - val_acc: 0.2858\n",
            "Epoch 20/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.5620 - acc: 0.3897\n",
            "Epoch 00020: val_loss did not improve from 1.75944\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 1.5636 - acc: 0.3869 - val_loss: 1.7895 - val_acc: 0.2875\n",
            "Epoch 21/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.5214 - acc: 0.4077\n",
            "Epoch 00021: val_loss did not improve from 1.75944\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.5251 - acc: 0.4053 - val_loss: 1.8044 - val_acc: 0.3233\n",
            "Epoch 22/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.5588 - acc: 0.3860\n",
            "Epoch 00022: val_loss did not improve from 1.75944\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 1.5597 - acc: 0.3850 - val_loss: 1.8584 - val_acc: 0.3242\n",
            "Epoch 23/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.5043 - acc: 0.4053\n",
            "Epoch 00023: val_loss did not improve from 1.75944\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.5069 - acc: 0.4033 - val_loss: 1.8751 - val_acc: 0.3008\n",
            "Epoch 24/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.4499 - acc: 0.4345\n",
            "Epoch 00024: val_loss improved from 1.75944 to 1.67435, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 1.4546 - acc: 0.4317 - val_loss: 1.6744 - val_acc: 0.3308\n",
            "Epoch 25/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.4394 - acc: 0.4329\n",
            "Epoch 00025: val_loss did not improve from 1.67435\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 1.4408 - acc: 0.4314 - val_loss: 1.7975 - val_acc: 0.3292\n",
            "Epoch 26/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.4141 - acc: 0.4520\n",
            "Epoch 00026: val_loss improved from 1.67435 to 1.60313, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 1.4227 - acc: 0.4503 - val_loss: 1.6031 - val_acc: 0.3775\n",
            "Epoch 27/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.3837 - acc: 0.4588\n",
            "Epoch 00027: val_loss did not improve from 1.60313\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.3922 - acc: 0.4539 - val_loss: 1.7816 - val_acc: 0.3450\n",
            "Epoch 28/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.3483 - acc: 0.4876\n",
            "Epoch 00028: val_loss did not improve from 1.60313\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.3558 - acc: 0.4861 - val_loss: 1.6260 - val_acc: 0.3633\n",
            "Epoch 29/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.3266 - acc: 0.4885\n",
            "Epoch 00029: val_loss improved from 1.60313 to 1.53250, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 1.3259 - acc: 0.4900 - val_loss: 1.5325 - val_acc: 0.3975\n",
            "Epoch 30/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.3159 - acc: 0.4963\n",
            "Epoch 00030: val_loss did not improve from 1.53250\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 1.3166 - acc: 0.4969 - val_loss: 1.6111 - val_acc: 0.3817\n",
            "Epoch 31/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.2733 - acc: 0.5157\n",
            "Epoch 00031: val_loss did not improve from 1.53250\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.2785 - acc: 0.5139 - val_loss: 1.7157 - val_acc: 0.3725\n",
            "Epoch 32/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.2345 - acc: 0.5234\n",
            "Epoch 00032: val_loss did not improve from 1.53250\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 1.2317 - acc: 0.5267 - val_loss: 1.6232 - val_acc: 0.4158\n",
            "Epoch 33/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.1923 - acc: 0.5512\n",
            "Epoch 00033: val_loss improved from 1.53250 to 1.45044, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 1.2004 - acc: 0.5481 - val_loss: 1.4504 - val_acc: 0.4450\n",
            "Epoch 34/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.1584 - acc: 0.5721\n",
            "Epoch 00034: val_loss did not improve from 1.45044\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.1624 - acc: 0.5714 - val_loss: 1.6275 - val_acc: 0.4033\n",
            "Epoch 35/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1663 - acc: 0.5542\n",
            "Epoch 00035: val_loss improved from 1.45044 to 1.44084, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 1.1705 - acc: 0.5556 - val_loss: 1.4408 - val_acc: 0.4575\n",
            "Epoch 36/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.1409 - acc: 0.5653\n",
            "Epoch 00036: val_loss did not improve from 1.44084\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 1.1410 - acc: 0.5644 - val_loss: 1.4466 - val_acc: 0.4717\n",
            "Epoch 37/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0956 - acc: 0.5915\n",
            "Epoch 00037: val_loss improved from 1.44084 to 1.40030, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 1.0985 - acc: 0.5914 - val_loss: 1.4003 - val_acc: 0.4842\n",
            "Epoch 38/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0691 - acc: 0.6042\n",
            "Epoch 00038: val_loss improved from 1.40030 to 1.38391, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 1.0656 - acc: 0.6044 - val_loss: 1.3839 - val_acc: 0.4858\n",
            "Epoch 39/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.0320 - acc: 0.6183\n",
            "Epoch 00039: val_loss improved from 1.38391 to 1.26939, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 1.0332 - acc: 0.6192 - val_loss: 1.2694 - val_acc: 0.5067\n",
            "Epoch 40/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0094 - acc: 0.6203\n",
            "Epoch 00040: val_loss did not improve from 1.26939\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 1.0182 - acc: 0.6167 - val_loss: 1.3247 - val_acc: 0.5158\n",
            "Epoch 41/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9965 - acc: 0.6363\n",
            "Epoch 00041: val_loss did not improve from 1.26939\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.9981 - acc: 0.6356 - val_loss: 1.2963 - val_acc: 0.5258\n",
            "Epoch 42/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9571 - acc: 0.6524\n",
            "Epoch 00042: val_loss did not improve from 1.26939\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.9574 - acc: 0.6533 - val_loss: 1.3254 - val_acc: 0.4942\n",
            "Epoch 43/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9406 - acc: 0.6503\n",
            "Epoch 00043: val_loss did not improve from 1.26939\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.9379 - acc: 0.6514 - val_loss: 1.2758 - val_acc: 0.5242\n",
            "Epoch 44/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9206 - acc: 0.6623\n",
            "Epoch 00044: val_loss did not improve from 1.26939\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.9197 - acc: 0.6608 - val_loss: 1.2777 - val_acc: 0.5175\n",
            "Epoch 45/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8559 - acc: 0.6906\n",
            "Epoch 00045: val_loss improved from 1.26939 to 1.16209, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.8574 - acc: 0.6867 - val_loss: 1.1621 - val_acc: 0.5475\n",
            "Epoch 46/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8803 - acc: 0.6762\n",
            "Epoch 00046: val_loss improved from 1.16209 to 1.11744, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.8812 - acc: 0.6753 - val_loss: 1.1174 - val_acc: 0.5633\n",
            "Epoch 47/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8266 - acc: 0.7112\n",
            "Epoch 00047: val_loss did not improve from 1.11744\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.8268 - acc: 0.7114 - val_loss: 1.1948 - val_acc: 0.5367\n",
            "Epoch 48/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8115 - acc: 0.7109\n",
            "Epoch 00048: val_loss improved from 1.11744 to 1.03388, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.8140 - acc: 0.7097 - val_loss: 1.0339 - val_acc: 0.6217\n",
            "Epoch 49/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7778 - acc: 0.7206\n",
            "Epoch 00049: val_loss did not improve from 1.03388\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.7734 - acc: 0.7239 - val_loss: 1.0896 - val_acc: 0.5825\n",
            "Epoch 50/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7884 - acc: 0.7212\n",
            "Epoch 00050: val_loss did not improve from 1.03388\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.7880 - acc: 0.7206 - val_loss: 1.0637 - val_acc: 0.5883\n",
            "Epoch 51/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7236 - acc: 0.7591\n",
            "Epoch 00051: val_loss improved from 1.03388 to 0.95888, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.7210 - acc: 0.7594 - val_loss: 0.9589 - val_acc: 0.6333\n",
            "Epoch 52/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7429 - acc: 0.7334\n",
            "Epoch 00052: val_loss did not improve from 0.95888\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.7475 - acc: 0.7322 - val_loss: 1.0095 - val_acc: 0.6167\n",
            "Epoch 53/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6971 - acc: 0.7589\n",
            "Epoch 00053: val_loss did not improve from 0.95888\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.6958 - acc: 0.7583 - val_loss: 0.9777 - val_acc: 0.6183\n",
            "Epoch 54/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.6834 - acc: 0.7670\n",
            "Epoch 00054: val_loss improved from 0.95888 to 0.91890, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.6894 - acc: 0.7636 - val_loss: 0.9189 - val_acc: 0.6533\n",
            "Epoch 55/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6837 - acc: 0.7643\n",
            "Epoch 00055: val_loss did not improve from 0.91890\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.6850 - acc: 0.7639 - val_loss: 0.9489 - val_acc: 0.6467\n",
            "Epoch 56/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6265 - acc: 0.7865\n",
            "Epoch 00056: val_loss improved from 0.91890 to 0.85135, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.6251 - acc: 0.7883 - val_loss: 0.8513 - val_acc: 0.6783\n",
            "Epoch 57/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.6118 - acc: 0.7937\n",
            "Epoch 00057: val_loss did not improve from 0.85135\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.6197 - acc: 0.7883 - val_loss: 0.8651 - val_acc: 0.6750\n",
            "Epoch 58/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5917 - acc: 0.7980\n",
            "Epoch 00058: val_loss improved from 0.85135 to 0.84003, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.5933 - acc: 0.7969 - val_loss: 0.8400 - val_acc: 0.6925\n",
            "Epoch 59/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5777 - acc: 0.8112\n",
            "Epoch 00059: val_loss improved from 0.84003 to 0.83774, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.5803 - acc: 0.8097 - val_loss: 0.8377 - val_acc: 0.6933\n",
            "Epoch 60/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5571 - acc: 0.8152\n",
            "Epoch 00060: val_loss did not improve from 0.83774\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.5627 - acc: 0.8139 - val_loss: 0.9466 - val_acc: 0.6375\n",
            "Epoch 61/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5445 - acc: 0.8200\n",
            "Epoch 00061: val_loss improved from 0.83774 to 0.76460, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.5506 - acc: 0.8181 - val_loss: 0.7646 - val_acc: 0.7142\n",
            "Epoch 62/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5567 - acc: 0.8135\n",
            "Epoch 00062: val_loss did not improve from 0.76460\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.5584 - acc: 0.8119 - val_loss: 0.7726 - val_acc: 0.7025\n",
            "Epoch 63/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.5496 - acc: 0.8228\n",
            "Epoch 00063: val_loss improved from 0.76460 to 0.70441, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.5422 - acc: 0.8278 - val_loss: 0.7044 - val_acc: 0.7500\n",
            "Epoch 64/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.5129 - acc: 0.8247\n",
            "Epoch 00064: val_loss improved from 0.70441 to 0.69208, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.5079 - acc: 0.8294 - val_loss: 0.6921 - val_acc: 0.7558\n",
            "Epoch 65/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5172 - acc: 0.8320\n",
            "Epoch 00065: val_loss did not improve from 0.69208\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.5187 - acc: 0.8308 - val_loss: 0.7358 - val_acc: 0.7392\n",
            "Epoch 66/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.4817 - acc: 0.8478\n",
            "Epoch 00066: val_loss did not improve from 0.69208\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.4830 - acc: 0.8475 - val_loss: 0.7624 - val_acc: 0.7133\n",
            "Epoch 67/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4801 - acc: 0.8512\n",
            "Epoch 00067: val_loss improved from 0.69208 to 0.68824, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.4832 - acc: 0.8494 - val_loss: 0.6882 - val_acc: 0.7525\n",
            "Epoch 68/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4661 - acc: 0.8539\n",
            "Epoch 00068: val_loss did not improve from 0.68824\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.4660 - acc: 0.8528 - val_loss: 0.6897 - val_acc: 0.7475\n",
            "Epoch 69/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4407 - acc: 0.8579\n",
            "Epoch 00069: val_loss improved from 0.68824 to 0.61286, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.4410 - acc: 0.8556 - val_loss: 0.6129 - val_acc: 0.7817\n",
            "Epoch 70/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4569 - acc: 0.8506\n",
            "Epoch 00070: val_loss did not improve from 0.61286\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.4562 - acc: 0.8528 - val_loss: 0.6562 - val_acc: 0.7500\n",
            "Epoch 71/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4199 - acc: 0.8676\n",
            "Epoch 00071: val_loss did not improve from 0.61286\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.4224 - acc: 0.8664 - val_loss: 0.6181 - val_acc: 0.7892\n",
            "Epoch 72/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4167 - acc: 0.8642\n",
            "Epoch 00072: val_loss did not improve from 0.61286\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.4202 - acc: 0.8631 - val_loss: 0.6903 - val_acc: 0.7367\n",
            "Epoch 73/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4165 - acc: 0.8756\n",
            "Epoch 00073: val_loss did not improve from 0.61286\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.4166 - acc: 0.8747 - val_loss: 0.6377 - val_acc: 0.7567\n",
            "Epoch 74/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4197 - acc: 0.8626\n",
            "Epoch 00074: val_loss improved from 0.61286 to 0.55409, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.4193 - acc: 0.8628 - val_loss: 0.5541 - val_acc: 0.8133\n",
            "Epoch 75/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4090 - acc: 0.8680\n",
            "Epoch 00075: val_loss did not improve from 0.55409\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.4077 - acc: 0.8689 - val_loss: 0.5949 - val_acc: 0.7842\n",
            "Epoch 76/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3981 - acc: 0.8785\n",
            "Epoch 00076: val_loss improved from 0.55409 to 0.54105, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.4044 - acc: 0.8744 - val_loss: 0.5410 - val_acc: 0.8217\n",
            "Epoch 77/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3848 - acc: 0.8724\n",
            "Epoch 00077: val_loss did not improve from 0.54105\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.3861 - acc: 0.8708 - val_loss: 0.5570 - val_acc: 0.8017\n",
            "Epoch 78/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3711 - acc: 0.8909\n",
            "Epoch 00078: val_loss improved from 0.54105 to 0.51804, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.3707 - acc: 0.8897 - val_loss: 0.5180 - val_acc: 0.8283\n",
            "Epoch 79/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3755 - acc: 0.8828\n",
            "Epoch 00079: val_loss did not improve from 0.51804\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.3749 - acc: 0.8822 - val_loss: 0.5265 - val_acc: 0.8367\n",
            "Epoch 80/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3380 - acc: 0.9036\n",
            "Epoch 00080: val_loss improved from 0.51804 to 0.51483, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.3435 - acc: 0.9014 - val_loss: 0.5148 - val_acc: 0.8217\n",
            "Epoch 81/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3487 - acc: 0.8906\n",
            "Epoch 00081: val_loss improved from 0.51483 to 0.49645, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.3521 - acc: 0.8883 - val_loss: 0.4965 - val_acc: 0.8350\n",
            "Epoch 82/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3467 - acc: 0.8940\n",
            "Epoch 00082: val_loss did not improve from 0.49645\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.3469 - acc: 0.8942 - val_loss: 0.5014 - val_acc: 0.8208\n",
            "Epoch 83/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3206 - acc: 0.9034\n",
            "Epoch 00083: val_loss improved from 0.49645 to 0.46596, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.3316 - acc: 0.8981 - val_loss: 0.4660 - val_acc: 0.8517\n",
            "Epoch 84/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3315 - acc: 0.8986\n",
            "Epoch 00084: val_loss did not improve from 0.46596\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.3336 - acc: 0.8978 - val_loss: 0.5099 - val_acc: 0.8392\n",
            "Epoch 85/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3217 - acc: 0.8997\n",
            "Epoch 00085: val_loss did not improve from 0.46596\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.3216 - acc: 0.8994 - val_loss: 0.4873 - val_acc: 0.8283\n",
            "Epoch 86/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3194 - acc: 0.9024\n",
            "Epoch 00086: val_loss improved from 0.46596 to 0.46272, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.3204 - acc: 0.9025 - val_loss: 0.4627 - val_acc: 0.8525\n",
            "Epoch 87/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3130 - acc: 0.9036\n",
            "Epoch 00087: val_loss did not improve from 0.46272\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.3154 - acc: 0.9025 - val_loss: 0.4706 - val_acc: 0.8433\n",
            "Epoch 88/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3071 - acc: 0.9017\n",
            "Epoch 00088: val_loss improved from 0.46272 to 0.45955, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.3064 - acc: 0.9017 - val_loss: 0.4595 - val_acc: 0.8350\n",
            "Epoch 89/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3105 - acc: 0.9024\n",
            "Epoch 00089: val_loss did not improve from 0.45955\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.3075 - acc: 0.9039 - val_loss: 0.4831 - val_acc: 0.8450\n",
            "Epoch 90/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2983 - acc: 0.9091\n",
            "Epoch 00090: val_loss improved from 0.45955 to 0.41760, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.3010 - acc: 0.9072 - val_loss: 0.4176 - val_acc: 0.8742\n",
            "Epoch 91/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2802 - acc: 0.9200\n",
            "Epoch 00091: val_loss improved from 0.41760 to 0.37126, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.2836 - acc: 0.9175 - val_loss: 0.3713 - val_acc: 0.8908\n",
            "Epoch 92/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2831 - acc: 0.9160\n",
            "Epoch 00092: val_loss did not improve from 0.37126\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2861 - acc: 0.9147 - val_loss: 0.3893 - val_acc: 0.8742\n",
            "Epoch 93/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2862 - acc: 0.9141\n",
            "Epoch 00093: val_loss did not improve from 0.37126\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2814 - acc: 0.9161 - val_loss: 0.3960 - val_acc: 0.8767\n",
            "Epoch 94/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.9066\n",
            "Epoch 00094: val_loss did not improve from 0.37126\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2933 - acc: 0.9072 - val_loss: 0.4029 - val_acc: 0.8592\n",
            "Epoch 95/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.9143\n",
            "Epoch 00095: val_loss did not improve from 0.37126\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2861 - acc: 0.9142 - val_loss: 0.3722 - val_acc: 0.8825\n",
            "Epoch 96/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2672 - acc: 0.9206\n",
            "Epoch 00096: val_loss improved from 0.37126 to 0.34680, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.2664 - acc: 0.9208 - val_loss: 0.3468 - val_acc: 0.8883\n",
            "Epoch 97/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2553 - acc: 0.9275\n",
            "Epoch 00097: val_loss did not improve from 0.34680\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2618 - acc: 0.9250 - val_loss: 0.3947 - val_acc: 0.8683\n",
            "Epoch 98/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2653 - acc: 0.9143\n",
            "Epoch 00098: val_loss did not improve from 0.34680\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2674 - acc: 0.9128 - val_loss: 0.3693 - val_acc: 0.8883\n",
            "Epoch 99/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2553 - acc: 0.9234\n",
            "Epoch 00099: val_loss did not improve from 0.34680\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2567 - acc: 0.9236 - val_loss: 0.3505 - val_acc: 0.8950\n",
            "Epoch 100/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2353 - acc: 0.9312\n",
            "Epoch 00100: val_loss improved from 0.34680 to 0.33579, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.2388 - acc: 0.9286 - val_loss: 0.3358 - val_acc: 0.8950\n",
            "Epoch 101/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2383 - acc: 0.9271\n",
            "Epoch 00101: val_loss did not improve from 0.33579\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2407 - acc: 0.9258 - val_loss: 0.3605 - val_acc: 0.8883\n",
            "Epoch 102/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2546 - acc: 0.9241\n",
            "Epoch 00102: val_loss improved from 0.33579 to 0.31404, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.2533 - acc: 0.9247 - val_loss: 0.3140 - val_acc: 0.9083\n",
            "Epoch 103/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2465 - acc: 0.9281\n",
            "Epoch 00103: val_loss did not improve from 0.31404\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2472 - acc: 0.9278 - val_loss: 0.3191 - val_acc: 0.9025\n",
            "Epoch 104/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2335 - acc: 0.9312\n",
            "Epoch 00104: val_loss did not improve from 0.31404\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.2372 - acc: 0.9292 - val_loss: 0.3189 - val_acc: 0.9042\n",
            "Epoch 105/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2414 - acc: 0.9279\n",
            "Epoch 00105: val_loss did not improve from 0.31404\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2454 - acc: 0.9250 - val_loss: 0.3727 - val_acc: 0.8800\n",
            "Epoch 106/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9317\n",
            "Epoch 00106: val_loss did not improve from 0.31404\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2399 - acc: 0.9306 - val_loss: 0.3329 - val_acc: 0.9067\n",
            "Epoch 107/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2308 - acc: 0.9334\n",
            "Epoch 00107: val_loss improved from 0.31404 to 0.28699, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.2311 - acc: 0.9333 - val_loss: 0.2870 - val_acc: 0.9267\n",
            "Epoch 108/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9397\n",
            "Epoch 00108: val_loss did not improve from 0.28699\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2167 - acc: 0.9392 - val_loss: 0.3214 - val_acc: 0.9017\n",
            "Epoch 109/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2009 - acc: 0.9450\n",
            "Epoch 00109: val_loss did not improve from 0.28699\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2010 - acc: 0.9450 - val_loss: 0.3099 - val_acc: 0.9008\n",
            "Epoch 110/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2170 - acc: 0.9371\n",
            "Epoch 00110: val_loss improved from 0.28699 to 0.27563, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.2170 - acc: 0.9364 - val_loss: 0.2756 - val_acc: 0.9250\n",
            "Epoch 111/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2165 - acc: 0.9348\n",
            "Epoch 00111: val_loss did not improve from 0.27563\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2217 - acc: 0.9314 - val_loss: 0.3458 - val_acc: 0.8883\n",
            "Epoch 112/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1995 - acc: 0.9400\n",
            "Epoch 00112: val_loss did not improve from 0.27563\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2054 - acc: 0.9383 - val_loss: 0.3030 - val_acc: 0.9133\n",
            "Epoch 113/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2079 - acc: 0.9432\n",
            "Epoch 00113: val_loss improved from 0.27563 to 0.27257, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.2109 - acc: 0.9422 - val_loss: 0.2726 - val_acc: 0.9200\n",
            "Epoch 114/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2013 - acc: 0.9412\n",
            "Epoch 00114: val_loss did not improve from 0.27257\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2046 - acc: 0.9397 - val_loss: 0.2886 - val_acc: 0.9183\n",
            "Epoch 115/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2133 - acc: 0.9348\n",
            "Epoch 00115: val_loss did not improve from 0.27257\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2164 - acc: 0.9325 - val_loss: 0.2945 - val_acc: 0.9192\n",
            "Epoch 116/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2030 - acc: 0.9421\n",
            "Epoch 00116: val_loss did not improve from 0.27257\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2012 - acc: 0.9433 - val_loss: 0.2819 - val_acc: 0.9175\n",
            "Epoch 117/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1959 - acc: 0.9415\n",
            "Epoch 00117: val_loss improved from 0.27257 to 0.27234, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1965 - acc: 0.9422 - val_loss: 0.2723 - val_acc: 0.9142\n",
            "Epoch 118/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1889 - acc: 0.9462\n",
            "Epoch 00118: val_loss improved from 0.27234 to 0.26620, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.1903 - acc: 0.9453 - val_loss: 0.2662 - val_acc: 0.9258\n",
            "Epoch 119/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1983 - acc: 0.9455\n",
            "Epoch 00119: val_loss improved from 0.26620 to 0.26197, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1992 - acc: 0.9447 - val_loss: 0.2620 - val_acc: 0.9242\n",
            "Epoch 120/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1879 - acc: 0.9442\n",
            "Epoch 00120: val_loss did not improve from 0.26197\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1883 - acc: 0.9439 - val_loss: 0.2659 - val_acc: 0.9167\n",
            "Epoch 121/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9429\n",
            "Epoch 00121: val_loss improved from 0.26197 to 0.25993, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1928 - acc: 0.9431 - val_loss: 0.2599 - val_acc: 0.9283\n",
            "Epoch 122/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2064 - acc: 0.9362\n",
            "Epoch 00122: val_loss improved from 0.25993 to 0.24295, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.2042 - acc: 0.9375 - val_loss: 0.2429 - val_acc: 0.9350\n",
            "Epoch 123/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1996 - acc: 0.9415\n",
            "Epoch 00123: val_loss did not improve from 0.24295\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2012 - acc: 0.9408 - val_loss: 0.2908 - val_acc: 0.9067\n",
            "Epoch 124/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1878 - acc: 0.9484\n",
            "Epoch 00124: val_loss did not improve from 0.24295\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1906 - acc: 0.9450 - val_loss: 0.2452 - val_acc: 0.9300\n",
            "Epoch 125/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9463\n",
            "Epoch 00125: val_loss improved from 0.24295 to 0.23601, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.1778 - acc: 0.9467 - val_loss: 0.2360 - val_acc: 0.9375\n",
            "Epoch 126/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1799 - acc: 0.9494\n",
            "Epoch 00126: val_loss did not improve from 0.23601\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1815 - acc: 0.9483 - val_loss: 0.2559 - val_acc: 0.9242\n",
            "Epoch 127/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9460\n",
            "Epoch 00127: val_loss did not improve from 0.23601\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1771 - acc: 0.9464 - val_loss: 0.2660 - val_acc: 0.9258\n",
            "Epoch 128/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2046 - acc: 0.9368\n",
            "Epoch 00128: val_loss did not improve from 0.23601\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.2048 - acc: 0.9364 - val_loss: 0.2618 - val_acc: 0.9250\n",
            "Epoch 129/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1838 - acc: 0.9478\n",
            "Epoch 00129: val_loss did not improve from 0.23601\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1828 - acc: 0.9494 - val_loss: 0.2433 - val_acc: 0.9367\n",
            "Epoch 130/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1663 - acc: 0.9538\n",
            "Epoch 00130: val_loss improved from 0.23601 to 0.20559, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1678 - acc: 0.9536 - val_loss: 0.2056 - val_acc: 0.9417\n",
            "Epoch 131/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1848 - acc: 0.9452\n",
            "Epoch 00131: val_loss did not improve from 0.20559\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1880 - acc: 0.9453 - val_loss: 0.2498 - val_acc: 0.9292\n",
            "Epoch 132/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1707 - acc: 0.9509\n",
            "Epoch 00132: val_loss did not improve from 0.20559\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1718 - acc: 0.9506 - val_loss: 0.2275 - val_acc: 0.9400\n",
            "Epoch 133/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9466\n",
            "Epoch 00133: val_loss did not improve from 0.20559\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1830 - acc: 0.9461 - val_loss: 0.2323 - val_acc: 0.9308\n",
            "Epoch 134/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1758 - acc: 0.9561\n",
            "Epoch 00134: val_loss did not improve from 0.20559\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1761 - acc: 0.9542 - val_loss: 0.2607 - val_acc: 0.9183\n",
            "Epoch 135/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1954 - acc: 0.9422\n",
            "Epoch 00135: val_loss did not improve from 0.20559\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2030 - acc: 0.9392 - val_loss: 0.2259 - val_acc: 0.9358\n",
            "Epoch 136/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1920 - acc: 0.9432\n",
            "Epoch 00136: val_loss improved from 0.20559 to 0.19894, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1905 - acc: 0.9433 - val_loss: 0.1989 - val_acc: 0.9558\n",
            "Epoch 137/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1743 - acc: 0.9500\n",
            "Epoch 00137: val_loss did not improve from 0.19894\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1754 - acc: 0.9489 - val_loss: 0.2553 - val_acc: 0.9200\n",
            "Epoch 138/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1823 - acc: 0.9444\n",
            "Epoch 00138: val_loss did not improve from 0.19894\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1853 - acc: 0.9422 - val_loss: 0.2804 - val_acc: 0.9142\n",
            "Epoch 139/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1736 - acc: 0.9482\n",
            "Epoch 00139: val_loss did not improve from 0.19894\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1757 - acc: 0.9475 - val_loss: 0.2702 - val_acc: 0.9125\n",
            "Epoch 140/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1676 - acc: 0.9522\n",
            "Epoch 00140: val_loss did not improve from 0.19894\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1649 - acc: 0.9531 - val_loss: 0.2170 - val_acc: 0.9358\n",
            "Epoch 141/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1621 - acc: 0.9518\n",
            "Epoch 00141: val_loss did not improve from 0.19894\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1634 - acc: 0.9514 - val_loss: 0.2382 - val_acc: 0.9317\n",
            "Epoch 142/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9540\n",
            "Epoch 00142: val_loss did not improve from 0.19894\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1620 - acc: 0.9528 - val_loss: 0.2583 - val_acc: 0.9217\n",
            "Epoch 143/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1625 - acc: 0.9559\n",
            "Epoch 00143: val_loss did not improve from 0.19894\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1637 - acc: 0.9553 - val_loss: 0.2403 - val_acc: 0.9300\n",
            "Epoch 144/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9483\n",
            "Epoch 00144: val_loss improved from 0.19894 to 0.19360, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1688 - acc: 0.9475 - val_loss: 0.1936 - val_acc: 0.9483\n",
            "Epoch 145/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1725 - acc: 0.9503\n",
            "Epoch 00145: val_loss did not improve from 0.19360\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1733 - acc: 0.9500 - val_loss: 0.2297 - val_acc: 0.9392\n",
            "Epoch 146/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1577 - acc: 0.9552\n",
            "Epoch 00146: val_loss did not improve from 0.19360\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1606 - acc: 0.9533 - val_loss: 0.2095 - val_acc: 0.9392\n",
            "Epoch 147/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1524 - acc: 0.9579\n",
            "Epoch 00147: val_loss improved from 0.19360 to 0.18924, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1543 - acc: 0.9569 - val_loss: 0.1892 - val_acc: 0.9417\n",
            "Epoch 148/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1502 - acc: 0.9566\n",
            "Epoch 00148: val_loss did not improve from 0.18924\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1544 - acc: 0.9553 - val_loss: 0.1992 - val_acc: 0.9433\n",
            "Epoch 149/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1531 - acc: 0.9574\n",
            "Epoch 00149: val_loss did not improve from 0.18924\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1568 - acc: 0.9558 - val_loss: 0.2248 - val_acc: 0.9425\n",
            "Epoch 150/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.9606\n",
            "Epoch 00150: val_loss did not improve from 0.18924\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1488 - acc: 0.9606 - val_loss: 0.2234 - val_acc: 0.9308\n",
            "Epoch 151/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1597 - acc: 0.9542\n",
            "Epoch 00151: val_loss did not improve from 0.18924\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1578 - acc: 0.9556 - val_loss: 0.2231 - val_acc: 0.9392\n",
            "Epoch 152/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1547 - acc: 0.9594\n",
            "Epoch 00152: val_loss did not improve from 0.18924\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1595 - acc: 0.9567 - val_loss: 0.2247 - val_acc: 0.9358\n",
            "Epoch 153/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1665 - acc: 0.9524\n",
            "Epoch 00153: val_loss did not improve from 0.18924\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1651 - acc: 0.9528 - val_loss: 0.1921 - val_acc: 0.9475\n",
            "Epoch 154/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9511\n",
            "Epoch 00154: val_loss did not improve from 0.18924\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1581 - acc: 0.9517 - val_loss: 0.2270 - val_acc: 0.9317\n",
            "Epoch 155/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1627 - acc: 0.9533\n",
            "Epoch 00155: val_loss did not improve from 0.18924\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1671 - acc: 0.9511 - val_loss: 0.2075 - val_acc: 0.9367\n",
            "Epoch 156/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1791 - acc: 0.9453\n",
            "Epoch 00156: val_loss improved from 0.18924 to 0.18359, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1772 - acc: 0.9464 - val_loss: 0.1836 - val_acc: 0.9458\n",
            "Epoch 157/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1855 - acc: 0.9421\n",
            "Epoch 00157: val_loss improved from 0.18359 to 0.17689, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1888 - acc: 0.9408 - val_loss: 0.1769 - val_acc: 0.9633\n",
            "Epoch 158/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9586\n",
            "Epoch 00158: val_loss did not improve from 0.17689\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1519 - acc: 0.9586 - val_loss: 0.1815 - val_acc: 0.9517\n",
            "Epoch 159/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1638 - acc: 0.9494\n",
            "Epoch 00159: val_loss did not improve from 0.17689\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1640 - acc: 0.9503 - val_loss: 0.2260 - val_acc: 0.9358\n",
            "Epoch 160/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1403 - acc: 0.9622\n",
            "Epoch 00160: val_loss improved from 0.17689 to 0.17677, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.1428 - acc: 0.9611 - val_loss: 0.1768 - val_acc: 0.9525\n",
            "Epoch 161/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1583 - acc: 0.9513\n",
            "Epoch 00161: val_loss did not improve from 0.17677\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1630 - acc: 0.9494 - val_loss: 0.1851 - val_acc: 0.9542\n",
            "Epoch 162/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9566\n",
            "Epoch 00162: val_loss did not improve from 0.17677\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1446 - acc: 0.9564 - val_loss: 0.2064 - val_acc: 0.9392\n",
            "Epoch 163/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9631\n",
            "Epoch 00163: val_loss did not improve from 0.17677\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1378 - acc: 0.9628 - val_loss: 0.1992 - val_acc: 0.9467\n",
            "Epoch 164/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1447 - acc: 0.9612\n",
            "Epoch 00164: val_loss did not improve from 0.17677\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1457 - acc: 0.9600 - val_loss: 0.1841 - val_acc: 0.9517\n",
            "Epoch 165/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1408 - acc: 0.9565\n",
            "Epoch 00165: val_loss improved from 0.17677 to 0.15921, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1426 - acc: 0.9553 - val_loss: 0.1592 - val_acc: 0.9608\n",
            "Epoch 166/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1442 - acc: 0.9582\n",
            "Epoch 00166: val_loss did not improve from 0.15921\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1440 - acc: 0.9583 - val_loss: 0.1983 - val_acc: 0.9408\n",
            "Epoch 167/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1587 - acc: 0.9494\n",
            "Epoch 00167: val_loss did not improve from 0.15921\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1640 - acc: 0.9472 - val_loss: 0.1856 - val_acc: 0.9567\n",
            "Epoch 168/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1481 - acc: 0.9574\n",
            "Epoch 00168: val_loss did not improve from 0.15921\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1507 - acc: 0.9575 - val_loss: 0.1593 - val_acc: 0.9542\n",
            "Epoch 169/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1302 - acc: 0.9631\n",
            "Epoch 00169: val_loss improved from 0.15921 to 0.15527, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1344 - acc: 0.9611 - val_loss: 0.1553 - val_acc: 0.9683\n",
            "Epoch 170/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1536 - acc: 0.9529\n",
            "Epoch 00170: val_loss did not improve from 0.15527\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1523 - acc: 0.9539 - val_loss: 0.2506 - val_acc: 0.9225\n",
            "Epoch 171/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.9534\n",
            "Epoch 00171: val_loss did not improve from 0.15527\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1460 - acc: 0.9531 - val_loss: 0.2199 - val_acc: 0.9358\n",
            "Epoch 172/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1397 - acc: 0.9600\n",
            "Epoch 00172: val_loss did not improve from 0.15527\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.1420 - acc: 0.9586 - val_loss: 0.1905 - val_acc: 0.9575\n",
            "Epoch 173/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1451 - acc: 0.9615\n",
            "Epoch 00173: val_loss did not improve from 0.15527\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1430 - acc: 0.9625 - val_loss: 0.1789 - val_acc: 0.9475\n",
            "Epoch 174/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1374 - acc: 0.9553\n",
            "Epoch 00174: val_loss did not improve from 0.15527\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1385 - acc: 0.9547 - val_loss: 0.1898 - val_acc: 0.9492\n",
            "Epoch 175/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1443 - acc: 0.9526\n",
            "Epoch 00175: val_loss did not improve from 0.15527\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1461 - acc: 0.9522 - val_loss: 0.2012 - val_acc: 0.9375\n",
            "Epoch 176/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9560\n",
            "Epoch 00176: val_loss did not improve from 0.15527\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1495 - acc: 0.9564 - val_loss: 0.2630 - val_acc: 0.9200\n",
            "Epoch 177/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1466 - acc: 0.9542\n",
            "Epoch 00177: val_loss did not improve from 0.15527\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1446 - acc: 0.9550 - val_loss: 0.1745 - val_acc: 0.9450\n",
            "Epoch 178/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1469 - acc: 0.9565\n",
            "Epoch 00178: val_loss did not improve from 0.15527\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1501 - acc: 0.9558 - val_loss: 0.2312 - val_acc: 0.9242\n",
            "Epoch 179/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1556 - acc: 0.9515\n",
            "Epoch 00179: val_loss did not improve from 0.15527\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1548 - acc: 0.9528 - val_loss: 0.1955 - val_acc: 0.9467\n",
            "Epoch 180/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1576 - acc: 0.9459\n",
            "Epoch 00180: val_loss did not improve from 0.15527\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1603 - acc: 0.9458 - val_loss: 0.1682 - val_acc: 0.9533\n",
            "Epoch 181/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1471 - acc: 0.9559\n",
            "Epoch 00181: val_loss did not improve from 0.15527\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1497 - acc: 0.9556 - val_loss: 0.1994 - val_acc: 0.9425\n",
            "Epoch 182/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1518 - acc: 0.9509\n",
            "Epoch 00182: val_loss did not improve from 0.15527\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1506 - acc: 0.9519 - val_loss: 0.1697 - val_acc: 0.9575\n",
            "Epoch 183/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9554\n",
            "Epoch 00183: val_loss did not improve from 0.15527\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1561 - acc: 0.9553 - val_loss: 0.1899 - val_acc: 0.9517\n",
            "Epoch 184/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9631\n",
            "Epoch 00184: val_loss improved from 0.15527 to 0.15263, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1200 - acc: 0.9636 - val_loss: 0.1526 - val_acc: 0.9650\n",
            "Epoch 185/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9586\n",
            "Epoch 00185: val_loss improved from 0.15263 to 0.14662, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.1428 - acc: 0.9586 - val_loss: 0.1466 - val_acc: 0.9642\n",
            "Epoch 186/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1337 - acc: 0.9606\n",
            "Epoch 00186: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1319 - acc: 0.9608 - val_loss: 0.1861 - val_acc: 0.9475\n",
            "Epoch 187/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9606\n",
            "Epoch 00187: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1282 - acc: 0.9614 - val_loss: 0.2180 - val_acc: 0.9283\n",
            "Epoch 188/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1349 - acc: 0.9621\n",
            "Epoch 00188: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1403 - acc: 0.9603 - val_loss: 0.1826 - val_acc: 0.9508\n",
            "Epoch 189/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1322 - acc: 0.9600\n",
            "Epoch 00189: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1327 - acc: 0.9603 - val_loss: 0.1710 - val_acc: 0.9542\n",
            "Epoch 190/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1591 - acc: 0.9503\n",
            "Epoch 00190: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1578 - acc: 0.9514 - val_loss: 0.1542 - val_acc: 0.9583\n",
            "Epoch 191/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9560\n",
            "Epoch 00191: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1427 - acc: 0.9564 - val_loss: 0.1677 - val_acc: 0.9575\n",
            "Epoch 192/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1345 - acc: 0.9606\n",
            "Epoch 00192: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1355 - acc: 0.9600 - val_loss: 0.1794 - val_acc: 0.9517\n",
            "Epoch 193/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1292 - acc: 0.9574\n",
            "Epoch 00193: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1291 - acc: 0.9578 - val_loss: 0.1826 - val_acc: 0.9492\n",
            "Epoch 194/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1465 - acc: 0.9588\n",
            "Epoch 00194: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1488 - acc: 0.9578 - val_loss: 0.1817 - val_acc: 0.9475\n",
            "Epoch 195/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1332 - acc: 0.9615\n",
            "Epoch 00195: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1327 - acc: 0.9617 - val_loss: 0.1553 - val_acc: 0.9642\n",
            "Epoch 196/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1185 - acc: 0.9675\n",
            "Epoch 00196: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1199 - acc: 0.9669 - val_loss: 0.1470 - val_acc: 0.9575\n",
            "Epoch 197/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1244 - acc: 0.9652\n",
            "Epoch 00197: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1281 - acc: 0.9636 - val_loss: 0.1553 - val_acc: 0.9625\n",
            "Epoch 198/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1358 - acc: 0.9603\n",
            "Epoch 00198: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1418 - acc: 0.9575 - val_loss: 0.2022 - val_acc: 0.9433\n",
            "Epoch 199/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1227 - acc: 0.9662\n",
            "Epoch 00199: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1229 - acc: 0.9658 - val_loss: 0.1616 - val_acc: 0.9558\n",
            "Epoch 200/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1186 - acc: 0.9648\n",
            "Epoch 00200: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1194 - acc: 0.9642 - val_loss: 0.1777 - val_acc: 0.9467\n",
            "Epoch 201/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.9586\n",
            "Epoch 00201: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1403 - acc: 0.9592 - val_loss: 0.1598 - val_acc: 0.9533\n",
            "Epoch 202/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1321 - acc: 0.9609\n",
            "Epoch 00202: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1337 - acc: 0.9611 - val_loss: 0.1623 - val_acc: 0.9542\n",
            "Epoch 203/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1130 - acc: 0.9700\n",
            "Epoch 00203: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1121 - acc: 0.9708 - val_loss: 0.1474 - val_acc: 0.9658\n",
            "Epoch 204/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1204 - acc: 0.9639\n",
            "Epoch 00204: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1186 - acc: 0.9647 - val_loss: 0.1898 - val_acc: 0.9442\n",
            "Epoch 205/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9663\n",
            "Epoch 00205: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1160 - acc: 0.9653 - val_loss: 0.1523 - val_acc: 0.9583\n",
            "Epoch 206/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1241 - acc: 0.9652\n",
            "Epoch 00206: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1244 - acc: 0.9653 - val_loss: 0.1792 - val_acc: 0.9467\n",
            "Epoch 207/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1189 - acc: 0.9656\n",
            "Epoch 00207: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1214 - acc: 0.9633 - val_loss: 0.1566 - val_acc: 0.9592\n",
            "Epoch 208/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1419 - acc: 0.9545\n",
            "Epoch 00208: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1445 - acc: 0.9525 - val_loss: 0.1691 - val_acc: 0.9500\n",
            "Epoch 209/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1342 - acc: 0.9612\n",
            "Epoch 00209: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1355 - acc: 0.9603 - val_loss: 0.1818 - val_acc: 0.9458\n",
            "Epoch 210/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9666\n",
            "Epoch 00210: val_loss did not improve from 0.14662\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1191 - acc: 0.9661 - val_loss: 0.1568 - val_acc: 0.9650\n",
            "Epoch 211/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1177 - acc: 0.9668\n",
            "Epoch 00211: val_loss improved from 0.14662 to 0.13608, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.1176 - acc: 0.9664 - val_loss: 0.1361 - val_acc: 0.9658\n",
            "Epoch 212/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9649\n",
            "Epoch 00212: val_loss did not improve from 0.13608\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1205 - acc: 0.9644 - val_loss: 0.1629 - val_acc: 0.9608\n",
            "Epoch 213/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1142 - acc: 0.9663\n",
            "Epoch 00213: val_loss did not improve from 0.13608\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1168 - acc: 0.9653 - val_loss: 0.1525 - val_acc: 0.9575\n",
            "Epoch 214/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1231 - acc: 0.9626\n",
            "Epoch 00214: val_loss improved from 0.13608 to 0.12565, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.1235 - acc: 0.9628 - val_loss: 0.1257 - val_acc: 0.9633\n",
            "Epoch 215/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9700\n",
            "Epoch 00215: val_loss did not improve from 0.12565\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1123 - acc: 0.9700 - val_loss: 0.1678 - val_acc: 0.9542\n",
            "Epoch 216/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9594\n",
            "Epoch 00216: val_loss did not improve from 0.12565\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1348 - acc: 0.9597 - val_loss: 0.1553 - val_acc: 0.9575\n",
            "Epoch 217/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9654\n",
            "Epoch 00217: val_loss improved from 0.12565 to 0.12255, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.1155 - acc: 0.9656 - val_loss: 0.1225 - val_acc: 0.9683\n",
            "Epoch 218/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1245 - acc: 0.9579\n",
            "Epoch 00218: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1268 - acc: 0.9581 - val_loss: 0.1443 - val_acc: 0.9617\n",
            "Epoch 219/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1330 - acc: 0.9585\n",
            "Epoch 00219: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1324 - acc: 0.9581 - val_loss: 0.1266 - val_acc: 0.9692\n",
            "Epoch 220/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1270 - acc: 0.9582\n",
            "Epoch 00220: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1270 - acc: 0.9578 - val_loss: 0.1506 - val_acc: 0.9567\n",
            "Epoch 221/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1503 - acc: 0.9527\n",
            "Epoch 00221: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1547 - acc: 0.9508 - val_loss: 0.1612 - val_acc: 0.9525\n",
            "Epoch 222/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1352 - acc: 0.9548\n",
            "Epoch 00222: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1366 - acc: 0.9536 - val_loss: 0.1580 - val_acc: 0.9550\n",
            "Epoch 223/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1315 - acc: 0.9594\n",
            "Epoch 00223: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1327 - acc: 0.9594 - val_loss: 0.1480 - val_acc: 0.9575\n",
            "Epoch 224/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1199 - acc: 0.9669\n",
            "Epoch 00224: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1213 - acc: 0.9661 - val_loss: 0.1390 - val_acc: 0.9592\n",
            "Epoch 225/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1261 - acc: 0.9621\n",
            "Epoch 00225: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1292 - acc: 0.9611 - val_loss: 0.1456 - val_acc: 0.9633\n",
            "Epoch 226/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1240 - acc: 0.9623\n",
            "Epoch 00226: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1241 - acc: 0.9619 - val_loss: 0.1502 - val_acc: 0.9633\n",
            "Epoch 227/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1160 - acc: 0.9679\n",
            "Epoch 00227: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1155 - acc: 0.9681 - val_loss: 0.1501 - val_acc: 0.9542\n",
            "Epoch 228/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9743\n",
            "Epoch 00228: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1033 - acc: 0.9736 - val_loss: 0.1378 - val_acc: 0.9617\n",
            "Epoch 229/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1129 - acc: 0.9659\n",
            "Epoch 00229: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1141 - acc: 0.9658 - val_loss: 0.1580 - val_acc: 0.9533\n",
            "Epoch 230/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1192 - acc: 0.9638\n",
            "Epoch 00230: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1235 - acc: 0.9614 - val_loss: 0.1703 - val_acc: 0.9517\n",
            "Epoch 231/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9643\n",
            "Epoch 00231: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1293 - acc: 0.9642 - val_loss: 0.2061 - val_acc: 0.9400\n",
            "Epoch 232/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1254 - acc: 0.9619\n",
            "Epoch 00232: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1231 - acc: 0.9619 - val_loss: 0.1393 - val_acc: 0.9675\n",
            "Epoch 233/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9646\n",
            "Epoch 00233: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1214 - acc: 0.9642 - val_loss: 0.1333 - val_acc: 0.9650\n",
            "Epoch 234/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1283 - acc: 0.9591\n",
            "Epoch 00234: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1315 - acc: 0.9581 - val_loss: 0.1421 - val_acc: 0.9575\n",
            "Epoch 235/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1117 - acc: 0.9669\n",
            "Epoch 00235: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1177 - acc: 0.9647 - val_loss: 0.1896 - val_acc: 0.9417\n",
            "Epoch 236/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1074 - acc: 0.9730\n",
            "Epoch 00236: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1070 - acc: 0.9725 - val_loss: 0.1290 - val_acc: 0.9683\n",
            "Epoch 237/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1063 - acc: 0.9676\n",
            "Epoch 00237: val_loss did not improve from 0.12255\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1038 - acc: 0.9689 - val_loss: 0.1462 - val_acc: 0.9592\n",
            "Epoch 238/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1119 - acc: 0.9676\n",
            "Epoch 00238: val_loss improved from 0.12255 to 0.11769, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1133 - acc: 0.9672 - val_loss: 0.1177 - val_acc: 0.9708\n",
            "Epoch 239/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1193 - acc: 0.9645\n",
            "Epoch 00239: val_loss improved from 0.11769 to 0.10982, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1199 - acc: 0.9653 - val_loss: 0.1098 - val_acc: 0.9742\n",
            "Epoch 240/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1217 - acc: 0.9597\n",
            "Epoch 00240: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1217 - acc: 0.9603 - val_loss: 0.1295 - val_acc: 0.9700\n",
            "Epoch 241/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1057 - acc: 0.9709\n",
            "Epoch 00241: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1066 - acc: 0.9708 - val_loss: 0.1210 - val_acc: 0.9725\n",
            "Epoch 242/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1174 - acc: 0.9650\n",
            "Epoch 00242: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1160 - acc: 0.9661 - val_loss: 0.1600 - val_acc: 0.9550\n",
            "Epoch 243/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1184 - acc: 0.9661\n",
            "Epoch 00243: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1192 - acc: 0.9656 - val_loss: 0.1334 - val_acc: 0.9667\n",
            "Epoch 244/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1076 - acc: 0.9682\n",
            "Epoch 00244: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1094 - acc: 0.9669 - val_loss: 0.1251 - val_acc: 0.9667\n",
            "Epoch 245/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1087 - acc: 0.9694\n",
            "Epoch 00245: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1132 - acc: 0.9678 - val_loss: 0.1115 - val_acc: 0.9708\n",
            "Epoch 246/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9680\n",
            "Epoch 00246: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1107 - acc: 0.9686 - val_loss: 0.1245 - val_acc: 0.9742\n",
            "Epoch 247/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1098 - acc: 0.9647\n",
            "Epoch 00247: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1109 - acc: 0.9642 - val_loss: 0.1486 - val_acc: 0.9592\n",
            "Epoch 248/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1122 - acc: 0.9694\n",
            "Epoch 00248: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1147 - acc: 0.9683 - val_loss: 0.1284 - val_acc: 0.9658\n",
            "Epoch 249/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1147 - acc: 0.9697\n",
            "Epoch 00249: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1155 - acc: 0.9686 - val_loss: 0.1375 - val_acc: 0.9617\n",
            "Epoch 250/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1133 - acc: 0.9650\n",
            "Epoch 00250: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1136 - acc: 0.9656 - val_loss: 0.1294 - val_acc: 0.9650\n",
            "Epoch 251/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9706\n",
            "Epoch 00251: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1008 - acc: 0.9694 - val_loss: 0.1276 - val_acc: 0.9667\n",
            "Epoch 252/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1066 - acc: 0.9684\n",
            "Epoch 00252: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1131 - acc: 0.9661 - val_loss: 0.1372 - val_acc: 0.9617\n",
            "Epoch 253/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1350 - acc: 0.9547\n",
            "Epoch 00253: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1344 - acc: 0.9550 - val_loss: 0.1421 - val_acc: 0.9650\n",
            "Epoch 254/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1277 - acc: 0.9639\n",
            "Epoch 00254: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1269 - acc: 0.9642 - val_loss: 0.1231 - val_acc: 0.9675\n",
            "Epoch 255/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9657\n",
            "Epoch 00255: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1219 - acc: 0.9658 - val_loss: 0.1490 - val_acc: 0.9633\n",
            "Epoch 256/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9694\n",
            "Epoch 00256: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1125 - acc: 0.9697 - val_loss: 0.1493 - val_acc: 0.9617\n",
            "Epoch 257/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1123 - acc: 0.9664\n",
            "Epoch 00257: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1130 - acc: 0.9672 - val_loss: 0.1429 - val_acc: 0.9558\n",
            "Epoch 258/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1121 - acc: 0.9664\n",
            "Epoch 00258: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1148 - acc: 0.9653 - val_loss: 0.1190 - val_acc: 0.9708\n",
            "Epoch 259/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1084 - acc: 0.9674\n",
            "Epoch 00259: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1071 - acc: 0.9681 - val_loss: 0.1136 - val_acc: 0.9675\n",
            "Epoch 260/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9717\n",
            "Epoch 00260: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0965 - acc: 0.9714 - val_loss: 0.1234 - val_acc: 0.9675\n",
            "Epoch 261/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9757\n",
            "Epoch 00261: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0936 - acc: 0.9750 - val_loss: 0.1329 - val_acc: 0.9650\n",
            "Epoch 262/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9740\n",
            "Epoch 00262: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0938 - acc: 0.9744 - val_loss: 0.1213 - val_acc: 0.9667\n",
            "Epoch 263/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0969 - acc: 0.9736\n",
            "Epoch 00263: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0979 - acc: 0.9725 - val_loss: 0.1327 - val_acc: 0.9642\n",
            "Epoch 264/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1123 - acc: 0.9694\n",
            "Epoch 00264: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1170 - acc: 0.9664 - val_loss: 0.1137 - val_acc: 0.9700\n",
            "Epoch 265/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1063 - acc: 0.9665\n",
            "Epoch 00265: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1045 - acc: 0.9675 - val_loss: 0.1314 - val_acc: 0.9708\n",
            "Epoch 266/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1082 - acc: 0.9676\n",
            "Epoch 00266: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1074 - acc: 0.9675 - val_loss: 0.1435 - val_acc: 0.9617\n",
            "Epoch 267/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0924 - acc: 0.9700\n",
            "Epoch 00267: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0925 - acc: 0.9700 - val_loss: 0.1400 - val_acc: 0.9558\n",
            "Epoch 268/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0995 - acc: 0.9700\n",
            "Epoch 00268: val_loss did not improve from 0.10982\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0987 - acc: 0.9714 - val_loss: 0.1232 - val_acc: 0.9683\n",
            "Epoch 269/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1085 - acc: 0.9670\n",
            "Epoch 00269: val_loss improved from 0.10982 to 0.10907, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1085 - acc: 0.9669 - val_loss: 0.1091 - val_acc: 0.9725\n",
            "Epoch 270/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9660\n",
            "Epoch 00270: val_loss did not improve from 0.10907\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1084 - acc: 0.9667 - val_loss: 0.1474 - val_acc: 0.9558\n",
            "Epoch 271/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0993 - acc: 0.9715\n",
            "Epoch 00271: val_loss did not improve from 0.10907\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0998 - acc: 0.9717 - val_loss: 0.1297 - val_acc: 0.9658\n",
            "Epoch 272/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1013 - acc: 0.9712\n",
            "Epoch 00272: val_loss did not improve from 0.10907\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1051 - acc: 0.9703 - val_loss: 0.1874 - val_acc: 0.9508\n",
            "Epoch 273/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1059 - acc: 0.9669\n",
            "Epoch 00273: val_loss did not improve from 0.10907\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1054 - acc: 0.9678 - val_loss: 0.1163 - val_acc: 0.9650\n",
            "Epoch 274/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1065 - acc: 0.9647\n",
            "Epoch 00274: val_loss did not improve from 0.10907\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1092 - acc: 0.9631 - val_loss: 0.1312 - val_acc: 0.9658\n",
            "Epoch 275/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1093 - acc: 0.9691\n",
            "Epoch 00275: val_loss did not improve from 0.10907\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1088 - acc: 0.9703 - val_loss: 0.1234 - val_acc: 0.9675\n",
            "Epoch 276/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0985 - acc: 0.9732\n",
            "Epoch 00276: val_loss did not improve from 0.10907\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0996 - acc: 0.9731 - val_loss: 0.1309 - val_acc: 0.9608\n",
            "Epoch 277/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1028 - acc: 0.9676\n",
            "Epoch 00277: val_loss did not improve from 0.10907\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1017 - acc: 0.9678 - val_loss: 0.1109 - val_acc: 0.9725\n",
            "Epoch 278/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0981 - acc: 0.9724\n",
            "Epoch 00278: val_loss did not improve from 0.10907\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0954 - acc: 0.9739 - val_loss: 0.1384 - val_acc: 0.9658\n",
            "Epoch 279/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1051 - acc: 0.9661\n",
            "Epoch 00279: val_loss improved from 0.10907 to 0.10025, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.1063 - acc: 0.9653 - val_loss: 0.1003 - val_acc: 0.9742\n",
            "Epoch 280/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1151 - acc: 0.9667\n",
            "Epoch 00280: val_loss did not improve from 0.10025\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1180 - acc: 0.9647 - val_loss: 0.1353 - val_acc: 0.9650\n",
            "Epoch 281/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9614\n",
            "Epoch 00281: val_loss did not improve from 0.10025\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1260 - acc: 0.9617 - val_loss: 0.1234 - val_acc: 0.9675\n",
            "Epoch 282/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1113 - acc: 0.9678\n",
            "Epoch 00282: val_loss did not improve from 0.10025\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1098 - acc: 0.9675 - val_loss: 0.1058 - val_acc: 0.9683\n",
            "Epoch 283/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0941 - acc: 0.9755\n",
            "Epoch 00283: val_loss did not improve from 0.10025\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0930 - acc: 0.9764 - val_loss: 0.1048 - val_acc: 0.9750\n",
            "Epoch 284/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9686\n",
            "Epoch 00284: val_loss did not improve from 0.10025\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1033 - acc: 0.9686 - val_loss: 0.1092 - val_acc: 0.9717\n",
            "Epoch 285/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0995 - acc: 0.9722\n",
            "Epoch 00285: val_loss did not improve from 0.10025\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0986 - acc: 0.9719 - val_loss: 0.1271 - val_acc: 0.9667\n",
            "Epoch 286/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9683\n",
            "Epoch 00286: val_loss did not improve from 0.10025\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1041 - acc: 0.9686 - val_loss: 0.1181 - val_acc: 0.9675\n",
            "Epoch 287/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1064 - acc: 0.9672\n",
            "Epoch 00287: val_loss did not improve from 0.10025\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1110 - acc: 0.9658 - val_loss: 0.1293 - val_acc: 0.9658\n",
            "Epoch 288/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1200 - acc: 0.9591\n",
            "Epoch 00288: val_loss did not improve from 0.10025\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1193 - acc: 0.9600 - val_loss: 0.1354 - val_acc: 0.9650\n",
            "Epoch 289/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1051 - acc: 0.9673\n",
            "Epoch 00289: val_loss did not improve from 0.10025\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1039 - acc: 0.9683 - val_loss: 0.1089 - val_acc: 0.9717\n",
            "Epoch 290/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1196 - acc: 0.9642\n",
            "Epoch 00290: val_loss improved from 0.10025 to 0.09680, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1173 - acc: 0.9656 - val_loss: 0.0968 - val_acc: 0.9758\n",
            "Epoch 291/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0964 - acc: 0.9718\n",
            "Epoch 00291: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0985 - acc: 0.9703 - val_loss: 0.1139 - val_acc: 0.9708\n",
            "Epoch 292/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1107 - acc: 0.9645\n",
            "Epoch 00292: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1110 - acc: 0.9644 - val_loss: 0.1345 - val_acc: 0.9667\n",
            "Epoch 293/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0924 - acc: 0.9721\n",
            "Epoch 00293: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0942 - acc: 0.9714 - val_loss: 0.1242 - val_acc: 0.9675\n",
            "Epoch 294/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1089 - acc: 0.9664\n",
            "Epoch 00294: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1090 - acc: 0.9667 - val_loss: 0.1013 - val_acc: 0.9817\n",
            "Epoch 295/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0878 - acc: 0.9779\n",
            "Epoch 00295: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0882 - acc: 0.9775 - val_loss: 0.1323 - val_acc: 0.9683\n",
            "Epoch 296/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1015 - acc: 0.9697\n",
            "Epoch 00296: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1065 - acc: 0.9669 - val_loss: 0.1273 - val_acc: 0.9667\n",
            "Epoch 297/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1030 - acc: 0.9706\n",
            "Epoch 00297: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1050 - acc: 0.9697 - val_loss: 0.1210 - val_acc: 0.9642\n",
            "Epoch 298/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1058 - acc: 0.9665\n",
            "Epoch 00298: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1054 - acc: 0.9664 - val_loss: 0.1112 - val_acc: 0.9692\n",
            "Epoch 299/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1125 - acc: 0.9650\n",
            "Epoch 00299: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1154 - acc: 0.9647 - val_loss: 0.1279 - val_acc: 0.9658\n",
            "Epoch 300/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1083 - acc: 0.9691\n",
            "Epoch 00300: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1084 - acc: 0.9686 - val_loss: 0.1018 - val_acc: 0.9725\n",
            "Epoch 301/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9740\n",
            "Epoch 00301: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0945 - acc: 0.9731 - val_loss: 0.1067 - val_acc: 0.9692\n",
            "Epoch 302/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1027 - acc: 0.9662\n",
            "Epoch 00302: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1040 - acc: 0.9658 - val_loss: 0.1189 - val_acc: 0.9658\n",
            "Epoch 303/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1063 - acc: 0.9670\n",
            "Epoch 00303: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1048 - acc: 0.9675 - val_loss: 0.1209 - val_acc: 0.9667\n",
            "Epoch 304/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9697\n",
            "Epoch 00304: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0989 - acc: 0.9703 - val_loss: 0.1107 - val_acc: 0.9708\n",
            "Epoch 305/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9709\n",
            "Epoch 00305: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0976 - acc: 0.9697 - val_loss: 0.1285 - val_acc: 0.9633\n",
            "Epoch 306/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1034 - acc: 0.9666\n",
            "Epoch 00306: val_loss improved from 0.09680 to 0.09415, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1045 - acc: 0.9653 - val_loss: 0.0942 - val_acc: 0.9792\n",
            "Epoch 307/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.9623\n",
            "Epoch 00307: val_loss did not improve from 0.09415\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1256 - acc: 0.9617 - val_loss: 0.1403 - val_acc: 0.9600\n",
            "Epoch 308/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9680\n",
            "Epoch 00308: val_loss did not improve from 0.09415\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1179 - acc: 0.9675 - val_loss: 0.1113 - val_acc: 0.9733\n",
            "Epoch 309/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1063 - acc: 0.9682\n",
            "Epoch 00309: val_loss did not improve from 0.09415\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1078 - acc: 0.9669 - val_loss: 0.1100 - val_acc: 0.9667\n",
            "Epoch 310/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1080 - acc: 0.9676\n",
            "Epoch 00310: val_loss improved from 0.09415 to 0.09345, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1095 - acc: 0.9664 - val_loss: 0.0935 - val_acc: 0.9783\n",
            "Epoch 311/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1002 - acc: 0.9685\n",
            "Epoch 00311: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0985 - acc: 0.9692 - val_loss: 0.1198 - val_acc: 0.9650\n",
            "Epoch 312/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9700\n",
            "Epoch 00312: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0982 - acc: 0.9689 - val_loss: 0.0969 - val_acc: 0.9750\n",
            "Epoch 313/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0961 - acc: 0.9724\n",
            "Epoch 00313: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1016 - acc: 0.9700 - val_loss: 0.1230 - val_acc: 0.9733\n",
            "Epoch 314/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1037 - acc: 0.9661\n",
            "Epoch 00314: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1045 - acc: 0.9653 - val_loss: 0.1290 - val_acc: 0.9692\n",
            "Epoch 315/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0995 - acc: 0.9706\n",
            "Epoch 00315: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1006 - acc: 0.9700 - val_loss: 0.1275 - val_acc: 0.9650\n",
            "Epoch 316/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0860 - acc: 0.9773\n",
            "Epoch 00316: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0852 - acc: 0.9772 - val_loss: 0.1196 - val_acc: 0.9708\n",
            "Epoch 317/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0824 - acc: 0.9776\n",
            "Epoch 00317: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0863 - acc: 0.9764 - val_loss: 0.1169 - val_acc: 0.9683\n",
            "Epoch 318/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1005 - acc: 0.9665\n",
            "Epoch 00318: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0995 - acc: 0.9672 - val_loss: 0.1210 - val_acc: 0.9675\n",
            "Epoch 319/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9726\n",
            "Epoch 00319: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0874 - acc: 0.9728 - val_loss: 0.1281 - val_acc: 0.9642\n",
            "Epoch 320/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9669\n",
            "Epoch 00320: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1051 - acc: 0.9667 - val_loss: 0.1155 - val_acc: 0.9683\n",
            "Epoch 321/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0960 - acc: 0.9730\n",
            "Epoch 00321: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0940 - acc: 0.9733 - val_loss: 0.1195 - val_acc: 0.9650\n",
            "Epoch 322/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0975 - acc: 0.9724\n",
            "Epoch 00322: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0988 - acc: 0.9711 - val_loss: 0.1146 - val_acc: 0.9708\n",
            "Epoch 323/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0918 - acc: 0.9691\n",
            "Epoch 00323: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0931 - acc: 0.9689 - val_loss: 0.1077 - val_acc: 0.9708\n",
            "Epoch 324/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1053 - acc: 0.9665\n",
            "Epoch 00324: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1042 - acc: 0.9664 - val_loss: 0.1591 - val_acc: 0.9542\n",
            "Epoch 325/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9729\n",
            "Epoch 00325: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0907 - acc: 0.9731 - val_loss: 0.1130 - val_acc: 0.9717\n",
            "Epoch 326/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1103 - acc: 0.9670\n",
            "Epoch 00326: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1119 - acc: 0.9656 - val_loss: 0.1295 - val_acc: 0.9658\n",
            "Epoch 327/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9714\n",
            "Epoch 00327: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0927 - acc: 0.9711 - val_loss: 0.1346 - val_acc: 0.9625\n",
            "Epoch 328/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1212 - acc: 0.9606\n",
            "Epoch 00328: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1221 - acc: 0.9606 - val_loss: 0.1213 - val_acc: 0.9642\n",
            "Epoch 329/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1102 - acc: 0.9667\n",
            "Epoch 00329: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1141 - acc: 0.9664 - val_loss: 0.1048 - val_acc: 0.9725\n",
            "Epoch 330/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0997 - acc: 0.9700\n",
            "Epoch 00330: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0997 - acc: 0.9689 - val_loss: 0.0991 - val_acc: 0.9758\n",
            "Epoch 331/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0984 - acc: 0.9715\n",
            "Epoch 00331: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0966 - acc: 0.9719 - val_loss: 0.1303 - val_acc: 0.9625\n",
            "Epoch 332/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9660\n",
            "Epoch 00332: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1016 - acc: 0.9667 - val_loss: 0.1185 - val_acc: 0.9692\n",
            "Epoch 333/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1007 - acc: 0.9712\n",
            "Epoch 00333: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1050 - acc: 0.9683 - val_loss: 0.0956 - val_acc: 0.9792\n",
            "Epoch 334/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0990 - acc: 0.9712\n",
            "Epoch 00334: val_loss did not improve from 0.09345\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1000 - acc: 0.9708 - val_loss: 0.0939 - val_acc: 0.9775\n",
            "Epoch 335/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1029 - acc: 0.9659\n",
            "Epoch 00335: val_loss improved from 0.09345 to 0.08883, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1012 - acc: 0.9672 - val_loss: 0.0888 - val_acc: 0.9792\n",
            "Epoch 336/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0828 - acc: 0.9779\n",
            "Epoch 00336: val_loss did not improve from 0.08883\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0855 - acc: 0.9767 - val_loss: 0.1029 - val_acc: 0.9750\n",
            "Epoch 337/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0866 - acc: 0.9779\n",
            "Epoch 00337: val_loss did not improve from 0.08883\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0867 - acc: 0.9778 - val_loss: 0.1050 - val_acc: 0.9717\n",
            "Epoch 338/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9666\n",
            "Epoch 00338: val_loss did not improve from 0.08883\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1074 - acc: 0.9664 - val_loss: 0.0964 - val_acc: 0.9750\n",
            "Epoch 339/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1042 - acc: 0.9709\n",
            "Epoch 00339: val_loss did not improve from 0.08883\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1041 - acc: 0.9703 - val_loss: 0.1081 - val_acc: 0.9717\n",
            "Epoch 340/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0897 - acc: 0.9706\n",
            "Epoch 00340: val_loss did not improve from 0.08883\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0905 - acc: 0.9700 - val_loss: 0.1132 - val_acc: 0.9700\n",
            "Epoch 341/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1026 - acc: 0.9682\n",
            "Epoch 00341: val_loss did not improve from 0.08883\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1005 - acc: 0.9694 - val_loss: 0.1122 - val_acc: 0.9708\n",
            "Epoch 342/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0903 - acc: 0.9759\n",
            "Epoch 00342: val_loss did not improve from 0.08883\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0893 - acc: 0.9758 - val_loss: 0.1134 - val_acc: 0.9717\n",
            "Epoch 343/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0865 - acc: 0.9719\n",
            "Epoch 00343: val_loss did not improve from 0.08883\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0907 - acc: 0.9708 - val_loss: 0.1474 - val_acc: 0.9592\n",
            "Epoch 344/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0805 - acc: 0.9779\n",
            "Epoch 00344: val_loss did not improve from 0.08883\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0814 - acc: 0.9775 - val_loss: 0.1151 - val_acc: 0.9717\n",
            "Epoch 345/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9726\n",
            "Epoch 00345: val_loss did not improve from 0.08883\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0891 - acc: 0.9725 - val_loss: 0.1011 - val_acc: 0.9750\n",
            "Epoch 346/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0886 - acc: 0.9736\n",
            "Epoch 00346: val_loss did not improve from 0.08883\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0878 - acc: 0.9747 - val_loss: 0.0933 - val_acc: 0.9792\n",
            "Epoch 347/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1031 - acc: 0.9650\n",
            "Epoch 00347: val_loss did not improve from 0.08883\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1044 - acc: 0.9653 - val_loss: 0.1394 - val_acc: 0.9650\n",
            "Epoch 348/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0760 - acc: 0.9803\n",
            "Epoch 00348: val_loss did not improve from 0.08883\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0787 - acc: 0.9783 - val_loss: 0.1133 - val_acc: 0.9675\n",
            "Epoch 349/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0908 - acc: 0.9724\n",
            "Epoch 00349: val_loss did not improve from 0.08883\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0916 - acc: 0.9717 - val_loss: 0.1008 - val_acc: 0.9750\n",
            "Epoch 350/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1114 - acc: 0.9697\n",
            "Epoch 00350: val_loss did not improve from 0.08883\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1081 - acc: 0.9708 - val_loss: 0.1079 - val_acc: 0.9742\n",
            "Epoch 351/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9714\n",
            "Epoch 00351: val_loss did not improve from 0.08883\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0966 - acc: 0.9706 - val_loss: 0.1099 - val_acc: 0.9667\n",
            "Epoch 352/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9691\n",
            "Epoch 00352: val_loss did not improve from 0.08883\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1054 - acc: 0.9686 - val_loss: 0.1073 - val_acc: 0.9742\n",
            "Epoch 353/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9671\n",
            "Epoch 00353: val_loss did not improve from 0.08883\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1025 - acc: 0.9675 - val_loss: 0.1316 - val_acc: 0.9625\n",
            "Epoch 354/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1046 - acc: 0.9676\n",
            "Epoch 00354: val_loss did not improve from 0.08883\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1034 - acc: 0.9683 - val_loss: 0.1270 - val_acc: 0.9700\n",
            "Epoch 355/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0906 - acc: 0.9771\n",
            "Epoch 00355: val_loss improved from 0.08883 to 0.08855, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.0894 - acc: 0.9775 - val_loss: 0.0886 - val_acc: 0.9733\n",
            "Epoch 356/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0903 - acc: 0.9721\n",
            "Epoch 00356: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0900 - acc: 0.9725 - val_loss: 0.1084 - val_acc: 0.9700\n",
            "Epoch 357/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0953 - acc: 0.9735\n",
            "Epoch 00357: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0952 - acc: 0.9733 - val_loss: 0.1088 - val_acc: 0.9717\n",
            "Epoch 358/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9723\n",
            "Epoch 00358: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0898 - acc: 0.9728 - val_loss: 0.1139 - val_acc: 0.9650\n",
            "Epoch 359/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0959 - acc: 0.9679\n",
            "Epoch 00359: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0964 - acc: 0.9678 - val_loss: 0.1109 - val_acc: 0.9758\n",
            "Epoch 360/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9760\n",
            "Epoch 00360: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0874 - acc: 0.9744 - val_loss: 0.1062 - val_acc: 0.9725\n",
            "Epoch 361/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0927 - acc: 0.9706\n",
            "Epoch 00361: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0956 - acc: 0.9697 - val_loss: 0.1222 - val_acc: 0.9667\n",
            "Epoch 362/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0924 - acc: 0.9721\n",
            "Epoch 00362: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0949 - acc: 0.9706 - val_loss: 0.1243 - val_acc: 0.9667\n",
            "Epoch 363/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9706\n",
            "Epoch 00363: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0957 - acc: 0.9700 - val_loss: 0.1411 - val_acc: 0.9575\n",
            "Epoch 364/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1112 - acc: 0.9639\n",
            "Epoch 00364: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1099 - acc: 0.9653 - val_loss: 0.1180 - val_acc: 0.9642\n",
            "Epoch 365/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9734\n",
            "Epoch 00365: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0904 - acc: 0.9739 - val_loss: 0.1166 - val_acc: 0.9642\n",
            "Epoch 366/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0944 - acc: 0.9733\n",
            "Epoch 00366: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0937 - acc: 0.9739 - val_loss: 0.1156 - val_acc: 0.9708\n",
            "Epoch 367/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9714\n",
            "Epoch 00367: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0942 - acc: 0.9706 - val_loss: 0.1208 - val_acc: 0.9675\n",
            "Epoch 368/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0816 - acc: 0.9761\n",
            "Epoch 00368: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0814 - acc: 0.9756 - val_loss: 0.0948 - val_acc: 0.9767\n",
            "Epoch 369/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0882 - acc: 0.9724\n",
            "Epoch 00369: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0874 - acc: 0.9736 - val_loss: 0.0965 - val_acc: 0.9725\n",
            "Epoch 370/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0985 - acc: 0.9681\n",
            "Epoch 00370: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0999 - acc: 0.9675 - val_loss: 0.1055 - val_acc: 0.9667\n",
            "Epoch 371/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0886 - acc: 0.9722\n",
            "Epoch 00371: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0883 - acc: 0.9717 - val_loss: 0.1049 - val_acc: 0.9742\n",
            "Epoch 372/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0905 - acc: 0.9715\n",
            "Epoch 00372: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0910 - acc: 0.9708 - val_loss: 0.1033 - val_acc: 0.9783\n",
            "Epoch 373/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0927 - acc: 0.9715\n",
            "Epoch 00373: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0919 - acc: 0.9717 - val_loss: 0.1074 - val_acc: 0.9700\n",
            "Epoch 374/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0988 - acc: 0.9726\n",
            "Epoch 00374: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1002 - acc: 0.9719 - val_loss: 0.1254 - val_acc: 0.9675\n",
            "Epoch 375/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1326 - acc: 0.9597\n",
            "Epoch 00375: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1325 - acc: 0.9597 - val_loss: 0.1091 - val_acc: 0.9658\n",
            "Epoch 376/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1125 - acc: 0.9641\n",
            "Epoch 00376: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1135 - acc: 0.9647 - val_loss: 0.1012 - val_acc: 0.9775\n",
            "Epoch 377/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1128 - acc: 0.9652\n",
            "Epoch 00377: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1120 - acc: 0.9667 - val_loss: 0.1040 - val_acc: 0.9700\n",
            "Epoch 378/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0896 - acc: 0.9736\n",
            "Epoch 00378: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0887 - acc: 0.9736 - val_loss: 0.1075 - val_acc: 0.9725\n",
            "Epoch 379/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0763 - acc: 0.9797\n",
            "Epoch 00379: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0774 - acc: 0.9797 - val_loss: 0.1046 - val_acc: 0.9700\n",
            "Epoch 380/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0832 - acc: 0.9781\n",
            "Epoch 00380: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0828 - acc: 0.9778 - val_loss: 0.1068 - val_acc: 0.9717\n",
            "Epoch 381/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0838 - acc: 0.9744\n",
            "Epoch 00381: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0846 - acc: 0.9742 - val_loss: 0.0966 - val_acc: 0.9717\n",
            "Epoch 382/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0897 - acc: 0.9727\n",
            "Epoch 00382: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0904 - acc: 0.9719 - val_loss: 0.0948 - val_acc: 0.9758\n",
            "Epoch 383/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0857 - acc: 0.9745\n",
            "Epoch 00383: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0870 - acc: 0.9747 - val_loss: 0.1260 - val_acc: 0.9625\n",
            "Epoch 384/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0936 - acc: 0.9712\n",
            "Epoch 00384: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0948 - acc: 0.9708 - val_loss: 0.0887 - val_acc: 0.9775\n",
            "Epoch 385/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0856 - acc: 0.9739\n",
            "Epoch 00385: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0822 - acc: 0.9758 - val_loss: 0.1125 - val_acc: 0.9650\n",
            "Epoch 386/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0857 - acc: 0.9735\n",
            "Epoch 00386: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0871 - acc: 0.9728 - val_loss: 0.0977 - val_acc: 0.9725\n",
            "Epoch 387/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0813 - acc: 0.9759\n",
            "Epoch 00387: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0811 - acc: 0.9767 - val_loss: 0.0955 - val_acc: 0.9725\n",
            "Epoch 388/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0919 - acc: 0.9730\n",
            "Epoch 00388: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0893 - acc: 0.9739 - val_loss: 0.0935 - val_acc: 0.9683\n",
            "Epoch 389/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9720\n",
            "Epoch 00389: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0882 - acc: 0.9711 - val_loss: 0.1006 - val_acc: 0.9692\n",
            "Epoch 390/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9786\n",
            "Epoch 00390: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0817 - acc: 0.9772 - val_loss: 0.1070 - val_acc: 0.9683\n",
            "Epoch 391/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0787 - acc: 0.9782\n",
            "Epoch 00391: val_loss did not improve from 0.08855\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0775 - acc: 0.9786 - val_loss: 0.0911 - val_acc: 0.9750\n",
            "Epoch 392/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0783 - acc: 0.9776\n",
            "Epoch 00392: val_loss improved from 0.08855 to 0.08781, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.0791 - acc: 0.9772 - val_loss: 0.0878 - val_acc: 0.9767\n",
            "Epoch 393/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0782 - acc: 0.9797\n",
            "Epoch 00393: val_loss did not improve from 0.08781\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0780 - acc: 0.9800 - val_loss: 0.0936 - val_acc: 0.9750\n",
            "Epoch 394/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0818 - acc: 0.9782\n",
            "Epoch 00394: val_loss did not improve from 0.08781\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0821 - acc: 0.9783 - val_loss: 0.1041 - val_acc: 0.9692\n",
            "Epoch 395/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0875 - acc: 0.9703\n",
            "Epoch 00395: val_loss did not improve from 0.08781\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0871 - acc: 0.9703 - val_loss: 0.0985 - val_acc: 0.9758\n",
            "Epoch 396/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1035 - acc: 0.9685\n",
            "Epoch 00396: val_loss did not improve from 0.08781\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1047 - acc: 0.9678 - val_loss: 0.1053 - val_acc: 0.9750\n",
            "Epoch 397/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0983 - acc: 0.9661\n",
            "Epoch 00397: val_loss did not improve from 0.08781\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0985 - acc: 0.9661 - val_loss: 0.1266 - val_acc: 0.9700\n",
            "Epoch 398/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0970 - acc: 0.9709\n",
            "Epoch 00398: val_loss did not improve from 0.08781\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0978 - acc: 0.9714 - val_loss: 0.0973 - val_acc: 0.9708\n",
            "Epoch 399/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0821 - acc: 0.9770\n",
            "Epoch 00399: val_loss did not improve from 0.08781\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0849 - acc: 0.9769 - val_loss: 0.1156 - val_acc: 0.9675\n",
            "Epoch 400/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0794 - acc: 0.9752\n",
            "Epoch 00400: val_loss did not improve from 0.08781\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0795 - acc: 0.9758 - val_loss: 0.0926 - val_acc: 0.9750\n",
            "1200/1200 [==============================] - 0s 119us/sample - loss: 0.0926 - acc: 0.9750\n",
            "[0.09258170162638028, 0.975]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvQJqLaHi9kY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "89a3a032-ea8d-4e93-9220-3882ed0b6751"
      },
      "source": [
        "for i in range(2, 3): # Итерација низ секој испитен примерок\n",
        "  print(f\"====================== Примерок ({i}) ======================\")\n",
        "  print(\"Вчитување тест податоци од испитниот примерок \" + str(i) + \"...\")\n",
        "  \n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_test_data = np.empty(0)\n",
        "  temp_test_events = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_test_set = 'S' + format(j, '02') + '/Test'\n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testData.mat\"\n",
        "    temp = loadmat(full_path)['testData']\n",
        "    if temp_test_data.size != 0:\n",
        "      temp_test_data = np.concatenate((temp_test_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_test_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_test_events.size != 0:\n",
        "        temp_test_events = np.append(temp_test_events, temp)\n",
        "      else:\n",
        "        temp_test_events = np.array(temp)\n",
        "\n",
        "    # Вчитување на бројот на runs \n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/runs_per_block.txt\"\n",
        "    with open(full_path, \"r\") as runs_per_block:\n",
        "      test_runs_per_block[i-1][j-1] = int(runs_per_block.read())\n",
        "\n",
        "    print(\"\\t - Тест податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "  # Зачувај ги тест податоците вчитани од испитниот примерок во низа\n",
        "  test_data.append(temp_test_data)\n",
        "  test_events.append(temp_test_events)\n",
        "  print(\"Тест податоците од испитниот примерок \" + str(i) + \" се вчитани.\\n\")\n",
        "\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_data: \" + str(test_data[i-1].shape)) # test_data to predict\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_events: \" + str(len(test_events[i-1]))) # test_events\n",
        "  for j in range (1,4):\n",
        "    print(\"SBJ\" + str(format(i-1, '02')) + \" / S\" + str(format(j-1, '02')) + \"| Runs per block: \" + str(test_runs_per_block[i-1][j-1])) # runs per block in SJB01, SJ00 \n",
        "\n",
        "  to_predict_data = reshape_data_to_mne_format(test_data[i-1])\n",
        "  predictions = model2.predict(to_predict_data)\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Predictions: \" + str(len(predictions)))\n",
        "  # np.savetxt(\"predictions.csv\", predictions, delimiter=\",\")\n",
        "\n",
        "\n",
        "  # ========= FALI USTE DA SE ISPARSIRA PREDICTIONOT... NE E SREDEN OVOJ KOD DOLE =======\n",
        "\n",
        "  int_pred = np.argmax(predictions, axis=1)\n",
        "  int_ytest = np.argmax(y_test, axis=1)\n",
        "\n",
        "  session_start = 0\n",
        "  start_prediction_index = 0\n",
        "  end_prediction_index = 0\n",
        "  for session in range(0, 3):\n",
        "    print(f\"============== Сесија ({session}) ==============\")\n",
        "    for block in range(0, 50):    \n",
        "      events_per_block = test_runs_per_block[i-1][session]\n",
        "\n",
        "      start_prediction_index = session_start + (block*events_per_block)*8\n",
        "      end_prediction_index = session_start + ((block+1)*events_per_block)*8\n",
        "\n",
        "      block_prediction = int_pred[start_prediction_index:end_prediction_index]\n",
        "      prediction = np.bincount(block_prediction).argmax()\n",
        "      df.iat[session+3,block+2] = prediction+1\n",
        "      # UNCOMMENT ZA PODOBAR PRIKAZ :)\n",
        "      # print(f\"Session {session} | Block: {block} | Prediction: {prediction} | Address: {end_prediction_index}\")\n",
        "\n",
        "      print(str(prediction+1) + \",\", end=\"\")\n",
        "    session_start = end_prediction_index\n",
        "    print(\"\")\n",
        "  print(\"Stigna li do kraj: \" + str(session_start == len(predictions)))\n",
        "  print(f\"====================== Примерок ({i}) ======================\\n\\n\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Примерок (2) ======================\n",
            "Вчитување тест податоци од испитниот примерок 2...\n",
            "\t - Тест податоците од сесија 1 се вчитани.\n",
            "\t - Тест податоците од сесија 2 се вчитани.\n",
            "\t - Тест податоците од сесија 3 се вчитани.\n",
            "Тест податоците од испитниот примерок 2 се вчитани.\n",
            "\n",
            "SBJ01| Test_data: (8, 350, 8800)\n",
            "SBJ01| Test_events: 8800\n",
            "SBJ01 / S00| Runs per block: 9\n",
            "SBJ01 / S01| Runs per block: 8\n",
            "SBJ01 / S02| Runs per block: 5\n",
            "SBJ01| Predictions: 8800\n",
            "============== Сесија (0) ==============\n",
            "8,8,8,8,8,8,8,8,8,8,8,8,8,3,8,8,8,8,8,8,8,8,8,8,8,7,8,8,3,8,5,3,8,8,7,8,8,7,8,8,8,8,8,4,8,4,8,7,8,8,\n",
            "============== Сесија (1) ==============\n",
            "2,7,6,6,6,6,7,6,2,6,6,6,6,2,6,2,2,6,2,6,6,7,2,6,6,6,2,2,2,5,6,2,2,6,5,2,2,2,6,6,6,7,2,6,6,2,6,6,2,2,\n",
            "============== Сесија (2) ==============\n",
            "2,7,2,6,7,4,2,6,6,1,6,7,2,6,6,2,2,2,6,2,6,2,2,7,7,2,7,7,7,7,1,7,7,2,7,2,2,7,2,6,2,6,2,6,2,2,7,2,2,2,\n",
            "Stigna li do kraj: True\n",
            "====================== Примерок (2) ======================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYNZO333mlWa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6beee587-839a-45a7-8acf-ad6fec9fa5be"
      },
      "source": [
        "df"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUBJECT</th>\n",
              "      <th>SESSION</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>Unnamed: 52</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    SUBJECT  SESSION  1  2  3  4  5  6  7  ... 43 44 45 46 47 48 49 50 Unnamed: 52\n",
              "0         1        1  6  6  3  6  6  3  6  ...  6  4  8  3  4  5  5  7         NaN\n",
              "1         1        2  6  3  2  1  2  1  3  ...  6  2  2  2  3  6  6  2         NaN\n",
              "2         1        3  3  3  3  3  3  3  3  ...  3  3  6  3  6  7  1  6         NaN\n",
              "3         2        1  8  8  8  8  8  8  8  ...  8  4  8  4  8  7  8  8         NaN\n",
              "4         2        2  2  7  6  6  6  6  7  ...  2  6  6  2  6  6  2  2         NaN\n",
              "5         2        3  2  7  2  6  7  4  2  ...  2  6  2  2  7  2  2  2         NaN\n",
              "6         3        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "7         3        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "8         3        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "9         4        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "10        4        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "11        4        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "12        5        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "13        5        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "14        5        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "15        6        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "16        6        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "17        6        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "18        7        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "19        7        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "20        7        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "21        8        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "22        8        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "23        8        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "24        9        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "25        9        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "26        9        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "27       10        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "28       10        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "29       10        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "30       11        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "31       11        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "32       11        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "33       12        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "34       12        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "35       12        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "36       13        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "37       13        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "38       13        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "39       14        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "40       14        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "41       14        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "42       15        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "43       15        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "44       15        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "\n",
              "[45 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l56NunpdpZUN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5a862a69-d591-4be7-9301-de041ebec9dc"
      },
      "source": [
        "for i in range(3, 4): # Итерација низ секој испитен примерок\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_data = np.empty(0)\n",
        "  temp_labels = np.empty(0)\n",
        "  temp_events = np.empty(0)\n",
        "  temp_targets = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainData.mat\"\n",
        "    temp = loadmat(full_path)['trainData']\n",
        "    if temp_data.size != 0:\n",
        "      temp_data = np.concatenate((temp_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на label-ите\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainLabels.txt\"\n",
        "    with open(full_path, \"r\") as file_labels:\n",
        "      temp = file_labels.read().splitlines()\n",
        "      temp = np.repeat(temp, 8*10)\n",
        "      if temp_labels.size != 0:\n",
        "        temp_labels = np.concatenate((temp_labels, temp))\n",
        "      else:\n",
        "        temp_labels = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_events.size != 0:\n",
        "        temp_events = np.append(temp_events, temp)\n",
        "      else:\n",
        "        temp_events = np.array(temp)\n",
        "      \n",
        "\n",
        "    # Вчитување на редоследот на објекти кои се target\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainTargets.txt\"\n",
        "    with open(full_path, \"r\") as file_targets:\n",
        "      temp = file_targets.read().splitlines()\n",
        "      if temp_targets.size != 0:\n",
        "        temp_targets = np.concatenate((temp_targets, temp))\n",
        "      else:\n",
        "        temp_targets = np.array(temp)\n",
        "    print(\"\\t - Податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "\n",
        "  for j in range(4, 8): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "      \n",
        "  # Зачувај ги податоците вчитани од испитниот примерок во низа\n",
        "  data.append(temp_data)\n",
        "  labels.append(temp_labels)\n",
        "  events.append(temp_events)\n",
        "  targets.append(temp_targets)\n",
        "\n",
        "  \n",
        "  print(\"Податоците од испитниот примерок \" + str(i) + \" се вчитани.\")\n",
        "\n",
        "\n",
        "  #data = target_events_data_scaled\n",
        "  mne_array = np.swapaxes(data[i-1], 0, 2) # (епохa, канал, настан). \n",
        "  mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "  mne_array = mne_array.reshape(mne_array.shape[0],1, 8,350)\n",
        "  print(mne_array.shape)\n",
        "\n",
        "  events_arr = events[i-1].astype(np.int)\n",
        "  labels_arr = labels[i-1].astype(np.int)\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(mne_array, labels_arr-1, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "  model3 = DeepConvNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "  model3.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = Adam(0.0009))\n",
        "  checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                                verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "  #clf = RandomForestClassifier(max_depth=5)\n",
        "  #clf.fit(X_train, y_train)\n",
        "  #score = clf.score(X_test, y_test)\n",
        "  # print(score)\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  num_batch_size=100\n",
        "  num_epochs=400\n",
        "  model3.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "            validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "  score = model3.evaluate(X_test, y_test, verbose=1)\n",
        "  print(score)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t - Податоците од сесија 1 се вчитани.\n",
            "\t - Податоците од сесија 2 се вчитани.\n",
            "\t - Податоците од сесија 3 се вчитани.\n",
            "Податоците од испитниот примерок 3 се вчитани.\n",
            "(4800, 1, 8, 350)\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Train on 3600 samples, validate on 1200 samples\n",
            "Epoch 1/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.3234 - acc: 0.1663\n",
            "Epoch 00001: val_loss improved from inf to 2.11654, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 4s 1ms/sample - loss: 2.3212 - acc: 0.1661 - val_loss: 2.1165 - val_acc: 0.1992\n",
            "Epoch 2/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.1766 - acc: 0.1866\n",
            "Epoch 00002: val_loss did not improve from 2.11654\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 2.1824 - acc: 0.1858 - val_loss: 2.2188 - val_acc: 0.1733\n",
            "Epoch 3/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.1105 - acc: 0.2232\n",
            "Epoch 00003: val_loss did not improve from 2.11654\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 2.1043 - acc: 0.2247 - val_loss: 2.3057 - val_acc: 0.1975\n",
            "Epoch 4/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.0467 - acc: 0.2544\n",
            "Epoch 00004: val_loss did not improve from 2.11654\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 2.0430 - acc: 0.2567 - val_loss: 2.3500 - val_acc: 0.1775\n",
            "Epoch 5/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.9749 - acc: 0.2670\n",
            "Epoch 00005: val_loss did not improve from 2.11654\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.9822 - acc: 0.2644 - val_loss: 2.1619 - val_acc: 0.2208\n",
            "Epoch 6/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.9406 - acc: 0.2759\n",
            "Epoch 00006: val_loss did not improve from 2.11654\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.9387 - acc: 0.2758 - val_loss: 2.4288 - val_acc: 0.1808\n",
            "Epoch 7/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.8437 - acc: 0.3041\n",
            "Epoch 00007: val_loss did not improve from 2.11654\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.8437 - acc: 0.3036 - val_loss: 2.2971 - val_acc: 0.2075\n",
            "Epoch 8/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.8127 - acc: 0.3268\n",
            "Epoch 00008: val_loss did not improve from 2.11654\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 1.8107 - acc: 0.3264 - val_loss: 2.1177 - val_acc: 0.2142\n",
            "Epoch 9/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.7712 - acc: 0.3375\n",
            "Epoch 00009: val_loss improved from 2.11654 to 1.97693, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 1.7734 - acc: 0.3367 - val_loss: 1.9769 - val_acc: 0.2742\n",
            "Epoch 10/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.7193 - acc: 0.3541\n",
            "Epoch 00010: val_loss did not improve from 1.97693\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.7222 - acc: 0.3536 - val_loss: 2.2385 - val_acc: 0.2325\n",
            "Epoch 11/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.6824 - acc: 0.3673\n",
            "Epoch 00011: val_loss did not improve from 1.97693\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.6860 - acc: 0.3653 - val_loss: 2.1751 - val_acc: 0.2417\n",
            "Epoch 12/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.6766 - acc: 0.3688\n",
            "Epoch 00012: val_loss did not improve from 1.97693\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 1.6769 - acc: 0.3675 - val_loss: 2.1708 - val_acc: 0.2467\n",
            "Epoch 13/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.6447 - acc: 0.3866\n",
            "Epoch 00013: val_loss did not improve from 1.97693\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.6483 - acc: 0.3853 - val_loss: 2.1260 - val_acc: 0.2508\n",
            "Epoch 14/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.6124 - acc: 0.4017\n",
            "Epoch 00014: val_loss improved from 1.97693 to 1.92322, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 1.6149 - acc: 0.4000 - val_loss: 1.9232 - val_acc: 0.2933\n",
            "Epoch 15/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.5785 - acc: 0.4041\n",
            "Epoch 00015: val_loss did not improve from 1.92322\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.5719 - acc: 0.4072 - val_loss: 2.0420 - val_acc: 0.2717\n",
            "Epoch 16/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.5411 - acc: 0.4200\n",
            "Epoch 00016: val_loss did not improve from 1.92322\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.5463 - acc: 0.4192 - val_loss: 2.1948 - val_acc: 0.2425\n",
            "Epoch 17/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.5090 - acc: 0.4323\n",
            "Epoch 00017: val_loss did not improve from 1.92322\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.5127 - acc: 0.4306 - val_loss: 2.2591 - val_acc: 0.2458\n",
            "Epoch 18/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4601 - acc: 0.4450\n",
            "Epoch 00018: val_loss improved from 1.92322 to 1.91642, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 1.4731 - acc: 0.4403 - val_loss: 1.9164 - val_acc: 0.3250\n",
            "Epoch 19/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.4554 - acc: 0.4491\n",
            "Epoch 00019: val_loss did not improve from 1.91642\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 1.4461 - acc: 0.4539 - val_loss: 2.0115 - val_acc: 0.2950\n",
            "Epoch 20/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.4458 - acc: 0.4582\n",
            "Epoch 00020: val_loss improved from 1.91642 to 1.85972, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 1.4461 - acc: 0.4556 - val_loss: 1.8597 - val_acc: 0.3408\n",
            "Epoch 21/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.3798 - acc: 0.4832\n",
            "Epoch 00021: val_loss did not improve from 1.85972\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 1.3851 - acc: 0.4806 - val_loss: 1.9150 - val_acc: 0.3142\n",
            "Epoch 22/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.3337 - acc: 0.5106\n",
            "Epoch 00022: val_loss improved from 1.85972 to 1.82923, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 1.3319 - acc: 0.5103 - val_loss: 1.8292 - val_acc: 0.3258\n",
            "Epoch 23/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.3142 - acc: 0.5177\n",
            "Epoch 00023: val_loss improved from 1.82923 to 1.64597, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 1.3175 - acc: 0.5178 - val_loss: 1.6460 - val_acc: 0.3975\n",
            "Epoch 24/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.2730 - acc: 0.5341\n",
            "Epoch 00024: val_loss did not improve from 1.64597\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.2779 - acc: 0.5297 - val_loss: 1.7088 - val_acc: 0.3475\n",
            "Epoch 25/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.2553 - acc: 0.5255\n",
            "Epoch 00025: val_loss did not improve from 1.64597\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.2589 - acc: 0.5242 - val_loss: 1.7361 - val_acc: 0.3542\n",
            "Epoch 26/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.2167 - acc: 0.5449\n",
            "Epoch 00026: val_loss did not improve from 1.64597\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 1.2266 - acc: 0.5406 - val_loss: 1.8143 - val_acc: 0.3508\n",
            "Epoch 27/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2275 - acc: 0.5379\n",
            "Epoch 00027: val_loss improved from 1.64597 to 1.50037, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 1.2300 - acc: 0.5378 - val_loss: 1.5004 - val_acc: 0.4358\n",
            "Epoch 28/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1941 - acc: 0.5597\n",
            "Epoch 00028: val_loss did not improve from 1.50037\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.2045 - acc: 0.5550 - val_loss: 1.5679 - val_acc: 0.4067\n",
            "Epoch 29/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.1561 - acc: 0.5722\n",
            "Epoch 00029: val_loss improved from 1.50037 to 1.48610, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 1.1537 - acc: 0.5744 - val_loss: 1.4861 - val_acc: 0.4475\n",
            "Epoch 30/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1171 - acc: 0.5861\n",
            "Epoch 00030: val_loss did not improve from 1.48610\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 1.1222 - acc: 0.5856 - val_loss: 1.4973 - val_acc: 0.4425\n",
            "Epoch 31/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0894 - acc: 0.5991\n",
            "Epoch 00031: val_loss improved from 1.48610 to 1.44741, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 1.0899 - acc: 0.5992 - val_loss: 1.4474 - val_acc: 0.4433\n",
            "Epoch 32/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.0755 - acc: 0.6059\n",
            "Epoch 00032: val_loss did not improve from 1.44741\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.0852 - acc: 0.6028 - val_loss: 1.4562 - val_acc: 0.4700\n",
            "Epoch 33/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0553 - acc: 0.6147\n",
            "Epoch 00033: val_loss improved from 1.44741 to 1.40075, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 1.0573 - acc: 0.6139 - val_loss: 1.4008 - val_acc: 0.4717\n",
            "Epoch 34/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.0441 - acc: 0.6189\n",
            "Epoch 00034: val_loss improved from 1.40075 to 1.38633, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 1.0435 - acc: 0.6189 - val_loss: 1.3863 - val_acc: 0.4750\n",
            "Epoch 35/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.9869 - acc: 0.6422\n",
            "Epoch 00035: val_loss did not improve from 1.38633\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 1.0049 - acc: 0.6328 - val_loss: 1.4253 - val_acc: 0.4750\n",
            "Epoch 36/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9507 - acc: 0.6552\n",
            "Epoch 00036: val_loss improved from 1.38633 to 1.30452, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.9650 - acc: 0.6494 - val_loss: 1.3045 - val_acc: 0.5125\n",
            "Epoch 37/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9644 - acc: 0.6511\n",
            "Epoch 00037: val_loss improved from 1.30452 to 1.30252, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.9638 - acc: 0.6519 - val_loss: 1.3025 - val_acc: 0.4958\n",
            "Epoch 38/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9406 - acc: 0.6657\n",
            "Epoch 00038: val_loss improved from 1.30252 to 1.22911, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.9421 - acc: 0.6650 - val_loss: 1.2291 - val_acc: 0.5375\n",
            "Epoch 39/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9010 - acc: 0.6755\n",
            "Epoch 00039: val_loss did not improve from 1.22911\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.9059 - acc: 0.6742 - val_loss: 1.3128 - val_acc: 0.5175\n",
            "Epoch 40/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8913 - acc: 0.6794\n",
            "Epoch 00040: val_loss did not improve from 1.22911\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.8984 - acc: 0.6761 - val_loss: 1.4072 - val_acc: 0.4700\n",
            "Epoch 41/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8824 - acc: 0.6941\n",
            "Epoch 00041: val_loss did not improve from 1.22911\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.8840 - acc: 0.6914 - val_loss: 1.3123 - val_acc: 0.5225\n",
            "Epoch 42/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8326 - acc: 0.7091\n",
            "Epoch 00042: val_loss improved from 1.22911 to 1.14514, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.8372 - acc: 0.7064 - val_loss: 1.1451 - val_acc: 0.5708\n",
            "Epoch 43/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8414 - acc: 0.7019\n",
            "Epoch 00043: val_loss did not improve from 1.14514\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.8469 - acc: 0.7017 - val_loss: 1.1862 - val_acc: 0.5417\n",
            "Epoch 44/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8443 - acc: 0.7006\n",
            "Epoch 00044: val_loss improved from 1.14514 to 1.08003, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.8499 - acc: 0.7008 - val_loss: 1.0800 - val_acc: 0.5958\n",
            "Epoch 45/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7841 - acc: 0.7239\n",
            "Epoch 00045: val_loss did not improve from 1.08003\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.7887 - acc: 0.7208 - val_loss: 1.1286 - val_acc: 0.5850\n",
            "Epoch 46/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7593 - acc: 0.7352\n",
            "Epoch 00046: val_loss improved from 1.08003 to 1.06623, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.7691 - acc: 0.7308 - val_loss: 1.0662 - val_acc: 0.6150\n",
            "Epoch 47/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7628 - acc: 0.7385\n",
            "Epoch 00047: val_loss improved from 1.06623 to 1.05483, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.7650 - acc: 0.7369 - val_loss: 1.0548 - val_acc: 0.5975\n",
            "Epoch 48/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7456 - acc: 0.7412\n",
            "Epoch 00048: val_loss improved from 1.05483 to 0.98750, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.7506 - acc: 0.7394 - val_loss: 0.9875 - val_acc: 0.6625\n",
            "Epoch 49/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7471 - acc: 0.7330\n",
            "Epoch 00049: val_loss improved from 0.98750 to 0.97890, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.7490 - acc: 0.7303 - val_loss: 0.9789 - val_acc: 0.6483\n",
            "Epoch 50/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7080 - acc: 0.7520\n",
            "Epoch 00050: val_loss did not improve from 0.97890\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.7082 - acc: 0.7511 - val_loss: 1.0228 - val_acc: 0.6342\n",
            "Epoch 51/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7200 - acc: 0.7433\n",
            "Epoch 00051: val_loss improved from 0.97890 to 0.96789, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.7145 - acc: 0.7461 - val_loss: 0.9679 - val_acc: 0.6558\n",
            "Epoch 52/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6840 - acc: 0.7603\n",
            "Epoch 00052: val_loss did not improve from 0.96789\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.6839 - acc: 0.7603 - val_loss: 0.9801 - val_acc: 0.6408\n",
            "Epoch 53/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.6844 - acc: 0.7597\n",
            "Epoch 00053: val_loss improved from 0.96789 to 0.95683, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.6841 - acc: 0.7583 - val_loss: 0.9568 - val_acc: 0.6517\n",
            "Epoch 54/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6651 - acc: 0.7806\n",
            "Epoch 00054: val_loss improved from 0.95683 to 0.93497, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.6708 - acc: 0.7772 - val_loss: 0.9350 - val_acc: 0.6517\n",
            "Epoch 55/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.6161 - acc: 0.7979\n",
            "Epoch 00055: val_loss improved from 0.93497 to 0.87693, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.6245 - acc: 0.7936 - val_loss: 0.8769 - val_acc: 0.6800\n",
            "Epoch 56/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6345 - acc: 0.7837\n",
            "Epoch 00056: val_loss improved from 0.87693 to 0.87307, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.6375 - acc: 0.7819 - val_loss: 0.8731 - val_acc: 0.6850\n",
            "Epoch 57/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.5899 - acc: 0.8062\n",
            "Epoch 00057: val_loss improved from 0.87307 to 0.82278, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.5920 - acc: 0.8047 - val_loss: 0.8228 - val_acc: 0.7158\n",
            "Epoch 58/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.6323 - acc: 0.7803\n",
            "Epoch 00058: val_loss did not improve from 0.82278\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.6325 - acc: 0.7781 - val_loss: 0.8653 - val_acc: 0.6958\n",
            "Epoch 59/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5786 - acc: 0.8051\n",
            "Epoch 00059: val_loss did not improve from 0.82278\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.5842 - acc: 0.8014 - val_loss: 0.8590 - val_acc: 0.7008\n",
            "Epoch 60/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5748 - acc: 0.8009\n",
            "Epoch 00060: val_loss did not improve from 0.82278\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.5790 - acc: 0.7997 - val_loss: 0.8678 - val_acc: 0.6875\n",
            "Epoch 61/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5417 - acc: 0.8323\n",
            "Epoch 00061: val_loss improved from 0.82278 to 0.80061, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.5416 - acc: 0.8322 - val_loss: 0.8006 - val_acc: 0.7117\n",
            "Epoch 62/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5509 - acc: 0.8097\n",
            "Epoch 00062: val_loss improved from 0.80061 to 0.76721, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.5516 - acc: 0.8103 - val_loss: 0.7672 - val_acc: 0.7258\n",
            "Epoch 63/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5473 - acc: 0.8170\n",
            "Epoch 00063: val_loss did not improve from 0.76721\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.5496 - acc: 0.8158 - val_loss: 0.7786 - val_acc: 0.7267\n",
            "Epoch 64/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5116 - acc: 0.8394\n",
            "Epoch 00064: val_loss improved from 0.76721 to 0.71982, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.5151 - acc: 0.8358 - val_loss: 0.7198 - val_acc: 0.7375\n",
            "Epoch 65/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4968 - acc: 0.8382\n",
            "Epoch 00065: val_loss did not improve from 0.71982\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.5037 - acc: 0.8353 - val_loss: 0.7701 - val_acc: 0.7225\n",
            "Epoch 66/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5141 - acc: 0.8300\n",
            "Epoch 00066: val_loss did not improve from 0.71982\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.5178 - acc: 0.8281 - val_loss: 0.7811 - val_acc: 0.7100\n",
            "Epoch 67/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4893 - acc: 0.8366\n",
            "Epoch 00067: val_loss improved from 0.71982 to 0.69454, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.4908 - acc: 0.8367 - val_loss: 0.6945 - val_acc: 0.7625\n",
            "Epoch 68/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4729 - acc: 0.8469\n",
            "Epoch 00068: val_loss did not improve from 0.69454\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.4739 - acc: 0.8478 - val_loss: 0.6970 - val_acc: 0.7517\n",
            "Epoch 69/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4592 - acc: 0.8509\n",
            "Epoch 00069: val_loss did not improve from 0.69454\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.4614 - acc: 0.8508 - val_loss: 0.7264 - val_acc: 0.7408\n",
            "Epoch 70/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.4670 - acc: 0.8509\n",
            "Epoch 00070: val_loss did not improve from 0.69454\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.4707 - acc: 0.8483 - val_loss: 0.7943 - val_acc: 0.7183\n",
            "Epoch 71/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4545 - acc: 0.8533\n",
            "Epoch 00071: val_loss improved from 0.69454 to 0.60710, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.4572 - acc: 0.8528 - val_loss: 0.6071 - val_acc: 0.7975\n",
            "Epoch 72/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4214 - acc: 0.8742\n",
            "Epoch 00072: val_loss improved from 0.60710 to 0.59731, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.4307 - acc: 0.8719 - val_loss: 0.5973 - val_acc: 0.8008\n",
            "Epoch 73/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.4382 - acc: 0.8628\n",
            "Epoch 00073: val_loss improved from 0.59731 to 0.58836, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.4409 - acc: 0.8625 - val_loss: 0.5884 - val_acc: 0.8133\n",
            "Epoch 74/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.4305 - acc: 0.8612\n",
            "Epoch 00074: val_loss did not improve from 0.58836\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.4354 - acc: 0.8606 - val_loss: 0.6087 - val_acc: 0.7908\n",
            "Epoch 75/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4149 - acc: 0.8665\n",
            "Epoch 00075: val_loss did not improve from 0.58836\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.4174 - acc: 0.8656 - val_loss: 0.6850 - val_acc: 0.7633\n",
            "Epoch 76/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4161 - acc: 0.8654\n",
            "Epoch 00076: val_loss did not improve from 0.58836\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.4199 - acc: 0.8628 - val_loss: 0.6083 - val_acc: 0.7875\n",
            "Epoch 77/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4186 - acc: 0.8660\n",
            "Epoch 00077: val_loss did not improve from 0.58836\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.4224 - acc: 0.8653 - val_loss: 0.6250 - val_acc: 0.7808\n",
            "Epoch 78/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3878 - acc: 0.8821\n",
            "Epoch 00078: val_loss improved from 0.58836 to 0.56019, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.3877 - acc: 0.8808 - val_loss: 0.5602 - val_acc: 0.8175\n",
            "Epoch 79/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.8757\n",
            "Epoch 00079: val_loss did not improve from 0.56019\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.3963 - acc: 0.8747 - val_loss: 0.6037 - val_acc: 0.7950\n",
            "Epoch 80/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3817 - acc: 0.8777\n",
            "Epoch 00080: val_loss did not improve from 0.56019\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.3851 - acc: 0.8767 - val_loss: 0.5757 - val_acc: 0.8225\n",
            "Epoch 81/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3799 - acc: 0.8831\n",
            "Epoch 00081: val_loss improved from 0.56019 to 0.52509, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.3768 - acc: 0.8850 - val_loss: 0.5251 - val_acc: 0.8292\n",
            "Epoch 82/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3964 - acc: 0.8674\n",
            "Epoch 00082: val_loss did not improve from 0.52509\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.3944 - acc: 0.8683 - val_loss: 0.5649 - val_acc: 0.7933\n",
            "Epoch 83/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3702 - acc: 0.8853\n",
            "Epoch 00083: val_loss did not improve from 0.52509\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.3672 - acc: 0.8844 - val_loss: 0.6088 - val_acc: 0.7742\n",
            "Epoch 84/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3740 - acc: 0.8735\n",
            "Epoch 00084: val_loss improved from 0.52509 to 0.50370, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.3765 - acc: 0.8733 - val_loss: 0.5037 - val_acc: 0.8367\n",
            "Epoch 85/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3699 - acc: 0.8774\n",
            "Epoch 00085: val_loss did not improve from 0.50370\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.3719 - acc: 0.8769 - val_loss: 0.5112 - val_acc: 0.8317\n",
            "Epoch 86/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3594 - acc: 0.8894\n",
            "Epoch 00086: val_loss did not improve from 0.50370\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.3610 - acc: 0.8881 - val_loss: 0.5149 - val_acc: 0.8317\n",
            "Epoch 87/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3321 - acc: 0.8985\n",
            "Epoch 00087: val_loss improved from 0.50370 to 0.45455, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.3326 - acc: 0.8981 - val_loss: 0.4546 - val_acc: 0.8658\n",
            "Epoch 88/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3510 - acc: 0.8909\n",
            "Epoch 00088: val_loss did not improve from 0.45455\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.3469 - acc: 0.8919 - val_loss: 0.4819 - val_acc: 0.8342\n",
            "Epoch 89/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8900\n",
            "Epoch 00089: val_loss did not improve from 0.45455\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.3491 - acc: 0.8897 - val_loss: 0.4720 - val_acc: 0.8425\n",
            "Epoch 90/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3530 - acc: 0.8861\n",
            "Epoch 00090: val_loss did not improve from 0.45455\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.3568 - acc: 0.8833 - val_loss: 0.5059 - val_acc: 0.8325\n",
            "Epoch 91/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3322 - acc: 0.8971\n",
            "Epoch 00091: val_loss did not improve from 0.45455\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.3321 - acc: 0.8972 - val_loss: 0.4653 - val_acc: 0.8542\n",
            "Epoch 92/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3406 - acc: 0.8909\n",
            "Epoch 00092: val_loss improved from 0.45455 to 0.43394, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.3427 - acc: 0.8894 - val_loss: 0.4339 - val_acc: 0.8625\n",
            "Epoch 93/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3041 - acc: 0.9078\n",
            "Epoch 00093: val_loss improved from 0.43394 to 0.43288, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.3138 - acc: 0.9019 - val_loss: 0.4329 - val_acc: 0.8817\n",
            "Epoch 94/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3194 - acc: 0.8961\n",
            "Epoch 00094: val_loss improved from 0.43288 to 0.41629, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.3202 - acc: 0.8961 - val_loss: 0.4163 - val_acc: 0.8708\n",
            "Epoch 95/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2846 - acc: 0.9150\n",
            "Epoch 00095: val_loss did not improve from 0.41629\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2874 - acc: 0.9142 - val_loss: 0.4198 - val_acc: 0.8633\n",
            "Epoch 96/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3027 - acc: 0.9088\n",
            "Epoch 00096: val_loss did not improve from 0.41629\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.3039 - acc: 0.9086 - val_loss: 0.4483 - val_acc: 0.8558\n",
            "Epoch 97/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.9066\n",
            "Epoch 00097: val_loss did not improve from 0.41629\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.2998 - acc: 0.9047 - val_loss: 0.4836 - val_acc: 0.8433\n",
            "Epoch 98/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2629 - acc: 0.9232\n",
            "Epoch 00098: val_loss improved from 0.41629 to 0.40705, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.2630 - acc: 0.9233 - val_loss: 0.4071 - val_acc: 0.8758\n",
            "Epoch 99/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2769 - acc: 0.9132\n",
            "Epoch 00099: val_loss improved from 0.40705 to 0.37161, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.2761 - acc: 0.9144 - val_loss: 0.3716 - val_acc: 0.8808\n",
            "Epoch 100/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2871 - acc: 0.9162\n",
            "Epoch 00100: val_loss did not improve from 0.37161\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2864 - acc: 0.9150 - val_loss: 0.3769 - val_acc: 0.8883\n",
            "Epoch 101/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2839 - acc: 0.9103\n",
            "Epoch 00101: val_loss improved from 0.37161 to 0.35977, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.2882 - acc: 0.9092 - val_loss: 0.3598 - val_acc: 0.9025\n",
            "Epoch 102/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2822 - acc: 0.9171\n",
            "Epoch 00102: val_loss did not improve from 0.35977\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2825 - acc: 0.9164 - val_loss: 0.4441 - val_acc: 0.8617\n",
            "Epoch 103/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2861 - acc: 0.9109\n",
            "Epoch 00103: val_loss did not improve from 0.35977\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2864 - acc: 0.9103 - val_loss: 0.3676 - val_acc: 0.8900\n",
            "Epoch 104/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2836 - acc: 0.9170\n",
            "Epoch 00104: val_loss did not improve from 0.35977\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2863 - acc: 0.9164 - val_loss: 0.4064 - val_acc: 0.8725\n",
            "Epoch 105/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2780 - acc: 0.9130\n",
            "Epoch 00105: val_loss did not improve from 0.35977\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2786 - acc: 0.9125 - val_loss: 0.3888 - val_acc: 0.8733\n",
            "Epoch 106/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2666 - acc: 0.9175\n",
            "Epoch 00106: val_loss did not improve from 0.35977\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2661 - acc: 0.9164 - val_loss: 0.4245 - val_acc: 0.8642\n",
            "Epoch 107/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2485 - acc: 0.9242\n",
            "Epoch 00107: val_loss improved from 0.35977 to 0.34849, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.2498 - acc: 0.9233 - val_loss: 0.3485 - val_acc: 0.9083\n",
            "Epoch 108/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2587 - acc: 0.9256\n",
            "Epoch 00108: val_loss did not improve from 0.34849\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2582 - acc: 0.9253 - val_loss: 0.3543 - val_acc: 0.8925\n",
            "Epoch 109/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9260\n",
            "Epoch 00109: val_loss did not improve from 0.34849\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2545 - acc: 0.9253 - val_loss: 0.3629 - val_acc: 0.8950\n",
            "Epoch 110/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2625 - acc: 0.9248\n",
            "Epoch 00110: val_loss improved from 0.34849 to 0.29791, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.2619 - acc: 0.9244 - val_loss: 0.2979 - val_acc: 0.9200\n",
            "Epoch 111/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2528 - acc: 0.9291\n",
            "Epoch 00111: val_loss did not improve from 0.29791\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2563 - acc: 0.9272 - val_loss: 0.3997 - val_acc: 0.8658\n",
            "Epoch 112/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2636 - acc: 0.9179\n",
            "Epoch 00112: val_loss improved from 0.29791 to 0.28707, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.2666 - acc: 0.9158 - val_loss: 0.2871 - val_acc: 0.9267\n",
            "Epoch 113/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2399 - acc: 0.9294\n",
            "Epoch 00113: val_loss did not improve from 0.28707\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2387 - acc: 0.9303 - val_loss: 0.2997 - val_acc: 0.9142\n",
            "Epoch 114/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2488 - acc: 0.9182\n",
            "Epoch 00114: val_loss did not improve from 0.28707\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2484 - acc: 0.9189 - val_loss: 0.3325 - val_acc: 0.9025\n",
            "Epoch 115/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2615 - acc: 0.9206\n",
            "Epoch 00115: val_loss did not improve from 0.28707\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2609 - acc: 0.9214 - val_loss: 0.3140 - val_acc: 0.9100\n",
            "Epoch 116/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2509 - acc: 0.9232\n",
            "Epoch 00116: val_loss did not improve from 0.28707\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2480 - acc: 0.9247 - val_loss: 0.3417 - val_acc: 0.8950\n",
            "Epoch 117/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2409 - acc: 0.9294\n",
            "Epoch 00117: val_loss did not improve from 0.28707\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2415 - acc: 0.9281 - val_loss: 0.3290 - val_acc: 0.8992\n",
            "Epoch 118/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2437 - acc: 0.9229\n",
            "Epoch 00118: val_loss did not improve from 0.28707\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2451 - acc: 0.9222 - val_loss: 0.3318 - val_acc: 0.9000\n",
            "Epoch 119/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2310 - acc: 0.9338\n",
            "Epoch 00119: val_loss improved from 0.28707 to 0.27689, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.2318 - acc: 0.9333 - val_loss: 0.2769 - val_acc: 0.9275\n",
            "Epoch 120/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2171 - acc: 0.9403\n",
            "Epoch 00120: val_loss did not improve from 0.27689\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2187 - acc: 0.9400 - val_loss: 0.2898 - val_acc: 0.9242\n",
            "Epoch 121/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2388 - acc: 0.9275\n",
            "Epoch 00121: val_loss did not improve from 0.27689\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.2361 - acc: 0.9292 - val_loss: 0.3196 - val_acc: 0.8992\n",
            "Epoch 122/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2356 - acc: 0.9262\n",
            "Epoch 00122: val_loss did not improve from 0.27689\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2414 - acc: 0.9250 - val_loss: 0.3389 - val_acc: 0.8975\n",
            "Epoch 123/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2409 - acc: 0.9271\n",
            "Epoch 00123: val_loss did not improve from 0.27689\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2413 - acc: 0.9267 - val_loss: 0.2948 - val_acc: 0.9092\n",
            "Epoch 124/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2086 - acc: 0.9416\n",
            "Epoch 00124: val_loss did not improve from 0.27689\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2088 - acc: 0.9428 - val_loss: 0.2865 - val_acc: 0.9208\n",
            "Epoch 125/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2213 - acc: 0.9360\n",
            "Epoch 00125: val_loss did not improve from 0.27689\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2215 - acc: 0.9367 - val_loss: 0.3609 - val_acc: 0.8875\n",
            "Epoch 126/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9280\n",
            "Epoch 00126: val_loss did not improve from 0.27689\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2360 - acc: 0.9264 - val_loss: 0.3439 - val_acc: 0.8900\n",
            "Epoch 127/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2150 - acc: 0.9359\n",
            "Epoch 00127: val_loss did not improve from 0.27689\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2168 - acc: 0.9353 - val_loss: 0.3094 - val_acc: 0.9042\n",
            "Epoch 128/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2345 - acc: 0.9282\n",
            "Epoch 00128: val_loss improved from 0.27689 to 0.26053, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.2354 - acc: 0.9289 - val_loss: 0.2605 - val_acc: 0.9250\n",
            "Epoch 129/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2210 - acc: 0.9382\n",
            "Epoch 00129: val_loss did not improve from 0.26053\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2250 - acc: 0.9361 - val_loss: 0.2794 - val_acc: 0.9217\n",
            "Epoch 130/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2190 - acc: 0.9326\n",
            "Epoch 00130: val_loss did not improve from 0.26053\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2180 - acc: 0.9322 - val_loss: 0.3099 - val_acc: 0.9125\n",
            "Epoch 131/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2089 - acc: 0.9379\n",
            "Epoch 00131: val_loss did not improve from 0.26053\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2097 - acc: 0.9378 - val_loss: 0.2697 - val_acc: 0.9267\n",
            "Epoch 132/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2026 - acc: 0.9418\n",
            "Epoch 00132: val_loss did not improve from 0.26053\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2043 - acc: 0.9411 - val_loss: 0.3020 - val_acc: 0.9092\n",
            "Epoch 133/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2178 - acc: 0.9322\n",
            "Epoch 00133: val_loss improved from 0.26053 to 0.25119, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.2223 - acc: 0.9308 - val_loss: 0.2512 - val_acc: 0.9308\n",
            "Epoch 134/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2169 - acc: 0.9352\n",
            "Epoch 00134: val_loss did not improve from 0.25119\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2172 - acc: 0.9336 - val_loss: 0.2546 - val_acc: 0.9317\n",
            "Epoch 135/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2202 - acc: 0.9287\n",
            "Epoch 00135: val_loss did not improve from 0.25119\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2182 - acc: 0.9319 - val_loss: 0.3552 - val_acc: 0.8933\n",
            "Epoch 136/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2168 - acc: 0.9312\n",
            "Epoch 00136: val_loss improved from 0.25119 to 0.23824, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.2192 - acc: 0.9297 - val_loss: 0.2382 - val_acc: 0.9408\n",
            "Epoch 137/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2253 - acc: 0.9291\n",
            "Epoch 00137: val_loss did not improve from 0.23824\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2225 - acc: 0.9306 - val_loss: 0.2788 - val_acc: 0.9233\n",
            "Epoch 138/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2090 - acc: 0.9424\n",
            "Epoch 00138: val_loss did not improve from 0.23824\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2066 - acc: 0.9439 - val_loss: 0.2525 - val_acc: 0.9242\n",
            "Epoch 139/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1951 - acc: 0.9409\n",
            "Epoch 00139: val_loss did not improve from 0.23824\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1960 - acc: 0.9408 - val_loss: 0.2698 - val_acc: 0.9183\n",
            "Epoch 140/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2071 - acc: 0.9326\n",
            "Epoch 00140: val_loss did not improve from 0.23824\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2064 - acc: 0.9325 - val_loss: 0.3490 - val_acc: 0.8883\n",
            "Epoch 141/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9394\n",
            "Epoch 00141: val_loss improved from 0.23824 to 0.23092, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.2001 - acc: 0.9400 - val_loss: 0.2309 - val_acc: 0.9317\n",
            "Epoch 142/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1977 - acc: 0.9406\n",
            "Epoch 00142: val_loss did not improve from 0.23092\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1991 - acc: 0.9403 - val_loss: 0.2587 - val_acc: 0.9275\n",
            "Epoch 143/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1931 - acc: 0.9444\n",
            "Epoch 00143: val_loss did not improve from 0.23092\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1977 - acc: 0.9422 - val_loss: 0.2801 - val_acc: 0.9142\n",
            "Epoch 144/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9397\n",
            "Epoch 00144: val_loss did not improve from 0.23092\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1955 - acc: 0.9403 - val_loss: 0.2986 - val_acc: 0.9008\n",
            "Epoch 145/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1783 - acc: 0.9503\n",
            "Epoch 00145: val_loss did not improve from 0.23092\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1785 - acc: 0.9503 - val_loss: 0.2692 - val_acc: 0.9125\n",
            "Epoch 146/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1934 - acc: 0.9411\n",
            "Epoch 00146: val_loss did not improve from 0.23092\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1935 - acc: 0.9414 - val_loss: 0.2382 - val_acc: 0.9283\n",
            "Epoch 147/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2021 - acc: 0.9379\n",
            "Epoch 00147: val_loss did not improve from 0.23092\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.2039 - acc: 0.9378 - val_loss: 0.2772 - val_acc: 0.9167\n",
            "Epoch 148/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1782 - acc: 0.9479\n",
            "Epoch 00148: val_loss did not improve from 0.23092\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1793 - acc: 0.9478 - val_loss: 0.2796 - val_acc: 0.9208\n",
            "Epoch 149/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1988 - acc: 0.9428\n",
            "Epoch 00149: val_loss did not improve from 0.23092\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2090 - acc: 0.9386 - val_loss: 0.2406 - val_acc: 0.9308\n",
            "Epoch 150/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1951 - acc: 0.9415\n",
            "Epoch 00150: val_loss did not improve from 0.23092\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1946 - acc: 0.9414 - val_loss: 0.2589 - val_acc: 0.9225\n",
            "Epoch 151/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1908 - acc: 0.9406\n",
            "Epoch 00151: val_loss did not improve from 0.23092\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1930 - acc: 0.9394 - val_loss: 0.2611 - val_acc: 0.9300\n",
            "Epoch 152/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9360\n",
            "Epoch 00152: val_loss did not improve from 0.23092\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2148 - acc: 0.9364 - val_loss: 0.2589 - val_acc: 0.9192\n",
            "Epoch 153/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2126 - acc: 0.9311\n",
            "Epoch 00153: val_loss did not improve from 0.23092\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2156 - acc: 0.9300 - val_loss: 0.2312 - val_acc: 0.9342\n",
            "Epoch 154/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1975 - acc: 0.9406\n",
            "Epoch 00154: val_loss improved from 0.23092 to 0.22182, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1973 - acc: 0.9406 - val_loss: 0.2218 - val_acc: 0.9400\n",
            "Epoch 155/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1885 - acc: 0.9429\n",
            "Epoch 00155: val_loss did not improve from 0.22182\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1878 - acc: 0.9431 - val_loss: 0.2382 - val_acc: 0.9425\n",
            "Epoch 156/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1926 - acc: 0.9397\n",
            "Epoch 00156: val_loss improved from 0.22182 to 0.21828, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1932 - acc: 0.9400 - val_loss: 0.2183 - val_acc: 0.9400\n",
            "Epoch 157/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1909 - acc: 0.9391\n",
            "Epoch 00157: val_loss did not improve from 0.21828\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1914 - acc: 0.9389 - val_loss: 0.2894 - val_acc: 0.9083\n",
            "Epoch 158/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9374\n",
            "Epoch 00158: val_loss did not improve from 0.21828\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2018 - acc: 0.9372 - val_loss: 0.2210 - val_acc: 0.9358\n",
            "Epoch 159/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1912 - acc: 0.9406\n",
            "Epoch 00159: val_loss did not improve from 0.21828\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1906 - acc: 0.9400 - val_loss: 0.2491 - val_acc: 0.9275\n",
            "Epoch 160/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1726 - acc: 0.9467\n",
            "Epoch 00160: val_loss improved from 0.21828 to 0.19861, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1723 - acc: 0.9478 - val_loss: 0.1986 - val_acc: 0.9483\n",
            "Epoch 161/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1802 - acc: 0.9525\n",
            "Epoch 00161: val_loss did not improve from 0.19861\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1871 - acc: 0.9492 - val_loss: 0.2636 - val_acc: 0.9208\n",
            "Epoch 162/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1915 - acc: 0.9417\n",
            "Epoch 00162: val_loss did not improve from 0.19861\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1899 - acc: 0.9425 - val_loss: 0.2085 - val_acc: 0.9417\n",
            "Epoch 163/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1760 - acc: 0.9463\n",
            "Epoch 00163: val_loss did not improve from 0.19861\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.1812 - acc: 0.9444 - val_loss: 0.2444 - val_acc: 0.9317\n",
            "Epoch 164/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1706 - acc: 0.9521\n",
            "Epoch 00164: val_loss did not improve from 0.19861\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1724 - acc: 0.9519 - val_loss: 0.2423 - val_acc: 0.9292\n",
            "Epoch 165/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1501 - acc: 0.9573\n",
            "Epoch 00165: val_loss did not improve from 0.19861\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1498 - acc: 0.9561 - val_loss: 0.2189 - val_acc: 0.9367\n",
            "Epoch 166/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1868 - acc: 0.9447\n",
            "Epoch 00166: val_loss did not improve from 0.19861\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1909 - acc: 0.9436 - val_loss: 0.2102 - val_acc: 0.9425\n",
            "Epoch 167/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1837 - acc: 0.9458\n",
            "Epoch 00167: val_loss improved from 0.19861 to 0.18029, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1831 - acc: 0.9456 - val_loss: 0.1803 - val_acc: 0.9517\n",
            "Epoch 168/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1840 - acc: 0.9409\n",
            "Epoch 00168: val_loss did not improve from 0.18029\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1851 - acc: 0.9403 - val_loss: 0.2070 - val_acc: 0.9358\n",
            "Epoch 169/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2003 - acc: 0.9403\n",
            "Epoch 00169: val_loss did not improve from 0.18029\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2010 - acc: 0.9400 - val_loss: 0.2151 - val_acc: 0.9408\n",
            "Epoch 170/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1831 - acc: 0.9447\n",
            "Epoch 00170: val_loss did not improve from 0.18029\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1849 - acc: 0.9439 - val_loss: 0.2476 - val_acc: 0.9267\n",
            "Epoch 171/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1940 - acc: 0.9406\n",
            "Epoch 00171: val_loss did not improve from 0.18029\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1935 - acc: 0.9414 - val_loss: 0.2434 - val_acc: 0.9292\n",
            "Epoch 172/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1803 - acc: 0.9448\n",
            "Epoch 00172: val_loss did not improve from 0.18029\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1794 - acc: 0.9450 - val_loss: 0.2270 - val_acc: 0.9367\n",
            "Epoch 173/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1634 - acc: 0.9497\n",
            "Epoch 00173: val_loss did not improve from 0.18029\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1637 - acc: 0.9497 - val_loss: 0.2014 - val_acc: 0.9433\n",
            "Epoch 174/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9497\n",
            "Epoch 00174: val_loss improved from 0.18029 to 0.17861, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1764 - acc: 0.9500 - val_loss: 0.1786 - val_acc: 0.9492\n",
            "Epoch 175/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9517\n",
            "Epoch 00175: val_loss improved from 0.17861 to 0.16940, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1616 - acc: 0.9514 - val_loss: 0.1694 - val_acc: 0.9625\n",
            "Epoch 176/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1598 - acc: 0.9541\n",
            "Epoch 00176: val_loss did not improve from 0.16940\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1634 - acc: 0.9519 - val_loss: 0.1945 - val_acc: 0.9467\n",
            "Epoch 177/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1693 - acc: 0.9464\n",
            "Epoch 00177: val_loss did not improve from 0.16940\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1682 - acc: 0.9467 - val_loss: 0.2217 - val_acc: 0.9358\n",
            "Epoch 178/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1777 - acc: 0.9466\n",
            "Epoch 00178: val_loss did not improve from 0.16940\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.1805 - acc: 0.9464 - val_loss: 0.2111 - val_acc: 0.9417\n",
            "Epoch 179/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9491\n",
            "Epoch 00179: val_loss did not improve from 0.16940\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1724 - acc: 0.9489 - val_loss: 0.1961 - val_acc: 0.9500\n",
            "Epoch 180/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1904 - acc: 0.9366\n",
            "Epoch 00180: val_loss did not improve from 0.16940\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1905 - acc: 0.9367 - val_loss: 0.1862 - val_acc: 0.9442\n",
            "Epoch 181/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9463\n",
            "Epoch 00181: val_loss did not improve from 0.16940\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1847 - acc: 0.9464 - val_loss: 0.2118 - val_acc: 0.9392\n",
            "Epoch 182/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1685 - acc: 0.9515\n",
            "Epoch 00182: val_loss did not improve from 0.16940\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1693 - acc: 0.9517 - val_loss: 0.1971 - val_acc: 0.9358\n",
            "Epoch 183/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1650 - acc: 0.9497\n",
            "Epoch 00183: val_loss improved from 0.16940 to 0.16601, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1631 - acc: 0.9503 - val_loss: 0.1660 - val_acc: 0.9567\n",
            "Epoch 184/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1770 - acc: 0.9453\n",
            "Epoch 00184: val_loss did not improve from 0.16601\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1763 - acc: 0.9450 - val_loss: 0.1804 - val_acc: 0.9483\n",
            "Epoch 185/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1700 - acc: 0.9494\n",
            "Epoch 00185: val_loss improved from 0.16601 to 0.16514, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1723 - acc: 0.9481 - val_loss: 0.1651 - val_acc: 0.9550\n",
            "Epoch 186/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1691 - acc: 0.9491\n",
            "Epoch 00186: val_loss did not improve from 0.16514\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1732 - acc: 0.9483 - val_loss: 0.2224 - val_acc: 0.9400\n",
            "Epoch 187/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1692 - acc: 0.9491\n",
            "Epoch 00187: val_loss did not improve from 0.16514\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1691 - acc: 0.9492 - val_loss: 0.1663 - val_acc: 0.9558\n",
            "Epoch 188/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1637 - acc: 0.9540\n",
            "Epoch 00188: val_loss did not improve from 0.16514\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1633 - acc: 0.9536 - val_loss: 0.1666 - val_acc: 0.9533\n",
            "Epoch 189/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1729 - acc: 0.9483\n",
            "Epoch 00189: val_loss did not improve from 0.16514\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1720 - acc: 0.9489 - val_loss: 0.2027 - val_acc: 0.9475\n",
            "Epoch 190/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1492 - acc: 0.9534\n",
            "Epoch 00190: val_loss did not improve from 0.16514\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1528 - acc: 0.9522 - val_loss: 0.2028 - val_acc: 0.9375\n",
            "Epoch 191/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1574 - acc: 0.9525\n",
            "Epoch 00191: val_loss did not improve from 0.16514\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1585 - acc: 0.9506 - val_loss: 0.1830 - val_acc: 0.9450\n",
            "Epoch 192/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1535 - acc: 0.9524\n",
            "Epoch 00192: val_loss did not improve from 0.16514\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1535 - acc: 0.9531 - val_loss: 0.1777 - val_acc: 0.9508\n",
            "Epoch 193/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9537\n",
            "Epoch 00193: val_loss did not improve from 0.16514\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1599 - acc: 0.9531 - val_loss: 0.2067 - val_acc: 0.9433\n",
            "Epoch 194/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1608 - acc: 0.9503\n",
            "Epoch 00194: val_loss did not improve from 0.16514\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1603 - acc: 0.9511 - val_loss: 0.1675 - val_acc: 0.9533\n",
            "Epoch 195/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1570 - acc: 0.9503\n",
            "Epoch 00195: val_loss improved from 0.16514 to 0.15270, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1592 - acc: 0.9492 - val_loss: 0.1527 - val_acc: 0.9592\n",
            "Epoch 196/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9446\n",
            "Epoch 00196: val_loss did not improve from 0.15270\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1654 - acc: 0.9444 - val_loss: 0.1650 - val_acc: 0.9567\n",
            "Epoch 197/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1496 - acc: 0.9582\n",
            "Epoch 00197: val_loss did not improve from 0.15270\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1481 - acc: 0.9586 - val_loss: 0.1675 - val_acc: 0.9575\n",
            "Epoch 198/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9603\n",
            "Epoch 00198: val_loss did not improve from 0.15270\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1412 - acc: 0.9600 - val_loss: 0.1549 - val_acc: 0.9592\n",
            "Epoch 199/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1470 - acc: 0.9566\n",
            "Epoch 00199: val_loss did not improve from 0.15270\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.1499 - acc: 0.9550 - val_loss: 0.1618 - val_acc: 0.9550\n",
            "Epoch 200/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1495 - acc: 0.9541\n",
            "Epoch 00200: val_loss did not improve from 0.15270\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1516 - acc: 0.9539 - val_loss: 0.1538 - val_acc: 0.9650\n",
            "Epoch 201/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1518 - acc: 0.9524\n",
            "Epoch 00201: val_loss did not improve from 0.15270\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1512 - acc: 0.9522 - val_loss: 0.1571 - val_acc: 0.9575\n",
            "Epoch 202/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9620\n",
            "Epoch 00202: val_loss did not improve from 0.15270\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1384 - acc: 0.9614 - val_loss: 0.1825 - val_acc: 0.9433\n",
            "Epoch 203/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1593 - acc: 0.9543\n",
            "Epoch 00203: val_loss did not improve from 0.15270\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1612 - acc: 0.9533 - val_loss: 0.1971 - val_acc: 0.9400\n",
            "Epoch 204/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1565 - acc: 0.9548\n",
            "Epoch 00204: val_loss did not improve from 0.15270\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1616 - acc: 0.9533 - val_loss: 0.1775 - val_acc: 0.9542\n",
            "Epoch 205/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1606 - acc: 0.9521\n",
            "Epoch 00205: val_loss did not improve from 0.15270\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1592 - acc: 0.9531 - val_loss: 0.1704 - val_acc: 0.9550\n",
            "Epoch 206/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1505 - acc: 0.9565\n",
            "Epoch 00206: val_loss did not improve from 0.15270\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1493 - acc: 0.9575 - val_loss: 0.1839 - val_acc: 0.9517\n",
            "Epoch 207/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9569\n",
            "Epoch 00207: val_loss improved from 0.15270 to 0.14769, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1581 - acc: 0.9575 - val_loss: 0.1477 - val_acc: 0.9592\n",
            "Epoch 208/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9514\n",
            "Epoch 00208: val_loss improved from 0.14769 to 0.14620, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1661 - acc: 0.9508 - val_loss: 0.1462 - val_acc: 0.9567\n",
            "Epoch 209/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1510 - acc: 0.9541\n",
            "Epoch 00209: val_loss did not improve from 0.14620\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1499 - acc: 0.9547 - val_loss: 0.1669 - val_acc: 0.9475\n",
            "Epoch 210/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1512 - acc: 0.9574\n",
            "Epoch 00210: val_loss did not improve from 0.14620\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1554 - acc: 0.9556 - val_loss: 0.1841 - val_acc: 0.9458\n",
            "Epoch 211/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1540 - acc: 0.9524\n",
            "Epoch 00211: val_loss did not improve from 0.14620\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1531 - acc: 0.9519 - val_loss: 0.1638 - val_acc: 0.9517\n",
            "Epoch 212/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1495 - acc: 0.9574\n",
            "Epoch 00212: val_loss did not improve from 0.14620\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1502 - acc: 0.9561 - val_loss: 0.1570 - val_acc: 0.9533\n",
            "Epoch 213/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1430 - acc: 0.9578\n",
            "Epoch 00213: val_loss improved from 0.14620 to 0.14480, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1430 - acc: 0.9586 - val_loss: 0.1448 - val_acc: 0.9592\n",
            "Epoch 214/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1522 - acc: 0.9513\n",
            "Epoch 00214: val_loss improved from 0.14480 to 0.14271, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1548 - acc: 0.9511 - val_loss: 0.1427 - val_acc: 0.9592\n",
            "Epoch 215/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1610 - acc: 0.9524\n",
            "Epoch 00215: val_loss did not improve from 0.14271\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1572 - acc: 0.9542 - val_loss: 0.1638 - val_acc: 0.9492\n",
            "Epoch 216/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1861 - acc: 0.9421\n",
            "Epoch 00216: val_loss did not improve from 0.14271\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1845 - acc: 0.9425 - val_loss: 0.1537 - val_acc: 0.9617\n",
            "Epoch 217/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1550 - acc: 0.9512\n",
            "Epoch 00217: val_loss did not improve from 0.14271\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1513 - acc: 0.9531 - val_loss: 0.1837 - val_acc: 0.9508\n",
            "Epoch 218/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1728 - acc: 0.9414\n",
            "Epoch 00218: val_loss did not improve from 0.14271\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1717 - acc: 0.9422 - val_loss: 0.1913 - val_acc: 0.9433\n",
            "Epoch 219/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1491 - acc: 0.9515\n",
            "Epoch 00219: val_loss improved from 0.14271 to 0.14111, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1505 - acc: 0.9511 - val_loss: 0.1411 - val_acc: 0.9650\n",
            "Epoch 220/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1414 - acc: 0.9624\n",
            "Epoch 00220: val_loss did not improve from 0.14111\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1426 - acc: 0.9611 - val_loss: 0.1676 - val_acc: 0.9542\n",
            "Epoch 221/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1409 - acc: 0.9582\n",
            "Epoch 00221: val_loss did not improve from 0.14111\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1417 - acc: 0.9572 - val_loss: 0.1483 - val_acc: 0.9675\n",
            "Epoch 222/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1546 - acc: 0.9491\n",
            "Epoch 00222: val_loss did not improve from 0.14111\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1541 - acc: 0.9494 - val_loss: 0.1488 - val_acc: 0.9642\n",
            "Epoch 223/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1480 - acc: 0.9566\n",
            "Epoch 00223: val_loss did not improve from 0.14111\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1492 - acc: 0.9564 - val_loss: 0.1814 - val_acc: 0.9500\n",
            "Epoch 224/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1486 - acc: 0.9559\n",
            "Epoch 00224: val_loss did not improve from 0.14111\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1470 - acc: 0.9564 - val_loss: 0.1491 - val_acc: 0.9592\n",
            "Epoch 225/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1351 - acc: 0.9588\n",
            "Epoch 00225: val_loss improved from 0.14111 to 0.13335, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1334 - acc: 0.9597 - val_loss: 0.1333 - val_acc: 0.9667\n",
            "Epoch 226/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1437 - acc: 0.9591\n",
            "Epoch 00226: val_loss did not improve from 0.13335\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1451 - acc: 0.9583 - val_loss: 0.1433 - val_acc: 0.9633\n",
            "Epoch 227/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9631\n",
            "Epoch 00227: val_loss did not improve from 0.13335\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1260 - acc: 0.9628 - val_loss: 0.1585 - val_acc: 0.9558\n",
            "Epoch 228/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1407 - acc: 0.9597\n",
            "Epoch 00228: val_loss did not improve from 0.13335\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1395 - acc: 0.9589 - val_loss: 0.1689 - val_acc: 0.9533\n",
            "Epoch 229/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1351 - acc: 0.9600\n",
            "Epoch 00229: val_loss did not improve from 0.13335\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1354 - acc: 0.9592 - val_loss: 0.1578 - val_acc: 0.9575\n",
            "Epoch 230/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1397 - acc: 0.9576\n",
            "Epoch 00230: val_loss improved from 0.13335 to 0.13143, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1404 - acc: 0.9575 - val_loss: 0.1314 - val_acc: 0.9750\n",
            "Epoch 231/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1371 - acc: 0.9612\n",
            "Epoch 00231: val_loss improved from 0.13143 to 0.12524, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.1362 - acc: 0.9625 - val_loss: 0.1252 - val_acc: 0.9725\n",
            "Epoch 232/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1385 - acc: 0.9582\n",
            "Epoch 00232: val_loss did not improve from 0.12524\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1378 - acc: 0.9578 - val_loss: 0.1378 - val_acc: 0.9650\n",
            "Epoch 233/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.9651\n",
            "Epoch 00233: val_loss did not improve from 0.12524\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1322 - acc: 0.9650 - val_loss: 0.1590 - val_acc: 0.9583\n",
            "Epoch 234/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1365 - acc: 0.9588\n",
            "Epoch 00234: val_loss improved from 0.12524 to 0.11340, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.1395 - acc: 0.9575 - val_loss: 0.1134 - val_acc: 0.9675\n",
            "Epoch 235/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9611\n",
            "Epoch 00235: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1313 - acc: 0.9608 - val_loss: 0.1349 - val_acc: 0.9667\n",
            "Epoch 236/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1391 - acc: 0.9579\n",
            "Epoch 00236: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1403 - acc: 0.9575 - val_loss: 0.1309 - val_acc: 0.9675\n",
            "Epoch 237/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1563 - acc: 0.9471\n",
            "Epoch 00237: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1559 - acc: 0.9472 - val_loss: 0.1365 - val_acc: 0.9708\n",
            "Epoch 238/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1362 - acc: 0.9588\n",
            "Epoch 00238: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1378 - acc: 0.9572 - val_loss: 0.1206 - val_acc: 0.9692\n",
            "Epoch 239/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1262 - acc: 0.9648\n",
            "Epoch 00239: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1308 - acc: 0.9619 - val_loss: 0.1245 - val_acc: 0.9708\n",
            "Epoch 240/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9563\n",
            "Epoch 00240: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1453 - acc: 0.9561 - val_loss: 0.1283 - val_acc: 0.9675\n",
            "Epoch 241/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1281 - acc: 0.9633\n",
            "Epoch 00241: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1264 - acc: 0.9639 - val_loss: 0.1183 - val_acc: 0.9675\n",
            "Epoch 242/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.9606\n",
            "Epoch 00242: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1338 - acc: 0.9597 - val_loss: 0.1846 - val_acc: 0.9475\n",
            "Epoch 243/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9557\n",
            "Epoch 00243: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1512 - acc: 0.9550 - val_loss: 0.1153 - val_acc: 0.9725\n",
            "Epoch 244/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1537 - acc: 0.9506\n",
            "Epoch 00244: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1584 - acc: 0.9481 - val_loss: 0.1408 - val_acc: 0.9667\n",
            "Epoch 245/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1525 - acc: 0.9550\n",
            "Epoch 00245: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1515 - acc: 0.9558 - val_loss: 0.1467 - val_acc: 0.9625\n",
            "Epoch 246/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1498 - acc: 0.9527\n",
            "Epoch 00246: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1482 - acc: 0.9533 - val_loss: 0.2579 - val_acc: 0.9117\n",
            "Epoch 247/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1475 - acc: 0.9571\n",
            "Epoch 00247: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1459 - acc: 0.9578 - val_loss: 0.1609 - val_acc: 0.9500\n",
            "Epoch 248/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1437 - acc: 0.9553\n",
            "Epoch 00248: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1440 - acc: 0.9553 - val_loss: 0.1416 - val_acc: 0.9625\n",
            "Epoch 249/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1345 - acc: 0.9629\n",
            "Epoch 00249: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1357 - acc: 0.9622 - val_loss: 0.1364 - val_acc: 0.9633\n",
            "Epoch 250/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1335 - acc: 0.9575\n",
            "Epoch 00250: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1344 - acc: 0.9575 - val_loss: 0.1575 - val_acc: 0.9558\n",
            "Epoch 251/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1219 - acc: 0.9644\n",
            "Epoch 00251: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1232 - acc: 0.9639 - val_loss: 0.1594 - val_acc: 0.9567\n",
            "Epoch 252/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1337 - acc: 0.9609\n",
            "Epoch 00252: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1308 - acc: 0.9625 - val_loss: 0.1570 - val_acc: 0.9592\n",
            "Epoch 253/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1288 - acc: 0.9594\n",
            "Epoch 00253: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1295 - acc: 0.9594 - val_loss: 0.1579 - val_acc: 0.9567\n",
            "Epoch 254/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9646\n",
            "Epoch 00254: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1322 - acc: 0.9644 - val_loss: 0.1498 - val_acc: 0.9533\n",
            "Epoch 255/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9586\n",
            "Epoch 00255: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1392 - acc: 0.9581 - val_loss: 0.1333 - val_acc: 0.9650\n",
            "Epoch 256/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1459 - acc: 0.9572\n",
            "Epoch 00256: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1463 - acc: 0.9564 - val_loss: 0.1368 - val_acc: 0.9600\n",
            "Epoch 257/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1396 - acc: 0.9559\n",
            "Epoch 00257: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1405 - acc: 0.9556 - val_loss: 0.1363 - val_acc: 0.9625\n",
            "Epoch 258/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.9560\n",
            "Epoch 00258: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1411 - acc: 0.9558 - val_loss: 0.1256 - val_acc: 0.9675\n",
            "Epoch 259/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1235 - acc: 0.9661\n",
            "Epoch 00259: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1277 - acc: 0.9636 - val_loss: 0.1342 - val_acc: 0.9600\n",
            "Epoch 260/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1430 - acc: 0.9566\n",
            "Epoch 00260: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1444 - acc: 0.9542 - val_loss: 0.1225 - val_acc: 0.9683\n",
            "Epoch 261/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1221 - acc: 0.9662\n",
            "Epoch 00261: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1219 - acc: 0.9664 - val_loss: 0.1150 - val_acc: 0.9658\n",
            "Epoch 262/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1397 - acc: 0.9552\n",
            "Epoch 00262: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1436 - acc: 0.9531 - val_loss: 0.1146 - val_acc: 0.9675\n",
            "Epoch 263/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1382 - acc: 0.9570\n",
            "Epoch 00263: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1379 - acc: 0.9569 - val_loss: 0.1309 - val_acc: 0.9600\n",
            "Epoch 264/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9631\n",
            "Epoch 00264: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1279 - acc: 0.9619 - val_loss: 0.1701 - val_acc: 0.9542\n",
            "Epoch 265/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9529\n",
            "Epoch 00265: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1496 - acc: 0.9525 - val_loss: 0.1645 - val_acc: 0.9517\n",
            "Epoch 266/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9523\n",
            "Epoch 00266: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1550 - acc: 0.9525 - val_loss: 0.1329 - val_acc: 0.9650\n",
            "Epoch 267/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9563\n",
            "Epoch 00267: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1425 - acc: 0.9561 - val_loss: 0.1246 - val_acc: 0.9717\n",
            "Epoch 268/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9611\n",
            "Epoch 00268: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1252 - acc: 0.9614 - val_loss: 0.1370 - val_acc: 0.9583\n",
            "Epoch 269/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9597\n",
            "Epoch 00269: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1299 - acc: 0.9603 - val_loss: 0.1211 - val_acc: 0.9625\n",
            "Epoch 270/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1234 - acc: 0.9632\n",
            "Epoch 00270: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1241 - acc: 0.9628 - val_loss: 0.1392 - val_acc: 0.9625\n",
            "Epoch 271/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9657\n",
            "Epoch 00271: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1218 - acc: 0.9639 - val_loss: 0.1492 - val_acc: 0.9617\n",
            "Epoch 272/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1366 - acc: 0.9618\n",
            "Epoch 00272: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1366 - acc: 0.9619 - val_loss: 0.1265 - val_acc: 0.9675\n",
            "Epoch 273/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1256 - acc: 0.9627\n",
            "Epoch 00273: val_loss did not improve from 0.11340\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1288 - acc: 0.9606 - val_loss: 0.1211 - val_acc: 0.9700\n",
            "Epoch 274/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1189 - acc: 0.9671\n",
            "Epoch 00274: val_loss improved from 0.11340 to 0.10510, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1204 - acc: 0.9661 - val_loss: 0.1051 - val_acc: 0.9708\n",
            "Epoch 275/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1180 - acc: 0.9659\n",
            "Epoch 00275: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1167 - acc: 0.9661 - val_loss: 0.1302 - val_acc: 0.9617\n",
            "Epoch 276/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1319 - acc: 0.9606\n",
            "Epoch 00276: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1344 - acc: 0.9592 - val_loss: 0.1598 - val_acc: 0.9567\n",
            "Epoch 277/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1367 - acc: 0.9570\n",
            "Epoch 00277: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1393 - acc: 0.9561 - val_loss: 0.1257 - val_acc: 0.9592\n",
            "Epoch 278/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1351 - acc: 0.9576\n",
            "Epoch 00278: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1344 - acc: 0.9572 - val_loss: 0.1129 - val_acc: 0.9692\n",
            "Epoch 279/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1295 - acc: 0.9618\n",
            "Epoch 00279: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1294 - acc: 0.9614 - val_loss: 0.1102 - val_acc: 0.9717\n",
            "Epoch 280/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1346 - acc: 0.9603\n",
            "Epoch 00280: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1328 - acc: 0.9611 - val_loss: 0.1345 - val_acc: 0.9608\n",
            "Epoch 281/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1305 - acc: 0.9576\n",
            "Epoch 00281: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1298 - acc: 0.9578 - val_loss: 0.1085 - val_acc: 0.9725\n",
            "Epoch 282/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1207 - acc: 0.9676\n",
            "Epoch 00282: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1223 - acc: 0.9667 - val_loss: 0.1130 - val_acc: 0.9708\n",
            "Epoch 283/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9597\n",
            "Epoch 00283: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1275 - acc: 0.9597 - val_loss: 0.1701 - val_acc: 0.9467\n",
            "Epoch 284/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9571\n",
            "Epoch 00284: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1394 - acc: 0.9569 - val_loss: 0.1251 - val_acc: 0.9658\n",
            "Epoch 285/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1286 - acc: 0.9624\n",
            "Epoch 00285: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1293 - acc: 0.9617 - val_loss: 0.1186 - val_acc: 0.9658\n",
            "Epoch 286/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9606\n",
            "Epoch 00286: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1193 - acc: 0.9606 - val_loss: 0.1055 - val_acc: 0.9675\n",
            "Epoch 287/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1205 - acc: 0.9625\n",
            "Epoch 00287: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1204 - acc: 0.9633 - val_loss: 0.1576 - val_acc: 0.9583\n",
            "Epoch 288/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1105 - acc: 0.9679\n",
            "Epoch 00288: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1111 - acc: 0.9683 - val_loss: 0.1453 - val_acc: 0.9533\n",
            "Epoch 289/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1284 - acc: 0.9568\n",
            "Epoch 00289: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1318 - acc: 0.9556 - val_loss: 0.1203 - val_acc: 0.9658\n",
            "Epoch 290/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1148 - acc: 0.9667\n",
            "Epoch 00290: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1149 - acc: 0.9669 - val_loss: 0.1337 - val_acc: 0.9633\n",
            "Epoch 291/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1255 - acc: 0.9653\n",
            "Epoch 00291: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1296 - acc: 0.9628 - val_loss: 0.1299 - val_acc: 0.9583\n",
            "Epoch 292/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1332 - acc: 0.9591\n",
            "Epoch 00292: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1341 - acc: 0.9589 - val_loss: 0.1148 - val_acc: 0.9708\n",
            "Epoch 293/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1235 - acc: 0.9603\n",
            "Epoch 00293: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1250 - acc: 0.9597 - val_loss: 0.1071 - val_acc: 0.9717\n",
            "Epoch 294/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1199 - acc: 0.9638\n",
            "Epoch 00294: val_loss did not improve from 0.10510\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1186 - acc: 0.9642 - val_loss: 0.1165 - val_acc: 0.9717\n",
            "Epoch 295/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1041 - acc: 0.9700\n",
            "Epoch 00295: val_loss improved from 0.10510 to 0.10026, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1051 - acc: 0.9686 - val_loss: 0.1003 - val_acc: 0.9742\n",
            "Epoch 296/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9634\n",
            "Epoch 00296: val_loss did not improve from 0.10026\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1208 - acc: 0.9633 - val_loss: 0.1094 - val_acc: 0.9692\n",
            "Epoch 297/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1139 - acc: 0.9684\n",
            "Epoch 00297: val_loss did not improve from 0.10026\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1170 - acc: 0.9686 - val_loss: 0.1325 - val_acc: 0.9650\n",
            "Epoch 298/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1153 - acc: 0.9645\n",
            "Epoch 00298: val_loss did not improve from 0.10026\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1158 - acc: 0.9650 - val_loss: 0.1159 - val_acc: 0.9667\n",
            "Epoch 299/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9657\n",
            "Epoch 00299: val_loss did not improve from 0.10026\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1139 - acc: 0.9658 - val_loss: 0.1085 - val_acc: 0.9742\n",
            "Epoch 300/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1122 - acc: 0.9682\n",
            "Epoch 00300: val_loss did not improve from 0.10026\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1129 - acc: 0.9678 - val_loss: 0.1232 - val_acc: 0.9675\n",
            "Epoch 301/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1148 - acc: 0.9674\n",
            "Epoch 00301: val_loss did not improve from 0.10026\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1146 - acc: 0.9678 - val_loss: 0.1460 - val_acc: 0.9608\n",
            "Epoch 302/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1140 - acc: 0.9667\n",
            "Epoch 00302: val_loss did not improve from 0.10026\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1114 - acc: 0.9678 - val_loss: 0.1500 - val_acc: 0.9592\n",
            "Epoch 303/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1205 - acc: 0.9621\n",
            "Epoch 00303: val_loss did not improve from 0.10026\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1217 - acc: 0.9622 - val_loss: 0.1682 - val_acc: 0.9525\n",
            "Epoch 304/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1278 - acc: 0.9622\n",
            "Epoch 00304: val_loss did not improve from 0.10026\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1329 - acc: 0.9614 - val_loss: 0.1119 - val_acc: 0.9733\n",
            "Epoch 305/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1304 - acc: 0.9552\n",
            "Epoch 00305: val_loss did not improve from 0.10026\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1306 - acc: 0.9547 - val_loss: 0.1527 - val_acc: 0.9575\n",
            "Epoch 306/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9620\n",
            "Epoch 00306: val_loss did not improve from 0.10026\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1251 - acc: 0.9625 - val_loss: 0.1058 - val_acc: 0.9733\n",
            "Epoch 307/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1092 - acc: 0.9700\n",
            "Epoch 00307: val_loss did not improve from 0.10026\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1058 - acc: 0.9711 - val_loss: 0.1299 - val_acc: 0.9650\n",
            "Epoch 308/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1148 - acc: 0.9628\n",
            "Epoch 00308: val_loss did not improve from 0.10026\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1186 - acc: 0.9617 - val_loss: 0.1025 - val_acc: 0.9758\n",
            "Epoch 309/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9523\n",
            "Epoch 00309: val_loss did not improve from 0.10026\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1441 - acc: 0.9531 - val_loss: 0.1041 - val_acc: 0.9700\n",
            "Epoch 310/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1341 - acc: 0.9562\n",
            "Epoch 00310: val_loss did not improve from 0.10026\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1343 - acc: 0.9564 - val_loss: 0.1147 - val_acc: 0.9725\n",
            "Epoch 311/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9674\n",
            "Epoch 00311: val_loss improved from 0.10026 to 0.09502, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1127 - acc: 0.9667 - val_loss: 0.0950 - val_acc: 0.9767\n",
            "Epoch 312/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1121 - acc: 0.9682\n",
            "Epoch 00312: val_loss did not improve from 0.09502\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1121 - acc: 0.9681 - val_loss: 0.1113 - val_acc: 0.9700\n",
            "Epoch 313/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1128 - acc: 0.9694\n",
            "Epoch 00313: val_loss did not improve from 0.09502\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1136 - acc: 0.9689 - val_loss: 0.0988 - val_acc: 0.9733\n",
            "Epoch 314/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1317 - acc: 0.9600\n",
            "Epoch 00314: val_loss did not improve from 0.09502\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1308 - acc: 0.9606 - val_loss: 0.1188 - val_acc: 0.9658\n",
            "Epoch 315/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1167 - acc: 0.9676\n",
            "Epoch 00315: val_loss did not improve from 0.09502\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1147 - acc: 0.9683 - val_loss: 0.1068 - val_acc: 0.9700\n",
            "Epoch 316/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1120 - acc: 0.9669\n",
            "Epoch 00316: val_loss did not improve from 0.09502\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1117 - acc: 0.9667 - val_loss: 0.1153 - val_acc: 0.9675\n",
            "Epoch 317/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1306 - acc: 0.9569\n",
            "Epoch 00317: val_loss did not improve from 0.09502\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1292 - acc: 0.9567 - val_loss: 0.1790 - val_acc: 0.9475\n",
            "Epoch 318/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1193 - acc: 0.9648\n",
            "Epoch 00318: val_loss did not improve from 0.09502\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1182 - acc: 0.9647 - val_loss: 0.1101 - val_acc: 0.9733\n",
            "Epoch 319/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1171 - acc: 0.9671\n",
            "Epoch 00319: val_loss did not improve from 0.09502\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1178 - acc: 0.9664 - val_loss: 0.1111 - val_acc: 0.9683\n",
            "Epoch 320/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1278 - acc: 0.9606\n",
            "Epoch 00320: val_loss did not improve from 0.09502\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1275 - acc: 0.9614 - val_loss: 0.1020 - val_acc: 0.9700\n",
            "Epoch 321/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1206 - acc: 0.9594\n",
            "Epoch 00321: val_loss did not improve from 0.09502\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1212 - acc: 0.9594 - val_loss: 0.1105 - val_acc: 0.9725\n",
            "Epoch 322/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1218 - acc: 0.9645\n",
            "Epoch 00322: val_loss did not improve from 0.09502\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1257 - acc: 0.9628 - val_loss: 0.1175 - val_acc: 0.9692\n",
            "Epoch 323/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1084 - acc: 0.9673\n",
            "Epoch 00323: val_loss did not improve from 0.09502\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1101 - acc: 0.9661 - val_loss: 0.1108 - val_acc: 0.9683\n",
            "Epoch 324/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1106 - acc: 0.9700\n",
            "Epoch 00324: val_loss did not improve from 0.09502\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1099 - acc: 0.9706 - val_loss: 0.1075 - val_acc: 0.9675\n",
            "Epoch 325/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1238 - acc: 0.9606\n",
            "Epoch 00325: val_loss did not improve from 0.09502\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1241 - acc: 0.9603 - val_loss: 0.1201 - val_acc: 0.9675\n",
            "Epoch 326/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9666\n",
            "Epoch 00326: val_loss did not improve from 0.09502\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1085 - acc: 0.9669 - val_loss: 0.1168 - val_acc: 0.9700\n",
            "Epoch 327/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1222 - acc: 0.9612\n",
            "Epoch 00327: val_loss did not improve from 0.09502\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1223 - acc: 0.9617 - val_loss: 0.1115 - val_acc: 0.9675\n",
            "Epoch 328/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1041 - acc: 0.9729\n",
            "Epoch 00328: val_loss improved from 0.09502 to 0.09069, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1043 - acc: 0.9728 - val_loss: 0.0907 - val_acc: 0.9758\n",
            "Epoch 329/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9674\n",
            "Epoch 00329: val_loss did not improve from 0.09069\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1083 - acc: 0.9672 - val_loss: 0.1026 - val_acc: 0.9717\n",
            "Epoch 330/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1186 - acc: 0.9624\n",
            "Epoch 00330: val_loss improved from 0.09069 to 0.08852, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1191 - acc: 0.9622 - val_loss: 0.0885 - val_acc: 0.9800\n",
            "Epoch 331/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1484 - acc: 0.9515\n",
            "Epoch 00331: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1462 - acc: 0.9525 - val_loss: 0.1120 - val_acc: 0.9733\n",
            "Epoch 332/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9646\n",
            "Epoch 00332: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1201 - acc: 0.9644 - val_loss: 0.1069 - val_acc: 0.9717\n",
            "Epoch 333/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9637\n",
            "Epoch 00333: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1239 - acc: 0.9628 - val_loss: 0.1076 - val_acc: 0.9800\n",
            "Epoch 334/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1206 - acc: 0.9653\n",
            "Epoch 00334: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1234 - acc: 0.9636 - val_loss: 0.1197 - val_acc: 0.9650\n",
            "Epoch 335/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1142 - acc: 0.9656\n",
            "Epoch 00335: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1174 - acc: 0.9644 - val_loss: 0.1060 - val_acc: 0.9742\n",
            "Epoch 336/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1082 - acc: 0.9694\n",
            "Epoch 00336: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1082 - acc: 0.9692 - val_loss: 0.1135 - val_acc: 0.9708\n",
            "Epoch 337/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1139 - acc: 0.9656\n",
            "Epoch 00337: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1190 - acc: 0.9644 - val_loss: 0.1265 - val_acc: 0.9642\n",
            "Epoch 338/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1261 - acc: 0.9626\n",
            "Epoch 00338: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1274 - acc: 0.9625 - val_loss: 0.1062 - val_acc: 0.9767\n",
            "Epoch 339/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9691\n",
            "Epoch 00339: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1043 - acc: 0.9686 - val_loss: 0.1172 - val_acc: 0.9692\n",
            "Epoch 340/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9591\n",
            "Epoch 00340: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1173 - acc: 0.9597 - val_loss: 0.1196 - val_acc: 0.9650\n",
            "Epoch 341/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1148 - acc: 0.9647\n",
            "Epoch 00341: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1139 - acc: 0.9656 - val_loss: 0.1023 - val_acc: 0.9700\n",
            "Epoch 342/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1260 - acc: 0.9603\n",
            "Epoch 00342: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1265 - acc: 0.9603 - val_loss: 0.1358 - val_acc: 0.9617\n",
            "Epoch 343/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1400 - acc: 0.9541\n",
            "Epoch 00343: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1385 - acc: 0.9544 - val_loss: 0.1063 - val_acc: 0.9692\n",
            "Epoch 344/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1216 - acc: 0.9656\n",
            "Epoch 00344: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1225 - acc: 0.9658 - val_loss: 0.1035 - val_acc: 0.9758\n",
            "Epoch 345/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1266 - acc: 0.9630\n",
            "Epoch 00345: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1252 - acc: 0.9639 - val_loss: 0.1054 - val_acc: 0.9742\n",
            "Epoch 346/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1123 - acc: 0.9658\n",
            "Epoch 00346: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1142 - acc: 0.9656 - val_loss: 0.1285 - val_acc: 0.9658\n",
            "Epoch 347/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1223 - acc: 0.9606\n",
            "Epoch 00347: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1225 - acc: 0.9614 - val_loss: 0.1158 - val_acc: 0.9725\n",
            "Epoch 348/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1215 - acc: 0.9633\n",
            "Epoch 00348: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1218 - acc: 0.9628 - val_loss: 0.1101 - val_acc: 0.9683\n",
            "Epoch 349/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1260 - acc: 0.9609\n",
            "Epoch 00349: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1264 - acc: 0.9611 - val_loss: 0.1098 - val_acc: 0.9658\n",
            "Epoch 350/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1206 - acc: 0.9634\n",
            "Epoch 00350: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1221 - acc: 0.9631 - val_loss: 0.1179 - val_acc: 0.9675\n",
            "Epoch 351/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1212 - acc: 0.9634\n",
            "Epoch 00351: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1185 - acc: 0.9650 - val_loss: 0.1367 - val_acc: 0.9592\n",
            "Epoch 352/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9669\n",
            "Epoch 00352: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1088 - acc: 0.9675 - val_loss: 0.1017 - val_acc: 0.9700\n",
            "Epoch 353/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9680\n",
            "Epoch 00353: val_loss did not improve from 0.08852\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1063 - acc: 0.9672 - val_loss: 0.0920 - val_acc: 0.9742\n",
            "Epoch 354/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9691\n",
            "Epoch 00354: val_loss improved from 0.08852 to 0.08521, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1070 - acc: 0.9689 - val_loss: 0.0852 - val_acc: 0.9808\n",
            "Epoch 355/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0993 - acc: 0.9694\n",
            "Epoch 00355: val_loss did not improve from 0.08521\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1018 - acc: 0.9683 - val_loss: 0.0862 - val_acc: 0.9767\n",
            "Epoch 356/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9677\n",
            "Epoch 00356: val_loss did not improve from 0.08521\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1088 - acc: 0.9675 - val_loss: 0.1238 - val_acc: 0.9575\n",
            "Epoch 357/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9646\n",
            "Epoch 00357: val_loss did not improve from 0.08521\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1161 - acc: 0.9653 - val_loss: 0.0942 - val_acc: 0.9733\n",
            "Epoch 358/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1057 - acc: 0.9694\n",
            "Epoch 00358: val_loss did not improve from 0.08521\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1067 - acc: 0.9672 - val_loss: 0.1059 - val_acc: 0.9658\n",
            "Epoch 359/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1100 - acc: 0.9667\n",
            "Epoch 00359: val_loss did not improve from 0.08521\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1117 - acc: 0.9653 - val_loss: 0.1168 - val_acc: 0.9717\n",
            "Epoch 360/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9666\n",
            "Epoch 00360: val_loss did not improve from 0.08521\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1080 - acc: 0.9667 - val_loss: 0.0907 - val_acc: 0.9742\n",
            "Epoch 361/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1276 - acc: 0.9603\n",
            "Epoch 00361: val_loss improved from 0.08521 to 0.08199, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1257 - acc: 0.9608 - val_loss: 0.0820 - val_acc: 0.9775\n",
            "Epoch 362/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1162 - acc: 0.9652\n",
            "Epoch 00362: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1179 - acc: 0.9650 - val_loss: 0.1034 - val_acc: 0.9725\n",
            "Epoch 363/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9591\n",
            "Epoch 00363: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1307 - acc: 0.9594 - val_loss: 0.1381 - val_acc: 0.9575\n",
            "Epoch 364/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9620\n",
            "Epoch 00364: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1224 - acc: 0.9617 - val_loss: 0.1169 - val_acc: 0.9717\n",
            "Epoch 365/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1119 - acc: 0.9685\n",
            "Epoch 00365: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 181us/sample - loss: 0.1119 - acc: 0.9683 - val_loss: 0.1044 - val_acc: 0.9758\n",
            "Epoch 366/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1145 - acc: 0.9668\n",
            "Epoch 00366: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1162 - acc: 0.9656 - val_loss: 0.1163 - val_acc: 0.9683\n",
            "Epoch 367/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9600\n",
            "Epoch 00367: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1274 - acc: 0.9606 - val_loss: 0.0918 - val_acc: 0.9692\n",
            "Epoch 368/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1073 - acc: 0.9666\n",
            "Epoch 00368: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.1107 - acc: 0.9653 - val_loss: 0.0970 - val_acc: 0.9742\n",
            "Epoch 369/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1238 - acc: 0.9642\n",
            "Epoch 00369: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1237 - acc: 0.9642 - val_loss: 0.1146 - val_acc: 0.9658\n",
            "Epoch 370/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9691\n",
            "Epoch 00370: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1130 - acc: 0.9689 - val_loss: 0.0948 - val_acc: 0.9700\n",
            "Epoch 371/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1007 - acc: 0.9719\n",
            "Epoch 00371: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.1012 - acc: 0.9706 - val_loss: 0.1083 - val_acc: 0.9700\n",
            "Epoch 372/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9700\n",
            "Epoch 00372: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1070 - acc: 0.9703 - val_loss: 0.0974 - val_acc: 0.9775\n",
            "Epoch 373/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1190 - acc: 0.9629\n",
            "Epoch 00373: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1214 - acc: 0.9617 - val_loss: 0.0866 - val_acc: 0.9825\n",
            "Epoch 374/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1064 - acc: 0.9694\n",
            "Epoch 00374: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1065 - acc: 0.9683 - val_loss: 0.1022 - val_acc: 0.9742\n",
            "Epoch 375/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0992 - acc: 0.9691\n",
            "Epoch 00375: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0976 - acc: 0.9697 - val_loss: 0.0960 - val_acc: 0.9775\n",
            "Epoch 376/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1186 - acc: 0.9627\n",
            "Epoch 00376: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1189 - acc: 0.9619 - val_loss: 0.1027 - val_acc: 0.9717\n",
            "Epoch 377/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9674\n",
            "Epoch 00377: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1066 - acc: 0.9675 - val_loss: 0.0973 - val_acc: 0.9733\n",
            "Epoch 378/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1004 - acc: 0.9685\n",
            "Epoch 00378: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1025 - acc: 0.9675 - val_loss: 0.1185 - val_acc: 0.9675\n",
            "Epoch 379/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1002 - acc: 0.9721\n",
            "Epoch 00379: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1023 - acc: 0.9711 - val_loss: 0.0850 - val_acc: 0.9792\n",
            "Epoch 380/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9654\n",
            "Epoch 00380: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1096 - acc: 0.9647 - val_loss: 0.1111 - val_acc: 0.9667\n",
            "Epoch 381/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1241 - acc: 0.9594\n",
            "Epoch 00381: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1220 - acc: 0.9594 - val_loss: 0.1084 - val_acc: 0.9742\n",
            "Epoch 382/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1015 - acc: 0.9679\n",
            "Epoch 00382: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1063 - acc: 0.9658 - val_loss: 0.1123 - val_acc: 0.9733\n",
            "Epoch 383/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1054 - acc: 0.9700\n",
            "Epoch 00383: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1076 - acc: 0.9683 - val_loss: 0.1055 - val_acc: 0.9742\n",
            "Epoch 384/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1117 - acc: 0.9639\n",
            "Epoch 00384: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1113 - acc: 0.9642 - val_loss: 0.0984 - val_acc: 0.9708\n",
            "Epoch 385/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1071 - acc: 0.9682\n",
            "Epoch 00385: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1084 - acc: 0.9669 - val_loss: 0.0849 - val_acc: 0.9808\n",
            "Epoch 386/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9654\n",
            "Epoch 00386: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1154 - acc: 0.9650 - val_loss: 0.1173 - val_acc: 0.9667\n",
            "Epoch 387/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1006 - acc: 0.9736\n",
            "Epoch 00387: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1049 - acc: 0.9725 - val_loss: 0.0980 - val_acc: 0.9742\n",
            "Epoch 388/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0908 - acc: 0.9762\n",
            "Epoch 00388: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0905 - acc: 0.9764 - val_loss: 0.0932 - val_acc: 0.9700\n",
            "Epoch 389/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0957 - acc: 0.9718\n",
            "Epoch 00389: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0946 - acc: 0.9722 - val_loss: 0.1145 - val_acc: 0.9650\n",
            "Epoch 390/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1002 - acc: 0.9672\n",
            "Epoch 00390: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1034 - acc: 0.9678 - val_loss: 0.1096 - val_acc: 0.9708\n",
            "Epoch 391/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1062 - acc: 0.9659\n",
            "Epoch 00391: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1069 - acc: 0.9650 - val_loss: 0.0976 - val_acc: 0.9708\n",
            "Epoch 392/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1109 - acc: 0.9633\n",
            "Epoch 00392: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1110 - acc: 0.9636 - val_loss: 0.0919 - val_acc: 0.9717\n",
            "Epoch 393/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9703\n",
            "Epoch 00393: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1040 - acc: 0.9703 - val_loss: 0.0989 - val_acc: 0.9700\n",
            "Epoch 394/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1060 - acc: 0.9682\n",
            "Epoch 00394: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1056 - acc: 0.9681 - val_loss: 0.0936 - val_acc: 0.9758\n",
            "Epoch 395/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1015 - acc: 0.9682\n",
            "Epoch 00395: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1063 - acc: 0.9667 - val_loss: 0.0921 - val_acc: 0.9758\n",
            "Epoch 396/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0958 - acc: 0.9744\n",
            "Epoch 00396: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0954 - acc: 0.9739 - val_loss: 0.0904 - val_acc: 0.9742\n",
            "Epoch 397/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1028 - acc: 0.9709\n",
            "Epoch 00397: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1018 - acc: 0.9711 - val_loss: 0.0864 - val_acc: 0.9758\n",
            "Epoch 398/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1081 - acc: 0.9669\n",
            "Epoch 00398: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1078 - acc: 0.9672 - val_loss: 0.0974 - val_acc: 0.9767\n",
            "Epoch 399/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9677\n",
            "Epoch 00399: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1016 - acc: 0.9683 - val_loss: 0.1076 - val_acc: 0.9717\n",
            "Epoch 400/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1092 - acc: 0.9676\n",
            "Epoch 00400: val_loss did not improve from 0.08199\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1085 - acc: 0.9678 - val_loss: 0.1315 - val_acc: 0.9642\n",
            "1200/1200 [==============================] - 0s 128us/sample - loss: 0.1315 - acc: 0.9642\n",
            "[0.1315487489104271, 0.96416664]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUhO5Bu-po1c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "592b0946-8f78-4589-dd49-9769f7d839c8"
      },
      "source": [
        "for i in range(3, 4): # Итерација низ секој испитен примерок\n",
        "  print(f\"====================== Примерок ({i}) ======================\")\n",
        "  print(\"Вчитување тест податоци од испитниот примерок \" + str(i) + \"...\")\n",
        "  \n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_test_data = np.empty(0)\n",
        "  temp_test_events = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_test_set = 'S' + format(j, '02') + '/Test'\n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testData.mat\"\n",
        "    temp = loadmat(full_path)['testData']\n",
        "    if temp_test_data.size != 0:\n",
        "      temp_test_data = np.concatenate((temp_test_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_test_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_test_events.size != 0:\n",
        "        temp_test_events = np.append(temp_test_events, temp)\n",
        "      else:\n",
        "        temp_test_events = np.array(temp)\n",
        "\n",
        "    # Вчитување на бројот на runs \n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/runs_per_block.txt\"\n",
        "    with open(full_path, \"r\") as runs_per_block:\n",
        "      test_runs_per_block[i-1][j-1] = int(runs_per_block.read())\n",
        "\n",
        "    print(\"\\t - Тест податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "  # Зачувај ги тест податоците вчитани од испитниот примерок во низа\n",
        "  test_data.append(temp_test_data)\n",
        "  test_events.append(temp_test_events)\n",
        "  print(\"Тест податоците од испитниот примерок \" + str(i) + \" се вчитани.\\n\")\n",
        "\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_data: \" + str(test_data[i-1].shape)) # test_data to predict\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_events: \" + str(len(test_events[i-1]))) # test_events\n",
        "  for j in range (1,4):\n",
        "    print(\"SBJ\" + str(format(i-1, '02')) + \" / S\" + str(format(j-1, '02')) + \"| Runs per block: \" + str(test_runs_per_block[i-1][j-1])) # runs per block in SJB01, SJ00 \n",
        "\n",
        "  to_predict_data = reshape_data_to_mne_format(test_data[i-1])\n",
        "  predictions = model3.predict(to_predict_data)\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Predictions: \" + str(len(predictions)))\n",
        "  # np.savetxt(\"predictions.csv\", predictions, delimiter=\",\")\n",
        "\n",
        "\n",
        "  # ========= FALI USTE DA SE ISPARSIRA PREDICTIONOT... NE E SREDEN OVOJ KOD DOLE =======\n",
        "\n",
        "  int_pred = np.argmax(predictions, axis=1)\n",
        "  int_ytest = np.argmax(y_test, axis=1)\n",
        "\n",
        "  session_start = 0\n",
        "  start_prediction_index = 0\n",
        "  end_prediction_index = 0\n",
        "  for session in range(0, 3):\n",
        "    print(f\"============== Сесија ({session}) ==============\")\n",
        "    for block in range(0, 50):    \n",
        "      events_per_block = test_runs_per_block[i-1][session]\n",
        "\n",
        "      start_prediction_index = session_start + (block*events_per_block)*8\n",
        "      end_prediction_index = session_start + ((block+1)*events_per_block)*8\n",
        "\n",
        "      block_prediction = int_pred[start_prediction_index:end_prediction_index]\n",
        "      prediction = np.bincount(block_prediction).argmax()\n",
        "      df.iat[session+6,block+2] = prediction+1\n",
        "      # UNCOMMENT ZA PODOBAR PRIKAZ :)\n",
        "      # print(f\"Session {session} | Block: {block} | Prediction: {prediction} | Address: {end_prediction_index}\")\n",
        "\n",
        "      print(str(prediction+1) + \",\", end=\"\")\n",
        "    session_start = end_prediction_index\n",
        "    print(\"\")\n",
        "  print(\"Stigna li do kraj: \" + str(session_start == len(predictions)))\n",
        "  print(f\"====================== Примерок ({i}) ======================\\n\\n\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Примерок (3) ======================\n",
            "Вчитување тест податоци од испитниот примерок 3...\n",
            "\t - Тест податоците од сесија 1 се вчитани.\n",
            "\t - Тест податоците од сесија 2 се вчитани.\n",
            "\t - Тест податоците од сесија 3 се вчитани.\n",
            "Тест податоците од испитниот примерок 3 се вчитани.\n",
            "\n",
            "SBJ02| Test_data: (8, 350, 7600)\n",
            "SBJ02| Test_events: 7600\n",
            "SBJ02 / S00| Runs per block: 6\n",
            "SBJ02 / S01| Runs per block: 6\n",
            "SBJ02 / S02| Runs per block: 7\n",
            "SBJ02| Predictions: 7600\n",
            "============== Сесија (0) ==============\n",
            "3,3,4,4,4,3,6,3,3,5,4,7,4,3,3,5,6,3,3,4,5,3,3,4,5,5,3,4,3,3,4,4,3,3,3,4,4,3,3,4,3,4,4,6,3,3,4,3,4,4,\n",
            "============== Сесија (1) ==============\n",
            "6,1,7,7,7,7,7,6,7,4,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,1,7,6,6,6,6,6,6,1,6,7,7,6,6,7,6,7,1,6,6,6,6,6,\n",
            "============== Сесија (2) ==============\n",
            "6,4,8,8,5,7,8,8,8,2,8,8,8,8,4,4,2,8,4,8,4,7,4,8,4,4,4,4,4,8,1,8,4,4,4,4,4,8,4,4,2,5,4,2,8,2,2,2,2,8,\n",
            "Stigna li do kraj: True\n",
            "====================== Примерок (3) ======================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20-3XdRBp0vw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6904123c-42b9-461d-ca57-5fca96d0eb07"
      },
      "source": [
        "df"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUBJECT</th>\n",
              "      <th>SESSION</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>Unnamed: 52</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    SUBJECT  SESSION  1  2  3  4  5  6  7  ... 43 44 45 46 47 48 49 50 Unnamed: 52\n",
              "0         1        1  6  6  3  6  6  3  6  ...  6  4  8  3  4  5  5  7         NaN\n",
              "1         1        2  6  3  2  1  2  1  3  ...  6  2  2  2  3  6  6  2         NaN\n",
              "2         1        3  3  3  3  3  3  3  3  ...  3  3  6  3  6  7  1  6         NaN\n",
              "3         2        1  8  8  8  8  8  8  8  ...  8  4  8  4  8  7  8  8         NaN\n",
              "4         2        2  2  7  6  6  6  6  7  ...  2  6  6  2  6  6  2  2         NaN\n",
              "5         2        3  2  7  2  6  7  4  2  ...  2  6  2  2  7  2  2  2         NaN\n",
              "6         3        1  3  3  4  4  4  3  6  ...  4  6  3  3  4  3  4  4         NaN\n",
              "7         3        2  6  1  7  7  7  7  7  ...  6  7  1  6  6  6  6  6         NaN\n",
              "8         3        3  6  4  8  8  5  7  8  ...  4  2  8  2  2  2  2  8         NaN\n",
              "9         4        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "10        4        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "11        4        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "12        5        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "13        5        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "14        5        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "15        6        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "16        6        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "17        6        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "18        7        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "19        7        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "20        7        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "21        8        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "22        8        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "23        8        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "24        9        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "25        9        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "26        9        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "27       10        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "28       10        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "29       10        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "30       11        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "31       11        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "32       11        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "33       12        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "34       12        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "35       12        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "36       13        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "37       13        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "38       13        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "39       14        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "40       14        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "41       14        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "42       15        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "43       15        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "44       15        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "\n",
              "[45 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB-9d2k-p1-l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8e8e0131-dd13-4e8c-b5ad-5f6f46b9cee6"
      },
      "source": [
        "for i in range(4, 5): # Итерација низ секој испитен примерок\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_data = np.empty(0)\n",
        "  temp_labels = np.empty(0)\n",
        "  temp_events = np.empty(0)\n",
        "  temp_targets = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainData.mat\"\n",
        "    temp = loadmat(full_path)['trainData']\n",
        "    if temp_data.size != 0:\n",
        "      temp_data = np.concatenate((temp_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на label-ите\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainLabels.txt\"\n",
        "    with open(full_path, \"r\") as file_labels:\n",
        "      temp = file_labels.read().splitlines()\n",
        "      temp = np.repeat(temp, 8*10)\n",
        "      if temp_labels.size != 0:\n",
        "        temp_labels = np.concatenate((temp_labels, temp))\n",
        "      else:\n",
        "        temp_labels = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_events.size != 0:\n",
        "        temp_events = np.append(temp_events, temp)\n",
        "      else:\n",
        "        temp_events = np.array(temp)\n",
        "      \n",
        "\n",
        "    # Вчитување на редоследот на објекти кои се target\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainTargets.txt\"\n",
        "    with open(full_path, \"r\") as file_targets:\n",
        "      temp = file_targets.read().splitlines()\n",
        "      if temp_targets.size != 0:\n",
        "        temp_targets = np.concatenate((temp_targets, temp))\n",
        "      else:\n",
        "        temp_targets = np.array(temp)\n",
        "    print(\"\\t - Податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "\n",
        "  for j in range(4, 8): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "      \n",
        "  # Зачувај ги податоците вчитани од испитниот примерок во низа\n",
        "  data.append(temp_data)\n",
        "  labels.append(temp_labels)\n",
        "  events.append(temp_events)\n",
        "  targets.append(temp_targets)\n",
        "\n",
        "  \n",
        "  print(\"Податоците од испитниот примерок \" + str(i) + \" се вчитани.\")\n",
        "\n",
        "\n",
        "  #data = target_events_data_scaled\n",
        "  mne_array = np.swapaxes(data[i-1], 0, 2) # (епохa, канал, настан). \n",
        "  mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "  mne_array = mne_array.reshape(mne_array.shape[0],1, 8,350)\n",
        "  print(mne_array.shape)\n",
        "\n",
        "  events_arr = events[i-1].astype(np.int)\n",
        "  labels_arr = labels[i-1].astype(np.int)\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(mne_array, labels_arr-1, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "  model4 = DeepConvNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "  model4.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = Adam(0.0009))\n",
        "  checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                                verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "  #clf = RandomForestClassifier(max_depth=5)\n",
        "  #clf.fit(X_train, y_train)\n",
        "  #score = clf.score(X_test, y_test)\n",
        "  # print(score)\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  num_batch_size=100\n",
        "  num_epochs=400\n",
        "  model4.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "            validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "  score = model4.evaluate(X_test, y_test, verbose=1)\n",
        "  print(score)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t - Податоците од сесија 1 се вчитани.\n",
            "\t - Податоците од сесија 2 се вчитани.\n",
            "\t - Податоците од сесија 3 се вчитани.\n",
            "Податоците од испитниот примерок 4 се вчитани.\n",
            "(4800, 1, 8, 350)\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Train on 3600 samples, validate on 1200 samples\n",
            "Epoch 1/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.4462 - acc: 0.1491\n",
            "Epoch 00001: val_loss improved from inf to 2.52206, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 4s 1ms/sample - loss: 2.4321 - acc: 0.1494 - val_loss: 2.5221 - val_acc: 0.1792\n",
            "Epoch 2/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.2292 - acc: 0.1742\n",
            "Epoch 00002: val_loss improved from 2.52206 to 2.35845, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 2.2261 - acc: 0.1736 - val_loss: 2.3585 - val_acc: 0.1142\n",
            "Epoch 3/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.1856 - acc: 0.1700\n",
            "Epoch 00003: val_loss did not improve from 2.35845\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 2.1860 - acc: 0.1717 - val_loss: 2.5164 - val_acc: 0.1800\n",
            "Epoch 4/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.1761 - acc: 0.1743\n",
            "Epoch 00004: val_loss improved from 2.35845 to 2.10170, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 2.1725 - acc: 0.1733 - val_loss: 2.1017 - val_acc: 0.1608\n",
            "Epoch 5/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 2.1403 - acc: 0.1756\n",
            "Epoch 00005: val_loss did not improve from 2.10170\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 2.1422 - acc: 0.1803 - val_loss: 2.4522 - val_acc: 0.1333\n",
            "Epoch 6/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.1538 - acc: 0.1876\n",
            "Epoch 00006: val_loss did not improve from 2.10170\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 2.1468 - acc: 0.1878 - val_loss: 2.2061 - val_acc: 0.0958\n",
            "Epoch 7/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.0984 - acc: 0.2040\n",
            "Epoch 00007: val_loss did not improve from 2.10170\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 2.1078 - acc: 0.2031 - val_loss: 3.3334 - val_acc: 0.1842\n",
            "Epoch 8/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 2.1004 - acc: 0.2100\n",
            "Epoch 00008: val_loss did not improve from 2.10170\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 2.0971 - acc: 0.2142 - val_loss: 2.1178 - val_acc: 0.2008\n",
            "Epoch 9/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.0621 - acc: 0.2297\n",
            "Epoch 00009: val_loss did not improve from 2.10170\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 2.0611 - acc: 0.2294 - val_loss: 2.1976 - val_acc: 0.1983\n",
            "Epoch 10/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.0306 - acc: 0.2363\n",
            "Epoch 00010: val_loss improved from 2.10170 to 2.05504, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 2.0345 - acc: 0.2331 - val_loss: 2.0550 - val_acc: 0.2192\n",
            "Epoch 11/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.0132 - acc: 0.2483\n",
            "Epoch 00011: val_loss did not improve from 2.05504\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 2.0116 - acc: 0.2494 - val_loss: 2.2739 - val_acc: 0.2033\n",
            "Epoch 12/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.9761 - acc: 0.2629\n",
            "Epoch 00012: val_loss did not improve from 2.05504\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 1.9744 - acc: 0.2658 - val_loss: 2.2148 - val_acc: 0.2192\n",
            "Epoch 13/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.9492 - acc: 0.2800\n",
            "Epoch 00013: val_loss improved from 2.05504 to 1.97858, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 1.9446 - acc: 0.2797 - val_loss: 1.9786 - val_acc: 0.2417\n",
            "Epoch 14/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.9485 - acc: 0.2785\n",
            "Epoch 00014: val_loss did not improve from 1.97858\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 1.9504 - acc: 0.2800 - val_loss: 2.1937 - val_acc: 0.2325\n",
            "Epoch 15/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.8495 - acc: 0.3085\n",
            "Epoch 00015: val_loss did not improve from 1.97858\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 1.8628 - acc: 0.3061 - val_loss: 2.3016 - val_acc: 0.2408\n",
            "Epoch 16/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.8672 - acc: 0.3071\n",
            "Epoch 00016: val_loss did not improve from 1.97858\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.8685 - acc: 0.3067 - val_loss: 1.9944 - val_acc: 0.1833\n",
            "Epoch 17/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.8523 - acc: 0.3100\n",
            "Epoch 00017: val_loss did not improve from 1.97858\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.8498 - acc: 0.3119 - val_loss: 2.1028 - val_acc: 0.2600\n",
            "Epoch 18/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.7792 - acc: 0.3474\n",
            "Epoch 00018: val_loss did not improve from 1.97858\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.7795 - acc: 0.3467 - val_loss: 2.1609 - val_acc: 0.2467\n",
            "Epoch 19/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.7888 - acc: 0.3337\n",
            "Epoch 00019: val_loss did not improve from 1.97858\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 1.7898 - acc: 0.3339 - val_loss: 2.2841 - val_acc: 0.2075\n",
            "Epoch 20/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.7558 - acc: 0.3512\n",
            "Epoch 00020: val_loss did not improve from 1.97858\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 1.7660 - acc: 0.3472 - val_loss: 2.4425 - val_acc: 0.2350\n",
            "Epoch 21/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.7253 - acc: 0.3474\n",
            "Epoch 00021: val_loss improved from 1.97858 to 1.90261, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 1.7270 - acc: 0.3492 - val_loss: 1.9026 - val_acc: 0.2883\n",
            "Epoch 22/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.7044 - acc: 0.3747\n",
            "Epoch 00022: val_loss did not improve from 1.90261\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.7029 - acc: 0.3750 - val_loss: 1.9866 - val_acc: 0.2817\n",
            "Epoch 23/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.6626 - acc: 0.3753\n",
            "Epoch 00023: val_loss did not improve from 1.90261\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 1.6676 - acc: 0.3714 - val_loss: 1.9041 - val_acc: 0.2600\n",
            "Epoch 24/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.6121 - acc: 0.3994\n",
            "Epoch 00024: val_loss did not improve from 1.90261\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.6137 - acc: 0.3981 - val_loss: 2.0893 - val_acc: 0.2950\n",
            "Epoch 25/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.5869 - acc: 0.4223\n",
            "Epoch 00025: val_loss did not improve from 1.90261\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.5830 - acc: 0.4247 - val_loss: 2.0451 - val_acc: 0.2850\n",
            "Epoch 26/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.5665 - acc: 0.4169\n",
            "Epoch 00026: val_loss did not improve from 1.90261\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 1.5731 - acc: 0.4153 - val_loss: 2.0901 - val_acc: 0.2692\n",
            "Epoch 27/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.5413 - acc: 0.4348\n",
            "Epoch 00027: val_loss did not improve from 1.90261\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.5419 - acc: 0.4356 - val_loss: 1.9207 - val_acc: 0.3175\n",
            "Epoch 28/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.4905 - acc: 0.4548\n",
            "Epoch 00028: val_loss did not improve from 1.90261\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 1.4985 - acc: 0.4531 - val_loss: 2.0237 - val_acc: 0.2692\n",
            "Epoch 29/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.5038 - acc: 0.4482\n",
            "Epoch 00029: val_loss improved from 1.90261 to 1.79274, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 1.5211 - acc: 0.4456 - val_loss: 1.7927 - val_acc: 0.3225\n",
            "Epoch 30/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.4765 - acc: 0.4497\n",
            "Epoch 00030: val_loss did not improve from 1.79274\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.4762 - acc: 0.4506 - val_loss: 1.9154 - val_acc: 0.3183\n",
            "Epoch 31/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.4429 - acc: 0.4694\n",
            "Epoch 00031: val_loss improved from 1.79274 to 1.75866, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 1.4470 - acc: 0.4692 - val_loss: 1.7587 - val_acc: 0.3525\n",
            "Epoch 32/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.4185 - acc: 0.4774\n",
            "Epoch 00032: val_loss did not improve from 1.75866\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 1.4269 - acc: 0.4756 - val_loss: 1.8994 - val_acc: 0.3333\n",
            "Epoch 33/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.3903 - acc: 0.4886\n",
            "Epoch 00033: val_loss improved from 1.75866 to 1.70946, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 1.3904 - acc: 0.4886 - val_loss: 1.7095 - val_acc: 0.3650\n",
            "Epoch 34/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.3671 - acc: 0.5076\n",
            "Epoch 00034: val_loss did not improve from 1.70946\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.3721 - acc: 0.5069 - val_loss: 1.7260 - val_acc: 0.3767\n",
            "Epoch 35/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.3661 - acc: 0.5029\n",
            "Epoch 00035: val_loss improved from 1.70946 to 1.70547, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 1.3653 - acc: 0.5017 - val_loss: 1.7055 - val_acc: 0.3733\n",
            "Epoch 36/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.3074 - acc: 0.5294\n",
            "Epoch 00036: val_loss did not improve from 1.70547\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.3080 - acc: 0.5278 - val_loss: 1.7268 - val_acc: 0.3800\n",
            "Epoch 37/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.2829 - acc: 0.5364\n",
            "Epoch 00037: val_loss did not improve from 1.70547\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.2936 - acc: 0.5339 - val_loss: 2.3070 - val_acc: 0.2700\n",
            "Epoch 38/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2542 - acc: 0.5447\n",
            "Epoch 00038: val_loss did not improve from 1.70547\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.2693 - acc: 0.5386 - val_loss: 2.1052 - val_acc: 0.2883\n",
            "Epoch 39/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2506 - acc: 0.5482\n",
            "Epoch 00039: val_loss improved from 1.70547 to 1.59584, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 1.2421 - acc: 0.5511 - val_loss: 1.5958 - val_acc: 0.3933\n",
            "Epoch 40/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.2057 - acc: 0.5667\n",
            "Epoch 00040: val_loss improved from 1.59584 to 1.58437, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 1.2110 - acc: 0.5631 - val_loss: 1.5844 - val_acc: 0.4075\n",
            "Epoch 41/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.1774 - acc: 0.5685\n",
            "Epoch 00041: val_loss did not improve from 1.58437\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.1758 - acc: 0.5739 - val_loss: 1.5857 - val_acc: 0.4192\n",
            "Epoch 42/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1368 - acc: 0.5982\n",
            "Epoch 00042: val_loss improved from 1.58437 to 1.54673, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 1.1384 - acc: 0.5953 - val_loss: 1.5467 - val_acc: 0.4317\n",
            "Epoch 43/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.1312 - acc: 0.6025\n",
            "Epoch 00043: val_loss did not improve from 1.54673\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.1339 - acc: 0.5978 - val_loss: 1.5728 - val_acc: 0.4625\n",
            "Epoch 44/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.1065 - acc: 0.6089\n",
            "Epoch 00044: val_loss improved from 1.54673 to 1.52009, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 1.1111 - acc: 0.6061 - val_loss: 1.5201 - val_acc: 0.4467\n",
            "Epoch 45/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0785 - acc: 0.6185\n",
            "Epoch 00045: val_loss improved from 1.52009 to 1.45029, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 1.0809 - acc: 0.6153 - val_loss: 1.4503 - val_acc: 0.4725\n",
            "Epoch 46/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.0459 - acc: 0.6226\n",
            "Epoch 00046: val_loss improved from 1.45029 to 1.44239, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 1.0440 - acc: 0.6244 - val_loss: 1.4424 - val_acc: 0.4625\n",
            "Epoch 47/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0442 - acc: 0.6309\n",
            "Epoch 00047: val_loss improved from 1.44239 to 1.39597, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 1.0398 - acc: 0.6325 - val_loss: 1.3960 - val_acc: 0.4867\n",
            "Epoch 48/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9792 - acc: 0.6541\n",
            "Epoch 00048: val_loss improved from 1.39597 to 1.34541, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.9769 - acc: 0.6547 - val_loss: 1.3454 - val_acc: 0.5092\n",
            "Epoch 49/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9635 - acc: 0.6612\n",
            "Epoch 00049: val_loss did not improve from 1.34541\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.9657 - acc: 0.6606 - val_loss: 1.3752 - val_acc: 0.4950\n",
            "Epoch 50/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9475 - acc: 0.6735\n",
            "Epoch 00050: val_loss did not improve from 1.34541\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.9499 - acc: 0.6717 - val_loss: 1.3692 - val_acc: 0.4900\n",
            "Epoch 51/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9527 - acc: 0.6634\n",
            "Epoch 00051: val_loss improved from 1.34541 to 1.27095, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.9505 - acc: 0.6644 - val_loss: 1.2709 - val_acc: 0.5383\n",
            "Epoch 52/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9346 - acc: 0.6754\n",
            "Epoch 00052: val_loss improved from 1.27095 to 1.26487, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.9361 - acc: 0.6750 - val_loss: 1.2649 - val_acc: 0.5400\n",
            "Epoch 53/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.8856 - acc: 0.6971\n",
            "Epoch 00053: val_loss improved from 1.26487 to 1.23986, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.8905 - acc: 0.6964 - val_loss: 1.2399 - val_acc: 0.5542\n",
            "Epoch 54/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8689 - acc: 0.7021\n",
            "Epoch 00054: val_loss improved from 1.23986 to 1.17819, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.8721 - acc: 0.7000 - val_loss: 1.1782 - val_acc: 0.5708\n",
            "Epoch 55/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8415 - acc: 0.7145\n",
            "Epoch 00055: val_loss improved from 1.17819 to 1.12352, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.8398 - acc: 0.7178 - val_loss: 1.1235 - val_acc: 0.5908\n",
            "Epoch 56/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7970 - acc: 0.7388\n",
            "Epoch 00056: val_loss did not improve from 1.12352\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.8044 - acc: 0.7361 - val_loss: 1.3091 - val_acc: 0.5267\n",
            "Epoch 57/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8101 - acc: 0.7325\n",
            "Epoch 00057: val_loss did not improve from 1.12352\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.8087 - acc: 0.7319 - val_loss: 1.1333 - val_acc: 0.5833\n",
            "Epoch 58/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7699 - acc: 0.7416\n",
            "Epoch 00058: val_loss did not improve from 1.12352\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.7737 - acc: 0.7375 - val_loss: 1.2030 - val_acc: 0.5600\n",
            "Epoch 59/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7547 - acc: 0.7489\n",
            "Epoch 00059: val_loss did not improve from 1.12352\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.7576 - acc: 0.7461 - val_loss: 1.3095 - val_acc: 0.5142\n",
            "Epoch 60/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7411 - acc: 0.7485\n",
            "Epoch 00060: val_loss improved from 1.12352 to 1.01274, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.7432 - acc: 0.7475 - val_loss: 1.0127 - val_acc: 0.6425\n",
            "Epoch 61/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7228 - acc: 0.7550\n",
            "Epoch 00061: val_loss did not improve from 1.01274\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.7301 - acc: 0.7519 - val_loss: 1.1093 - val_acc: 0.5950\n",
            "Epoch 62/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7010 - acc: 0.7678\n",
            "Epoch 00062: val_loss improved from 1.01274 to 0.97861, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.6939 - acc: 0.7708 - val_loss: 0.9786 - val_acc: 0.6542\n",
            "Epoch 63/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7082 - acc: 0.7471\n",
            "Epoch 00063: val_loss did not improve from 0.97861\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.7155 - acc: 0.7433 - val_loss: 1.0407 - val_acc: 0.6108\n",
            "Epoch 64/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7016 - acc: 0.7611\n",
            "Epoch 00064: val_loss improved from 0.97861 to 0.91844, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.7007 - acc: 0.7614 - val_loss: 0.9184 - val_acc: 0.6867\n",
            "Epoch 65/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6333 - acc: 0.7894\n",
            "Epoch 00065: val_loss did not improve from 0.91844\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.6363 - acc: 0.7886 - val_loss: 1.0319 - val_acc: 0.6375\n",
            "Epoch 66/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6143 - acc: 0.8024\n",
            "Epoch 00066: val_loss did not improve from 0.91844\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.6180 - acc: 0.8017 - val_loss: 0.9377 - val_acc: 0.6500\n",
            "Epoch 67/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.6191 - acc: 0.8041\n",
            "Epoch 00067: val_loss improved from 0.91844 to 0.85910, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.6262 - acc: 0.8022 - val_loss: 0.8591 - val_acc: 0.7067\n",
            "Epoch 68/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5900 - acc: 0.8147\n",
            "Epoch 00068: val_loss did not improve from 0.85910\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.5982 - acc: 0.8100 - val_loss: 1.0177 - val_acc: 0.6583\n",
            "Epoch 69/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6088 - acc: 0.8015\n",
            "Epoch 00069: val_loss did not improve from 0.85910\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.6079 - acc: 0.8028 - val_loss: 0.8800 - val_acc: 0.6925\n",
            "Epoch 70/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5675 - acc: 0.8137\n",
            "Epoch 00070: val_loss did not improve from 0.85910\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.5689 - acc: 0.8128 - val_loss: 1.1727 - val_acc: 0.5650\n",
            "Epoch 71/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5547 - acc: 0.8217\n",
            "Epoch 00071: val_loss improved from 0.85910 to 0.75267, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.5562 - acc: 0.8208 - val_loss: 0.7527 - val_acc: 0.7558\n",
            "Epoch 72/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.5731 - acc: 0.8147\n",
            "Epoch 00072: val_loss improved from 0.75267 to 0.74478, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.5769 - acc: 0.8153 - val_loss: 0.7448 - val_acc: 0.7458\n",
            "Epoch 73/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5541 - acc: 0.8188\n",
            "Epoch 00073: val_loss did not improve from 0.74478\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.5567 - acc: 0.8183 - val_loss: 0.8217 - val_acc: 0.7308\n",
            "Epoch 74/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5228 - acc: 0.8318\n",
            "Epoch 00074: val_loss improved from 0.74478 to 0.71423, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.5244 - acc: 0.8319 - val_loss: 0.7142 - val_acc: 0.7775\n",
            "Epoch 75/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5169 - acc: 0.8380\n",
            "Epoch 00075: val_loss improved from 0.71423 to 0.67626, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.5196 - acc: 0.8364 - val_loss: 0.6763 - val_acc: 0.7850\n",
            "Epoch 76/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5171 - acc: 0.8336\n",
            "Epoch 00076: val_loss did not improve from 0.67626\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.5214 - acc: 0.8317 - val_loss: 0.7137 - val_acc: 0.7608\n",
            "Epoch 77/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4781 - acc: 0.8549\n",
            "Epoch 00077: val_loss did not improve from 0.67626\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.4783 - acc: 0.8539 - val_loss: 0.7945 - val_acc: 0.7167\n",
            "Epoch 78/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.4775 - acc: 0.8516\n",
            "Epoch 00078: val_loss did not improve from 0.67626\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.4835 - acc: 0.8506 - val_loss: 0.7004 - val_acc: 0.7608\n",
            "Epoch 79/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4969 - acc: 0.8376\n",
            "Epoch 00079: val_loss did not improve from 0.67626\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.4970 - acc: 0.8367 - val_loss: 0.7040 - val_acc: 0.7692\n",
            "Epoch 80/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4816 - acc: 0.8491\n",
            "Epoch 00080: val_loss improved from 0.67626 to 0.67341, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.4858 - acc: 0.8475 - val_loss: 0.6734 - val_acc: 0.7725\n",
            "Epoch 81/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4514 - acc: 0.8631\n",
            "Epoch 00081: val_loss improved from 0.67341 to 0.57829, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.4497 - acc: 0.8633 - val_loss: 0.5783 - val_acc: 0.8050\n",
            "Epoch 82/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4266 - acc: 0.8700\n",
            "Epoch 00082: val_loss did not improve from 0.57829\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.4348 - acc: 0.8639 - val_loss: 0.7059 - val_acc: 0.7592\n",
            "Epoch 83/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4427 - acc: 0.8646\n",
            "Epoch 00083: val_loss did not improve from 0.57829\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.4436 - acc: 0.8650 - val_loss: 0.5985 - val_acc: 0.8075\n",
            "Epoch 84/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4252 - acc: 0.8671\n",
            "Epoch 00084: val_loss improved from 0.57829 to 0.51317, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.4256 - acc: 0.8669 - val_loss: 0.5132 - val_acc: 0.8400\n",
            "Epoch 85/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4218 - acc: 0.8676\n",
            "Epoch 00085: val_loss did not improve from 0.51317\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.4240 - acc: 0.8664 - val_loss: 0.6132 - val_acc: 0.7983\n",
            "Epoch 86/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4152 - acc: 0.8647\n",
            "Epoch 00086: val_loss did not improve from 0.51317\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.4192 - acc: 0.8622 - val_loss: 0.5881 - val_acc: 0.7942\n",
            "Epoch 87/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3980 - acc: 0.8780\n",
            "Epoch 00087: val_loss did not improve from 0.51317\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.3998 - acc: 0.8772 - val_loss: 0.5337 - val_acc: 0.8283\n",
            "Epoch 88/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3860 - acc: 0.8823\n",
            "Epoch 00088: val_loss did not improve from 0.51317\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.3869 - acc: 0.8814 - val_loss: 0.5210 - val_acc: 0.8383\n",
            "Epoch 89/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3762 - acc: 0.8897\n",
            "Epoch 00089: val_loss did not improve from 0.51317\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.3768 - acc: 0.8897 - val_loss: 0.5225 - val_acc: 0.8325\n",
            "Epoch 90/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3825 - acc: 0.8781\n",
            "Epoch 00090: val_loss did not improve from 0.51317\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.3862 - acc: 0.8767 - val_loss: 0.6548 - val_acc: 0.7600\n",
            "Epoch 91/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3912 - acc: 0.8775\n",
            "Epoch 00091: val_loss did not improve from 0.51317\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.3933 - acc: 0.8756 - val_loss: 0.5805 - val_acc: 0.8117\n",
            "Epoch 92/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3827 - acc: 0.8838\n",
            "Epoch 00092: val_loss improved from 0.51317 to 0.47746, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.3820 - acc: 0.8850 - val_loss: 0.4775 - val_acc: 0.8433\n",
            "Epoch 93/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3520 - acc: 0.8938\n",
            "Epoch 00093: val_loss did not improve from 0.47746\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.3543 - acc: 0.8931 - val_loss: 0.5285 - val_acc: 0.8267\n",
            "Epoch 94/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3522 - acc: 0.9013\n",
            "Epoch 00094: val_loss improved from 0.47746 to 0.46930, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.3493 - acc: 0.9025 - val_loss: 0.4693 - val_acc: 0.8558\n",
            "Epoch 95/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3594 - acc: 0.8986\n",
            "Epoch 00095: val_loss did not improve from 0.46930\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.3580 - acc: 0.8994 - val_loss: 0.4742 - val_acc: 0.8567\n",
            "Epoch 96/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3240 - acc: 0.9033\n",
            "Epoch 00096: val_loss did not improve from 0.46930\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.3269 - acc: 0.9033 - val_loss: 0.4973 - val_acc: 0.8408\n",
            "Epoch 97/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3368 - acc: 0.8985\n",
            "Epoch 00097: val_loss did not improve from 0.46930\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.3371 - acc: 0.8975 - val_loss: 0.5720 - val_acc: 0.8017\n",
            "Epoch 98/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3206 - acc: 0.9032\n",
            "Epoch 00098: val_loss improved from 0.46930 to 0.40969, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.3252 - acc: 0.9008 - val_loss: 0.4097 - val_acc: 0.8808\n",
            "Epoch 99/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3454 - acc: 0.8921\n",
            "Epoch 00099: val_loss improved from 0.40969 to 0.39350, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.3468 - acc: 0.8900 - val_loss: 0.3935 - val_acc: 0.8825\n",
            "Epoch 100/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3309 - acc: 0.9016\n",
            "Epoch 00100: val_loss did not improve from 0.39350\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.3315 - acc: 0.9006 - val_loss: 0.4351 - val_acc: 0.8650\n",
            "Epoch 101/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3329 - acc: 0.9039\n",
            "Epoch 00101: val_loss did not improve from 0.39350\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.3339 - acc: 0.9036 - val_loss: 0.7324 - val_acc: 0.7300\n",
            "Epoch 102/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3284 - acc: 0.9000\n",
            "Epoch 00102: val_loss improved from 0.39350 to 0.35942, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.3268 - acc: 0.9017 - val_loss: 0.3594 - val_acc: 0.8908\n",
            "Epoch 103/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2974 - acc: 0.9159\n",
            "Epoch 00103: val_loss did not improve from 0.35942\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.3013 - acc: 0.9156 - val_loss: 0.5587 - val_acc: 0.8142\n",
            "Epoch 104/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.9009\n",
            "Epoch 00104: val_loss did not improve from 0.35942\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.3252 - acc: 0.8994 - val_loss: 0.5116 - val_acc: 0.8292\n",
            "Epoch 105/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3003 - acc: 0.9121\n",
            "Epoch 00105: val_loss did not improve from 0.35942\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.2989 - acc: 0.9119 - val_loss: 0.3958 - val_acc: 0.8783\n",
            "Epoch 106/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2937 - acc: 0.9128\n",
            "Epoch 00106: val_loss did not improve from 0.35942\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.2994 - acc: 0.9089 - val_loss: 0.5113 - val_acc: 0.8383\n",
            "Epoch 107/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3089 - acc: 0.9080\n",
            "Epoch 00107: val_loss did not improve from 0.35942\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.3072 - acc: 0.9089 - val_loss: 0.4131 - val_acc: 0.8742\n",
            "Epoch 108/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2943 - acc: 0.9121\n",
            "Epoch 00108: val_loss improved from 0.35942 to 0.34838, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.2936 - acc: 0.9125 - val_loss: 0.3484 - val_acc: 0.8925\n",
            "Epoch 109/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2650 - acc: 0.9226\n",
            "Epoch 00109: val_loss did not improve from 0.34838\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2669 - acc: 0.9217 - val_loss: 0.3738 - val_acc: 0.8833\n",
            "Epoch 110/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9194\n",
            "Epoch 00110: val_loss improved from 0.34838 to 0.34471, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.2724 - acc: 0.9200 - val_loss: 0.3447 - val_acc: 0.8992\n",
            "Epoch 111/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2649 - acc: 0.9230\n",
            "Epoch 00111: val_loss did not improve from 0.34471\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2704 - acc: 0.9214 - val_loss: 0.5144 - val_acc: 0.8275\n",
            "Epoch 112/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2777 - acc: 0.9111\n",
            "Epoch 00112: val_loss improved from 0.34471 to 0.32277, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.2806 - acc: 0.9097 - val_loss: 0.3228 - val_acc: 0.8942\n",
            "Epoch 113/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2904 - acc: 0.9109\n",
            "Epoch 00113: val_loss did not improve from 0.32277\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2898 - acc: 0.9108 - val_loss: 0.3287 - val_acc: 0.8983\n",
            "Epoch 114/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2728 - acc: 0.9176\n",
            "Epoch 00114: val_loss did not improve from 0.32277\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2762 - acc: 0.9178 - val_loss: 0.4211 - val_acc: 0.8683\n",
            "Epoch 115/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2575 - acc: 0.9265\n",
            "Epoch 00115: val_loss did not improve from 0.32277\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2626 - acc: 0.9231 - val_loss: 0.4922 - val_acc: 0.8233\n",
            "Epoch 116/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2416 - acc: 0.9341\n",
            "Epoch 00116: val_loss did not improve from 0.32277\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.2445 - acc: 0.9325 - val_loss: 0.3459 - val_acc: 0.8925\n",
            "Epoch 117/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9206\n",
            "Epoch 00117: val_loss did not improve from 0.32277\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.2556 - acc: 0.9211 - val_loss: 0.4239 - val_acc: 0.8600\n",
            "Epoch 118/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2570 - acc: 0.9269\n",
            "Epoch 00118: val_loss improved from 0.32277 to 0.31538, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.2574 - acc: 0.9269 - val_loss: 0.3154 - val_acc: 0.9050\n",
            "Epoch 119/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2598 - acc: 0.9212\n",
            "Epoch 00119: val_loss did not improve from 0.31538\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2669 - acc: 0.9183 - val_loss: 0.3694 - val_acc: 0.8825\n",
            "Epoch 120/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.9234\n",
            "Epoch 00120: val_loss improved from 0.31538 to 0.30401, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.2600 - acc: 0.9244 - val_loss: 0.3040 - val_acc: 0.9133\n",
            "Epoch 121/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2501 - acc: 0.9249\n",
            "Epoch 00121: val_loss did not improve from 0.30401\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2491 - acc: 0.9253 - val_loss: 0.3264 - val_acc: 0.8992\n",
            "Epoch 122/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.9360\n",
            "Epoch 00122: val_loss improved from 0.30401 to 0.29552, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.2415 - acc: 0.9353 - val_loss: 0.2955 - val_acc: 0.9125\n",
            "Epoch 123/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2328 - acc: 0.9378\n",
            "Epoch 00123: val_loss improved from 0.29552 to 0.27595, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.2304 - acc: 0.9400 - val_loss: 0.2759 - val_acc: 0.9233\n",
            "Epoch 124/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2312 - acc: 0.9326\n",
            "Epoch 00124: val_loss did not improve from 0.27595\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2301 - acc: 0.9333 - val_loss: 0.3229 - val_acc: 0.9050\n",
            "Epoch 125/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2405 - acc: 0.9260\n",
            "Epoch 00125: val_loss did not improve from 0.27595\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2412 - acc: 0.9256 - val_loss: 0.3176 - val_acc: 0.9017\n",
            "Epoch 126/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2362 - acc: 0.9291\n",
            "Epoch 00126: val_loss did not improve from 0.27595\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2356 - acc: 0.9303 - val_loss: 0.3376 - val_acc: 0.8942\n",
            "Epoch 127/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2226 - acc: 0.9376\n",
            "Epoch 00127: val_loss did not improve from 0.27595\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2256 - acc: 0.9369 - val_loss: 0.3074 - val_acc: 0.9058\n",
            "Epoch 128/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2407 - acc: 0.9280\n",
            "Epoch 00128: val_loss did not improve from 0.27595\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2412 - acc: 0.9275 - val_loss: 0.3082 - val_acc: 0.9100\n",
            "Epoch 129/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2348 - acc: 0.9324\n",
            "Epoch 00129: val_loss did not improve from 0.27595\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2352 - acc: 0.9317 - val_loss: 0.3204 - val_acc: 0.8958\n",
            "Epoch 130/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9463\n",
            "Epoch 00130: val_loss did not improve from 0.27595\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.2085 - acc: 0.9453 - val_loss: 0.3122 - val_acc: 0.9017\n",
            "Epoch 131/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2014 - acc: 0.9397\n",
            "Epoch 00131: val_loss improved from 0.27595 to 0.26665, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.2097 - acc: 0.9356 - val_loss: 0.2666 - val_acc: 0.9167\n",
            "Epoch 132/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2415 - acc: 0.9254\n",
            "Epoch 00132: val_loss improved from 0.26665 to 0.25939, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.2422 - acc: 0.9244 - val_loss: 0.2594 - val_acc: 0.9200\n",
            "Epoch 133/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9389\n",
            "Epoch 00133: val_loss improved from 0.25939 to 0.25195, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.2108 - acc: 0.9397 - val_loss: 0.2520 - val_acc: 0.9233\n",
            "Epoch 134/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2384 - acc: 0.9285\n",
            "Epoch 00134: val_loss did not improve from 0.25195\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2406 - acc: 0.9269 - val_loss: 0.3036 - val_acc: 0.9017\n",
            "Epoch 135/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2321 - acc: 0.9294\n",
            "Epoch 00135: val_loss improved from 0.25195 to 0.23264, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.2303 - acc: 0.9297 - val_loss: 0.2326 - val_acc: 0.9383\n",
            "Epoch 136/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2237 - acc: 0.9369\n",
            "Epoch 00136: val_loss improved from 0.23264 to 0.21778, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.2248 - acc: 0.9356 - val_loss: 0.2178 - val_acc: 0.9475\n",
            "Epoch 137/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2102 - acc: 0.9367\n",
            "Epoch 00137: val_loss did not improve from 0.21778\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2102 - acc: 0.9372 - val_loss: 0.2376 - val_acc: 0.9308\n",
            "Epoch 138/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2175 - acc: 0.9369\n",
            "Epoch 00138: val_loss did not improve from 0.21778\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.2183 - acc: 0.9367 - val_loss: 0.3382 - val_acc: 0.8925\n",
            "Epoch 139/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9306\n",
            "Epoch 00139: val_loss did not improve from 0.21778\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2189 - acc: 0.9311 - val_loss: 0.2435 - val_acc: 0.9158\n",
            "Epoch 140/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2224 - acc: 0.9314\n",
            "Epoch 00140: val_loss did not improve from 0.21778\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.2244 - acc: 0.9306 - val_loss: 0.3210 - val_acc: 0.9033\n",
            "Epoch 141/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2062 - acc: 0.9389\n",
            "Epoch 00141: val_loss did not improve from 0.21778\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.2082 - acc: 0.9381 - val_loss: 0.3780 - val_acc: 0.8742\n",
            "Epoch 142/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2017 - acc: 0.9400\n",
            "Epoch 00142: val_loss did not improve from 0.21778\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2074 - acc: 0.9394 - val_loss: 0.3300 - val_acc: 0.8975\n",
            "Epoch 143/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9371\n",
            "Epoch 00143: val_loss improved from 0.21778 to 0.20622, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.2150 - acc: 0.9367 - val_loss: 0.2062 - val_acc: 0.9408\n",
            "Epoch 144/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2049 - acc: 0.9403\n",
            "Epoch 00144: val_loss did not improve from 0.20622\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2053 - acc: 0.9408 - val_loss: 0.2658 - val_acc: 0.9183\n",
            "Epoch 145/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2091 - acc: 0.9381\n",
            "Epoch 00145: val_loss improved from 0.20622 to 0.19374, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.2102 - acc: 0.9375 - val_loss: 0.1937 - val_acc: 0.9442\n",
            "Epoch 146/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2104 - acc: 0.9388\n",
            "Epoch 00146: val_loss did not improve from 0.19374\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2106 - acc: 0.9389 - val_loss: 0.2658 - val_acc: 0.9225\n",
            "Epoch 147/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1998 - acc: 0.9391\n",
            "Epoch 00147: val_loss did not improve from 0.19374\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.2033 - acc: 0.9383 - val_loss: 0.2288 - val_acc: 0.9333\n",
            "Epoch 148/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1988 - acc: 0.9411\n",
            "Epoch 00148: val_loss did not improve from 0.19374\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1977 - acc: 0.9414 - val_loss: 0.2857 - val_acc: 0.9133\n",
            "Epoch 149/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2010 - acc: 0.9370\n",
            "Epoch 00149: val_loss did not improve from 0.19374\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2082 - acc: 0.9344 - val_loss: 0.2237 - val_acc: 0.9358\n",
            "Epoch 150/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1968 - acc: 0.9441\n",
            "Epoch 00150: val_loss did not improve from 0.19374\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1955 - acc: 0.9444 - val_loss: 0.2319 - val_acc: 0.9250\n",
            "Epoch 151/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1864 - acc: 0.9473\n",
            "Epoch 00151: val_loss did not improve from 0.19374\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1854 - acc: 0.9469 - val_loss: 0.2481 - val_acc: 0.9250\n",
            "Epoch 152/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1794 - acc: 0.9459\n",
            "Epoch 00152: val_loss did not improve from 0.19374\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1774 - acc: 0.9472 - val_loss: 0.1954 - val_acc: 0.9492\n",
            "Epoch 153/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1948 - acc: 0.9419\n",
            "Epoch 00153: val_loss did not improve from 0.19374\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1923 - acc: 0.9428 - val_loss: 0.2047 - val_acc: 0.9442\n",
            "Epoch 154/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1922 - acc: 0.9462\n",
            "Epoch 00154: val_loss did not improve from 0.19374\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1899 - acc: 0.9478 - val_loss: 0.2057 - val_acc: 0.9425\n",
            "Epoch 155/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9386\n",
            "Epoch 00155: val_loss did not improve from 0.19374\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2006 - acc: 0.9372 - val_loss: 0.3048 - val_acc: 0.9008\n",
            "Epoch 156/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1779 - acc: 0.9524\n",
            "Epoch 00156: val_loss did not improve from 0.19374\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1782 - acc: 0.9522 - val_loss: 0.3223 - val_acc: 0.9000\n",
            "Epoch 157/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1875 - acc: 0.9506\n",
            "Epoch 00157: val_loss did not improve from 0.19374\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1890 - acc: 0.9497 - val_loss: 0.2263 - val_acc: 0.9275\n",
            "Epoch 158/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1976 - acc: 0.9441\n",
            "Epoch 00158: val_loss did not improve from 0.19374\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1977 - acc: 0.9442 - val_loss: 0.2033 - val_acc: 0.9392\n",
            "Epoch 159/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1857 - acc: 0.9503\n",
            "Epoch 00159: val_loss did not improve from 0.19374\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1866 - acc: 0.9500 - val_loss: 0.1994 - val_acc: 0.9458\n",
            "Epoch 160/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1952 - acc: 0.9415\n",
            "Epoch 00160: val_loss did not improve from 0.19374\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1956 - acc: 0.9411 - val_loss: 0.2373 - val_acc: 0.9275\n",
            "Epoch 161/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1743 - acc: 0.9471\n",
            "Epoch 00161: val_loss did not improve from 0.19374\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1749 - acc: 0.9464 - val_loss: 0.1998 - val_acc: 0.9442\n",
            "Epoch 162/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1777 - acc: 0.9485\n",
            "Epoch 00162: val_loss did not improve from 0.19374\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.1792 - acc: 0.9481 - val_loss: 0.2154 - val_acc: 0.9408\n",
            "Epoch 163/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1967 - acc: 0.9406\n",
            "Epoch 00163: val_loss did not improve from 0.19374\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1976 - acc: 0.9400 - val_loss: 0.2716 - val_acc: 0.9117\n",
            "Epoch 164/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9491\n",
            "Epoch 00164: val_loss improved from 0.19374 to 0.16864, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1729 - acc: 0.9500 - val_loss: 0.1686 - val_acc: 0.9625\n",
            "Epoch 165/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1856 - acc: 0.9447\n",
            "Epoch 00165: val_loss did not improve from 0.16864\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1876 - acc: 0.9453 - val_loss: 0.1899 - val_acc: 0.9442\n",
            "Epoch 166/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1765 - acc: 0.9471\n",
            "Epoch 00166: val_loss did not improve from 0.16864\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1792 - acc: 0.9453 - val_loss: 0.1954 - val_acc: 0.9450\n",
            "Epoch 167/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1870 - acc: 0.9438\n",
            "Epoch 00167: val_loss improved from 0.16864 to 0.16343, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1911 - acc: 0.9428 - val_loss: 0.1634 - val_acc: 0.9542\n",
            "Epoch 168/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2017 - acc: 0.9394\n",
            "Epoch 00168: val_loss did not improve from 0.16343\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1992 - acc: 0.9397 - val_loss: 0.2090 - val_acc: 0.9383\n",
            "Epoch 169/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.9429\n",
            "Epoch 00169: val_loss did not improve from 0.16343\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1991 - acc: 0.9436 - val_loss: 0.1899 - val_acc: 0.9483\n",
            "Epoch 170/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1861 - acc: 0.9469\n",
            "Epoch 00170: val_loss did not improve from 0.16343\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1844 - acc: 0.9475 - val_loss: 0.2125 - val_acc: 0.9425\n",
            "Epoch 171/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1669 - acc: 0.9511\n",
            "Epoch 00171: val_loss did not improve from 0.16343\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1690 - acc: 0.9497 - val_loss: 0.2053 - val_acc: 0.9383\n",
            "Epoch 172/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1763 - acc: 0.9503\n",
            "Epoch 00172: val_loss did not improve from 0.16343\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1772 - acc: 0.9500 - val_loss: 0.1787 - val_acc: 0.9450\n",
            "Epoch 173/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1684 - acc: 0.9530\n",
            "Epoch 00173: val_loss did not improve from 0.16343\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1714 - acc: 0.9506 - val_loss: 0.3363 - val_acc: 0.8850\n",
            "Epoch 174/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1743 - acc: 0.9497\n",
            "Epoch 00174: val_loss did not improve from 0.16343\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1761 - acc: 0.9492 - val_loss: 0.1766 - val_acc: 0.9533\n",
            "Epoch 175/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1602 - acc: 0.9518\n",
            "Epoch 00175: val_loss did not improve from 0.16343\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1626 - acc: 0.9508 - val_loss: 0.1697 - val_acc: 0.9517\n",
            "Epoch 176/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9540\n",
            "Epoch 00176: val_loss improved from 0.16343 to 0.14704, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1564 - acc: 0.9539 - val_loss: 0.1470 - val_acc: 0.9583\n",
            "Epoch 177/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1492 - acc: 0.9616\n",
            "Epoch 00177: val_loss did not improve from 0.14704\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1520 - acc: 0.9600 - val_loss: 0.2070 - val_acc: 0.9367\n",
            "Epoch 178/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1709 - acc: 0.9469\n",
            "Epoch 00178: val_loss did not improve from 0.14704\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.1818 - acc: 0.9425 - val_loss: 0.1973 - val_acc: 0.9408\n",
            "Epoch 179/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9537\n",
            "Epoch 00179: val_loss improved from 0.14704 to 0.14430, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1669 - acc: 0.9542 - val_loss: 0.1443 - val_acc: 0.9625\n",
            "Epoch 180/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1884 - acc: 0.9388\n",
            "Epoch 00180: val_loss did not improve from 0.14430\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1905 - acc: 0.9372 - val_loss: 0.1919 - val_acc: 0.9392\n",
            "Epoch 181/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1784 - acc: 0.9465\n",
            "Epoch 00181: val_loss did not improve from 0.14430\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1774 - acc: 0.9464 - val_loss: 0.1691 - val_acc: 0.9483\n",
            "Epoch 182/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9523\n",
            "Epoch 00182: val_loss did not improve from 0.14430\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1582 - acc: 0.9514 - val_loss: 0.2667 - val_acc: 0.9242\n",
            "Epoch 183/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1553 - acc: 0.9550\n",
            "Epoch 00183: val_loss did not improve from 0.14430\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1598 - acc: 0.9544 - val_loss: 0.2122 - val_acc: 0.9308\n",
            "Epoch 184/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1752 - acc: 0.9494\n",
            "Epoch 00184: val_loss did not improve from 0.14430\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1727 - acc: 0.9500 - val_loss: 0.1942 - val_acc: 0.9425\n",
            "Epoch 185/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1772 - acc: 0.9461\n",
            "Epoch 00185: val_loss did not improve from 0.14430\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1771 - acc: 0.9472 - val_loss: 0.1965 - val_acc: 0.9408\n",
            "Epoch 186/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1634 - acc: 0.9522\n",
            "Epoch 00186: val_loss improved from 0.14430 to 0.12820, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1617 - acc: 0.9525 - val_loss: 0.1282 - val_acc: 0.9708\n",
            "Epoch 187/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1719 - acc: 0.9528\n",
            "Epoch 00187: val_loss did not improve from 0.12820\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1713 - acc: 0.9517 - val_loss: 0.2011 - val_acc: 0.9367\n",
            "Epoch 188/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1560 - acc: 0.9553\n",
            "Epoch 00188: val_loss did not improve from 0.12820\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1596 - acc: 0.9536 - val_loss: 0.1619 - val_acc: 0.9542\n",
            "Epoch 189/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1557 - acc: 0.9547\n",
            "Epoch 00189: val_loss did not improve from 0.12820\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1579 - acc: 0.9550 - val_loss: 0.1551 - val_acc: 0.9550\n",
            "Epoch 190/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1733 - acc: 0.9480\n",
            "Epoch 00190: val_loss did not improve from 0.12820\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1707 - acc: 0.9492 - val_loss: 0.1668 - val_acc: 0.9533\n",
            "Epoch 191/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1817 - acc: 0.9431\n",
            "Epoch 00191: val_loss did not improve from 0.12820\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1865 - acc: 0.9417 - val_loss: 0.1409 - val_acc: 0.9633\n",
            "Epoch 192/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1812 - acc: 0.9463\n",
            "Epoch 00192: val_loss did not improve from 0.12820\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1807 - acc: 0.9464 - val_loss: 0.3026 - val_acc: 0.9008\n",
            "Epoch 193/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1695 - acc: 0.9520\n",
            "Epoch 00193: val_loss did not improve from 0.12820\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1700 - acc: 0.9517 - val_loss: 0.1588 - val_acc: 0.9608\n",
            "Epoch 194/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9457\n",
            "Epoch 00194: val_loss did not improve from 0.12820\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1799 - acc: 0.9464 - val_loss: 0.1942 - val_acc: 0.9517\n",
            "Epoch 195/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1488 - acc: 0.9616\n",
            "Epoch 00195: val_loss did not improve from 0.12820\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1493 - acc: 0.9603 - val_loss: 0.1710 - val_acc: 0.9533\n",
            "Epoch 196/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1542 - acc: 0.9569\n",
            "Epoch 00196: val_loss did not improve from 0.12820\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.1525 - acc: 0.9575 - val_loss: 0.1624 - val_acc: 0.9542\n",
            "Epoch 197/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1695 - acc: 0.9476\n",
            "Epoch 00197: val_loss did not improve from 0.12820\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1682 - acc: 0.9486 - val_loss: 0.1809 - val_acc: 0.9442\n",
            "Epoch 198/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1622 - acc: 0.9503\n",
            "Epoch 00198: val_loss did not improve from 0.12820\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1595 - acc: 0.9525 - val_loss: 0.1548 - val_acc: 0.9583\n",
            "Epoch 199/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1761 - acc: 0.9458\n",
            "Epoch 00199: val_loss did not improve from 0.12820\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1739 - acc: 0.9467 - val_loss: 0.1606 - val_acc: 0.9567\n",
            "Epoch 200/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9620\n",
            "Epoch 00200: val_loss did not improve from 0.12820\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1436 - acc: 0.9617 - val_loss: 0.2283 - val_acc: 0.9308\n",
            "Epoch 201/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1558 - acc: 0.9541\n",
            "Epoch 00201: val_loss did not improve from 0.12820\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1557 - acc: 0.9542 - val_loss: 0.1618 - val_acc: 0.9558\n",
            "Epoch 202/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1458 - acc: 0.9603\n",
            "Epoch 00202: val_loss did not improve from 0.12820\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1462 - acc: 0.9594 - val_loss: 0.1821 - val_acc: 0.9408\n",
            "Epoch 203/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1499 - acc: 0.9585\n",
            "Epoch 00203: val_loss did not improve from 0.12820\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.1482 - acc: 0.9592 - val_loss: 0.1868 - val_acc: 0.9442\n",
            "Epoch 204/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1526 - acc: 0.9544\n",
            "Epoch 00204: val_loss improved from 0.12820 to 0.12503, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1530 - acc: 0.9525 - val_loss: 0.1250 - val_acc: 0.9683\n",
            "Epoch 205/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1576 - acc: 0.9573\n",
            "Epoch 00205: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1577 - acc: 0.9575 - val_loss: 0.1429 - val_acc: 0.9625\n",
            "Epoch 206/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1523 - acc: 0.9594\n",
            "Epoch 00206: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1548 - acc: 0.9567 - val_loss: 0.1302 - val_acc: 0.9600\n",
            "Epoch 207/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1433 - acc: 0.9588\n",
            "Epoch 00207: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1440 - acc: 0.9589 - val_loss: 0.1707 - val_acc: 0.9467\n",
            "Epoch 208/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.9574\n",
            "Epoch 00208: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1571 - acc: 0.9572 - val_loss: 0.1534 - val_acc: 0.9550\n",
            "Epoch 209/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9569\n",
            "Epoch 00209: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1474 - acc: 0.9550 - val_loss: 0.1571 - val_acc: 0.9517\n",
            "Epoch 210/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1426 - acc: 0.9578\n",
            "Epoch 00210: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1396 - acc: 0.9592 - val_loss: 0.1459 - val_acc: 0.9550\n",
            "Epoch 211/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1532 - acc: 0.9521\n",
            "Epoch 00211: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1503 - acc: 0.9542 - val_loss: 0.1356 - val_acc: 0.9633\n",
            "Epoch 212/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1420 - acc: 0.9594\n",
            "Epoch 00212: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1411 - acc: 0.9592 - val_loss: 0.1320 - val_acc: 0.9683\n",
            "Epoch 213/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1670 - acc: 0.9491\n",
            "Epoch 00213: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1684 - acc: 0.9494 - val_loss: 0.2008 - val_acc: 0.9367\n",
            "Epoch 214/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9474\n",
            "Epoch 00214: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1648 - acc: 0.9461 - val_loss: 0.2379 - val_acc: 0.9208\n",
            "Epoch 215/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9494\n",
            "Epoch 00215: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1654 - acc: 0.9483 - val_loss: 0.1533 - val_acc: 0.9583\n",
            "Epoch 216/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1592 - acc: 0.9524\n",
            "Epoch 00216: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1587 - acc: 0.9528 - val_loss: 0.1433 - val_acc: 0.9567\n",
            "Epoch 217/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9423\n",
            "Epoch 00217: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1806 - acc: 0.9419 - val_loss: 0.2861 - val_acc: 0.9092\n",
            "Epoch 218/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1650 - acc: 0.9500\n",
            "Epoch 00218: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1625 - acc: 0.9508 - val_loss: 0.1288 - val_acc: 0.9633\n",
            "Epoch 219/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1635 - acc: 0.9554\n",
            "Epoch 00219: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1625 - acc: 0.9556 - val_loss: 0.1402 - val_acc: 0.9575\n",
            "Epoch 220/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9654\n",
            "Epoch 00220: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1314 - acc: 0.9642 - val_loss: 0.1503 - val_acc: 0.9567\n",
            "Epoch 221/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9491\n",
            "Epoch 00221: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1644 - acc: 0.9486 - val_loss: 0.1359 - val_acc: 0.9600\n",
            "Epoch 222/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1445 - acc: 0.9570\n",
            "Epoch 00222: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1484 - acc: 0.9556 - val_loss: 0.1769 - val_acc: 0.9442\n",
            "Epoch 223/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1605 - acc: 0.9541\n",
            "Epoch 00223: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1613 - acc: 0.9547 - val_loss: 0.1713 - val_acc: 0.9442\n",
            "Epoch 224/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9489\n",
            "Epoch 00224: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1656 - acc: 0.9497 - val_loss: 0.2066 - val_acc: 0.9400\n",
            "Epoch 225/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1624 - acc: 0.9500\n",
            "Epoch 00225: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1601 - acc: 0.9514 - val_loss: 0.1465 - val_acc: 0.9600\n",
            "Epoch 226/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1440 - acc: 0.9539\n",
            "Epoch 00226: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1428 - acc: 0.9550 - val_loss: 0.1476 - val_acc: 0.9567\n",
            "Epoch 227/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9620\n",
            "Epoch 00227: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1378 - acc: 0.9622 - val_loss: 0.1402 - val_acc: 0.9592\n",
            "Epoch 228/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1495 - acc: 0.9549\n",
            "Epoch 00228: val_loss did not improve from 0.12503\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1496 - acc: 0.9550 - val_loss: 0.1269 - val_acc: 0.9625\n",
            "Epoch 229/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1409 - acc: 0.9584\n",
            "Epoch 00229: val_loss improved from 0.12503 to 0.12025, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1439 - acc: 0.9569 - val_loss: 0.1202 - val_acc: 0.9683\n",
            "Epoch 230/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1252 - acc: 0.9615\n",
            "Epoch 00230: val_loss did not improve from 0.12025\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1277 - acc: 0.9608 - val_loss: 0.1338 - val_acc: 0.9617\n",
            "Epoch 231/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1510 - acc: 0.9547\n",
            "Epoch 00231: val_loss did not improve from 0.12025\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1561 - acc: 0.9531 - val_loss: 0.1269 - val_acc: 0.9625\n",
            "Epoch 232/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.9549\n",
            "Epoch 00232: val_loss did not improve from 0.12025\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1446 - acc: 0.9533 - val_loss: 0.1656 - val_acc: 0.9483\n",
            "Epoch 233/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9603\n",
            "Epoch 00233: val_loss did not improve from 0.12025\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1387 - acc: 0.9603 - val_loss: 0.1476 - val_acc: 0.9608\n",
            "Epoch 234/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1518 - acc: 0.9518\n",
            "Epoch 00234: val_loss did not improve from 0.12025\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1532 - acc: 0.9514 - val_loss: 0.1573 - val_acc: 0.9575\n",
            "Epoch 235/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9634\n",
            "Epoch 00235: val_loss improved from 0.12025 to 0.10565, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1320 - acc: 0.9639 - val_loss: 0.1057 - val_acc: 0.9733\n",
            "Epoch 236/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1480 - acc: 0.9575\n",
            "Epoch 00236: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1484 - acc: 0.9578 - val_loss: 0.1221 - val_acc: 0.9692\n",
            "Epoch 237/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9549\n",
            "Epoch 00237: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1432 - acc: 0.9550 - val_loss: 0.1378 - val_acc: 0.9642\n",
            "Epoch 238/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1306 - acc: 0.9647\n",
            "Epoch 00238: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1318 - acc: 0.9642 - val_loss: 0.1773 - val_acc: 0.9450\n",
            "Epoch 239/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1572 - acc: 0.9547\n",
            "Epoch 00239: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.1570 - acc: 0.9536 - val_loss: 0.1151 - val_acc: 0.9683\n",
            "Epoch 240/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1433 - acc: 0.9585\n",
            "Epoch 00240: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1411 - acc: 0.9594 - val_loss: 0.1360 - val_acc: 0.9583\n",
            "Epoch 241/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1511 - acc: 0.9534\n",
            "Epoch 00241: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1543 - acc: 0.9522 - val_loss: 0.1894 - val_acc: 0.9433\n",
            "Epoch 242/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1493 - acc: 0.9534\n",
            "Epoch 00242: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1469 - acc: 0.9558 - val_loss: 0.1222 - val_acc: 0.9692\n",
            "Epoch 243/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1445 - acc: 0.9541\n",
            "Epoch 00243: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1446 - acc: 0.9536 - val_loss: 0.1266 - val_acc: 0.9658\n",
            "Epoch 244/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1624 - acc: 0.9515\n",
            "Epoch 00244: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1651 - acc: 0.9506 - val_loss: 0.1171 - val_acc: 0.9667\n",
            "Epoch 245/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1452 - acc: 0.9582\n",
            "Epoch 00245: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1450 - acc: 0.9586 - val_loss: 0.1320 - val_acc: 0.9550\n",
            "Epoch 246/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.9617\n",
            "Epoch 00246: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1347 - acc: 0.9625 - val_loss: 0.1450 - val_acc: 0.9542\n",
            "Epoch 247/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1506 - acc: 0.9524\n",
            "Epoch 00247: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.1493 - acc: 0.9536 - val_loss: 0.1433 - val_acc: 0.9592\n",
            "Epoch 248/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9531\n",
            "Epoch 00248: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1456 - acc: 0.9531 - val_loss: 0.1490 - val_acc: 0.9592\n",
            "Epoch 249/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.9589\n",
            "Epoch 00249: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1361 - acc: 0.9589 - val_loss: 0.1205 - val_acc: 0.9625\n",
            "Epoch 250/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1367 - acc: 0.9588\n",
            "Epoch 00250: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1383 - acc: 0.9586 - val_loss: 0.1184 - val_acc: 0.9717\n",
            "Epoch 251/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9649\n",
            "Epoch 00251: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1236 - acc: 0.9650 - val_loss: 0.1343 - val_acc: 0.9642\n",
            "Epoch 252/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1313 - acc: 0.9653\n",
            "Epoch 00252: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1340 - acc: 0.9631 - val_loss: 0.1162 - val_acc: 0.9667\n",
            "Epoch 253/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1299 - acc: 0.9662\n",
            "Epoch 00253: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1316 - acc: 0.9667 - val_loss: 0.1058 - val_acc: 0.9708\n",
            "Epoch 254/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1319 - acc: 0.9609\n",
            "Epoch 00254: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1323 - acc: 0.9608 - val_loss: 0.1106 - val_acc: 0.9708\n",
            "Epoch 255/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1367 - acc: 0.9591\n",
            "Epoch 00255: val_loss did not improve from 0.10565\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1381 - acc: 0.9589 - val_loss: 0.1997 - val_acc: 0.9508\n",
            "Epoch 256/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9606\n",
            "Epoch 00256: val_loss improved from 0.10565 to 0.10527, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1324 - acc: 0.9606 - val_loss: 0.1053 - val_acc: 0.9733\n",
            "Epoch 257/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1278 - acc: 0.9632\n",
            "Epoch 00257: val_loss did not improve from 0.10527\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1273 - acc: 0.9631 - val_loss: 0.1183 - val_acc: 0.9658\n",
            "Epoch 258/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1245 - acc: 0.9670\n",
            "Epoch 00258: val_loss did not improve from 0.10527\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1241 - acc: 0.9667 - val_loss: 0.1104 - val_acc: 0.9717\n",
            "Epoch 259/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9620\n",
            "Epoch 00259: val_loss did not improve from 0.10527\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1323 - acc: 0.9625 - val_loss: 0.1193 - val_acc: 0.9683\n",
            "Epoch 260/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9620\n",
            "Epoch 00260: val_loss did not improve from 0.10527\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1360 - acc: 0.9608 - val_loss: 0.1110 - val_acc: 0.9692\n",
            "Epoch 261/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1291 - acc: 0.9661\n",
            "Epoch 00261: val_loss did not improve from 0.10527\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1285 - acc: 0.9661 - val_loss: 0.1342 - val_acc: 0.9658\n",
            "Epoch 262/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1249 - acc: 0.9606\n",
            "Epoch 00262: val_loss did not improve from 0.10527\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1257 - acc: 0.9600 - val_loss: 0.1226 - val_acc: 0.9650\n",
            "Epoch 263/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9669\n",
            "Epoch 00263: val_loss did not improve from 0.10527\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1256 - acc: 0.9664 - val_loss: 0.1514 - val_acc: 0.9575\n",
            "Epoch 264/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1137 - acc: 0.9668\n",
            "Epoch 00264: val_loss did not improve from 0.10527\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1117 - acc: 0.9675 - val_loss: 0.1249 - val_acc: 0.9642\n",
            "Epoch 265/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9660\n",
            "Epoch 00265: val_loss improved from 0.10527 to 0.09775, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.1226 - acc: 0.9661 - val_loss: 0.0977 - val_acc: 0.9742\n",
            "Epoch 266/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1334 - acc: 0.9612\n",
            "Epoch 00266: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1341 - acc: 0.9608 - val_loss: 0.1126 - val_acc: 0.9683\n",
            "Epoch 267/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1266 - acc: 0.9646\n",
            "Epoch 00267: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1285 - acc: 0.9636 - val_loss: 0.1859 - val_acc: 0.9383\n",
            "Epoch 268/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1342 - acc: 0.9600\n",
            "Epoch 00268: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1352 - acc: 0.9594 - val_loss: 0.1436 - val_acc: 0.9550\n",
            "Epoch 269/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1397 - acc: 0.9594\n",
            "Epoch 00269: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1382 - acc: 0.9600 - val_loss: 0.1362 - val_acc: 0.9617\n",
            "Epoch 270/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1222 - acc: 0.9631\n",
            "Epoch 00270: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1229 - acc: 0.9625 - val_loss: 0.1069 - val_acc: 0.9683\n",
            "Epoch 271/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1332 - acc: 0.9603\n",
            "Epoch 00271: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1351 - acc: 0.9600 - val_loss: 0.1185 - val_acc: 0.9658\n",
            "Epoch 272/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9614\n",
            "Epoch 00272: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1210 - acc: 0.9622 - val_loss: 0.1553 - val_acc: 0.9608\n",
            "Epoch 273/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1035 - acc: 0.9745\n",
            "Epoch 00273: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1034 - acc: 0.9744 - val_loss: 0.1337 - val_acc: 0.9650\n",
            "Epoch 274/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1233 - acc: 0.9631\n",
            "Epoch 00274: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1243 - acc: 0.9628 - val_loss: 0.1008 - val_acc: 0.9767\n",
            "Epoch 275/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1284 - acc: 0.9621\n",
            "Epoch 00275: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1295 - acc: 0.9622 - val_loss: 0.1117 - val_acc: 0.9692\n",
            "Epoch 276/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9566\n",
            "Epoch 00276: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1337 - acc: 0.9561 - val_loss: 0.1172 - val_acc: 0.9592\n",
            "Epoch 277/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1197 - acc: 0.9649\n",
            "Epoch 00277: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1189 - acc: 0.9656 - val_loss: 0.1855 - val_acc: 0.9383\n",
            "Epoch 278/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1159 - acc: 0.9682\n",
            "Epoch 00278: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1163 - acc: 0.9675 - val_loss: 0.1307 - val_acc: 0.9617\n",
            "Epoch 279/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1140 - acc: 0.9691\n",
            "Epoch 00279: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1136 - acc: 0.9683 - val_loss: 0.1240 - val_acc: 0.9625\n",
            "Epoch 280/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1088 - acc: 0.9715\n",
            "Epoch 00280: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1075 - acc: 0.9711 - val_loss: 0.1363 - val_acc: 0.9550\n",
            "Epoch 281/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9663\n",
            "Epoch 00281: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1076 - acc: 0.9661 - val_loss: 0.1131 - val_acc: 0.9650\n",
            "Epoch 282/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9746\n",
            "Epoch 00282: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0985 - acc: 0.9739 - val_loss: 0.1295 - val_acc: 0.9658\n",
            "Epoch 283/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1176 - acc: 0.9656\n",
            "Epoch 00283: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 181us/sample - loss: 0.1207 - acc: 0.9647 - val_loss: 0.1537 - val_acc: 0.9550\n",
            "Epoch 284/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1358 - acc: 0.9578\n",
            "Epoch 00284: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1416 - acc: 0.9558 - val_loss: 0.1529 - val_acc: 0.9550\n",
            "Epoch 285/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1254 - acc: 0.9668\n",
            "Epoch 00285: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1248 - acc: 0.9667 - val_loss: 0.1037 - val_acc: 0.9708\n",
            "Epoch 286/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1137 - acc: 0.9694\n",
            "Epoch 00286: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1128 - acc: 0.9697 - val_loss: 0.1174 - val_acc: 0.9700\n",
            "Epoch 287/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1153 - acc: 0.9641\n",
            "Epoch 00287: val_loss did not improve from 0.09775\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1145 - acc: 0.9647 - val_loss: 0.1017 - val_acc: 0.9708\n",
            "Epoch 288/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1203 - acc: 0.9664\n",
            "Epoch 00288: val_loss improved from 0.09775 to 0.09720, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1181 - acc: 0.9681 - val_loss: 0.0972 - val_acc: 0.9725\n",
            "Epoch 289/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1119 - acc: 0.9663\n",
            "Epoch 00289: val_loss did not improve from 0.09720\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1108 - acc: 0.9681 - val_loss: 0.1456 - val_acc: 0.9608\n",
            "Epoch 290/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1115 - acc: 0.9679\n",
            "Epoch 00290: val_loss did not improve from 0.09720\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1101 - acc: 0.9686 - val_loss: 0.1156 - val_acc: 0.9725\n",
            "Epoch 291/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9711\n",
            "Epoch 00291: val_loss did not improve from 0.09720\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1052 - acc: 0.9714 - val_loss: 0.1043 - val_acc: 0.9692\n",
            "Epoch 292/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1040 - acc: 0.9700\n",
            "Epoch 00292: val_loss did not improve from 0.09720\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1099 - acc: 0.9675 - val_loss: 0.2192 - val_acc: 0.9300\n",
            "Epoch 293/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9631\n",
            "Epoch 00293: val_loss improved from 0.09720 to 0.09235, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1258 - acc: 0.9617 - val_loss: 0.0924 - val_acc: 0.9783\n",
            "Epoch 294/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9637\n",
            "Epoch 00294: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1212 - acc: 0.9631 - val_loss: 0.1186 - val_acc: 0.9642\n",
            "Epoch 295/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9706\n",
            "Epoch 00295: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1105 - acc: 0.9703 - val_loss: 0.1062 - val_acc: 0.9683\n",
            "Epoch 296/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1119 - acc: 0.9691\n",
            "Epoch 00296: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1160 - acc: 0.9675 - val_loss: 0.2239 - val_acc: 0.9300\n",
            "Epoch 297/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1205 - acc: 0.9650\n",
            "Epoch 00297: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1228 - acc: 0.9639 - val_loss: 0.1222 - val_acc: 0.9650\n",
            "Epoch 298/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1390 - acc: 0.9615\n",
            "Epoch 00298: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1388 - acc: 0.9617 - val_loss: 0.1103 - val_acc: 0.9625\n",
            "Epoch 299/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1284 - acc: 0.9638\n",
            "Epoch 00299: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1306 - acc: 0.9628 - val_loss: 0.1115 - val_acc: 0.9692\n",
            "Epoch 300/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9634\n",
            "Epoch 00300: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1261 - acc: 0.9642 - val_loss: 0.1012 - val_acc: 0.9708\n",
            "Epoch 301/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9683\n",
            "Epoch 00301: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1072 - acc: 0.9675 - val_loss: 0.0998 - val_acc: 0.9700\n",
            "Epoch 302/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9669\n",
            "Epoch 00302: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1160 - acc: 0.9675 - val_loss: 0.0968 - val_acc: 0.9708\n",
            "Epoch 303/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1315 - acc: 0.9622\n",
            "Epoch 00303: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1288 - acc: 0.9633 - val_loss: 0.1311 - val_acc: 0.9642\n",
            "Epoch 304/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1113 - acc: 0.9697\n",
            "Epoch 00304: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.1136 - acc: 0.9692 - val_loss: 0.1023 - val_acc: 0.9717\n",
            "Epoch 305/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1148 - acc: 0.9647\n",
            "Epoch 00305: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1155 - acc: 0.9639 - val_loss: 0.0943 - val_acc: 0.9750\n",
            "Epoch 306/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1290 - acc: 0.9606\n",
            "Epoch 00306: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1301 - acc: 0.9600 - val_loss: 0.1065 - val_acc: 0.9667\n",
            "Epoch 307/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1271 - acc: 0.9616\n",
            "Epoch 00307: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1290 - acc: 0.9600 - val_loss: 0.1034 - val_acc: 0.9767\n",
            "Epoch 308/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1233 - acc: 0.9609\n",
            "Epoch 00308: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1235 - acc: 0.9606 - val_loss: 0.1139 - val_acc: 0.9675\n",
            "Epoch 309/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1247 - acc: 0.9612\n",
            "Epoch 00309: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1256 - acc: 0.9617 - val_loss: 0.0982 - val_acc: 0.9767\n",
            "Epoch 310/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1241 - acc: 0.9645\n",
            "Epoch 00310: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1259 - acc: 0.9631 - val_loss: 0.1073 - val_acc: 0.9692\n",
            "Epoch 311/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1099 - acc: 0.9642\n",
            "Epoch 00311: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 181us/sample - loss: 0.1158 - acc: 0.9633 - val_loss: 0.1035 - val_acc: 0.9717\n",
            "Epoch 312/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1118 - acc: 0.9682\n",
            "Epoch 00312: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.1183 - acc: 0.9656 - val_loss: 0.1648 - val_acc: 0.9600\n",
            "Epoch 313/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1147 - acc: 0.9691\n",
            "Epoch 00313: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.1140 - acc: 0.9689 - val_loss: 0.0976 - val_acc: 0.9742\n",
            "Epoch 314/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9577\n",
            "Epoch 00314: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1361 - acc: 0.9578 - val_loss: 0.1097 - val_acc: 0.9708\n",
            "Epoch 315/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9640\n",
            "Epoch 00315: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1210 - acc: 0.9636 - val_loss: 0.1115 - val_acc: 0.9750\n",
            "Epoch 316/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1401 - acc: 0.9582\n",
            "Epoch 00316: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 181us/sample - loss: 0.1422 - acc: 0.9578 - val_loss: 0.1370 - val_acc: 0.9617\n",
            "Epoch 317/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1351 - acc: 0.9585\n",
            "Epoch 00317: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1337 - acc: 0.9592 - val_loss: 0.1885 - val_acc: 0.9417\n",
            "Epoch 318/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1151 - acc: 0.9645\n",
            "Epoch 00318: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1128 - acc: 0.9656 - val_loss: 0.1024 - val_acc: 0.9767\n",
            "Epoch 319/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1081 - acc: 0.9691\n",
            "Epoch 00319: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.1078 - acc: 0.9692 - val_loss: 0.1100 - val_acc: 0.9675\n",
            "Epoch 320/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1119 - acc: 0.9682\n",
            "Epoch 00320: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1137 - acc: 0.9669 - val_loss: 0.1382 - val_acc: 0.9617\n",
            "Epoch 321/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9669\n",
            "Epoch 00321: val_loss did not improve from 0.09235\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1130 - acc: 0.9667 - val_loss: 0.1102 - val_acc: 0.9717\n",
            "Epoch 322/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9706\n",
            "Epoch 00322: val_loss improved from 0.09235 to 0.08584, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1060 - acc: 0.9706 - val_loss: 0.0858 - val_acc: 0.9767\n",
            "Epoch 323/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9637\n",
            "Epoch 00323: val_loss improved from 0.08584 to 0.07565, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1184 - acc: 0.9636 - val_loss: 0.0757 - val_acc: 0.9808\n",
            "Epoch 324/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1515 - acc: 0.9532\n",
            "Epoch 00324: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1490 - acc: 0.9544 - val_loss: 0.1053 - val_acc: 0.9642\n",
            "Epoch 325/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1173 - acc: 0.9673\n",
            "Epoch 00325: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1198 - acc: 0.9669 - val_loss: 0.0912 - val_acc: 0.9717\n",
            "Epoch 326/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1187 - acc: 0.9650\n",
            "Epoch 00326: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1192 - acc: 0.9644 - val_loss: 0.1030 - val_acc: 0.9692\n",
            "Epoch 327/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9634\n",
            "Epoch 00327: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1219 - acc: 0.9639 - val_loss: 0.1122 - val_acc: 0.9708\n",
            "Epoch 328/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9694\n",
            "Epoch 00328: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1131 - acc: 0.9694 - val_loss: 0.0957 - val_acc: 0.9733\n",
            "Epoch 329/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9674\n",
            "Epoch 00329: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1123 - acc: 0.9678 - val_loss: 0.1117 - val_acc: 0.9633\n",
            "Epoch 330/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9720\n",
            "Epoch 00330: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1045 - acc: 0.9719 - val_loss: 0.0889 - val_acc: 0.9792\n",
            "Epoch 331/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1006 - acc: 0.9712\n",
            "Epoch 00331: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1049 - acc: 0.9694 - val_loss: 0.1221 - val_acc: 0.9750\n",
            "Epoch 332/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1027 - acc: 0.9681\n",
            "Epoch 00332: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.0997 - acc: 0.9692 - val_loss: 0.0902 - val_acc: 0.9767\n",
            "Epoch 333/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1089 - acc: 0.9672\n",
            "Epoch 00333: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1102 - acc: 0.9661 - val_loss: 0.0990 - val_acc: 0.9708\n",
            "Epoch 334/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0955 - acc: 0.9752\n",
            "Epoch 00334: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1003 - acc: 0.9736 - val_loss: 0.2286 - val_acc: 0.9317\n",
            "Epoch 335/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1091 - acc: 0.9681\n",
            "Epoch 00335: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1125 - acc: 0.9672 - val_loss: 0.1076 - val_acc: 0.9733\n",
            "Epoch 336/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1078 - acc: 0.9663\n",
            "Epoch 00336: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1121 - acc: 0.9650 - val_loss: 0.1049 - val_acc: 0.9717\n",
            "Epoch 337/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9703\n",
            "Epoch 00337: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1054 - acc: 0.9697 - val_loss: 0.0955 - val_acc: 0.9783\n",
            "Epoch 338/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1226 - acc: 0.9624\n",
            "Epoch 00338: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1206 - acc: 0.9633 - val_loss: 0.1138 - val_acc: 0.9675\n",
            "Epoch 339/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1187 - acc: 0.9630\n",
            "Epoch 00339: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1177 - acc: 0.9631 - val_loss: 0.1151 - val_acc: 0.9650\n",
            "Epoch 340/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9669\n",
            "Epoch 00340: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1145 - acc: 0.9672 - val_loss: 0.0841 - val_acc: 0.9767\n",
            "Epoch 341/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9709\n",
            "Epoch 00341: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0973 - acc: 0.9717 - val_loss: 0.0989 - val_acc: 0.9692\n",
            "Epoch 342/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1134 - acc: 0.9665\n",
            "Epoch 00342: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1126 - acc: 0.9672 - val_loss: 0.0803 - val_acc: 0.9775\n",
            "Epoch 343/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9597\n",
            "Epoch 00343: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1327 - acc: 0.9594 - val_loss: 0.1087 - val_acc: 0.9700\n",
            "Epoch 344/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1378 - acc: 0.9565\n",
            "Epoch 00344: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1400 - acc: 0.9556 - val_loss: 0.0980 - val_acc: 0.9692\n",
            "Epoch 345/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1282 - acc: 0.9591\n",
            "Epoch 00345: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1280 - acc: 0.9589 - val_loss: 0.1171 - val_acc: 0.9658\n",
            "Epoch 346/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0993 - acc: 0.9728\n",
            "Epoch 00346: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.0975 - acc: 0.9733 - val_loss: 0.0931 - val_acc: 0.9775\n",
            "Epoch 347/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1060 - acc: 0.9682\n",
            "Epoch 00347: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1086 - acc: 0.9683 - val_loss: 0.0811 - val_acc: 0.9833\n",
            "Epoch 348/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9711\n",
            "Epoch 00348: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1025 - acc: 0.9711 - val_loss: 0.1304 - val_acc: 0.9642\n",
            "Epoch 349/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1059 - acc: 0.9630\n",
            "Epoch 00349: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1068 - acc: 0.9622 - val_loss: 0.1097 - val_acc: 0.9708\n",
            "Epoch 350/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1319 - acc: 0.9606\n",
            "Epoch 00350: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1297 - acc: 0.9606 - val_loss: 0.1155 - val_acc: 0.9683\n",
            "Epoch 351/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9737\n",
            "Epoch 00351: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1021 - acc: 0.9733 - val_loss: 0.0874 - val_acc: 0.9750\n",
            "Epoch 352/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9683\n",
            "Epoch 00352: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1035 - acc: 0.9675 - val_loss: 0.1299 - val_acc: 0.9658\n",
            "Epoch 353/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9714\n",
            "Epoch 00353: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1069 - acc: 0.9717 - val_loss: 0.0870 - val_acc: 0.9750\n",
            "Epoch 354/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9723\n",
            "Epoch 00354: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1018 - acc: 0.9722 - val_loss: 0.1011 - val_acc: 0.9675\n",
            "Epoch 355/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0958 - acc: 0.9725\n",
            "Epoch 00355: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0972 - acc: 0.9706 - val_loss: 0.1132 - val_acc: 0.9675\n",
            "Epoch 356/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1057 - acc: 0.9706\n",
            "Epoch 00356: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1045 - acc: 0.9700 - val_loss: 0.1024 - val_acc: 0.9692\n",
            "Epoch 357/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1028 - acc: 0.9697\n",
            "Epoch 00357: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1048 - acc: 0.9686 - val_loss: 0.0830 - val_acc: 0.9775\n",
            "Epoch 358/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9631\n",
            "Epoch 00358: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1239 - acc: 0.9628 - val_loss: 0.1223 - val_acc: 0.9625\n",
            "Epoch 359/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1116 - acc: 0.9691\n",
            "Epoch 00359: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1144 - acc: 0.9667 - val_loss: 0.0981 - val_acc: 0.9750\n",
            "Epoch 360/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9611\n",
            "Epoch 00360: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1302 - acc: 0.9608 - val_loss: 0.1070 - val_acc: 0.9700\n",
            "Epoch 361/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9694\n",
            "Epoch 00361: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1068 - acc: 0.9694 - val_loss: 0.1306 - val_acc: 0.9608\n",
            "Epoch 362/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1208 - acc: 0.9615\n",
            "Epoch 00362: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1198 - acc: 0.9614 - val_loss: 0.1009 - val_acc: 0.9683\n",
            "Epoch 363/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9686\n",
            "Epoch 00363: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1123 - acc: 0.9686 - val_loss: 0.0820 - val_acc: 0.9808\n",
            "Epoch 364/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9729\n",
            "Epoch 00364: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0995 - acc: 0.9731 - val_loss: 0.1078 - val_acc: 0.9675\n",
            "Epoch 365/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9703\n",
            "Epoch 00365: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1032 - acc: 0.9706 - val_loss: 0.0970 - val_acc: 0.9700\n",
            "Epoch 366/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9669\n",
            "Epoch 00366: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1128 - acc: 0.9669 - val_loss: 0.0865 - val_acc: 0.9750\n",
            "Epoch 367/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1100 - acc: 0.9667\n",
            "Epoch 00367: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1114 - acc: 0.9664 - val_loss: 0.1492 - val_acc: 0.9617\n",
            "Epoch 368/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1081 - acc: 0.9697\n",
            "Epoch 00368: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1077 - acc: 0.9697 - val_loss: 0.1130 - val_acc: 0.9642\n",
            "Epoch 369/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0819 - acc: 0.9779\n",
            "Epoch 00369: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.0901 - acc: 0.9747 - val_loss: 0.1567 - val_acc: 0.9558\n",
            "Epoch 370/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1142 - acc: 0.9669\n",
            "Epoch 00370: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1146 - acc: 0.9667 - val_loss: 0.1114 - val_acc: 0.9700\n",
            "Epoch 371/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9703\n",
            "Epoch 00371: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1026 - acc: 0.9706 - val_loss: 0.1036 - val_acc: 0.9683\n",
            "Epoch 372/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1170 - acc: 0.9656\n",
            "Epoch 00372: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1181 - acc: 0.9647 - val_loss: 0.0789 - val_acc: 0.9808\n",
            "Epoch 373/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9609\n",
            "Epoch 00373: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1296 - acc: 0.9603 - val_loss: 0.1107 - val_acc: 0.9667\n",
            "Epoch 374/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1234 - acc: 0.9609\n",
            "Epoch 00374: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1227 - acc: 0.9606 - val_loss: 0.0789 - val_acc: 0.9800\n",
            "Epoch 375/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1164 - acc: 0.9650\n",
            "Epoch 00375: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1121 - acc: 0.9667 - val_loss: 0.0907 - val_acc: 0.9733\n",
            "Epoch 376/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1098 - acc: 0.9697\n",
            "Epoch 00376: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1090 - acc: 0.9697 - val_loss: 0.1047 - val_acc: 0.9708\n",
            "Epoch 377/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1094 - acc: 0.9688\n",
            "Epoch 00377: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1090 - acc: 0.9689 - val_loss: 0.1070 - val_acc: 0.9717\n",
            "Epoch 378/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0956 - acc: 0.9747\n",
            "Epoch 00378: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.0949 - acc: 0.9742 - val_loss: 0.1607 - val_acc: 0.9525\n",
            "Epoch 379/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1226 - acc: 0.9600\n",
            "Epoch 00379: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1194 - acc: 0.9625 - val_loss: 0.0855 - val_acc: 0.9733\n",
            "Epoch 380/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1091 - acc: 0.9679\n",
            "Epoch 00380: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1110 - acc: 0.9669 - val_loss: 0.0909 - val_acc: 0.9733\n",
            "Epoch 381/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1018 - acc: 0.9718\n",
            "Epoch 00381: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1024 - acc: 0.9714 - val_loss: 0.1244 - val_acc: 0.9633\n",
            "Epoch 382/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0938 - acc: 0.9718\n",
            "Epoch 00382: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0961 - acc: 0.9719 - val_loss: 0.1082 - val_acc: 0.9725\n",
            "Epoch 383/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1056 - acc: 0.9664\n",
            "Epoch 00383: val_loss did not improve from 0.07565\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1046 - acc: 0.9667 - val_loss: 0.0843 - val_acc: 0.9767\n",
            "Epoch 384/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1151 - acc: 0.9613\n",
            "Epoch 00384: val_loss improved from 0.07565 to 0.07549, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1154 - acc: 0.9611 - val_loss: 0.0755 - val_acc: 0.9833\n",
            "Epoch 385/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.9663\n",
            "Epoch 00385: val_loss did not improve from 0.07549\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1126 - acc: 0.9658 - val_loss: 0.1504 - val_acc: 0.9508\n",
            "Epoch 386/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9640\n",
            "Epoch 00386: val_loss improved from 0.07549 to 0.06775, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1173 - acc: 0.9647 - val_loss: 0.0677 - val_acc: 0.9833\n",
            "Epoch 387/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1050 - acc: 0.9676\n",
            "Epoch 00387: val_loss did not improve from 0.06775\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1094 - acc: 0.9669 - val_loss: 0.0744 - val_acc: 0.9783\n",
            "Epoch 388/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9680\n",
            "Epoch 00388: val_loss did not improve from 0.06775\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1056 - acc: 0.9678 - val_loss: 0.0907 - val_acc: 0.9708\n",
            "Epoch 389/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0927 - acc: 0.9768\n",
            "Epoch 00389: val_loss did not improve from 0.06775\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0925 - acc: 0.9769 - val_loss: 0.0914 - val_acc: 0.9717\n",
            "Epoch 390/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0991 - acc: 0.9706\n",
            "Epoch 00390: val_loss did not improve from 0.06775\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1029 - acc: 0.9697 - val_loss: 0.0689 - val_acc: 0.9817\n",
            "Epoch 391/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9709\n",
            "Epoch 00391: val_loss did not improve from 0.06775\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1041 - acc: 0.9706 - val_loss: 0.1009 - val_acc: 0.9708\n",
            "Epoch 392/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1038 - acc: 0.9725\n",
            "Epoch 00392: val_loss did not improve from 0.06775\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1039 - acc: 0.9717 - val_loss: 0.1015 - val_acc: 0.9692\n",
            "Epoch 393/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0982 - acc: 0.9725\n",
            "Epoch 00393: val_loss did not improve from 0.06775\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0959 - acc: 0.9728 - val_loss: 0.0874 - val_acc: 0.9733\n",
            "Epoch 394/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0878 - acc: 0.9730\n",
            "Epoch 00394: val_loss did not improve from 0.06775\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0891 - acc: 0.9725 - val_loss: 0.1087 - val_acc: 0.9650\n",
            "Epoch 395/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1004 - acc: 0.9718\n",
            "Epoch 00395: val_loss did not improve from 0.06775\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0997 - acc: 0.9717 - val_loss: 0.0812 - val_acc: 0.9792\n",
            "Epoch 396/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0934 - acc: 0.9741\n",
            "Epoch 00396: val_loss did not improve from 0.06775\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0945 - acc: 0.9733 - val_loss: 0.0903 - val_acc: 0.9733\n",
            "Epoch 397/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9697\n",
            "Epoch 00397: val_loss did not improve from 0.06775\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0977 - acc: 0.9700 - val_loss: 0.0904 - val_acc: 0.9775\n",
            "Epoch 398/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0941 - acc: 0.9735\n",
            "Epoch 00398: val_loss did not improve from 0.06775\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0938 - acc: 0.9736 - val_loss: 0.0851 - val_acc: 0.9758\n",
            "Epoch 399/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1461 - acc: 0.9513\n",
            "Epoch 00399: val_loss did not improve from 0.06775\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1430 - acc: 0.9533 - val_loss: 0.1210 - val_acc: 0.9625\n",
            "Epoch 400/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9611\n",
            "Epoch 00400: val_loss did not improve from 0.06775\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1177 - acc: 0.9617 - val_loss: 0.1071 - val_acc: 0.9700\n",
            "1200/1200 [==============================] - 0s 115us/sample - loss: 0.1071 - acc: 0.9700\n",
            "[0.10707788288593292, 0.97]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKtg_WhdqJKn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "1e684d74-8769-47bd-d566-4e541d1e5efc"
      },
      "source": [
        "for i in range(4, 5): # Итерација низ секој испитен примерок\n",
        "  print(f\"====================== Примерок ({i}) ======================\")\n",
        "  print(\"Вчитување тест податоци од испитниот примерок \" + str(i) + \"...\")\n",
        "  \n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_test_data = np.empty(0)\n",
        "  temp_test_events = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_test_set = 'S' + format(j, '02') + '/Test'\n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testData.mat\"\n",
        "    temp = loadmat(full_path)['testData']\n",
        "    if temp_test_data.size != 0:\n",
        "      temp_test_data = np.concatenate((temp_test_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_test_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_test_events.size != 0:\n",
        "        temp_test_events = np.append(temp_test_events, temp)\n",
        "      else:\n",
        "        temp_test_events = np.array(temp)\n",
        "\n",
        "    # Вчитување на бројот на runs \n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/runs_per_block.txt\"\n",
        "    with open(full_path, \"r\") as runs_per_block:\n",
        "      test_runs_per_block[i-1][j-1] = int(runs_per_block.read())\n",
        "\n",
        "    print(\"\\t - Тест податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "  # Зачувај ги тест податоците вчитани од испитниот примерок во низа\n",
        "  test_data.append(temp_test_data)\n",
        "  test_events.append(temp_test_events)\n",
        "  print(\"Тест податоците од испитниот примерок \" + str(i) + \" се вчитани.\\n\")\n",
        "\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_data: \" + str(test_data[i-1].shape)) # test_data to predict\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_events: \" + str(len(test_events[i-1]))) # test_events\n",
        "  for j in range (1,4):\n",
        "    print(\"SBJ\" + str(format(i-1, '02')) + \" / S\" + str(format(j-1, '02')) + \"| Runs per block: \" + str(test_runs_per_block[i-1][j-1])) # runs per block in SJB01, SJ00 \n",
        "\n",
        "  to_predict_data = reshape_data_to_mne_format(test_data[i-1])\n",
        "  predictions = model4.predict(to_predict_data)\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Predictions: \" + str(len(predictions)))\n",
        "  # np.savetxt(\"predictions.csv\", predictions, delimiter=\",\")\n",
        "\n",
        "\n",
        "  # ========= FALI USTE DA SE ISPARSIRA PREDICTIONOT... NE E SREDEN OVOJ KOD DOLE =======\n",
        "\n",
        "  int_pred = np.argmax(predictions, axis=1)\n",
        "  int_ytest = np.argmax(y_test, axis=1)\n",
        "\n",
        "  session_start = 0\n",
        "  start_prediction_index = 0\n",
        "  end_prediction_index = 0\n",
        "  for session in range(0, 3):\n",
        "    print(f\"============== Сесија ({session}) ==============\")\n",
        "    for block in range(0, 50):    \n",
        "      events_per_block = test_runs_per_block[i-1][session]\n",
        "\n",
        "      start_prediction_index = session_start + (block*events_per_block)*8\n",
        "      end_prediction_index = session_start + ((block+1)*events_per_block)*8\n",
        "\n",
        "      block_prediction = int_pred[start_prediction_index:end_prediction_index]\n",
        "      prediction = np.bincount(block_prediction).argmax()\n",
        "      df.iat[session+9,block+2] = prediction+1\n",
        "      # UNCOMMENT ZA PODOBAR PRIKAZ :)\n",
        "      # print(f\"Session {session} | Block: {block} | Prediction: {prediction} | Address: {end_prediction_index}\")\n",
        "\n",
        "      print(str(prediction+1) + \",\", end=\"\")\n",
        "    session_start = end_prediction_index\n",
        "    print(\"\")\n",
        "  print(\"Stigna li do kraj: \" + str(session_start == len(predictions)))\n",
        "  print(f\"====================== Примерок ({i}) ======================\\n\\n\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Примерок (4) ======================\n",
            "Вчитување тест податоци од испитниот примерок 4...\n",
            "\t - Тест податоците од сесија 1 се вчитани.\n",
            "\t - Тест податоците од сесија 2 се вчитани.\n",
            "\t - Тест податоците од сесија 3 се вчитани.\n",
            "Тест податоците од испитниот примерок 4 се вчитани.\n",
            "\n",
            "SBJ03| Test_data: (8, 350, 6800)\n",
            "SBJ03| Test_events: 6800\n",
            "SBJ03 / S00| Runs per block: 5\n",
            "SBJ03 / S01| Runs per block: 5\n",
            "SBJ03 / S02| Runs per block: 7\n",
            "SBJ03| Predictions: 6800\n",
            "============== Сесија (0) ==============\n",
            "1,1,2,1,5,5,6,5,5,5,6,5,1,1,6,1,1,1,6,1,5,6,6,5,2,6,1,5,6,6,2,1,1,1,6,2,6,2,6,1,1,1,1,1,7,1,6,6,6,6,\n",
            "============== Сесија (1) ==============\n",
            "7,7,7,7,6,6,7,5,7,8,7,5,7,6,6,3,1,6,6,6,5,5,5,5,5,8,6,5,6,5,6,7,4,7,6,6,5,6,5,6,6,7,1,5,5,5,5,5,2,6,\n",
            "============== Сесија (2) ==============\n",
            "7,3,7,3,6,6,7,5,6,1,6,6,6,7,6,6,6,6,6,7,7,4,4,7,6,7,6,6,7,4,7,7,1,6,6,6,1,1,1,1,6,6,6,1,4,4,4,6,6,6,\n",
            "Stigna li do kraj: True\n",
            "====================== Примерок (4) ======================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XncH00U9qSrv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2aeb7ba3-9e88-41b8-f6b0-c28e7e3088fa"
      },
      "source": [
        "df"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUBJECT</th>\n",
              "      <th>SESSION</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>Unnamed: 52</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    SUBJECT  SESSION  1  2  3  4  5  6  7  ... 43 44 45 46 47 48 49 50 Unnamed: 52\n",
              "0         1        1  6  6  3  6  6  3  6  ...  6  4  8  3  4  5  5  7         NaN\n",
              "1         1        2  6  3  2  1  2  1  3  ...  6  2  2  2  3  6  6  2         NaN\n",
              "2         1        3  3  3  3  3  3  3  3  ...  3  3  6  3  6  7  1  6         NaN\n",
              "3         2        1  8  8  8  8  8  8  8  ...  8  4  8  4  8  7  8  8         NaN\n",
              "4         2        2  2  7  6  6  6  6  7  ...  2  6  6  2  6  6  2  2         NaN\n",
              "5         2        3  2  7  2  6  7  4  2  ...  2  6  2  2  7  2  2  2         NaN\n",
              "6         3        1  3  3  4  4  4  3  6  ...  4  6  3  3  4  3  4  4         NaN\n",
              "7         3        2  6  1  7  7  7  7  7  ...  6  7  1  6  6  6  6  6         NaN\n",
              "8         3        3  6  4  8  8  5  7  8  ...  4  2  8  2  2  2  2  8         NaN\n",
              "9         4        1  1  1  2  1  5  5  6  ...  1  1  7  1  6  6  6  6         NaN\n",
              "10        4        2  7  7  7  7  6  6  7  ...  1  5  5  5  5  5  2  6         NaN\n",
              "11        4        3  7  3  7  3  6  6  7  ...  6  1  4  4  4  6  6  6         NaN\n",
              "12        5        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "13        5        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "14        5        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "15        6        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "16        6        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "17        6        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "18        7        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "19        7        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "20        7        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "21        8        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "22        8        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "23        8        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "24        9        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "25        9        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "26        9        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "27       10        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "28       10        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "29       10        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "30       11        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "31       11        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "32       11        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "33       12        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "34       12        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "35       12        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "36       13        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "37       13        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "38       13        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "39       14        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "40       14        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "41       14        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "42       15        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "43       15        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "44       15        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "\n",
              "[45 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F0ge8jhqb_T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5583eaff-2a3e-48f3-a8a2-668b24a244a0"
      },
      "source": [
        "for i in range(5, 6): # Итерација низ секој испитен примерок\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_data = np.empty(0)\n",
        "  temp_labels = np.empty(0)\n",
        "  temp_events = np.empty(0)\n",
        "  temp_targets = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainData.mat\"\n",
        "    temp = loadmat(full_path)['trainData']\n",
        "    if temp_data.size != 0:\n",
        "      temp_data = np.concatenate((temp_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на label-ите\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainLabels.txt\"\n",
        "    with open(full_path, \"r\") as file_labels:\n",
        "      temp = file_labels.read().splitlines()\n",
        "      temp = np.repeat(temp, 8*10)\n",
        "      if temp_labels.size != 0:\n",
        "        temp_labels = np.concatenate((temp_labels, temp))\n",
        "      else:\n",
        "        temp_labels = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_events.size != 0:\n",
        "        temp_events = np.append(temp_events, temp)\n",
        "      else:\n",
        "        temp_events = np.array(temp)\n",
        "      \n",
        "\n",
        "    # Вчитување на редоследот на објекти кои се target\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainTargets.txt\"\n",
        "    with open(full_path, \"r\") as file_targets:\n",
        "      temp = file_targets.read().splitlines()\n",
        "      if temp_targets.size != 0:\n",
        "        temp_targets = np.concatenate((temp_targets, temp))\n",
        "      else:\n",
        "        temp_targets = np.array(temp)\n",
        "    print(\"\\t - Податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "\n",
        "  for j in range(4, 8): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "      \n",
        "  # Зачувај ги податоците вчитани од испитниот примерок во низа\n",
        "  data.append(temp_data)\n",
        "  labels.append(temp_labels)\n",
        "  events.append(temp_events)\n",
        "  targets.append(temp_targets)\n",
        "\n",
        "  \n",
        "  print(\"Податоците од испитниот примерок \" + str(i) + \" се вчитани.\")\n",
        "\n",
        "\n",
        "  #data = target_events_data_scaled\n",
        "  mne_array = np.swapaxes(data[i-1], 0, 2) # (епохa, канал, настан). \n",
        "  mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "  mne_array = mne_array.reshape(mne_array.shape[0],1, 8,350)\n",
        "  print(mne_array.shape)\n",
        "\n",
        "  events_arr = events[i-1].astype(np.int)\n",
        "  labels_arr = labels[i-1].astype(np.int)\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(mne_array, labels_arr-1, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "  model5 = DeepConvNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "  model5.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = Adam(0.0009))\n",
        "  checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                                verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "  #clf = RandomForestClassifier(max_depth=5)\n",
        "  #clf.fit(X_train, y_train)\n",
        "  #score = clf.score(X_test, y_test)\n",
        "  # print(score)\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  num_batch_size=100\n",
        "  num_epochs=400\n",
        "  model5.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "            validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "  score = model5.evaluate(X_test, y_test, verbose=1)\n",
        "  print(score)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t - Податоците од сесија 1 се вчитани.\n",
            "\t - Податоците од сесија 2 се вчитани.\n",
            "\t - Податоците од сесија 3 се вчитани.\n",
            "Податоците од испитниот примерок 5 се вчитани.\n",
            "(4800, 1, 8, 350)\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Train on 3600 samples, validate on 1200 samples\n",
            "Epoch 1/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.3944 - acc: 0.1579\n",
            "Epoch 00001: val_loss improved from inf to 2.15632, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 5s 1ms/sample - loss: 2.3860 - acc: 0.1589 - val_loss: 2.1563 - val_acc: 0.1475\n",
            "Epoch 2/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.2740 - acc: 0.1579\n",
            "Epoch 00002: val_loss did not improve from 2.15632\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 2.2708 - acc: 0.1600 - val_loss: 2.6325 - val_acc: 0.0908\n",
            "Epoch 3/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.2063 - acc: 0.1718\n",
            "Epoch 00003: val_loss did not improve from 2.15632\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 2.2059 - acc: 0.1728 - val_loss: 2.3037 - val_acc: 0.1417\n",
            "Epoch 4/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.1553 - acc: 0.2049\n",
            "Epoch 00004: val_loss did not improve from 2.15632\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 2.1598 - acc: 0.2028 - val_loss: 2.2359 - val_acc: 0.1475\n",
            "Epoch 5/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.1094 - acc: 0.2226\n",
            "Epoch 00005: val_loss improved from 2.15632 to 2.14428, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 2.1013 - acc: 0.2267 - val_loss: 2.1443 - val_acc: 0.1992\n",
            "Epoch 6/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.8842 - acc: 0.3003\n",
            "Epoch 00006: val_loss improved from 2.14428 to 2.13135, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 1.8817 - acc: 0.3019 - val_loss: 2.1314 - val_acc: 0.2458\n",
            "Epoch 7/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.7661 - acc: 0.3385\n",
            "Epoch 00007: val_loss did not improve from 2.13135\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.7632 - acc: 0.3383 - val_loss: 2.1828 - val_acc: 0.2575\n",
            "Epoch 8/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.7004 - acc: 0.3553\n",
            "Epoch 00008: val_loss improved from 2.13135 to 1.75101, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 1.6989 - acc: 0.3575 - val_loss: 1.7510 - val_acc: 0.3225\n",
            "Epoch 9/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.6630 - acc: 0.3794\n",
            "Epoch 00009: val_loss improved from 1.75101 to 1.64850, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 1.6552 - acc: 0.3828 - val_loss: 1.6485 - val_acc: 0.3317\n",
            "Epoch 10/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.5649 - acc: 0.4109\n",
            "Epoch 00010: val_loss did not improve from 1.64850\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.5665 - acc: 0.4097 - val_loss: 1.7800 - val_acc: 0.3342\n",
            "Epoch 11/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.5192 - acc: 0.4265\n",
            "Epoch 00011: val_loss did not improve from 1.64850\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 1.5225 - acc: 0.4233 - val_loss: 1.9221 - val_acc: 0.2625\n",
            "Epoch 12/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4859 - acc: 0.4424\n",
            "Epoch 00012: val_loss did not improve from 1.64850\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.4929 - acc: 0.4389 - val_loss: 1.7081 - val_acc: 0.3467\n",
            "Epoch 13/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.4288 - acc: 0.4597\n",
            "Epoch 00013: val_loss did not improve from 1.64850\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.4315 - acc: 0.4600 - val_loss: 1.9962 - val_acc: 0.2783\n",
            "Epoch 14/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.3815 - acc: 0.4830\n",
            "Epoch 00014: val_loss did not improve from 1.64850\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.3864 - acc: 0.4789 - val_loss: 1.7288 - val_acc: 0.3233\n",
            "Epoch 15/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.3359 - acc: 0.5075\n",
            "Epoch 00015: val_loss improved from 1.64850 to 1.62157, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 1.3376 - acc: 0.5039 - val_loss: 1.6216 - val_acc: 0.3700\n",
            "Epoch 16/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.3452 - acc: 0.4994\n",
            "Epoch 00016: val_loss improved from 1.62157 to 1.52998, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 1.3416 - acc: 0.4994 - val_loss: 1.5300 - val_acc: 0.4250\n",
            "Epoch 17/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.2706 - acc: 0.5154\n",
            "Epoch 00017: val_loss did not improve from 1.52998\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.2715 - acc: 0.5153 - val_loss: 1.7152 - val_acc: 0.3567\n",
            "Epoch 18/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.2202 - acc: 0.5400\n",
            "Epoch 00018: val_loss improved from 1.52998 to 1.45470, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 1.2268 - acc: 0.5389 - val_loss: 1.4547 - val_acc: 0.4242\n",
            "Epoch 19/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.1791 - acc: 0.5629\n",
            "Epoch 00019: val_loss did not improve from 1.45470\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.1840 - acc: 0.5611 - val_loss: 1.5266 - val_acc: 0.4092\n",
            "Epoch 20/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1611 - acc: 0.5661\n",
            "Epoch 00020: val_loss did not improve from 1.45470\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 1.1608 - acc: 0.5650 - val_loss: 1.5274 - val_acc: 0.4258\n",
            "Epoch 21/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1200 - acc: 0.5776\n",
            "Epoch 00021: val_loss did not improve from 1.45470\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.1217 - acc: 0.5778 - val_loss: 1.5499 - val_acc: 0.4333\n",
            "Epoch 22/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.0929 - acc: 0.5889\n",
            "Epoch 00022: val_loss improved from 1.45470 to 1.31566, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 1.0911 - acc: 0.5886 - val_loss: 1.3157 - val_acc: 0.4958\n",
            "Epoch 23/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0748 - acc: 0.5974\n",
            "Epoch 00023: val_loss did not improve from 1.31566\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.0755 - acc: 0.5947 - val_loss: 1.3651 - val_acc: 0.4875\n",
            "Epoch 24/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9659 - acc: 0.6470\n",
            "Epoch 00024: val_loss improved from 1.31566 to 1.12983, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.9801 - acc: 0.6400 - val_loss: 1.1298 - val_acc: 0.5850\n",
            "Epoch 25/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9725 - acc: 0.6391\n",
            "Epoch 00025: val_loss did not improve from 1.12983\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.9835 - acc: 0.6350 - val_loss: 1.1957 - val_acc: 0.5442\n",
            "Epoch 26/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9538 - acc: 0.6530\n",
            "Epoch 00026: val_loss did not improve from 1.12983\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.9573 - acc: 0.6525 - val_loss: 1.1965 - val_acc: 0.5500\n",
            "Epoch 27/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8828 - acc: 0.6706\n",
            "Epoch 00027: val_loss did not improve from 1.12983\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.8856 - acc: 0.6689 - val_loss: 1.1886 - val_acc: 0.5425\n",
            "Epoch 28/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9041 - acc: 0.6553\n",
            "Epoch 00028: val_loss did not improve from 1.12983\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.9005 - acc: 0.6575 - val_loss: 1.1908 - val_acc: 0.5475\n",
            "Epoch 29/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8391 - acc: 0.6875\n",
            "Epoch 00029: val_loss improved from 1.12983 to 1.11656, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.8408 - acc: 0.6858 - val_loss: 1.1166 - val_acc: 0.5850\n",
            "Epoch 30/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7825 - acc: 0.7136\n",
            "Epoch 00030: val_loss improved from 1.11656 to 1.00332, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.7885 - acc: 0.7117 - val_loss: 1.0033 - val_acc: 0.6175\n",
            "Epoch 31/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7967 - acc: 0.7074\n",
            "Epoch 00031: val_loss improved from 1.00332 to 0.95711, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.8042 - acc: 0.7033 - val_loss: 0.9571 - val_acc: 0.6217\n",
            "Epoch 32/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7658 - acc: 0.7300\n",
            "Epoch 00032: val_loss improved from 0.95711 to 0.88822, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.7661 - acc: 0.7289 - val_loss: 0.8882 - val_acc: 0.6792\n",
            "Epoch 33/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7394 - acc: 0.7369\n",
            "Epoch 00033: val_loss improved from 0.88822 to 0.87799, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.7465 - acc: 0.7344 - val_loss: 0.8780 - val_acc: 0.6817\n",
            "Epoch 34/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7192 - acc: 0.7420\n",
            "Epoch 00034: val_loss improved from 0.87799 to 0.85973, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.7162 - acc: 0.7436 - val_loss: 0.8597 - val_acc: 0.6658\n",
            "Epoch 35/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6816 - acc: 0.7556\n",
            "Epoch 00035: val_loss did not improve from 0.85973\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.6836 - acc: 0.7536 - val_loss: 0.8800 - val_acc: 0.6608\n",
            "Epoch 36/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.6647 - acc: 0.7653\n",
            "Epoch 00036: val_loss did not improve from 0.85973\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.6747 - acc: 0.7606 - val_loss: 0.8756 - val_acc: 0.6608\n",
            "Epoch 37/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6233 - acc: 0.7797\n",
            "Epoch 00037: val_loss improved from 0.85973 to 0.84918, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.6222 - acc: 0.7800 - val_loss: 0.8492 - val_acc: 0.6792\n",
            "Epoch 38/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5945 - acc: 0.7818\n",
            "Epoch 00038: val_loss improved from 0.84918 to 0.74598, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.5925 - acc: 0.7856 - val_loss: 0.7460 - val_acc: 0.7225\n",
            "Epoch 39/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6082 - acc: 0.7846\n",
            "Epoch 00039: val_loss improved from 0.74598 to 0.68019, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.6081 - acc: 0.7847 - val_loss: 0.6802 - val_acc: 0.7558\n",
            "Epoch 40/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5518 - acc: 0.8085\n",
            "Epoch 00040: val_loss did not improve from 0.68019\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.5575 - acc: 0.8056 - val_loss: 0.6922 - val_acc: 0.7425\n",
            "Epoch 41/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.5356 - acc: 0.8169\n",
            "Epoch 00041: val_loss did not improve from 0.68019\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.5403 - acc: 0.8156 - val_loss: 0.7191 - val_acc: 0.7475\n",
            "Epoch 42/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5289 - acc: 0.8156\n",
            "Epoch 00042: val_loss did not improve from 0.68019\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.5296 - acc: 0.8144 - val_loss: 0.7788 - val_acc: 0.7042\n",
            "Epoch 43/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.5058 - acc: 0.8266\n",
            "Epoch 00043: val_loss improved from 0.68019 to 0.66939, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.5063 - acc: 0.8256 - val_loss: 0.6694 - val_acc: 0.7600\n",
            "Epoch 44/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4963 - acc: 0.8285\n",
            "Epoch 00044: val_loss improved from 0.66939 to 0.66692, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.4953 - acc: 0.8283 - val_loss: 0.6669 - val_acc: 0.7542\n",
            "Epoch 45/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4544 - acc: 0.8436\n",
            "Epoch 00045: val_loss improved from 0.66692 to 0.57279, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.4665 - acc: 0.8375 - val_loss: 0.5728 - val_acc: 0.8108\n",
            "Epoch 46/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4674 - acc: 0.8411\n",
            "Epoch 00046: val_loss did not improve from 0.57279\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.4676 - acc: 0.8406 - val_loss: 0.6131 - val_acc: 0.7900\n",
            "Epoch 47/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4404 - acc: 0.8456\n",
            "Epoch 00047: val_loss improved from 0.57279 to 0.51468, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.4430 - acc: 0.8461 - val_loss: 0.5147 - val_acc: 0.8292\n",
            "Epoch 48/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4350 - acc: 0.8521\n",
            "Epoch 00048: val_loss improved from 0.51468 to 0.50222, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.4354 - acc: 0.8508 - val_loss: 0.5022 - val_acc: 0.8300\n",
            "Epoch 49/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4184 - acc: 0.8597\n",
            "Epoch 00049: val_loss did not improve from 0.50222\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.4200 - acc: 0.8594 - val_loss: 0.5598 - val_acc: 0.8108\n",
            "Epoch 50/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3978 - acc: 0.8647\n",
            "Epoch 00050: val_loss did not improve from 0.50222\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.4040 - acc: 0.8614 - val_loss: 0.5285 - val_acc: 0.8225\n",
            "Epoch 51/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3977 - acc: 0.8674\n",
            "Epoch 00051: val_loss did not improve from 0.50222\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.3988 - acc: 0.8681 - val_loss: 0.5547 - val_acc: 0.8075\n",
            "Epoch 52/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3834 - acc: 0.8782\n",
            "Epoch 00052: val_loss improved from 0.50222 to 0.50002, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.3843 - acc: 0.8781 - val_loss: 0.5000 - val_acc: 0.8342\n",
            "Epoch 53/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3418 - acc: 0.8900\n",
            "Epoch 00053: val_loss improved from 0.50002 to 0.47776, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.3463 - acc: 0.8878 - val_loss: 0.4778 - val_acc: 0.8475\n",
            "Epoch 54/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3650 - acc: 0.8762\n",
            "Epoch 00054: val_loss did not improve from 0.47776\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.3633 - acc: 0.8764 - val_loss: 0.4925 - val_acc: 0.8308\n",
            "Epoch 55/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3395 - acc: 0.8888\n",
            "Epoch 00055: val_loss did not improve from 0.47776\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.3459 - acc: 0.8861 - val_loss: 0.4895 - val_acc: 0.8300\n",
            "Epoch 56/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3371 - acc: 0.8936\n",
            "Epoch 00056: val_loss improved from 0.47776 to 0.38621, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.3380 - acc: 0.8919 - val_loss: 0.3862 - val_acc: 0.8733\n",
            "Epoch 57/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3342 - acc: 0.8914\n",
            "Epoch 00057: val_loss did not improve from 0.38621\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.3329 - acc: 0.8925 - val_loss: 0.4226 - val_acc: 0.8592\n",
            "Epoch 58/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3129 - acc: 0.9021\n",
            "Epoch 00058: val_loss improved from 0.38621 to 0.35987, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.3124 - acc: 0.9022 - val_loss: 0.3599 - val_acc: 0.8842\n",
            "Epoch 59/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3213 - acc: 0.9003\n",
            "Epoch 00059: val_loss did not improve from 0.35987\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.3215 - acc: 0.9000 - val_loss: 0.3775 - val_acc: 0.8833\n",
            "Epoch 60/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3011 - acc: 0.9018\n",
            "Epoch 00060: val_loss did not improve from 0.35987\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.3037 - acc: 0.9003 - val_loss: 0.4484 - val_acc: 0.8417\n",
            "Epoch 61/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2922 - acc: 0.9079\n",
            "Epoch 00061: val_loss did not improve from 0.35987\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2933 - acc: 0.9069 - val_loss: 0.3829 - val_acc: 0.8742\n",
            "Epoch 62/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2878 - acc: 0.9094\n",
            "Epoch 00062: val_loss did not improve from 0.35987\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2925 - acc: 0.9069 - val_loss: 0.3651 - val_acc: 0.8817\n",
            "Epoch 63/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3029 - acc: 0.8976\n",
            "Epoch 00063: val_loss improved from 0.35987 to 0.30463, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.3024 - acc: 0.8975 - val_loss: 0.3046 - val_acc: 0.9208\n",
            "Epoch 64/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2812 - acc: 0.9160\n",
            "Epoch 00064: val_loss did not improve from 0.30463\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2805 - acc: 0.9164 - val_loss: 0.3751 - val_acc: 0.8792\n",
            "Epoch 65/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2665 - acc: 0.9188\n",
            "Epoch 00065: val_loss did not improve from 0.30463\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2665 - acc: 0.9181 - val_loss: 0.3550 - val_acc: 0.8883\n",
            "Epoch 66/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.9269\n",
            "Epoch 00066: val_loss did not improve from 0.30463\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2451 - acc: 0.9283 - val_loss: 0.3173 - val_acc: 0.9150\n",
            "Epoch 67/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2579 - acc: 0.9185\n",
            "Epoch 00067: val_loss did not improve from 0.30463\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2633 - acc: 0.9158 - val_loss: 0.3336 - val_acc: 0.8958\n",
            "Epoch 68/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2644 - acc: 0.9136\n",
            "Epoch 00068: val_loss did not improve from 0.30463\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2673 - acc: 0.9131 - val_loss: 0.3163 - val_acc: 0.9042\n",
            "Epoch 69/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2565 - acc: 0.9182\n",
            "Epoch 00069: val_loss did not improve from 0.30463\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2579 - acc: 0.9172 - val_loss: 0.3722 - val_acc: 0.8700\n",
            "Epoch 70/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.9246\n",
            "Epoch 00070: val_loss did not improve from 0.30463\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2526 - acc: 0.9236 - val_loss: 0.3618 - val_acc: 0.8867\n",
            "Epoch 71/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2538 - acc: 0.9229\n",
            "Epoch 00071: val_loss did not improve from 0.30463\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2544 - acc: 0.9231 - val_loss: 0.3061 - val_acc: 0.9042\n",
            "Epoch 72/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2218 - acc: 0.9347\n",
            "Epoch 00072: val_loss improved from 0.30463 to 0.26242, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.2245 - acc: 0.9347 - val_loss: 0.2624 - val_acc: 0.9242\n",
            "Epoch 73/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2196 - acc: 0.9370\n",
            "Epoch 00073: val_loss did not improve from 0.26242\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2195 - acc: 0.9367 - val_loss: 0.3415 - val_acc: 0.8742\n",
            "Epoch 74/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2275 - acc: 0.9325\n",
            "Epoch 00074: val_loss did not improve from 0.26242\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2307 - acc: 0.9311 - val_loss: 0.2694 - val_acc: 0.9142\n",
            "Epoch 75/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2143 - acc: 0.9374\n",
            "Epoch 00075: val_loss did not improve from 0.26242\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2129 - acc: 0.9372 - val_loss: 0.2673 - val_acc: 0.9217\n",
            "Epoch 76/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9343\n",
            "Epoch 00076: val_loss improved from 0.26242 to 0.25746, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.2107 - acc: 0.9347 - val_loss: 0.2575 - val_acc: 0.9250\n",
            "Epoch 77/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2112 - acc: 0.9347\n",
            "Epoch 00077: val_loss did not improve from 0.25746\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2121 - acc: 0.9350 - val_loss: 0.3136 - val_acc: 0.8950\n",
            "Epoch 78/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2049 - acc: 0.9400\n",
            "Epoch 00078: val_loss did not improve from 0.25746\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2053 - acc: 0.9400 - val_loss: 0.2580 - val_acc: 0.9250\n",
            "Epoch 79/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9449\n",
            "Epoch 00079: val_loss did not improve from 0.25746\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1903 - acc: 0.9447 - val_loss: 0.2844 - val_acc: 0.9083\n",
            "Epoch 80/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2098 - acc: 0.9333\n",
            "Epoch 00080: val_loss improved from 0.25746 to 0.24683, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.2089 - acc: 0.9342 - val_loss: 0.2468 - val_acc: 0.9292\n",
            "Epoch 81/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2148 - acc: 0.9326\n",
            "Epoch 00081: val_loss did not improve from 0.24683\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2162 - acc: 0.9322 - val_loss: 0.3266 - val_acc: 0.8900\n",
            "Epoch 82/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1809 - acc: 0.9462\n",
            "Epoch 00082: val_loss did not improve from 0.24683\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1819 - acc: 0.9458 - val_loss: 0.2907 - val_acc: 0.9092\n",
            "Epoch 83/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1898 - acc: 0.9409\n",
            "Epoch 00083: val_loss did not improve from 0.24683\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1899 - acc: 0.9411 - val_loss: 0.2679 - val_acc: 0.9217\n",
            "Epoch 84/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1838 - acc: 0.9453\n",
            "Epoch 00084: val_loss improved from 0.24683 to 0.21629, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1862 - acc: 0.9447 - val_loss: 0.2163 - val_acc: 0.9300\n",
            "Epoch 85/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1997 - acc: 0.9349\n",
            "Epoch 00085: val_loss did not improve from 0.21629\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1991 - acc: 0.9356 - val_loss: 0.2524 - val_acc: 0.9175\n",
            "Epoch 86/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1818 - acc: 0.9468\n",
            "Epoch 00086: val_loss improved from 0.21629 to 0.19836, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1818 - acc: 0.9461 - val_loss: 0.1984 - val_acc: 0.9400\n",
            "Epoch 87/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1791 - acc: 0.9463\n",
            "Epoch 00087: val_loss did not improve from 0.19836\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1797 - acc: 0.9461 - val_loss: 0.2297 - val_acc: 0.9292\n",
            "Epoch 88/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1776 - acc: 0.9456\n",
            "Epoch 00088: val_loss did not improve from 0.19836\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1820 - acc: 0.9439 - val_loss: 0.3502 - val_acc: 0.8675\n",
            "Epoch 89/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 0.9480\n",
            "Epoch 00089: val_loss did not improve from 0.19836\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1748 - acc: 0.9475 - val_loss: 0.2075 - val_acc: 0.9375\n",
            "Epoch 90/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.9457\n",
            "Epoch 00090: val_loss did not improve from 0.19836\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1714 - acc: 0.9461 - val_loss: 0.2185 - val_acc: 0.9283\n",
            "Epoch 91/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.9520\n",
            "Epoch 00091: val_loss improved from 0.19836 to 0.19402, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1675 - acc: 0.9525 - val_loss: 0.1940 - val_acc: 0.9467\n",
            "Epoch 92/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1690 - acc: 0.9485\n",
            "Epoch 00092: val_loss did not improve from 0.19402\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1653 - acc: 0.9506 - val_loss: 0.2223 - val_acc: 0.9317\n",
            "Epoch 93/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1460 - acc: 0.9603\n",
            "Epoch 00093: val_loss did not improve from 0.19402\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1467 - acc: 0.9600 - val_loss: 0.2071 - val_acc: 0.9400\n",
            "Epoch 94/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1523 - acc: 0.9578\n",
            "Epoch 00094: val_loss did not improve from 0.19402\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1526 - acc: 0.9569 - val_loss: 0.1941 - val_acc: 0.9408\n",
            "Epoch 95/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9554\n",
            "Epoch 00095: val_loss did not improve from 0.19402\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1493 - acc: 0.9558 - val_loss: 0.1983 - val_acc: 0.9450\n",
            "Epoch 96/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9569\n",
            "Epoch 00096: val_loss improved from 0.19402 to 0.18763, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1504 - acc: 0.9575 - val_loss: 0.1876 - val_acc: 0.9492\n",
            "Epoch 97/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9543\n",
            "Epoch 00097: val_loss did not improve from 0.18763\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1609 - acc: 0.9525 - val_loss: 0.2957 - val_acc: 0.9025\n",
            "Epoch 98/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1580 - acc: 0.9534\n",
            "Epoch 00098: val_loss did not improve from 0.18763\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1563 - acc: 0.9536 - val_loss: 0.2034 - val_acc: 0.9325\n",
            "Epoch 99/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1456 - acc: 0.9603\n",
            "Epoch 00099: val_loss did not improve from 0.18763\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1479 - acc: 0.9589 - val_loss: 0.2552 - val_acc: 0.9100\n",
            "Epoch 100/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1405 - acc: 0.9609\n",
            "Epoch 00100: val_loss improved from 0.18763 to 0.18510, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1419 - acc: 0.9606 - val_loss: 0.1851 - val_acc: 0.9375\n",
            "Epoch 101/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1458 - acc: 0.9525\n",
            "Epoch 00101: val_loss improved from 0.18510 to 0.15130, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1437 - acc: 0.9539 - val_loss: 0.1513 - val_acc: 0.9633\n",
            "Epoch 102/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1398 - acc: 0.9619\n",
            "Epoch 00102: val_loss did not improve from 0.15130\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1410 - acc: 0.9608 - val_loss: 0.1829 - val_acc: 0.9508\n",
            "Epoch 103/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9646\n",
            "Epoch 00103: val_loss did not improve from 0.15130\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1397 - acc: 0.9644 - val_loss: 0.1778 - val_acc: 0.9475\n",
            "Epoch 104/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1458 - acc: 0.9588\n",
            "Epoch 00104: val_loss did not improve from 0.15130\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1436 - acc: 0.9592 - val_loss: 0.1848 - val_acc: 0.9492\n",
            "Epoch 105/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1475 - acc: 0.9552\n",
            "Epoch 00105: val_loss did not improve from 0.15130\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1497 - acc: 0.9544 - val_loss: 0.1728 - val_acc: 0.9442\n",
            "Epoch 106/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1315 - acc: 0.9624\n",
            "Epoch 00106: val_loss did not improve from 0.15130\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1314 - acc: 0.9619 - val_loss: 0.2067 - val_acc: 0.9383\n",
            "Epoch 107/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1321 - acc: 0.9606\n",
            "Epoch 00107: val_loss did not improve from 0.15130\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1327 - acc: 0.9608 - val_loss: 0.1647 - val_acc: 0.9525\n",
            "Epoch 108/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1372 - acc: 0.9597\n",
            "Epoch 00108: val_loss did not improve from 0.15130\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1368 - acc: 0.9594 - val_loss: 0.2480 - val_acc: 0.9150\n",
            "Epoch 109/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1240 - acc: 0.9656\n",
            "Epoch 00109: val_loss did not improve from 0.15130\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1230 - acc: 0.9667 - val_loss: 0.1905 - val_acc: 0.9367\n",
            "Epoch 110/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1234 - acc: 0.9669\n",
            "Epoch 00110: val_loss improved from 0.15130 to 0.13319, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1265 - acc: 0.9644 - val_loss: 0.1332 - val_acc: 0.9617\n",
            "Epoch 111/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1400 - acc: 0.9588\n",
            "Epoch 00111: val_loss did not improve from 0.13319\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1406 - acc: 0.9589 - val_loss: 0.2009 - val_acc: 0.9367\n",
            "Epoch 112/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9657\n",
            "Epoch 00112: val_loss did not improve from 0.13319\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1240 - acc: 0.9658 - val_loss: 0.1629 - val_acc: 0.9508\n",
            "Epoch 113/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1291 - acc: 0.9614\n",
            "Epoch 00113: val_loss did not improve from 0.13319\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1312 - acc: 0.9614 - val_loss: 0.1998 - val_acc: 0.9417\n",
            "Epoch 114/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1267 - acc: 0.9634\n",
            "Epoch 00114: val_loss did not improve from 0.13319\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1277 - acc: 0.9625 - val_loss: 0.1607 - val_acc: 0.9508\n",
            "Epoch 115/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9611\n",
            "Epoch 00115: val_loss did not improve from 0.13319\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1306 - acc: 0.9611 - val_loss: 0.1743 - val_acc: 0.9433\n",
            "Epoch 116/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1285 - acc: 0.9625\n",
            "Epoch 00116: val_loss did not improve from 0.13319\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1302 - acc: 0.9628 - val_loss: 0.1573 - val_acc: 0.9533\n",
            "Epoch 117/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9603\n",
            "Epoch 00117: val_loss did not improve from 0.13319\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1343 - acc: 0.9608 - val_loss: 0.1812 - val_acc: 0.9450\n",
            "Epoch 118/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9583\n",
            "Epoch 00118: val_loss did not improve from 0.13319\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1298 - acc: 0.9575 - val_loss: 0.2173 - val_acc: 0.9325\n",
            "Epoch 119/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9669\n",
            "Epoch 00119: val_loss did not improve from 0.13319\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1214 - acc: 0.9664 - val_loss: 0.1809 - val_acc: 0.9450\n",
            "Epoch 120/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1409 - acc: 0.9566\n",
            "Epoch 00120: val_loss did not improve from 0.13319\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1424 - acc: 0.9547 - val_loss: 0.1895 - val_acc: 0.9408\n",
            "Epoch 121/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1205 - acc: 0.9694\n",
            "Epoch 00121: val_loss did not improve from 0.13319\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1202 - acc: 0.9689 - val_loss: 0.1598 - val_acc: 0.9533\n",
            "Epoch 122/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1197 - acc: 0.9662\n",
            "Epoch 00122: val_loss did not improve from 0.13319\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1193 - acc: 0.9664 - val_loss: 0.1551 - val_acc: 0.9508\n",
            "Epoch 123/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1295 - acc: 0.9588\n",
            "Epoch 00123: val_loss improved from 0.13319 to 0.11249, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1311 - acc: 0.9586 - val_loss: 0.1125 - val_acc: 0.9692\n",
            "Epoch 124/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1310 - acc: 0.9588\n",
            "Epoch 00124: val_loss did not improve from 0.11249\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1357 - acc: 0.9569 - val_loss: 0.1820 - val_acc: 0.9408\n",
            "Epoch 125/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9609\n",
            "Epoch 00125: val_loss did not improve from 0.11249\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1281 - acc: 0.9614 - val_loss: 0.1629 - val_acc: 0.9433\n",
            "Epoch 126/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1184 - acc: 0.9665\n",
            "Epoch 00126: val_loss did not improve from 0.11249\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1231 - acc: 0.9642 - val_loss: 0.1926 - val_acc: 0.9383\n",
            "Epoch 127/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1247 - acc: 0.9600\n",
            "Epoch 00127: val_loss did not improve from 0.11249\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1288 - acc: 0.9572 - val_loss: 0.1450 - val_acc: 0.9558\n",
            "Epoch 128/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9574\n",
            "Epoch 00128: val_loss did not improve from 0.11249\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1439 - acc: 0.9561 - val_loss: 0.1411 - val_acc: 0.9625\n",
            "Epoch 129/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1192 - acc: 0.9653\n",
            "Epoch 00129: val_loss did not improve from 0.11249\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1187 - acc: 0.9658 - val_loss: 0.1789 - val_acc: 0.9433\n",
            "Epoch 130/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1251 - acc: 0.9653\n",
            "Epoch 00130: val_loss did not improve from 0.11249\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1253 - acc: 0.9653 - val_loss: 0.2056 - val_acc: 0.9367\n",
            "Epoch 131/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1351 - acc: 0.9585\n",
            "Epoch 00131: val_loss did not improve from 0.11249\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1363 - acc: 0.9581 - val_loss: 0.1507 - val_acc: 0.9500\n",
            "Epoch 132/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9631\n",
            "Epoch 00132: val_loss did not improve from 0.11249\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1226 - acc: 0.9633 - val_loss: 0.1172 - val_acc: 0.9700\n",
            "Epoch 133/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1213 - acc: 0.9644\n",
            "Epoch 00133: val_loss did not improve from 0.11249\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1230 - acc: 0.9636 - val_loss: 0.1287 - val_acc: 0.9675\n",
            "Epoch 134/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1231 - acc: 0.9591\n",
            "Epoch 00134: val_loss did not improve from 0.11249\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1235 - acc: 0.9589 - val_loss: 0.1561 - val_acc: 0.9500\n",
            "Epoch 135/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1211 - acc: 0.9653\n",
            "Epoch 00135: val_loss did not improve from 0.11249\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1235 - acc: 0.9644 - val_loss: 0.1916 - val_acc: 0.9383\n",
            "Epoch 136/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1190 - acc: 0.9625\n",
            "Epoch 00136: val_loss did not improve from 0.11249\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1205 - acc: 0.9625 - val_loss: 0.1555 - val_acc: 0.9508\n",
            "Epoch 137/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1139 - acc: 0.9670\n",
            "Epoch 00137: val_loss did not improve from 0.11249\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1143 - acc: 0.9672 - val_loss: 0.1250 - val_acc: 0.9625\n",
            "Epoch 138/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1160 - acc: 0.9600\n",
            "Epoch 00138: val_loss did not improve from 0.11249\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1128 - acc: 0.9622 - val_loss: 0.1245 - val_acc: 0.9675\n",
            "Epoch 139/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1059 - acc: 0.9703\n",
            "Epoch 00139: val_loss did not improve from 0.11249\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1067 - acc: 0.9689 - val_loss: 0.1504 - val_acc: 0.9483\n",
            "Epoch 140/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1029 - acc: 0.9709\n",
            "Epoch 00140: val_loss did not improve from 0.11249\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1020 - acc: 0.9717 - val_loss: 0.1137 - val_acc: 0.9725\n",
            "Epoch 141/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9643\n",
            "Epoch 00141: val_loss did not improve from 0.11249\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1140 - acc: 0.9644 - val_loss: 0.1140 - val_acc: 0.9633\n",
            "Epoch 142/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9717\n",
            "Epoch 00142: val_loss improved from 0.11249 to 0.10192, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1040 - acc: 0.9719 - val_loss: 0.1019 - val_acc: 0.9733\n",
            "Epoch 143/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9674\n",
            "Epoch 00143: val_loss did not improve from 0.10192\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1112 - acc: 0.9669 - val_loss: 0.1299 - val_acc: 0.9658\n",
            "Epoch 144/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1005 - acc: 0.9703\n",
            "Epoch 00144: val_loss did not improve from 0.10192\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1047 - acc: 0.9678 - val_loss: 0.1394 - val_acc: 0.9608\n",
            "Epoch 145/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0939 - acc: 0.9766\n",
            "Epoch 00145: val_loss did not improve from 0.10192\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.0948 - acc: 0.9764 - val_loss: 0.1268 - val_acc: 0.9608\n",
            "Epoch 146/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1017 - acc: 0.9688\n",
            "Epoch 00146: val_loss did not improve from 0.10192\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1007 - acc: 0.9694 - val_loss: 0.1232 - val_acc: 0.9633\n",
            "Epoch 147/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1004 - acc: 0.9709\n",
            "Epoch 00147: val_loss did not improve from 0.10192\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0986 - acc: 0.9714 - val_loss: 0.1194 - val_acc: 0.9642\n",
            "Epoch 148/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1188 - acc: 0.9606\n",
            "Epoch 00148: val_loss did not improve from 0.10192\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1181 - acc: 0.9619 - val_loss: 0.1262 - val_acc: 0.9617\n",
            "Epoch 149/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1073 - acc: 0.9709\n",
            "Epoch 00149: val_loss did not improve from 0.10192\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1074 - acc: 0.9714 - val_loss: 0.1355 - val_acc: 0.9575\n",
            "Epoch 150/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9637\n",
            "Epoch 00150: val_loss did not improve from 0.10192\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1197 - acc: 0.9647 - val_loss: 0.1255 - val_acc: 0.9642\n",
            "Epoch 151/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1156 - acc: 0.9655\n",
            "Epoch 00151: val_loss did not improve from 0.10192\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1141 - acc: 0.9658 - val_loss: 0.1215 - val_acc: 0.9642\n",
            "Epoch 152/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9697\n",
            "Epoch 00152: val_loss did not improve from 0.10192\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1108 - acc: 0.9694 - val_loss: 0.1385 - val_acc: 0.9575\n",
            "Epoch 153/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0998 - acc: 0.9700\n",
            "Epoch 00153: val_loss did not improve from 0.10192\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0998 - acc: 0.9700 - val_loss: 0.1391 - val_acc: 0.9558\n",
            "Epoch 154/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0999 - acc: 0.9709\n",
            "Epoch 00154: val_loss did not improve from 0.10192\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0998 - acc: 0.9714 - val_loss: 0.1095 - val_acc: 0.9767\n",
            "Epoch 155/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0945 - acc: 0.9729\n",
            "Epoch 00155: val_loss did not improve from 0.10192\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0965 - acc: 0.9725 - val_loss: 0.1044 - val_acc: 0.9700\n",
            "Epoch 156/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1095 - acc: 0.9706\n",
            "Epoch 00156: val_loss did not improve from 0.10192\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1116 - acc: 0.9686 - val_loss: 0.1158 - val_acc: 0.9650\n",
            "Epoch 157/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0901 - acc: 0.9762\n",
            "Epoch 00157: val_loss improved from 0.10192 to 0.08945, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.0897 - acc: 0.9764 - val_loss: 0.0895 - val_acc: 0.9750\n",
            "Epoch 158/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1059 - acc: 0.9666\n",
            "Epoch 00158: val_loss did not improve from 0.08945\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1057 - acc: 0.9675 - val_loss: 0.1039 - val_acc: 0.9708\n",
            "Epoch 159/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1059 - acc: 0.9706\n",
            "Epoch 00159: val_loss did not improve from 0.08945\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1036 - acc: 0.9717 - val_loss: 0.1126 - val_acc: 0.9642\n",
            "Epoch 160/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9706\n",
            "Epoch 00160: val_loss did not improve from 0.08945\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0984 - acc: 0.9700 - val_loss: 0.1264 - val_acc: 0.9642\n",
            "Epoch 161/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9720\n",
            "Epoch 00161: val_loss did not improve from 0.08945\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0964 - acc: 0.9719 - val_loss: 0.0998 - val_acc: 0.9717\n",
            "Epoch 162/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.9731\n",
            "Epoch 00162: val_loss did not improve from 0.08945\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0981 - acc: 0.9728 - val_loss: 0.1551 - val_acc: 0.9550\n",
            "Epoch 163/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.9723\n",
            "Epoch 00163: val_loss did not improve from 0.08945\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0933 - acc: 0.9728 - val_loss: 0.1145 - val_acc: 0.9642\n",
            "Epoch 164/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0926 - acc: 0.9755\n",
            "Epoch 00164: val_loss did not improve from 0.08945\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0933 - acc: 0.9747 - val_loss: 0.1391 - val_acc: 0.9550\n",
            "Epoch 165/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9737\n",
            "Epoch 00165: val_loss did not improve from 0.08945\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0956 - acc: 0.9731 - val_loss: 0.1444 - val_acc: 0.9575\n",
            "Epoch 166/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0945 - acc: 0.9738\n",
            "Epoch 00166: val_loss did not improve from 0.08945\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0932 - acc: 0.9742 - val_loss: 0.1447 - val_acc: 0.9542\n",
            "Epoch 167/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0870 - acc: 0.9745\n",
            "Epoch 00167: val_loss did not improve from 0.08945\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0865 - acc: 0.9747 - val_loss: 0.1010 - val_acc: 0.9700\n",
            "Epoch 168/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0903 - acc: 0.9748\n",
            "Epoch 00168: val_loss did not improve from 0.08945\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.0922 - acc: 0.9733 - val_loss: 0.1088 - val_acc: 0.9725\n",
            "Epoch 169/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1017 - acc: 0.9681\n",
            "Epoch 00169: val_loss did not improve from 0.08945\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1017 - acc: 0.9681 - val_loss: 0.1022 - val_acc: 0.9708\n",
            "Epoch 170/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0983 - acc: 0.9732\n",
            "Epoch 00170: val_loss did not improve from 0.08945\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0964 - acc: 0.9742 - val_loss: 0.0969 - val_acc: 0.9708\n",
            "Epoch 171/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0989 - acc: 0.9706\n",
            "Epoch 00171: val_loss did not improve from 0.08945\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0992 - acc: 0.9703 - val_loss: 0.1058 - val_acc: 0.9733\n",
            "Epoch 172/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9720\n",
            "Epoch 00172: val_loss did not improve from 0.08945\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0947 - acc: 0.9719 - val_loss: 0.1304 - val_acc: 0.9567\n",
            "Epoch 173/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0877 - acc: 0.9742\n",
            "Epoch 00173: val_loss improved from 0.08945 to 0.08640, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.0869 - acc: 0.9750 - val_loss: 0.0864 - val_acc: 0.9758\n",
            "Epoch 174/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0981 - acc: 0.9712\n",
            "Epoch 00174: val_loss did not improve from 0.08640\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0962 - acc: 0.9719 - val_loss: 0.1063 - val_acc: 0.9692\n",
            "Epoch 175/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0997 - acc: 0.9739\n",
            "Epoch 00175: val_loss did not improve from 0.08640\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1019 - acc: 0.9733 - val_loss: 0.1696 - val_acc: 0.9442\n",
            "Epoch 176/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1096 - acc: 0.9656\n",
            "Epoch 00176: val_loss did not improve from 0.08640\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1101 - acc: 0.9658 - val_loss: 0.0996 - val_acc: 0.9733\n",
            "Epoch 177/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0939 - acc: 0.9697\n",
            "Epoch 00177: val_loss did not improve from 0.08640\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0949 - acc: 0.9697 - val_loss: 0.1227 - val_acc: 0.9642\n",
            "Epoch 178/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9717\n",
            "Epoch 00178: val_loss did not improve from 0.08640\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1002 - acc: 0.9708 - val_loss: 0.0912 - val_acc: 0.9717\n",
            "Epoch 179/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0905 - acc: 0.9724\n",
            "Epoch 00179: val_loss improved from 0.08640 to 0.07707, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.0917 - acc: 0.9711 - val_loss: 0.0771 - val_acc: 0.9792\n",
            "Epoch 180/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0993 - acc: 0.9697\n",
            "Epoch 00180: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0993 - acc: 0.9697 - val_loss: 0.1476 - val_acc: 0.9492\n",
            "Epoch 181/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9680\n",
            "Epoch 00181: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1028 - acc: 0.9681 - val_loss: 0.1581 - val_acc: 0.9458\n",
            "Epoch 182/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1058 - acc: 0.9703\n",
            "Epoch 00182: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1063 - acc: 0.9694 - val_loss: 0.0815 - val_acc: 0.9792\n",
            "Epoch 183/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9780\n",
            "Epoch 00183: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0836 - acc: 0.9781 - val_loss: 0.0897 - val_acc: 0.9758\n",
            "Epoch 184/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0801 - acc: 0.9779\n",
            "Epoch 00184: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0812 - acc: 0.9767 - val_loss: 0.0835 - val_acc: 0.9750\n",
            "Epoch 185/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0925 - acc: 0.9747\n",
            "Epoch 00185: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0926 - acc: 0.9744 - val_loss: 0.1006 - val_acc: 0.9700\n",
            "Epoch 186/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9734\n",
            "Epoch 00186: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0969 - acc: 0.9725 - val_loss: 0.1281 - val_acc: 0.9617\n",
            "Epoch 187/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0954 - acc: 0.9727\n",
            "Epoch 00187: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0961 - acc: 0.9719 - val_loss: 0.1841 - val_acc: 0.9400\n",
            "Epoch 188/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9691\n",
            "Epoch 00188: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1012 - acc: 0.9683 - val_loss: 0.0798 - val_acc: 0.9767\n",
            "Epoch 189/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9714\n",
            "Epoch 00189: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0927 - acc: 0.9708 - val_loss: 0.1031 - val_acc: 0.9700\n",
            "Epoch 190/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1273 - acc: 0.9566\n",
            "Epoch 00190: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1301 - acc: 0.9558 - val_loss: 0.1185 - val_acc: 0.9650\n",
            "Epoch 191/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0971 - acc: 0.9729\n",
            "Epoch 00191: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0977 - acc: 0.9725 - val_loss: 0.1105 - val_acc: 0.9642\n",
            "Epoch 192/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1066 - acc: 0.9688\n",
            "Epoch 00192: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1066 - acc: 0.9678 - val_loss: 0.0871 - val_acc: 0.9733\n",
            "Epoch 193/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1093 - acc: 0.9659\n",
            "Epoch 00193: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1063 - acc: 0.9672 - val_loss: 0.0901 - val_acc: 0.9758\n",
            "Epoch 194/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0971 - acc: 0.9682\n",
            "Epoch 00194: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1046 - acc: 0.9656 - val_loss: 0.1056 - val_acc: 0.9675\n",
            "Epoch 195/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9689\n",
            "Epoch 00195: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1015 - acc: 0.9692 - val_loss: 0.0972 - val_acc: 0.9750\n",
            "Epoch 196/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1026 - acc: 0.9703\n",
            "Epoch 00196: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1015 - acc: 0.9708 - val_loss: 0.1298 - val_acc: 0.9617\n",
            "Epoch 197/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9769\n",
            "Epoch 00197: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0831 - acc: 0.9769 - val_loss: 0.0922 - val_acc: 0.9758\n",
            "Epoch 198/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9746\n",
            "Epoch 00198: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0903 - acc: 0.9742 - val_loss: 0.1294 - val_acc: 0.9600\n",
            "Epoch 199/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1127 - acc: 0.9653\n",
            "Epoch 00199: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1120 - acc: 0.9647 - val_loss: 0.1480 - val_acc: 0.9525\n",
            "Epoch 200/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0962 - acc: 0.9728\n",
            "Epoch 00200: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0961 - acc: 0.9722 - val_loss: 0.1084 - val_acc: 0.9692\n",
            "Epoch 201/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0953 - acc: 0.9709\n",
            "Epoch 00201: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0961 - acc: 0.9714 - val_loss: 0.1195 - val_acc: 0.9658\n",
            "Epoch 202/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1022 - acc: 0.9729\n",
            "Epoch 00202: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1000 - acc: 0.9739 - val_loss: 0.1252 - val_acc: 0.9608\n",
            "Epoch 203/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9757\n",
            "Epoch 00203: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0820 - acc: 0.9756 - val_loss: 0.0839 - val_acc: 0.9817\n",
            "Epoch 204/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0859 - acc: 0.9773\n",
            "Epoch 00204: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0883 - acc: 0.9769 - val_loss: 0.1033 - val_acc: 0.9675\n",
            "Epoch 205/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9671\n",
            "Epoch 00205: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1090 - acc: 0.9669 - val_loss: 0.1298 - val_acc: 0.9533\n",
            "Epoch 206/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9726\n",
            "Epoch 00206: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0983 - acc: 0.9731 - val_loss: 0.0863 - val_acc: 0.9775\n",
            "Epoch 207/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0920 - acc: 0.9739\n",
            "Epoch 00207: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0930 - acc: 0.9736 - val_loss: 0.1105 - val_acc: 0.9650\n",
            "Epoch 208/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1139 - acc: 0.9616\n",
            "Epoch 00208: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1134 - acc: 0.9633 - val_loss: 0.1148 - val_acc: 0.9650\n",
            "Epoch 209/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0961 - acc: 0.9676\n",
            "Epoch 00209: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0957 - acc: 0.9669 - val_loss: 0.1061 - val_acc: 0.9692\n",
            "Epoch 210/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0870 - acc: 0.9727\n",
            "Epoch 00210: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0864 - acc: 0.9728 - val_loss: 0.1235 - val_acc: 0.9642\n",
            "Epoch 211/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9829\n",
            "Epoch 00211: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0766 - acc: 0.9825 - val_loss: 0.1159 - val_acc: 0.9650\n",
            "Epoch 212/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9717\n",
            "Epoch 00212: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0842 - acc: 0.9725 - val_loss: 0.1060 - val_acc: 0.9667\n",
            "Epoch 213/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9731\n",
            "Epoch 00213: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0882 - acc: 0.9733 - val_loss: 0.1094 - val_acc: 0.9725\n",
            "Epoch 214/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0959 - acc: 0.9718\n",
            "Epoch 00214: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0961 - acc: 0.9719 - val_loss: 0.1360 - val_acc: 0.9642\n",
            "Epoch 215/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0893 - acc: 0.9742\n",
            "Epoch 00215: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0899 - acc: 0.9739 - val_loss: 0.0913 - val_acc: 0.9725\n",
            "Epoch 216/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9763\n",
            "Epoch 00216: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0807 - acc: 0.9764 - val_loss: 0.0935 - val_acc: 0.9750\n",
            "Epoch 217/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0882 - acc: 0.9722\n",
            "Epoch 00217: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.0874 - acc: 0.9731 - val_loss: 0.1004 - val_acc: 0.9700\n",
            "Epoch 218/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0877 - acc: 0.9762\n",
            "Epoch 00218: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0889 - acc: 0.9758 - val_loss: 0.1342 - val_acc: 0.9558\n",
            "Epoch 219/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9720\n",
            "Epoch 00219: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0952 - acc: 0.9717 - val_loss: 0.1131 - val_acc: 0.9675\n",
            "Epoch 220/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0903 - acc: 0.9724\n",
            "Epoch 00220: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0906 - acc: 0.9725 - val_loss: 0.0961 - val_acc: 0.9742\n",
            "Epoch 221/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0798 - acc: 0.9773\n",
            "Epoch 00221: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0806 - acc: 0.9764 - val_loss: 0.0921 - val_acc: 0.9742\n",
            "Epoch 222/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0721 - acc: 0.9816\n",
            "Epoch 00222: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0728 - acc: 0.9808 - val_loss: 0.0985 - val_acc: 0.9683\n",
            "Epoch 223/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0813 - acc: 0.9782\n",
            "Epoch 00223: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0835 - acc: 0.9772 - val_loss: 0.0981 - val_acc: 0.9758\n",
            "Epoch 224/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0984 - acc: 0.9691\n",
            "Epoch 00224: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0971 - acc: 0.9697 - val_loss: 0.1091 - val_acc: 0.9658\n",
            "Epoch 225/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9760\n",
            "Epoch 00225: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0828 - acc: 0.9761 - val_loss: 0.0803 - val_acc: 0.9783\n",
            "Epoch 226/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0924 - acc: 0.9703\n",
            "Epoch 00226: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0929 - acc: 0.9694 - val_loss: 0.0827 - val_acc: 0.9767\n",
            "Epoch 227/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0835 - acc: 0.9745\n",
            "Epoch 00227: val_loss did not improve from 0.07707\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0827 - acc: 0.9750 - val_loss: 0.0870 - val_acc: 0.9758\n",
            "Epoch 228/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0917 - acc: 0.9712\n",
            "Epoch 00228: val_loss improved from 0.07707 to 0.07643, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.0907 - acc: 0.9717 - val_loss: 0.0764 - val_acc: 0.9775\n",
            "Epoch 229/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0765 - acc: 0.9794\n",
            "Epoch 00229: val_loss did not improve from 0.07643\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0765 - acc: 0.9794 - val_loss: 0.1103 - val_acc: 0.9667\n",
            "Epoch 230/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0857 - acc: 0.9755\n",
            "Epoch 00230: val_loss did not improve from 0.07643\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0846 - acc: 0.9758 - val_loss: 0.0931 - val_acc: 0.9767\n",
            "Epoch 231/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0784 - acc: 0.9765\n",
            "Epoch 00231: val_loss did not improve from 0.07643\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0768 - acc: 0.9772 - val_loss: 0.0821 - val_acc: 0.9775\n",
            "Epoch 232/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0794 - acc: 0.9747\n",
            "Epoch 00232: val_loss did not improve from 0.07643\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0802 - acc: 0.9750 - val_loss: 0.0940 - val_acc: 0.9742\n",
            "Epoch 233/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0822 - acc: 0.9770\n",
            "Epoch 00233: val_loss did not improve from 0.07643\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0827 - acc: 0.9772 - val_loss: 0.0993 - val_acc: 0.9667\n",
            "Epoch 234/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0929 - acc: 0.9761\n",
            "Epoch 00234: val_loss did not improve from 0.07643\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0925 - acc: 0.9758 - val_loss: 0.1039 - val_acc: 0.9650\n",
            "Epoch 235/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1009 - acc: 0.9673\n",
            "Epoch 00235: val_loss did not improve from 0.07643\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1016 - acc: 0.9675 - val_loss: 0.1025 - val_acc: 0.9675\n",
            "Epoch 236/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0819 - acc: 0.9756\n",
            "Epoch 00236: val_loss did not improve from 0.07643\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0823 - acc: 0.9753 - val_loss: 0.0969 - val_acc: 0.9692\n",
            "Epoch 237/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9789\n",
            "Epoch 00237: val_loss did not improve from 0.07643\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0773 - acc: 0.9794 - val_loss: 0.1529 - val_acc: 0.9500\n",
            "Epoch 238/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0741 - acc: 0.9782\n",
            "Epoch 00238: val_loss did not improve from 0.07643\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0744 - acc: 0.9783 - val_loss: 0.1203 - val_acc: 0.9608\n",
            "Epoch 239/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0895 - acc: 0.9718\n",
            "Epoch 00239: val_loss did not improve from 0.07643\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0894 - acc: 0.9719 - val_loss: 0.1033 - val_acc: 0.9708\n",
            "Epoch 240/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0776 - acc: 0.9785\n",
            "Epoch 00240: val_loss did not improve from 0.07643\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0788 - acc: 0.9781 - val_loss: 0.0883 - val_acc: 0.9767\n",
            "Epoch 241/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0756 - acc: 0.9776\n",
            "Epoch 00241: val_loss improved from 0.07643 to 0.07431, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.0753 - acc: 0.9783 - val_loss: 0.0743 - val_acc: 0.9808\n",
            "Epoch 242/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9786\n",
            "Epoch 00242: val_loss did not improve from 0.07431\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0741 - acc: 0.9775 - val_loss: 0.1011 - val_acc: 0.9650\n",
            "Epoch 243/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0878 - acc: 0.9724\n",
            "Epoch 00243: val_loss did not improve from 0.07431\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0899 - acc: 0.9717 - val_loss: 0.0827 - val_acc: 0.9742\n",
            "Epoch 244/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9746\n",
            "Epoch 00244: val_loss did not improve from 0.07431\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0851 - acc: 0.9750 - val_loss: 0.0840 - val_acc: 0.9725\n",
            "Epoch 245/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0848 - acc: 0.9770\n",
            "Epoch 00245: val_loss did not improve from 0.07431\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0825 - acc: 0.9778 - val_loss: 0.1065 - val_acc: 0.9667\n",
            "Epoch 246/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0757 - acc: 0.9776\n",
            "Epoch 00246: val_loss did not improve from 0.07431\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0768 - acc: 0.9772 - val_loss: 0.0776 - val_acc: 0.9742\n",
            "Epoch 247/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9777\n",
            "Epoch 00247: val_loss did not improve from 0.07431\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0829 - acc: 0.9769 - val_loss: 0.0869 - val_acc: 0.9750\n",
            "Epoch 248/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0718 - acc: 0.9803\n",
            "Epoch 00248: val_loss did not improve from 0.07431\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0716 - acc: 0.9794 - val_loss: 0.1076 - val_acc: 0.9625\n",
            "Epoch 249/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0681 - acc: 0.9800\n",
            "Epoch 00249: val_loss did not improve from 0.07431\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0718 - acc: 0.9783 - val_loss: 0.1017 - val_acc: 0.9692\n",
            "Epoch 250/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0951 - acc: 0.9691\n",
            "Epoch 00250: val_loss did not improve from 0.07431\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0978 - acc: 0.9672 - val_loss: 0.0942 - val_acc: 0.9750\n",
            "Epoch 251/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0898 - acc: 0.9712\n",
            "Epoch 00251: val_loss did not improve from 0.07431\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0899 - acc: 0.9711 - val_loss: 0.0999 - val_acc: 0.9633\n",
            "Epoch 252/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0670 - acc: 0.9824\n",
            "Epoch 00252: val_loss did not improve from 0.07431\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0669 - acc: 0.9825 - val_loss: 0.1099 - val_acc: 0.9592\n",
            "Epoch 253/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0714 - acc: 0.9812\n",
            "Epoch 00253: val_loss did not improve from 0.07431\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0730 - acc: 0.9794 - val_loss: 0.0879 - val_acc: 0.9758\n",
            "Epoch 254/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0868 - acc: 0.9753\n",
            "Epoch 00254: val_loss did not improve from 0.07431\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0881 - acc: 0.9736 - val_loss: 0.0867 - val_acc: 0.9758\n",
            "Epoch 255/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9797\n",
            "Epoch 00255: val_loss did not improve from 0.07431\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0723 - acc: 0.9794 - val_loss: 0.0783 - val_acc: 0.9692\n",
            "Epoch 256/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0761 - acc: 0.9776\n",
            "Epoch 00256: val_loss improved from 0.07431 to 0.07254, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.0750 - acc: 0.9781 - val_loss: 0.0725 - val_acc: 0.9750\n",
            "Epoch 257/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0839 - acc: 0.9750\n",
            "Epoch 00257: val_loss did not improve from 0.07254\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0823 - acc: 0.9753 - val_loss: 0.0832 - val_acc: 0.9775\n",
            "Epoch 258/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9746\n",
            "Epoch 00258: val_loss did not improve from 0.07254\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0840 - acc: 0.9747 - val_loss: 0.1003 - val_acc: 0.9717\n",
            "Epoch 259/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9751\n",
            "Epoch 00259: val_loss did not improve from 0.07254\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0833 - acc: 0.9750 - val_loss: 0.0835 - val_acc: 0.9733\n",
            "Epoch 260/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9769\n",
            "Epoch 00260: val_loss improved from 0.07254 to 0.06999, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.0768 - acc: 0.9769 - val_loss: 0.0700 - val_acc: 0.9775\n",
            "Epoch 261/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0722 - acc: 0.9761\n",
            "Epoch 00261: val_loss did not improve from 0.06999\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0727 - acc: 0.9758 - val_loss: 0.0945 - val_acc: 0.9733\n",
            "Epoch 262/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9751\n",
            "Epoch 00262: val_loss did not improve from 0.06999\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0797 - acc: 0.9756 - val_loss: 0.0853 - val_acc: 0.9717\n",
            "Epoch 263/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9780\n",
            "Epoch 00263: val_loss did not improve from 0.06999\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0723 - acc: 0.9786 - val_loss: 0.0867 - val_acc: 0.9750\n",
            "Epoch 264/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0699 - acc: 0.9806\n",
            "Epoch 00264: val_loss did not improve from 0.06999\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0714 - acc: 0.9803 - val_loss: 0.0778 - val_acc: 0.9767\n",
            "Epoch 265/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0695 - acc: 0.9800\n",
            "Epoch 00265: val_loss did not improve from 0.06999\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0702 - acc: 0.9797 - val_loss: 0.0795 - val_acc: 0.9750\n",
            "Epoch 266/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0825 - acc: 0.9744\n",
            "Epoch 00266: val_loss did not improve from 0.06999\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0813 - acc: 0.9747 - val_loss: 0.0888 - val_acc: 0.9742\n",
            "Epoch 267/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0909 - acc: 0.9725\n",
            "Epoch 00267: val_loss did not improve from 0.06999\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0884 - acc: 0.9736 - val_loss: 0.1285 - val_acc: 0.9583\n",
            "Epoch 268/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9689\n",
            "Epoch 00268: val_loss did not improve from 0.06999\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0973 - acc: 0.9689 - val_loss: 0.0927 - val_acc: 0.9742\n",
            "Epoch 269/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0780 - acc: 0.9753\n",
            "Epoch 00269: val_loss did not improve from 0.06999\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0770 - acc: 0.9758 - val_loss: 0.0832 - val_acc: 0.9750\n",
            "Epoch 270/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0738 - acc: 0.9791\n",
            "Epoch 00270: val_loss did not improve from 0.06999\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0743 - acc: 0.9789 - val_loss: 0.0883 - val_acc: 0.9733\n",
            "Epoch 271/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0708 - acc: 0.9791\n",
            "Epoch 00271: val_loss did not improve from 0.06999\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0701 - acc: 0.9794 - val_loss: 0.0781 - val_acc: 0.9725\n",
            "Epoch 272/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0741 - acc: 0.9794\n",
            "Epoch 00272: val_loss did not improve from 0.06999\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0725 - acc: 0.9803 - val_loss: 0.0822 - val_acc: 0.9742\n",
            "Epoch 273/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9703\n",
            "Epoch 00273: val_loss did not improve from 0.06999\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0811 - acc: 0.9706 - val_loss: 0.0837 - val_acc: 0.9717\n",
            "Epoch 274/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9754\n",
            "Epoch 00274: val_loss did not improve from 0.06999\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0808 - acc: 0.9756 - val_loss: 0.0854 - val_acc: 0.9792\n",
            "Epoch 275/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0918 - acc: 0.9747\n",
            "Epoch 00275: val_loss did not improve from 0.06999\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0897 - acc: 0.9756 - val_loss: 0.0728 - val_acc: 0.9767\n",
            "Epoch 276/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0894 - acc: 0.9706\n",
            "Epoch 00276: val_loss did not improve from 0.06999\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0893 - acc: 0.9706 - val_loss: 0.0896 - val_acc: 0.9750\n",
            "Epoch 277/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0815 - acc: 0.9739\n",
            "Epoch 00277: val_loss did not improve from 0.06999\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0807 - acc: 0.9744 - val_loss: 0.0911 - val_acc: 0.9717\n",
            "Epoch 278/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9740\n",
            "Epoch 00278: val_loss did not improve from 0.06999\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0828 - acc: 0.9744 - val_loss: 0.1058 - val_acc: 0.9700\n",
            "Epoch 279/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9803\n",
            "Epoch 00279: val_loss improved from 0.06999 to 0.06883, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.0709 - acc: 0.9806 - val_loss: 0.0688 - val_acc: 0.9842\n",
            "Epoch 280/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9823\n",
            "Epoch 00280: val_loss improved from 0.06883 to 0.06834, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.0667 - acc: 0.9819 - val_loss: 0.0683 - val_acc: 0.9808\n",
            "Epoch 281/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9749\n",
            "Epoch 00281: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0813 - acc: 0.9756 - val_loss: 0.0765 - val_acc: 0.9800\n",
            "Epoch 282/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0681 - acc: 0.9833\n",
            "Epoch 00282: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0684 - acc: 0.9831 - val_loss: 0.0970 - val_acc: 0.9708\n",
            "Epoch 283/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9774\n",
            "Epoch 00283: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0768 - acc: 0.9772 - val_loss: 0.0864 - val_acc: 0.9775\n",
            "Epoch 284/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0794 - acc: 0.9779\n",
            "Epoch 00284: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0801 - acc: 0.9775 - val_loss: 0.1152 - val_acc: 0.9675\n",
            "Epoch 285/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9749\n",
            "Epoch 00285: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0844 - acc: 0.9747 - val_loss: 0.0939 - val_acc: 0.9692\n",
            "Epoch 286/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9769\n",
            "Epoch 00286: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0750 - acc: 0.9769 - val_loss: 0.0840 - val_acc: 0.9750\n",
            "Epoch 287/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0867 - acc: 0.9724\n",
            "Epoch 00287: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0910 - acc: 0.9706 - val_loss: 0.0995 - val_acc: 0.9725\n",
            "Epoch 288/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0813 - acc: 0.9756\n",
            "Epoch 00288: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0799 - acc: 0.9764 - val_loss: 0.1024 - val_acc: 0.9692\n",
            "Epoch 289/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0802 - acc: 0.9775\n",
            "Epoch 00289: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0802 - acc: 0.9769 - val_loss: 0.1254 - val_acc: 0.9617\n",
            "Epoch 290/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9800\n",
            "Epoch 00290: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0657 - acc: 0.9800 - val_loss: 0.1082 - val_acc: 0.9667\n",
            "Epoch 291/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0724 - acc: 0.9774\n",
            "Epoch 00291: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0734 - acc: 0.9775 - val_loss: 0.0806 - val_acc: 0.9775\n",
            "Epoch 292/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0657 - acc: 0.9812\n",
            "Epoch 00292: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0637 - acc: 0.9819 - val_loss: 0.0839 - val_acc: 0.9750\n",
            "Epoch 293/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0783 - acc: 0.9747\n",
            "Epoch 00293: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0769 - acc: 0.9756 - val_loss: 0.0943 - val_acc: 0.9683\n",
            "Epoch 294/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9740\n",
            "Epoch 00294: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0851 - acc: 0.9742 - val_loss: 0.0857 - val_acc: 0.9750\n",
            "Epoch 295/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0789 - acc: 0.9761\n",
            "Epoch 00295: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0789 - acc: 0.9756 - val_loss: 0.0940 - val_acc: 0.9725\n",
            "Epoch 296/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0847 - acc: 0.9742\n",
            "Epoch 00296: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0845 - acc: 0.9747 - val_loss: 0.0933 - val_acc: 0.9750\n",
            "Epoch 297/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0879 - acc: 0.9718\n",
            "Epoch 00297: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0887 - acc: 0.9711 - val_loss: 0.0815 - val_acc: 0.9792\n",
            "Epoch 298/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9740\n",
            "Epoch 00298: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0833 - acc: 0.9742 - val_loss: 0.0848 - val_acc: 0.9783\n",
            "Epoch 299/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0790 - acc: 0.9782\n",
            "Epoch 00299: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0800 - acc: 0.9775 - val_loss: 0.0889 - val_acc: 0.9750\n",
            "Epoch 300/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0763 - acc: 0.9785\n",
            "Epoch 00300: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0778 - acc: 0.9775 - val_loss: 0.0982 - val_acc: 0.9717\n",
            "Epoch 301/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0789 - acc: 0.9775\n",
            "Epoch 00301: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0774 - acc: 0.9781 - val_loss: 0.0794 - val_acc: 0.9767\n",
            "Epoch 302/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0736 - acc: 0.9787\n",
            "Epoch 00302: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0734 - acc: 0.9792 - val_loss: 0.0888 - val_acc: 0.9725\n",
            "Epoch 303/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0700 - acc: 0.9815\n",
            "Epoch 00303: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0698 - acc: 0.9817 - val_loss: 0.0817 - val_acc: 0.9758\n",
            "Epoch 304/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0795 - acc: 0.9785\n",
            "Epoch 00304: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0784 - acc: 0.9789 - val_loss: 0.0764 - val_acc: 0.9758\n",
            "Epoch 305/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0750 - acc: 0.9806\n",
            "Epoch 00305: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0748 - acc: 0.9803 - val_loss: 0.0883 - val_acc: 0.9725\n",
            "Epoch 306/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9746\n",
            "Epoch 00306: val_loss did not improve from 0.06834\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0789 - acc: 0.9750 - val_loss: 0.0768 - val_acc: 0.9775\n",
            "Epoch 307/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9714\n",
            "Epoch 00307: val_loss improved from 0.06834 to 0.06502, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.0892 - acc: 0.9714 - val_loss: 0.0650 - val_acc: 0.9825\n",
            "Epoch 308/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9771\n",
            "Epoch 00308: val_loss did not improve from 0.06502\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0794 - acc: 0.9778 - val_loss: 0.0955 - val_acc: 0.9725\n",
            "Epoch 309/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0827 - acc: 0.9741\n",
            "Epoch 00309: val_loss did not improve from 0.06502\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0835 - acc: 0.9733 - val_loss: 0.1047 - val_acc: 0.9700\n",
            "Epoch 310/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0778 - acc: 0.9764\n",
            "Epoch 00310: val_loss did not improve from 0.06502\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0838 - acc: 0.9742 - val_loss: 0.0930 - val_acc: 0.9725\n",
            "Epoch 311/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0999 - acc: 0.9703\n",
            "Epoch 00311: val_loss did not improve from 0.06502\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0988 - acc: 0.9703 - val_loss: 0.0995 - val_acc: 0.9708\n",
            "Epoch 312/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0747 - acc: 0.9747\n",
            "Epoch 00312: val_loss did not improve from 0.06502\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0762 - acc: 0.9744 - val_loss: 0.1024 - val_acc: 0.9692\n",
            "Epoch 313/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0710 - acc: 0.9806\n",
            "Epoch 00313: val_loss did not improve from 0.06502\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0699 - acc: 0.9808 - val_loss: 0.0841 - val_acc: 0.9725\n",
            "Epoch 314/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0730 - acc: 0.9782\n",
            "Epoch 00314: val_loss did not improve from 0.06502\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0734 - acc: 0.9778 - val_loss: 0.0876 - val_acc: 0.9717\n",
            "Epoch 315/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9754\n",
            "Epoch 00315: val_loss did not improve from 0.06502\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0817 - acc: 0.9758 - val_loss: 0.0742 - val_acc: 0.9808\n",
            "Epoch 316/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0687 - acc: 0.9803\n",
            "Epoch 00316: val_loss did not improve from 0.06502\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0684 - acc: 0.9800 - val_loss: 0.0654 - val_acc: 0.9817\n",
            "Epoch 317/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0740 - acc: 0.9797\n",
            "Epoch 00317: val_loss improved from 0.06502 to 0.05874, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.0732 - acc: 0.9806 - val_loss: 0.0587 - val_acc: 0.9842\n",
            "Epoch 318/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9846\n",
            "Epoch 00318: val_loss improved from 0.05874 to 0.05846, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.0648 - acc: 0.9847 - val_loss: 0.0585 - val_acc: 0.9842\n",
            "Epoch 319/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0753 - acc: 0.9762\n",
            "Epoch 00319: val_loss did not improve from 0.05846\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0760 - acc: 0.9761 - val_loss: 0.0629 - val_acc: 0.9858\n",
            "Epoch 320/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0833 - acc: 0.9732\n",
            "Epoch 00320: val_loss did not improve from 0.05846\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0836 - acc: 0.9736 - val_loss: 0.1244 - val_acc: 0.9575\n",
            "Epoch 321/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0763 - acc: 0.9809\n",
            "Epoch 00321: val_loss did not improve from 0.05846\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0742 - acc: 0.9817 - val_loss: 0.0706 - val_acc: 0.9808\n",
            "Epoch 322/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9817\n",
            "Epoch 00322: val_loss improved from 0.05846 to 0.05744, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.0619 - acc: 0.9817 - val_loss: 0.0574 - val_acc: 0.9875\n",
            "Epoch 323/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0714 - acc: 0.9797\n",
            "Epoch 00323: val_loss did not improve from 0.05744\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0720 - acc: 0.9794 - val_loss: 0.0614 - val_acc: 0.9800\n",
            "Epoch 324/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9806\n",
            "Epoch 00324: val_loss did not improve from 0.05744\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0650 - acc: 0.9803 - val_loss: 0.0635 - val_acc: 0.9850\n",
            "Epoch 325/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0628 - acc: 0.9826\n",
            "Epoch 00325: val_loss improved from 0.05744 to 0.05680, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.0629 - acc: 0.9828 - val_loss: 0.0568 - val_acc: 0.9833\n",
            "Epoch 326/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0664 - acc: 0.9784\n",
            "Epoch 00326: val_loss did not improve from 0.05680\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0668 - acc: 0.9783 - val_loss: 0.0853 - val_acc: 0.9733\n",
            "Epoch 327/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0788 - acc: 0.9759\n",
            "Epoch 00327: val_loss did not improve from 0.05680\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0781 - acc: 0.9761 - val_loss: 0.0666 - val_acc: 0.9833\n",
            "Epoch 328/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0626 - acc: 0.9806\n",
            "Epoch 00328: val_loss did not improve from 0.05680\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0622 - acc: 0.9806 - val_loss: 0.0658 - val_acc: 0.9850\n",
            "Epoch 329/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0615 - acc: 0.9821\n",
            "Epoch 00329: val_loss did not improve from 0.05680\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0641 - acc: 0.9803 - val_loss: 0.0637 - val_acc: 0.9800\n",
            "Epoch 330/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0810 - acc: 0.9753\n",
            "Epoch 00330: val_loss did not improve from 0.05680\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0825 - acc: 0.9744 - val_loss: 0.0765 - val_acc: 0.9808\n",
            "Epoch 331/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0711 - acc: 0.9788\n",
            "Epoch 00331: val_loss did not improve from 0.05680\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0713 - acc: 0.9789 - val_loss: 0.0685 - val_acc: 0.9792\n",
            "Epoch 332/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9811\n",
            "Epoch 00332: val_loss did not improve from 0.05680\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0643 - acc: 0.9811 - val_loss: 0.0821 - val_acc: 0.9700\n",
            "Epoch 333/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0654 - acc: 0.9797\n",
            "Epoch 00333: val_loss improved from 0.05680 to 0.05618, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.0661 - acc: 0.9800 - val_loss: 0.0562 - val_acc: 0.9883\n",
            "Epoch 334/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.9694\n",
            "Epoch 00334: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0933 - acc: 0.9692 - val_loss: 0.1225 - val_acc: 0.9583\n",
            "Epoch 335/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0773 - acc: 0.9733\n",
            "Epoch 00335: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0802 - acc: 0.9725 - val_loss: 0.0941 - val_acc: 0.9725\n",
            "Epoch 336/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9803\n",
            "Epoch 00336: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0719 - acc: 0.9808 - val_loss: 0.0736 - val_acc: 0.9808\n",
            "Epoch 337/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0664 - acc: 0.9794\n",
            "Epoch 00337: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0695 - acc: 0.9781 - val_loss: 0.0791 - val_acc: 0.9792\n",
            "Epoch 338/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0748 - acc: 0.9781\n",
            "Epoch 00338: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0755 - acc: 0.9778 - val_loss: 0.1177 - val_acc: 0.9600\n",
            "Epoch 339/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9809\n",
            "Epoch 00339: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0708 - acc: 0.9803 - val_loss: 0.0859 - val_acc: 0.9775\n",
            "Epoch 340/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9717\n",
            "Epoch 00340: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0828 - acc: 0.9722 - val_loss: 0.0937 - val_acc: 0.9675\n",
            "Epoch 341/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0784 - acc: 0.9785\n",
            "Epoch 00341: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0759 - acc: 0.9797 - val_loss: 0.1041 - val_acc: 0.9675\n",
            "Epoch 342/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9806\n",
            "Epoch 00342: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0663 - acc: 0.9803 - val_loss: 0.0859 - val_acc: 0.9758\n",
            "Epoch 343/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0651 - acc: 0.9803\n",
            "Epoch 00343: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0703 - acc: 0.9781 - val_loss: 0.0760 - val_acc: 0.9767\n",
            "Epoch 344/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9749\n",
            "Epoch 00344: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0880 - acc: 0.9750 - val_loss: 0.0757 - val_acc: 0.9783\n",
            "Epoch 345/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0793 - acc: 0.9756\n",
            "Epoch 00345: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0823 - acc: 0.9742 - val_loss: 0.0816 - val_acc: 0.9758\n",
            "Epoch 346/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9674\n",
            "Epoch 00346: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0858 - acc: 0.9675 - val_loss: 0.1137 - val_acc: 0.9642\n",
            "Epoch 347/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0731 - acc: 0.9779\n",
            "Epoch 00347: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0749 - acc: 0.9767 - val_loss: 0.1192 - val_acc: 0.9650\n",
            "Epoch 348/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0813 - acc: 0.9738\n",
            "Epoch 00348: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0802 - acc: 0.9747 - val_loss: 0.0717 - val_acc: 0.9800\n",
            "Epoch 349/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0639 - acc: 0.9829\n",
            "Epoch 00349: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0634 - acc: 0.9833 - val_loss: 0.0638 - val_acc: 0.9808\n",
            "Epoch 350/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0607 - acc: 0.9830\n",
            "Epoch 00350: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.0632 - acc: 0.9822 - val_loss: 0.0650 - val_acc: 0.9767\n",
            "Epoch 351/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0646 - acc: 0.9815\n",
            "Epoch 00351: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0651 - acc: 0.9814 - val_loss: 0.0679 - val_acc: 0.9808\n",
            "Epoch 352/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0765 - acc: 0.9761\n",
            "Epoch 00352: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0758 - acc: 0.9761 - val_loss: 0.0643 - val_acc: 0.9833\n",
            "Epoch 353/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9740\n",
            "Epoch 00353: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0750 - acc: 0.9744 - val_loss: 0.0656 - val_acc: 0.9833\n",
            "Epoch 354/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0606 - acc: 0.9830\n",
            "Epoch 00354: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0604 - acc: 0.9833 - val_loss: 0.0786 - val_acc: 0.9750\n",
            "Epoch 355/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0575 - acc: 0.9850\n",
            "Epoch 00355: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0591 - acc: 0.9850 - val_loss: 0.0611 - val_acc: 0.9875\n",
            "Epoch 356/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0716 - acc: 0.9787\n",
            "Epoch 00356: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0707 - acc: 0.9789 - val_loss: 0.0762 - val_acc: 0.9758\n",
            "Epoch 357/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0734 - acc: 0.9747\n",
            "Epoch 00357: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0734 - acc: 0.9747 - val_loss: 0.0661 - val_acc: 0.9808\n",
            "Epoch 358/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9780\n",
            "Epoch 00358: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0756 - acc: 0.9781 - val_loss: 0.0790 - val_acc: 0.9733\n",
            "Epoch 359/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9794\n",
            "Epoch 00359: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0724 - acc: 0.9794 - val_loss: 0.0722 - val_acc: 0.9758\n",
            "Epoch 360/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9723\n",
            "Epoch 00360: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0825 - acc: 0.9725 - val_loss: 0.0590 - val_acc: 0.9842\n",
            "Epoch 361/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.9686\n",
            "Epoch 00361: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0947 - acc: 0.9686 - val_loss: 0.0832 - val_acc: 0.9750\n",
            "Epoch 362/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0818 - acc: 0.9755\n",
            "Epoch 00362: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.0818 - acc: 0.9747 - val_loss: 0.0778 - val_acc: 0.9792\n",
            "Epoch 363/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0730 - acc: 0.9781\n",
            "Epoch 00363: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0707 - acc: 0.9789 - val_loss: 0.0785 - val_acc: 0.9792\n",
            "Epoch 364/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0695 - acc: 0.9794\n",
            "Epoch 00364: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0692 - acc: 0.9797 - val_loss: 0.0789 - val_acc: 0.9775\n",
            "Epoch 365/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0754 - acc: 0.9782\n",
            "Epoch 00365: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0757 - acc: 0.9783 - val_loss: 0.0733 - val_acc: 0.9800\n",
            "Epoch 366/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0749 - acc: 0.9759\n",
            "Epoch 00366: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0765 - acc: 0.9747 - val_loss: 0.0608 - val_acc: 0.9867\n",
            "Epoch 367/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0899 - acc: 0.9697\n",
            "Epoch 00367: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0884 - acc: 0.9706 - val_loss: 0.0956 - val_acc: 0.9708\n",
            "Epoch 368/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9837\n",
            "Epoch 00368: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0617 - acc: 0.9839 - val_loss: 0.0650 - val_acc: 0.9825\n",
            "Epoch 369/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0774 - acc: 0.9762\n",
            "Epoch 00369: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0805 - acc: 0.9753 - val_loss: 0.0618 - val_acc: 0.9825\n",
            "Epoch 370/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9740\n",
            "Epoch 00370: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0758 - acc: 0.9736 - val_loss: 0.0798 - val_acc: 0.9767\n",
            "Epoch 371/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0768 - acc: 0.9762\n",
            "Epoch 00371: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0766 - acc: 0.9761 - val_loss: 0.0627 - val_acc: 0.9792\n",
            "Epoch 372/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9834\n",
            "Epoch 00372: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0649 - acc: 0.9833 - val_loss: 0.0675 - val_acc: 0.9800\n",
            "Epoch 373/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0572 - acc: 0.9842\n",
            "Epoch 00373: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0568 - acc: 0.9844 - val_loss: 0.0576 - val_acc: 0.9833\n",
            "Epoch 374/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9803\n",
            "Epoch 00374: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0642 - acc: 0.9803 - val_loss: 0.0752 - val_acc: 0.9775\n",
            "Epoch 375/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0713 - acc: 0.9806\n",
            "Epoch 00375: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0721 - acc: 0.9800 - val_loss: 0.0630 - val_acc: 0.9833\n",
            "Epoch 376/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9803\n",
            "Epoch 00376: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0683 - acc: 0.9797 - val_loss: 0.0589 - val_acc: 0.9842\n",
            "Epoch 377/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9820\n",
            "Epoch 00377: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0609 - acc: 0.9822 - val_loss: 0.0574 - val_acc: 0.9825\n",
            "Epoch 378/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0763 - acc: 0.9748\n",
            "Epoch 00378: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0784 - acc: 0.9742 - val_loss: 0.0691 - val_acc: 0.9817\n",
            "Epoch 379/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0775 - acc: 0.9759\n",
            "Epoch 00379: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0763 - acc: 0.9764 - val_loss: 0.0633 - val_acc: 0.9808\n",
            "Epoch 380/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0639 - acc: 0.9831\n",
            "Epoch 00380: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0656 - acc: 0.9822 - val_loss: 0.0705 - val_acc: 0.9817\n",
            "Epoch 381/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0681 - acc: 0.9794\n",
            "Epoch 00381: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0666 - acc: 0.9800 - val_loss: 0.0775 - val_acc: 0.9758\n",
            "Epoch 382/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9840\n",
            "Epoch 00382: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0603 - acc: 0.9839 - val_loss: 0.0815 - val_acc: 0.9717\n",
            "Epoch 383/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9766\n",
            "Epoch 00383: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0678 - acc: 0.9764 - val_loss: 0.1136 - val_acc: 0.9650\n",
            "Epoch 384/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9800\n",
            "Epoch 00384: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0723 - acc: 0.9803 - val_loss: 0.0659 - val_acc: 0.9817\n",
            "Epoch 385/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9831\n",
            "Epoch 00385: val_loss did not improve from 0.05618\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0589 - acc: 0.9831 - val_loss: 0.0595 - val_acc: 0.9875\n",
            "Epoch 386/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0602 - acc: 0.9815\n",
            "Epoch 00386: val_loss improved from 0.05618 to 0.05616, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.0598 - acc: 0.9817 - val_loss: 0.0562 - val_acc: 0.9850\n",
            "Epoch 387/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0619 - acc: 0.9818\n",
            "Epoch 00387: val_loss did not improve from 0.05616\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0613 - acc: 0.9822 - val_loss: 0.0595 - val_acc: 0.9792\n",
            "Epoch 388/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9803\n",
            "Epoch 00388: val_loss did not improve from 0.05616\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0659 - acc: 0.9808 - val_loss: 0.0686 - val_acc: 0.9783\n",
            "Epoch 389/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0808 - acc: 0.9752\n",
            "Epoch 00389: val_loss did not improve from 0.05616\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0788 - acc: 0.9761 - val_loss: 0.0928 - val_acc: 0.9725\n",
            "Epoch 390/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0759 - acc: 0.9762\n",
            "Epoch 00390: val_loss did not improve from 0.05616\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0783 - acc: 0.9758 - val_loss: 0.0855 - val_acc: 0.9775\n",
            "Epoch 391/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0632 - acc: 0.9847\n",
            "Epoch 00391: val_loss did not improve from 0.05616\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0614 - acc: 0.9853 - val_loss: 0.0703 - val_acc: 0.9825\n",
            "Epoch 392/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9829\n",
            "Epoch 00392: val_loss did not improve from 0.05616\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0590 - acc: 0.9833 - val_loss: 0.0641 - val_acc: 0.9800\n",
            "Epoch 393/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0644 - acc: 0.9812\n",
            "Epoch 00393: val_loss did not improve from 0.05616\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0652 - acc: 0.9806 - val_loss: 0.0776 - val_acc: 0.9767\n",
            "Epoch 394/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0707 - acc: 0.9784\n",
            "Epoch 00394: val_loss did not improve from 0.05616\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0700 - acc: 0.9792 - val_loss: 0.0704 - val_acc: 0.9783\n",
            "Epoch 395/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9774\n",
            "Epoch 00395: val_loss did not improve from 0.05616\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0671 - acc: 0.9772 - val_loss: 0.0643 - val_acc: 0.9817\n",
            "Epoch 396/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0726 - acc: 0.9756\n",
            "Epoch 00396: val_loss did not improve from 0.05616\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0703 - acc: 0.9769 - val_loss: 0.0680 - val_acc: 0.9808\n",
            "Epoch 397/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0594 - acc: 0.9826\n",
            "Epoch 00397: val_loss did not improve from 0.05616\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0612 - acc: 0.9822 - val_loss: 0.0623 - val_acc: 0.9833\n",
            "Epoch 398/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0751 - acc: 0.9773\n",
            "Epoch 00398: val_loss did not improve from 0.05616\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0758 - acc: 0.9775 - val_loss: 0.0856 - val_acc: 0.9733\n",
            "Epoch 399/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0657 - acc: 0.9824\n",
            "Epoch 00399: val_loss did not improve from 0.05616\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0677 - acc: 0.9817 - val_loss: 0.1026 - val_acc: 0.9667\n",
            "Epoch 400/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0758 - acc: 0.9761\n",
            "Epoch 00400: val_loss did not improve from 0.05616\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0763 - acc: 0.9756 - val_loss: 0.0862 - val_acc: 0.9758\n",
            "1200/1200 [==============================] - 0s 122us/sample - loss: 0.0862 - acc: 0.9758\n",
            "[0.08623584138850371, 0.97583336]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKcxyQoNqoOd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "93589e16-5045-40ad-922f-f04c2b675229"
      },
      "source": [
        "for i in range(5, 6): # Итерација низ секој испитен примерок\n",
        "  print(f\"====================== Примерок ({i}) ======================\")\n",
        "  print(\"Вчитување тест податоци од испитниот примерок \" + str(i) + \"...\")\n",
        "  \n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_test_data = np.empty(0)\n",
        "  temp_test_events = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_test_set = 'S' + format(j, '02') + '/Test'\n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testData.mat\"\n",
        "    temp = loadmat(full_path)['testData']\n",
        "    if temp_test_data.size != 0:\n",
        "      temp_test_data = np.concatenate((temp_test_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_test_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_test_events.size != 0:\n",
        "        temp_test_events = np.append(temp_test_events, temp)\n",
        "      else:\n",
        "        temp_test_events = np.array(temp)\n",
        "\n",
        "    # Вчитување на бројот на runs \n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/runs_per_block.txt\"\n",
        "    with open(full_path, \"r\") as runs_per_block:\n",
        "      test_runs_per_block[i-1][j-1] = int(runs_per_block.read())\n",
        "\n",
        "    print(\"\\t - Тест податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "  # Зачувај ги тест податоците вчитани од испитниот примерок во низа\n",
        "  test_data.append(temp_test_data)\n",
        "  test_events.append(temp_test_events)\n",
        "  print(\"Тест податоците од испитниот примерок \" + str(i) + \" се вчитани.\\n\")\n",
        "\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_data: \" + str(test_data[i-1].shape)) # test_data to predict\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_events: \" + str(len(test_events[i-1]))) # test_events\n",
        "  for j in range (1,4):\n",
        "    print(\"SBJ\" + str(format(i-1, '02')) + \" / S\" + str(format(j-1, '02')) + \"| Runs per block: \" + str(test_runs_per_block[i-1][j-1])) # runs per block in SJB01, SJ00 \n",
        "\n",
        "  to_predict_data = reshape_data_to_mne_format(test_data[i-1])\n",
        "  predictions = model5.predict(to_predict_data)\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Predictions: \" + str(len(predictions)))\n",
        "  # np.savetxt(\"predictions.csv\", predictions, delimiter=\",\")\n",
        "\n",
        "\n",
        "  # ========= FALI USTE DA SE ISPARSIRA PREDICTIONOT... NE E SREDEN OVOJ KOD DOLE =======\n",
        "\n",
        "  int_pred = np.argmax(predictions, axis=1)\n",
        "  int_ytest = np.argmax(y_test, axis=1)\n",
        "\n",
        "  session_start = 0\n",
        "  start_prediction_index = 0\n",
        "  end_prediction_index = 0\n",
        "  for session in range(0, 3):\n",
        "    print(f\"============== Сесија ({session}) ==============\")\n",
        "    for block in range(0, 50):    \n",
        "      events_per_block = test_runs_per_block[i-1][session]\n",
        "\n",
        "      start_prediction_index = session_start + (block*events_per_block)*8\n",
        "      end_prediction_index = session_start + ((block+1)*events_per_block)*8\n",
        "\n",
        "      block_prediction = int_pred[start_prediction_index:end_prediction_index]\n",
        "      prediction = np.bincount(block_prediction).argmax()\n",
        "      df.iat[session+12,block+2] = prediction+1\n",
        "      # UNCOMMENT ZA PODOBAR PRIKAZ :)\n",
        "      # print(f\"Session {session} | Block: {block} | Prediction: {prediction} | Address: {end_prediction_index}\")\n",
        "\n",
        "      print(str(prediction+1) + \",\", end=\"\")\n",
        "    session_start = end_prediction_index\n",
        "    print(\"\")\n",
        "  print(\"Stigna li do kraj: \" + str(session_start == len(predictions)))\n",
        "  print(f\"====================== Примерок ({i}) ======================\\n\\n\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Примерок (5) ======================\n",
            "Вчитување тест податоци од испитниот примерок 5...\n",
            "\t - Тест податоците од сесија 1 се вчитани.\n",
            "\t - Тест податоците од сесија 2 се вчитани.\n",
            "\t - Тест податоците од сесија 3 се вчитани.\n",
            "Тест податоците од испитниот примерок 5 се вчитани.\n",
            "\n",
            "SBJ04| Test_data: (8, 350, 8800)\n",
            "SBJ04| Test_events: 8800\n",
            "SBJ04 / S00| Runs per block: 8\n",
            "SBJ04 / S01| Runs per block: 7\n",
            "SBJ04 / S02| Runs per block: 7\n",
            "SBJ04| Predictions: 8800\n",
            "============== Сесија (0) ==============\n",
            "5,4,4,4,6,4,5,4,2,6,3,6,3,4,3,5,4,2,1,4,1,1,3,6,3,6,4,1,4,6,2,4,6,2,4,4,6,3,3,4,1,3,3,1,4,4,1,3,3,5,\n",
            "============== Сесија (1) ==============\n",
            "3,6,3,5,2,6,7,7,6,6,6,4,6,6,6,6,6,6,6,6,4,6,6,6,4,6,6,2,6,4,6,1,1,6,4,6,4,6,4,6,4,4,6,6,6,6,6,6,6,4,\n",
            "============== Сесија (2) ==============\n",
            "4,3,3,6,5,7,6,4,6,4,4,6,6,6,3,2,6,5,2,5,5,2,6,2,7,6,4,4,4,4,5,5,2,2,2,6,1,7,1,6,6,7,7,2,2,7,5,6,4,2,\n",
            "Stigna li do kraj: True\n",
            "====================== Примерок (5) ======================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txm1NT2MrFd7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e3177741-533e-4a4e-a750-47aa440604f5"
      },
      "source": [
        "df"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUBJECT</th>\n",
              "      <th>SESSION</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>Unnamed: 52</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    SUBJECT  SESSION  1  2  3  4  5  6  7  ... 43 44 45 46 47 48 49 50 Unnamed: 52\n",
              "0         1        1  6  6  3  6  6  3  6  ...  6  4  8  3  4  5  5  7         NaN\n",
              "1         1        2  6  3  2  1  2  1  3  ...  6  2  2  2  3  6  6  2         NaN\n",
              "2         1        3  3  3  3  3  3  3  3  ...  3  3  6  3  6  7  1  6         NaN\n",
              "3         2        1  8  8  8  8  8  8  8  ...  8  4  8  4  8  7  8  8         NaN\n",
              "4         2        2  2  7  6  6  6  6  7  ...  2  6  6  2  6  6  2  2         NaN\n",
              "5         2        3  2  7  2  6  7  4  2  ...  2  6  2  2  7  2  2  2         NaN\n",
              "6         3        1  3  3  4  4  4  3  6  ...  4  6  3  3  4  3  4  4         NaN\n",
              "7         3        2  6  1  7  7  7  7  7  ...  6  7  1  6  6  6  6  6         NaN\n",
              "8         3        3  6  4  8  8  5  7  8  ...  4  2  8  2  2  2  2  8         NaN\n",
              "9         4        1  1  1  2  1  5  5  6  ...  1  1  7  1  6  6  6  6         NaN\n",
              "10        4        2  7  7  7  7  6  6  7  ...  1  5  5  5  5  5  2  6         NaN\n",
              "11        4        3  7  3  7  3  6  6  7  ...  6  1  4  4  4  6  6  6         NaN\n",
              "12        5        1  5  4  4  4  6  4  5  ...  3  1  4  4  1  3  3  5         NaN\n",
              "13        5        2  3  6  3  5  2  6  7  ...  6  6  6  6  6  6  6  4         NaN\n",
              "14        5        3  4  3  3  6  5  7  6  ...  7  2  2  7  5  6  4  2         NaN\n",
              "15        6        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "16        6        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "17        6        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "18        7        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "19        7        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "20        7        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "21        8        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "22        8        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "23        8        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "24        9        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "25        9        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "26        9        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "27       10        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "28       10        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "29       10        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "30       11        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "31       11        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "32       11        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "33       12        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "34       12        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "35       12        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "36       13        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "37       13        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "38       13        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "39       14        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "40       14        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "41       14        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "42       15        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "43       15        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "44       15        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "\n",
              "[45 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z0wcr59rO7A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d6f933c1-45b2-43e5-fdb0-1321f3f26b31"
      },
      "source": [
        "for i in range(6, 7): # Итерација низ секој испитен примерок\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_data = np.empty(0)\n",
        "  temp_labels = np.empty(0)\n",
        "  temp_events = np.empty(0)\n",
        "  temp_targets = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainData.mat\"\n",
        "    temp = loadmat(full_path)['trainData']\n",
        "    if temp_data.size != 0:\n",
        "      temp_data = np.concatenate((temp_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на label-ите\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainLabels.txt\"\n",
        "    with open(full_path, \"r\") as file_labels:\n",
        "      temp = file_labels.read().splitlines()\n",
        "      temp = np.repeat(temp, 8*10)\n",
        "      if temp_labels.size != 0:\n",
        "        temp_labels = np.concatenate((temp_labels, temp))\n",
        "      else:\n",
        "        temp_labels = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_events.size != 0:\n",
        "        temp_events = np.append(temp_events, temp)\n",
        "      else:\n",
        "        temp_events = np.array(temp)\n",
        "      \n",
        "\n",
        "    # Вчитување на редоследот на објекти кои се target\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainTargets.txt\"\n",
        "    with open(full_path, \"r\") as file_targets:\n",
        "      temp = file_targets.read().splitlines()\n",
        "      if temp_targets.size != 0:\n",
        "        temp_targets = np.concatenate((temp_targets, temp))\n",
        "      else:\n",
        "        temp_targets = np.array(temp)\n",
        "    print(\"\\t - Податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "\n",
        "  for j in range(4, 8): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "      \n",
        "  # Зачувај ги податоците вчитани од испитниот примерок во низа\n",
        "  data.append(temp_data)\n",
        "  labels.append(temp_labels)\n",
        "  events.append(temp_events)\n",
        "  targets.append(temp_targets)\n",
        "\n",
        "  \n",
        "  print(\"Податоците од испитниот примерок \" + str(i) + \" се вчитани.\")\n",
        "\n",
        "\n",
        "  #data = target_events_data_scaled\n",
        "  mne_array = np.swapaxes(data[i-1], 0, 2) # (епохa, канал, настан). \n",
        "  mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "  mne_array = mne_array.reshape(mne_array.shape[0],1, 8,350)\n",
        "  print(mne_array.shape)\n",
        "\n",
        "  events_arr = events[i-1].astype(np.int)\n",
        "  labels_arr = labels[i-1].astype(np.int)\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(mne_array, labels_arr-1, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "  model6 = DeepConvNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "  model6.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = Adam(0.0009))\n",
        "  checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                                verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "  #clf = RandomForestClassifier(max_depth=5)\n",
        "  #clf.fit(X_train, y_train)\n",
        "  #score = clf.score(X_test, y_test)\n",
        "  # print(score)\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  num_batch_size=100\n",
        "  num_epochs=400\n",
        "  model6.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "            validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "  score = model6.evaluate(X_test, y_test, verbose=1)\n",
        "  print(score)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t - Податоците од сесија 1 се вчитани.\n",
            "\t - Податоците од сесија 2 се вчитани.\n",
            "\t - Податоците од сесија 3 се вчитани.\n",
            "Податоците од испитниот примерок 6 се вчитани.\n",
            "(4800, 1, 8, 350)\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Train on 3600 samples, validate on 1200 samples\n",
            "Epoch 1/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 2.3842 - acc: 0.1406\n",
            "Epoch 00001: val_loss improved from inf to 2.12568, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 5s 1ms/sample - loss: 2.3824 - acc: 0.1422 - val_loss: 2.1257 - val_acc: 0.1825\n",
            "Epoch 2/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.2966 - acc: 0.1642\n",
            "Epoch 00002: val_loss did not improve from 2.12568\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 2.2969 - acc: 0.1636 - val_loss: 2.2256 - val_acc: 0.1467\n",
            "Epoch 3/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.2152 - acc: 0.1764\n",
            "Epoch 00003: val_loss did not improve from 2.12568\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 2.2129 - acc: 0.1803 - val_loss: 2.2240 - val_acc: 0.1158\n",
            "Epoch 4/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 2.1521 - acc: 0.1959\n",
            "Epoch 00004: val_loss did not improve from 2.12568\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 2.1547 - acc: 0.1908 - val_loss: 2.1654 - val_acc: 0.1650\n",
            "Epoch 5/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.0859 - acc: 0.2132\n",
            "Epoch 00005: val_loss did not improve from 2.12568\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 2.0875 - acc: 0.2131 - val_loss: 2.4887 - val_acc: 0.1575\n",
            "Epoch 6/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.9998 - acc: 0.2489\n",
            "Epoch 00006: val_loss did not improve from 2.12568\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 2.0034 - acc: 0.2483 - val_loss: 2.2490 - val_acc: 0.1483\n",
            "Epoch 7/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.9732 - acc: 0.2549\n",
            "Epoch 00007: val_loss improved from 2.12568 to 2.07194, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 1.9730 - acc: 0.2544 - val_loss: 2.0719 - val_acc: 0.1875\n",
            "Epoch 8/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.9326 - acc: 0.2631\n",
            "Epoch 00008: val_loss did not improve from 2.07194\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.9419 - acc: 0.2617 - val_loss: 2.0951 - val_acc: 0.2350\n",
            "Epoch 9/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.9395 - acc: 0.2642\n",
            "Epoch 00009: val_loss did not improve from 2.07194\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.9439 - acc: 0.2619 - val_loss: 2.1521 - val_acc: 0.2200\n",
            "Epoch 10/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.8808 - acc: 0.2766\n",
            "Epoch 00010: val_loss did not improve from 2.07194\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.8780 - acc: 0.2783 - val_loss: 2.0889 - val_acc: 0.2333\n",
            "Epoch 11/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.8579 - acc: 0.3012\n",
            "Epoch 00011: val_loss did not improve from 2.07194\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.8636 - acc: 0.2981 - val_loss: 2.2293 - val_acc: 0.2333\n",
            "Epoch 12/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.8239 - acc: 0.3027\n",
            "Epoch 00012: val_loss did not improve from 2.07194\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 1.8331 - acc: 0.2997 - val_loss: 2.2909 - val_acc: 0.2175\n",
            "Epoch 13/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.7910 - acc: 0.3283\n",
            "Epoch 00013: val_loss did not improve from 2.07194\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.7944 - acc: 0.3264 - val_loss: 2.1822 - val_acc: 0.2217\n",
            "Epoch 14/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.7565 - acc: 0.3428\n",
            "Epoch 00014: val_loss improved from 2.07194 to 1.95656, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 1.7607 - acc: 0.3367 - val_loss: 1.9566 - val_acc: 0.2392\n",
            "Epoch 15/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.7346 - acc: 0.3391\n",
            "Epoch 00015: val_loss did not improve from 1.95656\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.7399 - acc: 0.3386 - val_loss: 1.9653 - val_acc: 0.2692\n",
            "Epoch 16/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.7223 - acc: 0.3450\n",
            "Epoch 00016: val_loss did not improve from 1.95656\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.7215 - acc: 0.3456 - val_loss: 2.0986 - val_acc: 0.2450\n",
            "Epoch 17/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.6790 - acc: 0.3626\n",
            "Epoch 00017: val_loss did not improve from 1.95656\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.6811 - acc: 0.3622 - val_loss: 2.1036 - val_acc: 0.2467\n",
            "Epoch 18/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.6568 - acc: 0.3666\n",
            "Epoch 00018: val_loss improved from 1.95656 to 1.94208, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 1.6610 - acc: 0.3644 - val_loss: 1.9421 - val_acc: 0.2692\n",
            "Epoch 19/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.6401 - acc: 0.3897\n",
            "Epoch 00019: val_loss did not improve from 1.94208\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.6390 - acc: 0.3861 - val_loss: 2.0024 - val_acc: 0.2625\n",
            "Epoch 20/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.6074 - acc: 0.4009\n",
            "Epoch 00020: val_loss improved from 1.94208 to 1.85511, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 1.6065 - acc: 0.4014 - val_loss: 1.8551 - val_acc: 0.3017\n",
            "Epoch 21/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.5601 - acc: 0.4044\n",
            "Epoch 00021: val_loss did not improve from 1.85511\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 1.5772 - acc: 0.4028 - val_loss: 1.9010 - val_acc: 0.3158\n",
            "Epoch 22/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.5283 - acc: 0.4348\n",
            "Epoch 00022: val_loss did not improve from 1.85511\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.5348 - acc: 0.4297 - val_loss: 1.8990 - val_acc: 0.3142\n",
            "Epoch 23/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4916 - acc: 0.4388\n",
            "Epoch 00023: val_loss improved from 1.85511 to 1.76664, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 1.5022 - acc: 0.4358 - val_loss: 1.7666 - val_acc: 0.3233\n",
            "Epoch 24/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.4883 - acc: 0.4443\n",
            "Epoch 00024: val_loss did not improve from 1.76664\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.4844 - acc: 0.4456 - val_loss: 1.7954 - val_acc: 0.3208\n",
            "Epoch 25/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4498 - acc: 0.4562\n",
            "Epoch 00025: val_loss did not improve from 1.76664\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.4510 - acc: 0.4542 - val_loss: 1.8506 - val_acc: 0.3300\n",
            "Epoch 26/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4435 - acc: 0.4671\n",
            "Epoch 00026: val_loss did not improve from 1.76664\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 1.4453 - acc: 0.4642 - val_loss: 1.7710 - val_acc: 0.3500\n",
            "Epoch 27/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4164 - acc: 0.4759\n",
            "Epoch 00027: val_loss did not improve from 1.76664\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.4260 - acc: 0.4708 - val_loss: 1.8063 - val_acc: 0.3350\n",
            "Epoch 28/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.3996 - acc: 0.4806\n",
            "Epoch 00028: val_loss improved from 1.76664 to 1.74416, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 1.4023 - acc: 0.4808 - val_loss: 1.7442 - val_acc: 0.3325\n",
            "Epoch 29/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.3881 - acc: 0.4779\n",
            "Epoch 00029: val_loss did not improve from 1.74416\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.3906 - acc: 0.4781 - val_loss: 1.7456 - val_acc: 0.3583\n",
            "Epoch 30/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.3276 - acc: 0.5068\n",
            "Epoch 00030: val_loss improved from 1.74416 to 1.61066, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 1.3400 - acc: 0.5014 - val_loss: 1.6107 - val_acc: 0.3933\n",
            "Epoch 31/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.3236 - acc: 0.5082\n",
            "Epoch 00031: val_loss did not improve from 1.61066\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.3281 - acc: 0.5058 - val_loss: 1.6796 - val_acc: 0.3692\n",
            "Epoch 32/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2991 - acc: 0.5238\n",
            "Epoch 00032: val_loss improved from 1.61066 to 1.59388, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 1.2997 - acc: 0.5242 - val_loss: 1.5939 - val_acc: 0.3833\n",
            "Epoch 33/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.2574 - acc: 0.5382\n",
            "Epoch 00033: val_loss did not improve from 1.59388\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 1.2658 - acc: 0.5356 - val_loss: 1.6776 - val_acc: 0.3708\n",
            "Epoch 34/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.2291 - acc: 0.5480\n",
            "Epoch 00034: val_loss improved from 1.59388 to 1.59373, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 1.2315 - acc: 0.5481 - val_loss: 1.5937 - val_acc: 0.4083\n",
            "Epoch 35/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.2248 - acc: 0.5571\n",
            "Epoch 00035: val_loss improved from 1.59373 to 1.57629, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 1.2235 - acc: 0.5589 - val_loss: 1.5763 - val_acc: 0.3958\n",
            "Epoch 36/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2252 - acc: 0.5426\n",
            "Epoch 00036: val_loss improved from 1.57629 to 1.53543, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 1.2287 - acc: 0.5433 - val_loss: 1.5354 - val_acc: 0.4092\n",
            "Epoch 37/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.1734 - acc: 0.5719\n",
            "Epoch 00037: val_loss improved from 1.53543 to 1.50241, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 1.1844 - acc: 0.5686 - val_loss: 1.5024 - val_acc: 0.4433\n",
            "Epoch 38/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1397 - acc: 0.5845\n",
            "Epoch 00038: val_loss did not improve from 1.50241\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.1431 - acc: 0.5850 - val_loss: 1.5797 - val_acc: 0.4100\n",
            "Epoch 39/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.1044 - acc: 0.6022\n",
            "Epoch 00039: val_loss improved from 1.50241 to 1.45497, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 1.1102 - acc: 0.6008 - val_loss: 1.4550 - val_acc: 0.4642\n",
            "Epoch 40/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1259 - acc: 0.5961\n",
            "Epoch 00040: val_loss improved from 1.45497 to 1.44429, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 1.1251 - acc: 0.5942 - val_loss: 1.4443 - val_acc: 0.4717\n",
            "Epoch 41/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.1017 - acc: 0.6071\n",
            "Epoch 00041: val_loss improved from 1.44429 to 1.40704, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 1.1083 - acc: 0.6036 - val_loss: 1.4070 - val_acc: 0.4758\n",
            "Epoch 42/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.0535 - acc: 0.6240\n",
            "Epoch 00042: val_loss improved from 1.40704 to 1.38452, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 1.0510 - acc: 0.6250 - val_loss: 1.3845 - val_acc: 0.4800\n",
            "Epoch 43/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.0343 - acc: 0.6409\n",
            "Epoch 00043: val_loss improved from 1.38452 to 1.32434, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 1.0393 - acc: 0.6375 - val_loss: 1.3243 - val_acc: 0.5158\n",
            "Epoch 44/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0224 - acc: 0.6406\n",
            "Epoch 00044: val_loss improved from 1.32434 to 1.30020, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 1.0232 - acc: 0.6408 - val_loss: 1.3002 - val_acc: 0.5258\n",
            "Epoch 45/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0053 - acc: 0.6403\n",
            "Epoch 00045: val_loss improved from 1.30020 to 1.24962, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 1.0084 - acc: 0.6386 - val_loss: 1.2496 - val_acc: 0.5358\n",
            "Epoch 46/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9427 - acc: 0.6694\n",
            "Epoch 00046: val_loss improved from 1.24962 to 1.24534, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.9506 - acc: 0.6686 - val_loss: 1.2453 - val_acc: 0.5325\n",
            "Epoch 47/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9656 - acc: 0.6520\n",
            "Epoch 00047: val_loss did not improve from 1.24534\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.9683 - acc: 0.6500 - val_loss: 1.2475 - val_acc: 0.5350\n",
            "Epoch 48/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9456 - acc: 0.6643\n",
            "Epoch 00048: val_loss improved from 1.24534 to 1.19525, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.9427 - acc: 0.6667 - val_loss: 1.1953 - val_acc: 0.5658\n",
            "Epoch 49/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8837 - acc: 0.6997\n",
            "Epoch 00049: val_loss improved from 1.19525 to 1.19373, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.8869 - acc: 0.6992 - val_loss: 1.1937 - val_acc: 0.5683\n",
            "Epoch 50/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8677 - acc: 0.7012\n",
            "Epoch 00050: val_loss improved from 1.19373 to 1.13361, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.8733 - acc: 0.6997 - val_loss: 1.1336 - val_acc: 0.6033\n",
            "Epoch 51/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8674 - acc: 0.6979\n",
            "Epoch 00051: val_loss improved from 1.13361 to 1.09480, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.8666 - acc: 0.6969 - val_loss: 1.0948 - val_acc: 0.5992\n",
            "Epoch 52/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8373 - acc: 0.7067\n",
            "Epoch 00052: val_loss did not improve from 1.09480\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.8378 - acc: 0.7069 - val_loss: 1.1525 - val_acc: 0.5742\n",
            "Epoch 53/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8349 - acc: 0.7141\n",
            "Epoch 00053: val_loss improved from 1.09480 to 1.08305, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.8311 - acc: 0.7164 - val_loss: 1.0830 - val_acc: 0.6158\n",
            "Epoch 54/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7713 - acc: 0.7344\n",
            "Epoch 00054: val_loss improved from 1.08305 to 1.04070, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.7873 - acc: 0.7278 - val_loss: 1.0407 - val_acc: 0.6300\n",
            "Epoch 55/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7855 - acc: 0.7357\n",
            "Epoch 00055: val_loss did not improve from 1.04070\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.7873 - acc: 0.7353 - val_loss: 1.0632 - val_acc: 0.6142\n",
            "Epoch 56/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7751 - acc: 0.7286\n",
            "Epoch 00056: val_loss improved from 1.04070 to 0.96490, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.7744 - acc: 0.7294 - val_loss: 0.9649 - val_acc: 0.6550\n",
            "Epoch 57/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7424 - acc: 0.7450\n",
            "Epoch 00057: val_loss did not improve from 0.96490\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.7476 - acc: 0.7422 - val_loss: 1.0631 - val_acc: 0.6042\n",
            "Epoch 58/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7281 - acc: 0.7515\n",
            "Epoch 00058: val_loss improved from 0.96490 to 0.95043, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.7361 - acc: 0.7472 - val_loss: 0.9504 - val_acc: 0.6733\n",
            "Epoch 59/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7437 - acc: 0.7471\n",
            "Epoch 00059: val_loss improved from 0.95043 to 0.92095, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.7479 - acc: 0.7461 - val_loss: 0.9210 - val_acc: 0.6992\n",
            "Epoch 60/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6888 - acc: 0.7686\n",
            "Epoch 00060: val_loss did not improve from 0.92095\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.6899 - acc: 0.7669 - val_loss: 0.9285 - val_acc: 0.6908\n",
            "Epoch 61/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6727 - acc: 0.7760\n",
            "Epoch 00061: val_loss improved from 0.92095 to 0.85527, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.6768 - acc: 0.7731 - val_loss: 0.8553 - val_acc: 0.7167\n",
            "Epoch 62/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6756 - acc: 0.7829\n",
            "Epoch 00062: val_loss did not improve from 0.85527\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.6760 - acc: 0.7811 - val_loss: 0.8918 - val_acc: 0.7100\n",
            "Epoch 63/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6761 - acc: 0.7666\n",
            "Epoch 00063: val_loss improved from 0.85527 to 0.85285, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.6782 - acc: 0.7658 - val_loss: 0.8528 - val_acc: 0.6942\n",
            "Epoch 64/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6353 - acc: 0.7894\n",
            "Epoch 00064: val_loss did not improve from 0.85285\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.6421 - acc: 0.7858 - val_loss: 0.8635 - val_acc: 0.6950\n",
            "Epoch 65/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6289 - acc: 0.7846\n",
            "Epoch 00065: val_loss improved from 0.85285 to 0.83926, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.6287 - acc: 0.7856 - val_loss: 0.8393 - val_acc: 0.7225\n",
            "Epoch 66/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6015 - acc: 0.8024\n",
            "Epoch 00066: val_loss improved from 0.83926 to 0.75164, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.6041 - acc: 0.8019 - val_loss: 0.7516 - val_acc: 0.7608\n",
            "Epoch 67/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5892 - acc: 0.8121\n",
            "Epoch 00067: val_loss did not improve from 0.75164\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.5891 - acc: 0.8136 - val_loss: 0.7605 - val_acc: 0.7392\n",
            "Epoch 68/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5862 - acc: 0.8012\n",
            "Epoch 00068: val_loss did not improve from 0.75164\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.5878 - acc: 0.8011 - val_loss: 0.7642 - val_acc: 0.7292\n",
            "Epoch 69/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.5760 - acc: 0.8103\n",
            "Epoch 00069: val_loss did not improve from 0.75164\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.5766 - acc: 0.8083 - val_loss: 0.7534 - val_acc: 0.7542\n",
            "Epoch 70/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5525 - acc: 0.8194\n",
            "Epoch 00070: val_loss improved from 0.75164 to 0.70106, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.5550 - acc: 0.8192 - val_loss: 0.7011 - val_acc: 0.7783\n",
            "Epoch 71/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5510 - acc: 0.8203\n",
            "Epoch 00071: val_loss did not improve from 0.70106\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.5530 - acc: 0.8189 - val_loss: 0.7370 - val_acc: 0.7625\n",
            "Epoch 72/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5274 - acc: 0.8315\n",
            "Epoch 00072: val_loss improved from 0.70106 to 0.68202, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.5311 - acc: 0.8294 - val_loss: 0.6820 - val_acc: 0.7842\n",
            "Epoch 73/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5237 - acc: 0.8334\n",
            "Epoch 00073: val_loss did not improve from 0.68202\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.5256 - acc: 0.8325 - val_loss: 0.7052 - val_acc: 0.7633\n",
            "Epoch 74/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4949 - acc: 0.8509\n",
            "Epoch 00074: val_loss improved from 0.68202 to 0.67351, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.4986 - acc: 0.8478 - val_loss: 0.6735 - val_acc: 0.7758\n",
            "Epoch 75/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4778 - acc: 0.8515\n",
            "Epoch 00075: val_loss improved from 0.67351 to 0.66427, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.4787 - acc: 0.8528 - val_loss: 0.6643 - val_acc: 0.7842\n",
            "Epoch 76/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4825 - acc: 0.8445\n",
            "Epoch 00076: val_loss did not improve from 0.66427\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.4910 - acc: 0.8403 - val_loss: 0.6669 - val_acc: 0.7642\n",
            "Epoch 77/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4768 - acc: 0.8456\n",
            "Epoch 00077: val_loss improved from 0.66427 to 0.60881, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.4752 - acc: 0.8456 - val_loss: 0.6088 - val_acc: 0.8158\n",
            "Epoch 78/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4610 - acc: 0.8618\n",
            "Epoch 00078: val_loss improved from 0.60881 to 0.58598, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.4653 - acc: 0.8594 - val_loss: 0.5860 - val_acc: 0.8117\n",
            "Epoch 79/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4594 - acc: 0.8618\n",
            "Epoch 00079: val_loss did not improve from 0.58598\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.4613 - acc: 0.8597 - val_loss: 0.6070 - val_acc: 0.8092\n",
            "Epoch 80/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4508 - acc: 0.8532\n",
            "Epoch 00080: val_loss did not improve from 0.58598\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.4548 - acc: 0.8497 - val_loss: 0.5874 - val_acc: 0.8033\n",
            "Epoch 81/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4412 - acc: 0.8600\n",
            "Epoch 00081: val_loss improved from 0.58598 to 0.56585, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.4370 - acc: 0.8611 - val_loss: 0.5658 - val_acc: 0.8217\n",
            "Epoch 82/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4216 - acc: 0.8709\n",
            "Epoch 00082: val_loss improved from 0.56585 to 0.51317, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.4245 - acc: 0.8692 - val_loss: 0.5132 - val_acc: 0.8458\n",
            "Epoch 83/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.4233 - acc: 0.8681\n",
            "Epoch 00083: val_loss did not improve from 0.51317\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.4303 - acc: 0.8647 - val_loss: 0.5298 - val_acc: 0.8192\n",
            "Epoch 84/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.4440 - acc: 0.8541\n",
            "Epoch 00084: val_loss did not improve from 0.51317\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.4479 - acc: 0.8539 - val_loss: 0.5271 - val_acc: 0.8492\n",
            "Epoch 85/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4029 - acc: 0.8712\n",
            "Epoch 00085: val_loss improved from 0.51317 to 0.51271, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.4050 - acc: 0.8708 - val_loss: 0.5127 - val_acc: 0.8450\n",
            "Epoch 86/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4239 - acc: 0.8600\n",
            "Epoch 00086: val_loss improved from 0.51271 to 0.49476, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.4233 - acc: 0.8608 - val_loss: 0.4948 - val_acc: 0.8575\n",
            "Epoch 87/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3868 - acc: 0.8789\n",
            "Epoch 00087: val_loss did not improve from 0.49476\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.3884 - acc: 0.8781 - val_loss: 0.5067 - val_acc: 0.8350\n",
            "Epoch 88/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3902 - acc: 0.8839\n",
            "Epoch 00088: val_loss did not improve from 0.49476\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.3963 - acc: 0.8817 - val_loss: 0.5302 - val_acc: 0.8233\n",
            "Epoch 89/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3910 - acc: 0.8803\n",
            "Epoch 00089: val_loss improved from 0.49476 to 0.44218, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.3907 - acc: 0.8794 - val_loss: 0.4422 - val_acc: 0.8658\n",
            "Epoch 90/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3943 - acc: 0.8759\n",
            "Epoch 00090: val_loss did not improve from 0.44218\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.3918 - acc: 0.8778 - val_loss: 0.4594 - val_acc: 0.8667\n",
            "Epoch 91/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3868 - acc: 0.8785\n",
            "Epoch 00091: val_loss did not improve from 0.44218\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.3889 - acc: 0.8792 - val_loss: 0.4615 - val_acc: 0.8608\n",
            "Epoch 92/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3740 - acc: 0.8830\n",
            "Epoch 00092: val_loss did not improve from 0.44218\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.3776 - acc: 0.8819 - val_loss: 0.5008 - val_acc: 0.8342\n",
            "Epoch 93/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3712 - acc: 0.8894\n",
            "Epoch 00093: val_loss improved from 0.44218 to 0.43967, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.3786 - acc: 0.8861 - val_loss: 0.4397 - val_acc: 0.8692\n",
            "Epoch 94/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3622 - acc: 0.8874\n",
            "Epoch 00094: val_loss did not improve from 0.43967\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.3616 - acc: 0.8867 - val_loss: 0.4736 - val_acc: 0.8458\n",
            "Epoch 95/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3532 - acc: 0.8903\n",
            "Epoch 00095: val_loss improved from 0.43967 to 0.43696, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.3525 - acc: 0.8931 - val_loss: 0.4370 - val_acc: 0.8658\n",
            "Epoch 96/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3425 - acc: 0.8965\n",
            "Epoch 00096: val_loss did not improve from 0.43696\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.3457 - acc: 0.8956 - val_loss: 0.4467 - val_acc: 0.8667\n",
            "Epoch 97/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3345 - acc: 0.9038\n",
            "Epoch 00097: val_loss did not improve from 0.43696\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.3417 - acc: 0.9003 - val_loss: 0.4529 - val_acc: 0.8575\n",
            "Epoch 98/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3358 - acc: 0.8960\n",
            "Epoch 00098: val_loss improved from 0.43696 to 0.42155, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.3365 - acc: 0.8958 - val_loss: 0.4216 - val_acc: 0.8783\n",
            "Epoch 99/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3473 - acc: 0.8897\n",
            "Epoch 00099: val_loss did not improve from 0.42155\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.3485 - acc: 0.8894 - val_loss: 0.4651 - val_acc: 0.8483\n",
            "Epoch 100/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3273 - acc: 0.9057\n",
            "Epoch 00100: val_loss did not improve from 0.42155\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.3267 - acc: 0.9061 - val_loss: 0.4405 - val_acc: 0.8558\n",
            "Epoch 101/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3169 - acc: 0.9041\n",
            "Epoch 00101: val_loss improved from 0.42155 to 0.42131, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.3174 - acc: 0.9053 - val_loss: 0.4213 - val_acc: 0.8575\n",
            "Epoch 102/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3323 - acc: 0.8994\n",
            "Epoch 00102: val_loss improved from 0.42131 to 0.39994, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.3327 - acc: 0.8992 - val_loss: 0.3999 - val_acc: 0.8892\n",
            "Epoch 103/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3394 - acc: 0.8919\n",
            "Epoch 00103: val_loss improved from 0.39994 to 0.37627, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.3417 - acc: 0.8914 - val_loss: 0.3763 - val_acc: 0.8875\n",
            "Epoch 104/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3067 - acc: 0.9094\n",
            "Epoch 00104: val_loss did not improve from 0.37627\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.3072 - acc: 0.9094 - val_loss: 0.4308 - val_acc: 0.8642\n",
            "Epoch 105/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3096 - acc: 0.9031\n",
            "Epoch 00105: val_loss did not improve from 0.37627\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.3105 - acc: 0.9033 - val_loss: 0.4087 - val_acc: 0.8733\n",
            "Epoch 106/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3250 - acc: 0.9000\n",
            "Epoch 00106: val_loss did not improve from 0.37627\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.3240 - acc: 0.9011 - val_loss: 0.3801 - val_acc: 0.8883\n",
            "Epoch 107/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9097\n",
            "Epoch 00107: val_loss improved from 0.37627 to 0.37013, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.2997 - acc: 0.9081 - val_loss: 0.3701 - val_acc: 0.8875\n",
            "Epoch 108/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2953 - acc: 0.9109\n",
            "Epoch 00108: val_loss did not improve from 0.37013\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2928 - acc: 0.9111 - val_loss: 0.3719 - val_acc: 0.8967\n",
            "Epoch 109/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3095 - acc: 0.8994\n",
            "Epoch 00109: val_loss improved from 0.37013 to 0.33737, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.3108 - acc: 0.8989 - val_loss: 0.3374 - val_acc: 0.9092\n",
            "Epoch 110/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2836 - acc: 0.9183\n",
            "Epoch 00110: val_loss improved from 0.33737 to 0.33364, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.2831 - acc: 0.9189 - val_loss: 0.3336 - val_acc: 0.9042\n",
            "Epoch 111/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2932 - acc: 0.9072\n",
            "Epoch 00111: val_loss improved from 0.33364 to 0.32281, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.2937 - acc: 0.9078 - val_loss: 0.3228 - val_acc: 0.9075\n",
            "Epoch 112/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2720 - acc: 0.9206\n",
            "Epoch 00112: val_loss did not improve from 0.32281\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2714 - acc: 0.9206 - val_loss: 0.3361 - val_acc: 0.8983\n",
            "Epoch 113/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2689 - acc: 0.9216\n",
            "Epoch 00113: val_loss did not improve from 0.32281\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.2726 - acc: 0.9208 - val_loss: 0.3395 - val_acc: 0.9075\n",
            "Epoch 114/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2743 - acc: 0.9234\n",
            "Epoch 00114: val_loss improved from 0.32281 to 0.31153, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.2743 - acc: 0.9236 - val_loss: 0.3115 - val_acc: 0.9217\n",
            "Epoch 115/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2721 - acc: 0.9203\n",
            "Epoch 00115: val_loss did not improve from 0.31153\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2736 - acc: 0.9192 - val_loss: 0.3372 - val_acc: 0.9083\n",
            "Epoch 116/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2936 - acc: 0.9060\n",
            "Epoch 00116: val_loss did not improve from 0.31153\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2932 - acc: 0.9064 - val_loss: 0.3681 - val_acc: 0.8967\n",
            "Epoch 117/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2936 - acc: 0.9106\n",
            "Epoch 00117: val_loss improved from 0.31153 to 0.30980, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.2890 - acc: 0.9108 - val_loss: 0.3098 - val_acc: 0.9183\n",
            "Epoch 118/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2587 - acc: 0.9253\n",
            "Epoch 00118: val_loss did not improve from 0.30980\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2636 - acc: 0.9244 - val_loss: 0.3334 - val_acc: 0.9042\n",
            "Epoch 119/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2651 - acc: 0.9248\n",
            "Epoch 00119: val_loss did not improve from 0.30980\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2692 - acc: 0.9228 - val_loss: 0.3261 - val_acc: 0.9092\n",
            "Epoch 120/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9263\n",
            "Epoch 00120: val_loss did not improve from 0.30980\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2554 - acc: 0.9247 - val_loss: 0.3134 - val_acc: 0.9067\n",
            "Epoch 121/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2701 - acc: 0.9197\n",
            "Epoch 00121: val_loss improved from 0.30980 to 0.30174, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.2667 - acc: 0.9219 - val_loss: 0.3017 - val_acc: 0.9125\n",
            "Epoch 122/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2713 - acc: 0.9158\n",
            "Epoch 00122: val_loss did not improve from 0.30174\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2714 - acc: 0.9144 - val_loss: 0.3148 - val_acc: 0.9142\n",
            "Epoch 123/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2706 - acc: 0.9184\n",
            "Epoch 00123: val_loss did not improve from 0.30174\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2689 - acc: 0.9186 - val_loss: 0.3181 - val_acc: 0.9058\n",
            "Epoch 124/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2430 - acc: 0.9294\n",
            "Epoch 00124: val_loss did not improve from 0.30174\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2431 - acc: 0.9281 - val_loss: 0.3278 - val_acc: 0.8967\n",
            "Epoch 125/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2396 - acc: 0.9300\n",
            "Epoch 00125: val_loss did not improve from 0.30174\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2411 - acc: 0.9294 - val_loss: 0.3273 - val_acc: 0.8992\n",
            "Epoch 126/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2440 - acc: 0.9272\n",
            "Epoch 00126: val_loss did not improve from 0.30174\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.2467 - acc: 0.9269 - val_loss: 0.3411 - val_acc: 0.9008\n",
            "Epoch 127/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2518 - acc: 0.9239\n",
            "Epoch 00127: val_loss improved from 0.30174 to 0.27930, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.2544 - acc: 0.9222 - val_loss: 0.2793 - val_acc: 0.9233\n",
            "Epoch 128/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2707 - acc: 0.9136\n",
            "Epoch 00128: val_loss did not improve from 0.27930\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2756 - acc: 0.9122 - val_loss: 0.3583 - val_acc: 0.8883\n",
            "Epoch 129/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2735 - acc: 0.9111\n",
            "Epoch 00129: val_loss did not improve from 0.27930\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.2711 - acc: 0.9122 - val_loss: 0.3160 - val_acc: 0.9008\n",
            "Epoch 130/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2623 - acc: 0.9147\n",
            "Epoch 00130: val_loss did not improve from 0.27930\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2603 - acc: 0.9150 - val_loss: 0.3196 - val_acc: 0.9142\n",
            "Epoch 131/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2268 - acc: 0.9324\n",
            "Epoch 00131: val_loss did not improve from 0.27930\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2276 - acc: 0.9322 - val_loss: 0.2932 - val_acc: 0.9192\n",
            "Epoch 132/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2468 - acc: 0.9209\n",
            "Epoch 00132: val_loss improved from 0.27930 to 0.27044, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.2461 - acc: 0.9228 - val_loss: 0.2704 - val_acc: 0.9308\n",
            "Epoch 133/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2397 - acc: 0.9244\n",
            "Epoch 00133: val_loss did not improve from 0.27044\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2420 - acc: 0.9242 - val_loss: 0.2732 - val_acc: 0.9200\n",
            "Epoch 134/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9289\n",
            "Epoch 00134: val_loss improved from 0.27044 to 0.24821, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.2316 - acc: 0.9294 - val_loss: 0.2482 - val_acc: 0.9458\n",
            "Epoch 135/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9303\n",
            "Epoch 00135: val_loss did not improve from 0.24821\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2291 - acc: 0.9306 - val_loss: 0.2706 - val_acc: 0.9200\n",
            "Epoch 136/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9371\n",
            "Epoch 00136: val_loss did not improve from 0.24821\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2124 - acc: 0.9361 - val_loss: 0.2702 - val_acc: 0.9300\n",
            "Epoch 137/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2076 - acc: 0.9418\n",
            "Epoch 00137: val_loss did not improve from 0.24821\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2058 - acc: 0.9422 - val_loss: 0.2985 - val_acc: 0.9058\n",
            "Epoch 138/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2427 - acc: 0.9200\n",
            "Epoch 00138: val_loss improved from 0.24821 to 0.24700, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.2426 - acc: 0.9200 - val_loss: 0.2470 - val_acc: 0.9383\n",
            "Epoch 139/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2270 - acc: 0.9309\n",
            "Epoch 00139: val_loss did not improve from 0.24700\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2300 - acc: 0.9303 - val_loss: 0.2692 - val_acc: 0.9250\n",
            "Epoch 140/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2419 - acc: 0.9277\n",
            "Epoch 00140: val_loss did not improve from 0.24700\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2413 - acc: 0.9278 - val_loss: 0.2701 - val_acc: 0.9267\n",
            "Epoch 141/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9409\n",
            "Epoch 00141: val_loss did not improve from 0.24700\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2092 - acc: 0.9397 - val_loss: 0.2518 - val_acc: 0.9350\n",
            "Epoch 142/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2496 - acc: 0.9206\n",
            "Epoch 00142: val_loss did not improve from 0.24700\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2490 - acc: 0.9208 - val_loss: 0.2780 - val_acc: 0.9208\n",
            "Epoch 143/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9320\n",
            "Epoch 00143: val_loss did not improve from 0.24700\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2237 - acc: 0.9303 - val_loss: 0.2846 - val_acc: 0.9125\n",
            "Epoch 144/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9420\n",
            "Epoch 00144: val_loss improved from 0.24700 to 0.24400, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.2084 - acc: 0.9419 - val_loss: 0.2440 - val_acc: 0.9342\n",
            "Epoch 145/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2055 - acc: 0.9458\n",
            "Epoch 00145: val_loss did not improve from 0.24400\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2167 - acc: 0.9414 - val_loss: 0.2755 - val_acc: 0.9242\n",
            "Epoch 146/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2186 - acc: 0.9347\n",
            "Epoch 00146: val_loss improved from 0.24400 to 0.22879, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.2197 - acc: 0.9342 - val_loss: 0.2288 - val_acc: 0.9450\n",
            "Epoch 147/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2200 - acc: 0.9321\n",
            "Epoch 00147: val_loss did not improve from 0.22879\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2174 - acc: 0.9336 - val_loss: 0.2692 - val_acc: 0.9267\n",
            "Epoch 148/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2156 - acc: 0.9353\n",
            "Epoch 00148: val_loss did not improve from 0.22879\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2234 - acc: 0.9314 - val_loss: 0.2397 - val_acc: 0.9283\n",
            "Epoch 149/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2235 - acc: 0.9336\n",
            "Epoch 00149: val_loss did not improve from 0.22879\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2267 - acc: 0.9333 - val_loss: 0.2488 - val_acc: 0.9358\n",
            "Epoch 150/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2279 - acc: 0.9254\n",
            "Epoch 00150: val_loss improved from 0.22879 to 0.22226, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.2282 - acc: 0.9256 - val_loss: 0.2223 - val_acc: 0.9408\n",
            "Epoch 151/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2168 - acc: 0.9416\n",
            "Epoch 00151: val_loss did not improve from 0.22226\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.2120 - acc: 0.9433 - val_loss: 0.3020 - val_acc: 0.9075\n",
            "Epoch 152/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2155 - acc: 0.9359\n",
            "Epoch 00152: val_loss did not improve from 0.22226\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2182 - acc: 0.9353 - val_loss: 0.2869 - val_acc: 0.9108\n",
            "Epoch 153/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1986 - acc: 0.9442\n",
            "Epoch 00153: val_loss improved from 0.22226 to 0.20303, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.2039 - acc: 0.9417 - val_loss: 0.2030 - val_acc: 0.9508\n",
            "Epoch 154/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9420\n",
            "Epoch 00154: val_loss did not improve from 0.20303\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2039 - acc: 0.9414 - val_loss: 0.2581 - val_acc: 0.9200\n",
            "Epoch 155/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1907 - acc: 0.9432\n",
            "Epoch 00155: val_loss did not improve from 0.20303\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1938 - acc: 0.9417 - val_loss: 0.2304 - val_acc: 0.9433\n",
            "Epoch 156/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1891 - acc: 0.9450\n",
            "Epoch 00156: val_loss did not improve from 0.20303\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1940 - acc: 0.9433 - val_loss: 0.2604 - val_acc: 0.9283\n",
            "Epoch 157/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2062 - acc: 0.9426\n",
            "Epoch 00157: val_loss improved from 0.20303 to 0.20175, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.2094 - acc: 0.9414 - val_loss: 0.2017 - val_acc: 0.9567\n",
            "Epoch 158/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1918 - acc: 0.9500\n",
            "Epoch 00158: val_loss did not improve from 0.20175\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1933 - acc: 0.9494 - val_loss: 0.2200 - val_acc: 0.9467\n",
            "Epoch 159/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1882 - acc: 0.9439\n",
            "Epoch 00159: val_loss did not improve from 0.20175\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1915 - acc: 0.9422 - val_loss: 0.2300 - val_acc: 0.9358\n",
            "Epoch 160/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1994 - acc: 0.9397\n",
            "Epoch 00160: val_loss did not improve from 0.20175\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1988 - acc: 0.9403 - val_loss: 0.2149 - val_acc: 0.9450\n",
            "Epoch 161/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9471\n",
            "Epoch 00161: val_loss did not improve from 0.20175\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1892 - acc: 0.9464 - val_loss: 0.2247 - val_acc: 0.9325\n",
            "Epoch 162/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1897 - acc: 0.9439\n",
            "Epoch 00162: val_loss did not improve from 0.20175\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1932 - acc: 0.9433 - val_loss: 0.2104 - val_acc: 0.9475\n",
            "Epoch 163/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1861 - acc: 0.9429\n",
            "Epoch 00163: val_loss did not improve from 0.20175\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1837 - acc: 0.9442 - val_loss: 0.2425 - val_acc: 0.9375\n",
            "Epoch 164/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1997 - acc: 0.9391\n",
            "Epoch 00164: val_loss did not improve from 0.20175\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1982 - acc: 0.9408 - val_loss: 0.2083 - val_acc: 0.9542\n",
            "Epoch 165/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2002 - acc: 0.9418\n",
            "Epoch 00165: val_loss did not improve from 0.20175\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1986 - acc: 0.9422 - val_loss: 0.2289 - val_acc: 0.9433\n",
            "Epoch 166/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1947 - acc: 0.9400\n",
            "Epoch 00166: val_loss did not improve from 0.20175\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1976 - acc: 0.9392 - val_loss: 0.2554 - val_acc: 0.9267\n",
            "Epoch 167/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2004 - acc: 0.9416\n",
            "Epoch 00167: val_loss did not improve from 0.20175\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2050 - acc: 0.9397 - val_loss: 0.2281 - val_acc: 0.9258\n",
            "Epoch 168/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1879 - acc: 0.9494\n",
            "Epoch 00168: val_loss improved from 0.20175 to 0.19703, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1897 - acc: 0.9481 - val_loss: 0.1970 - val_acc: 0.9550\n",
            "Epoch 169/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9431\n",
            "Epoch 00169: val_loss improved from 0.19703 to 0.19344, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1857 - acc: 0.9428 - val_loss: 0.1934 - val_acc: 0.9508\n",
            "Epoch 170/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1862 - acc: 0.9456\n",
            "Epoch 00170: val_loss did not improve from 0.19344\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1910 - acc: 0.9433 - val_loss: 0.2349 - val_acc: 0.9317\n",
            "Epoch 171/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1824 - acc: 0.9500\n",
            "Epoch 00171: val_loss did not improve from 0.19344\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1834 - acc: 0.9492 - val_loss: 0.2469 - val_acc: 0.9258\n",
            "Epoch 172/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1839 - acc: 0.9436\n",
            "Epoch 00172: val_loss did not improve from 0.19344\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1862 - acc: 0.9439 - val_loss: 0.2177 - val_acc: 0.9475\n",
            "Epoch 173/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1906 - acc: 0.9476\n",
            "Epoch 00173: val_loss did not improve from 0.19344\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1939 - acc: 0.9456 - val_loss: 0.2013 - val_acc: 0.9475\n",
            "Epoch 174/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1989 - acc: 0.9385\n",
            "Epoch 00174: val_loss did not improve from 0.19344\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2009 - acc: 0.9364 - val_loss: 0.2448 - val_acc: 0.9383\n",
            "Epoch 175/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9434\n",
            "Epoch 00175: val_loss did not improve from 0.19344\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1944 - acc: 0.9433 - val_loss: 0.2342 - val_acc: 0.9333\n",
            "Epoch 176/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1862 - acc: 0.9444\n",
            "Epoch 00176: val_loss improved from 0.19344 to 0.17485, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1850 - acc: 0.9436 - val_loss: 0.1749 - val_acc: 0.9567\n",
            "Epoch 177/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1990 - acc: 0.9353\n",
            "Epoch 00177: val_loss did not improve from 0.17485\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1985 - acc: 0.9339 - val_loss: 0.1793 - val_acc: 0.9550\n",
            "Epoch 178/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2033 - acc: 0.9394\n",
            "Epoch 00178: val_loss did not improve from 0.17485\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2081 - acc: 0.9381 - val_loss: 0.1887 - val_acc: 0.9567\n",
            "Epoch 179/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1739 - acc: 0.9491\n",
            "Epoch 00179: val_loss did not improve from 0.17485\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1736 - acc: 0.9486 - val_loss: 0.1815 - val_acc: 0.9542\n",
            "Epoch 180/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1744 - acc: 0.9497\n",
            "Epoch 00180: val_loss did not improve from 0.17485\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1739 - acc: 0.9503 - val_loss: 0.2063 - val_acc: 0.9417\n",
            "Epoch 181/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9457\n",
            "Epoch 00181: val_loss did not improve from 0.17485\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1863 - acc: 0.9456 - val_loss: 0.1917 - val_acc: 0.9508\n",
            "Epoch 182/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9351\n",
            "Epoch 00182: val_loss did not improve from 0.17485\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2047 - acc: 0.9347 - val_loss: 0.1908 - val_acc: 0.9533\n",
            "Epoch 183/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1980 - acc: 0.9382\n",
            "Epoch 00183: val_loss did not improve from 0.17485\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1957 - acc: 0.9397 - val_loss: 0.2097 - val_acc: 0.9450\n",
            "Epoch 184/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2088 - acc: 0.9357\n",
            "Epoch 00184: val_loss did not improve from 0.17485\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2076 - acc: 0.9364 - val_loss: 0.1914 - val_acc: 0.9425\n",
            "Epoch 185/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1745 - acc: 0.9441\n",
            "Epoch 00185: val_loss improved from 0.17485 to 0.16993, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1751 - acc: 0.9447 - val_loss: 0.1699 - val_acc: 0.9567\n",
            "Epoch 186/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1514 - acc: 0.9615\n",
            "Epoch 00186: val_loss improved from 0.16993 to 0.16218, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1571 - acc: 0.9583 - val_loss: 0.1622 - val_acc: 0.9625\n",
            "Epoch 187/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9454\n",
            "Epoch 00187: val_loss did not improve from 0.16218\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1823 - acc: 0.9458 - val_loss: 0.1969 - val_acc: 0.9500\n",
            "Epoch 188/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1719 - acc: 0.9500\n",
            "Epoch 00188: val_loss did not improve from 0.16218\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1760 - acc: 0.9478 - val_loss: 0.1666 - val_acc: 0.9558\n",
            "Epoch 189/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1770 - acc: 0.9459\n",
            "Epoch 00189: val_loss did not improve from 0.16218\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1798 - acc: 0.9442 - val_loss: 0.2113 - val_acc: 0.9408\n",
            "Epoch 190/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1883 - acc: 0.9479\n",
            "Epoch 00190: val_loss did not improve from 0.16218\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1837 - acc: 0.9489 - val_loss: 0.1654 - val_acc: 0.9675\n",
            "Epoch 191/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1913 - acc: 0.9423\n",
            "Epoch 00191: val_loss did not improve from 0.16218\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1915 - acc: 0.9419 - val_loss: 0.1824 - val_acc: 0.9583\n",
            "Epoch 192/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1650 - acc: 0.9528\n",
            "Epoch 00192: val_loss did not improve from 0.16218\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1699 - acc: 0.9503 - val_loss: 0.2084 - val_acc: 0.9417\n",
            "Epoch 193/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1787 - acc: 0.9469\n",
            "Epoch 00193: val_loss improved from 0.16218 to 0.15830, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.1785 - acc: 0.9475 - val_loss: 0.1583 - val_acc: 0.9633\n",
            "Epoch 194/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1851 - acc: 0.9488\n",
            "Epoch 00194: val_loss did not improve from 0.15830\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1847 - acc: 0.9492 - val_loss: 0.2013 - val_acc: 0.9525\n",
            "Epoch 195/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1928 - acc: 0.9397\n",
            "Epoch 00195: val_loss did not improve from 0.15830\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1939 - acc: 0.9392 - val_loss: 0.1890 - val_acc: 0.9558\n",
            "Epoch 196/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1863 - acc: 0.9461\n",
            "Epoch 00196: val_loss did not improve from 0.15830\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1827 - acc: 0.9478 - val_loss: 0.1803 - val_acc: 0.9508\n",
            "Epoch 197/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1850 - acc: 0.9409\n",
            "Epoch 00197: val_loss did not improve from 0.15830\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1881 - acc: 0.9403 - val_loss: 0.1846 - val_acc: 0.9558\n",
            "Epoch 198/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1975 - acc: 0.9370\n",
            "Epoch 00198: val_loss did not improve from 0.15830\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1960 - acc: 0.9372 - val_loss: 0.2235 - val_acc: 0.9300\n",
            "Epoch 199/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1690 - acc: 0.9524\n",
            "Epoch 00199: val_loss did not improve from 0.15830\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1710 - acc: 0.9517 - val_loss: 0.2070 - val_acc: 0.9458\n",
            "Epoch 200/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1682 - acc: 0.9485\n",
            "Epoch 00200: val_loss did not improve from 0.15830\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1687 - acc: 0.9478 - val_loss: 0.1987 - val_acc: 0.9350\n",
            "Epoch 201/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9526\n",
            "Epoch 00201: val_loss did not improve from 0.15830\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1673 - acc: 0.9528 - val_loss: 0.1715 - val_acc: 0.9575\n",
            "Epoch 202/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1524 - acc: 0.9562\n",
            "Epoch 00202: val_loss did not improve from 0.15830\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1513 - acc: 0.9561 - val_loss: 0.1871 - val_acc: 0.9425\n",
            "Epoch 203/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9426\n",
            "Epoch 00203: val_loss did not improve from 0.15830\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1807 - acc: 0.9425 - val_loss: 0.1645 - val_acc: 0.9575\n",
            "Epoch 204/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1710 - acc: 0.9425\n",
            "Epoch 00204: val_loss did not improve from 0.15830\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1696 - acc: 0.9422 - val_loss: 0.1840 - val_acc: 0.9467\n",
            "Epoch 205/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1691 - acc: 0.9506\n",
            "Epoch 00205: val_loss did not improve from 0.15830\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1696 - acc: 0.9492 - val_loss: 0.1920 - val_acc: 0.9467\n",
            "Epoch 206/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1525 - acc: 0.9555\n",
            "Epoch 00206: val_loss did not improve from 0.15830\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1531 - acc: 0.9544 - val_loss: 0.1749 - val_acc: 0.9483\n",
            "Epoch 207/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1599 - acc: 0.9535\n",
            "Epoch 00207: val_loss did not improve from 0.15830\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1614 - acc: 0.9533 - val_loss: 0.1634 - val_acc: 0.9608\n",
            "Epoch 208/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1722 - acc: 0.9468\n",
            "Epoch 00208: val_loss did not improve from 0.15830\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1710 - acc: 0.9475 - val_loss: 0.1590 - val_acc: 0.9625\n",
            "Epoch 209/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1756 - acc: 0.9444\n",
            "Epoch 00209: val_loss improved from 0.15830 to 0.15320, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1749 - acc: 0.9458 - val_loss: 0.1532 - val_acc: 0.9642\n",
            "Epoch 210/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1709 - acc: 0.9479\n",
            "Epoch 00210: val_loss did not improve from 0.15320\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1684 - acc: 0.9494 - val_loss: 0.1791 - val_acc: 0.9467\n",
            "Epoch 211/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1536 - acc: 0.9542\n",
            "Epoch 00211: val_loss did not improve from 0.15320\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1538 - acc: 0.9539 - val_loss: 0.1631 - val_acc: 0.9533\n",
            "Epoch 212/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1693 - acc: 0.9470\n",
            "Epoch 00212: val_loss did not improve from 0.15320\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1658 - acc: 0.9494 - val_loss: 0.1539 - val_acc: 0.9625\n",
            "Epoch 213/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1526 - acc: 0.9555\n",
            "Epoch 00213: val_loss did not improve from 0.15320\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1606 - acc: 0.9508 - val_loss: 0.1593 - val_acc: 0.9633\n",
            "Epoch 214/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1677 - acc: 0.9524\n",
            "Epoch 00214: val_loss did not improve from 0.15320\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1672 - acc: 0.9519 - val_loss: 0.1914 - val_acc: 0.9583\n",
            "Epoch 215/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1650 - acc: 0.9503\n",
            "Epoch 00215: val_loss did not improve from 0.15320\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1650 - acc: 0.9506 - val_loss: 0.1604 - val_acc: 0.9575\n",
            "Epoch 216/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1648 - acc: 0.9497\n",
            "Epoch 00216: val_loss did not improve from 0.15320\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1626 - acc: 0.9508 - val_loss: 0.1815 - val_acc: 0.9492\n",
            "Epoch 217/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1503 - acc: 0.9576\n",
            "Epoch 00217: val_loss did not improve from 0.15320\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1510 - acc: 0.9578 - val_loss: 0.1814 - val_acc: 0.9483\n",
            "Epoch 218/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1585 - acc: 0.9561\n",
            "Epoch 00218: val_loss improved from 0.15320 to 0.15162, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1596 - acc: 0.9553 - val_loss: 0.1516 - val_acc: 0.9617\n",
            "Epoch 219/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9574\n",
            "Epoch 00219: val_loss did not improve from 0.15162\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1459 - acc: 0.9578 - val_loss: 0.1631 - val_acc: 0.9592\n",
            "Epoch 220/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1533 - acc: 0.9555\n",
            "Epoch 00220: val_loss improved from 0.15162 to 0.14707, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1546 - acc: 0.9539 - val_loss: 0.1471 - val_acc: 0.9625\n",
            "Epoch 221/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1551 - acc: 0.9552\n",
            "Epoch 00221: val_loss did not improve from 0.14707\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1548 - acc: 0.9547 - val_loss: 0.1516 - val_acc: 0.9567\n",
            "Epoch 222/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9571\n",
            "Epoch 00222: val_loss did not improve from 0.14707\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1557 - acc: 0.9564 - val_loss: 0.1476 - val_acc: 0.9658\n",
            "Epoch 223/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1434 - acc: 0.9579\n",
            "Epoch 00223: val_loss improved from 0.14707 to 0.13357, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1517 - acc: 0.9539 - val_loss: 0.1336 - val_acc: 0.9658\n",
            "Epoch 224/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1638 - acc: 0.9497\n",
            "Epoch 00224: val_loss did not improve from 0.13357\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1625 - acc: 0.9489 - val_loss: 0.1995 - val_acc: 0.9400\n",
            "Epoch 225/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1527 - acc: 0.9542\n",
            "Epoch 00225: val_loss did not improve from 0.13357\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1508 - acc: 0.9553 - val_loss: 0.1502 - val_acc: 0.9650\n",
            "Epoch 226/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1629 - acc: 0.9525\n",
            "Epoch 00226: val_loss improved from 0.13357 to 0.12263, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1631 - acc: 0.9519 - val_loss: 0.1226 - val_acc: 0.9733\n",
            "Epoch 227/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1603 - acc: 0.9544\n",
            "Epoch 00227: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1604 - acc: 0.9536 - val_loss: 0.1350 - val_acc: 0.9733\n",
            "Epoch 228/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1596 - acc: 0.9500\n",
            "Epoch 00228: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1602 - acc: 0.9497 - val_loss: 0.2282 - val_acc: 0.9300\n",
            "Epoch 229/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1581 - acc: 0.9509\n",
            "Epoch 00229: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1588 - acc: 0.9517 - val_loss: 0.1570 - val_acc: 0.9633\n",
            "Epoch 230/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1549 - acc: 0.9553\n",
            "Epoch 00230: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1556 - acc: 0.9539 - val_loss: 0.1416 - val_acc: 0.9642\n",
            "Epoch 231/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9477\n",
            "Epoch 00231: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1653 - acc: 0.9478 - val_loss: 0.1902 - val_acc: 0.9492\n",
            "Epoch 232/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1604 - acc: 0.9509\n",
            "Epoch 00232: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1628 - acc: 0.9506 - val_loss: 0.1464 - val_acc: 0.9633\n",
            "Epoch 233/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1660 - acc: 0.9491\n",
            "Epoch 00233: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1634 - acc: 0.9506 - val_loss: 0.1494 - val_acc: 0.9642\n",
            "Epoch 234/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1390 - acc: 0.9591\n",
            "Epoch 00234: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1442 - acc: 0.9578 - val_loss: 0.1356 - val_acc: 0.9675\n",
            "Epoch 235/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1525 - acc: 0.9515\n",
            "Epoch 00235: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1552 - acc: 0.9489 - val_loss: 0.1632 - val_acc: 0.9533\n",
            "Epoch 236/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9551\n",
            "Epoch 00236: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1489 - acc: 0.9539 - val_loss: 0.1472 - val_acc: 0.9625\n",
            "Epoch 237/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1720 - acc: 0.9453\n",
            "Epoch 00237: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1700 - acc: 0.9461 - val_loss: 0.1723 - val_acc: 0.9558\n",
            "Epoch 238/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1521 - acc: 0.9544\n",
            "Epoch 00238: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1514 - acc: 0.9550 - val_loss: 0.1614 - val_acc: 0.9650\n",
            "Epoch 239/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1441 - acc: 0.9564\n",
            "Epoch 00239: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1495 - acc: 0.9533 - val_loss: 0.1428 - val_acc: 0.9683\n",
            "Epoch 240/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1430 - acc: 0.9594\n",
            "Epoch 00240: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1432 - acc: 0.9592 - val_loss: 0.1381 - val_acc: 0.9692\n",
            "Epoch 241/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1321 - acc: 0.9629\n",
            "Epoch 00241: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1326 - acc: 0.9628 - val_loss: 0.1303 - val_acc: 0.9683\n",
            "Epoch 242/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1260 - acc: 0.9634\n",
            "Epoch 00242: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1297 - acc: 0.9628 - val_loss: 0.1424 - val_acc: 0.9633\n",
            "Epoch 243/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1471 - acc: 0.9564\n",
            "Epoch 00243: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1456 - acc: 0.9578 - val_loss: 0.1342 - val_acc: 0.9692\n",
            "Epoch 244/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9654\n",
            "Epoch 00244: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1344 - acc: 0.9642 - val_loss: 0.1732 - val_acc: 0.9508\n",
            "Epoch 245/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.9620\n",
            "Epoch 00245: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1355 - acc: 0.9619 - val_loss: 0.1259 - val_acc: 0.9717\n",
            "Epoch 246/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1346 - acc: 0.9606\n",
            "Epoch 00246: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1354 - acc: 0.9603 - val_loss: 0.1271 - val_acc: 0.9692\n",
            "Epoch 247/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1324 - acc: 0.9585\n",
            "Epoch 00247: val_loss did not improve from 0.12263\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1321 - acc: 0.9586 - val_loss: 0.1439 - val_acc: 0.9617\n",
            "Epoch 248/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1424 - acc: 0.9556\n",
            "Epoch 00248: val_loss improved from 0.12263 to 0.11658, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1447 - acc: 0.9553 - val_loss: 0.1166 - val_acc: 0.9683\n",
            "Epoch 249/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9617\n",
            "Epoch 00249: val_loss did not improve from 0.11658\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1412 - acc: 0.9619 - val_loss: 0.1491 - val_acc: 0.9592\n",
            "Epoch 250/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1368 - acc: 0.9636\n",
            "Epoch 00250: val_loss did not improve from 0.11658\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1367 - acc: 0.9631 - val_loss: 0.1448 - val_acc: 0.9642\n",
            "Epoch 251/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1436 - acc: 0.9600\n",
            "Epoch 00251: val_loss did not improve from 0.11658\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1482 - acc: 0.9578 - val_loss: 0.1518 - val_acc: 0.9617\n",
            "Epoch 252/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1448 - acc: 0.9612\n",
            "Epoch 00252: val_loss did not improve from 0.11658\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1462 - acc: 0.9603 - val_loss: 0.1630 - val_acc: 0.9567\n",
            "Epoch 253/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1531 - acc: 0.9518\n",
            "Epoch 00253: val_loss did not improve from 0.11658\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1547 - acc: 0.9511 - val_loss: 0.1335 - val_acc: 0.9700\n",
            "Epoch 254/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9520\n",
            "Epoch 00254: val_loss did not improve from 0.11658\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1557 - acc: 0.9519 - val_loss: 0.1455 - val_acc: 0.9625\n",
            "Epoch 255/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1371 - acc: 0.9626\n",
            "Epoch 00255: val_loss did not improve from 0.11658\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1407 - acc: 0.9608 - val_loss: 0.1455 - val_acc: 0.9583\n",
            "Epoch 256/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1474 - acc: 0.9606\n",
            "Epoch 00256: val_loss did not improve from 0.11658\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1486 - acc: 0.9592 - val_loss: 0.1308 - val_acc: 0.9642\n",
            "Epoch 257/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1371 - acc: 0.9579\n",
            "Epoch 00257: val_loss did not improve from 0.11658\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1407 - acc: 0.9569 - val_loss: 0.1603 - val_acc: 0.9558\n",
            "Epoch 258/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9549\n",
            "Epoch 00258: val_loss did not improve from 0.11658\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1484 - acc: 0.9539 - val_loss: 0.1305 - val_acc: 0.9642\n",
            "Epoch 259/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1416 - acc: 0.9576\n",
            "Epoch 00259: val_loss did not improve from 0.11658\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1420 - acc: 0.9564 - val_loss: 0.1474 - val_acc: 0.9642\n",
            "Epoch 260/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9611\n",
            "Epoch 00260: val_loss did not improve from 0.11658\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1346 - acc: 0.9611 - val_loss: 0.1512 - val_acc: 0.9567\n",
            "Epoch 261/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1520 - acc: 0.9509\n",
            "Epoch 00261: val_loss did not improve from 0.11658\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1497 - acc: 0.9522 - val_loss: 0.1349 - val_acc: 0.9700\n",
            "Epoch 262/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.9577\n",
            "Epoch 00262: val_loss improved from 0.11658 to 0.11630, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1458 - acc: 0.9581 - val_loss: 0.1163 - val_acc: 0.9733\n",
            "Epoch 263/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9537\n",
            "Epoch 00263: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1435 - acc: 0.9536 - val_loss: 0.1461 - val_acc: 0.9650\n",
            "Epoch 264/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1419 - acc: 0.9591\n",
            "Epoch 00264: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1432 - acc: 0.9583 - val_loss: 0.1479 - val_acc: 0.9675\n",
            "Epoch 265/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1440 - acc: 0.9591\n",
            "Epoch 00265: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1417 - acc: 0.9600 - val_loss: 0.1563 - val_acc: 0.9608\n",
            "Epoch 266/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1367 - acc: 0.9582\n",
            "Epoch 00266: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1406 - acc: 0.9572 - val_loss: 0.1323 - val_acc: 0.9692\n",
            "Epoch 267/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1513 - acc: 0.9528\n",
            "Epoch 00267: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1545 - acc: 0.9528 - val_loss: 0.1179 - val_acc: 0.9733\n",
            "Epoch 268/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9503\n",
            "Epoch 00268: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1597 - acc: 0.9497 - val_loss: 0.1479 - val_acc: 0.9525\n",
            "Epoch 269/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1708 - acc: 0.9438\n",
            "Epoch 00269: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1753 - acc: 0.9422 - val_loss: 0.1497 - val_acc: 0.9608\n",
            "Epoch 270/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1482 - acc: 0.9515\n",
            "Epoch 00270: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1471 - acc: 0.9528 - val_loss: 0.1445 - val_acc: 0.9650\n",
            "Epoch 271/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1415 - acc: 0.9571\n",
            "Epoch 00271: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1413 - acc: 0.9558 - val_loss: 0.1481 - val_acc: 0.9617\n",
            "Epoch 272/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1454 - acc: 0.9544\n",
            "Epoch 00272: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1462 - acc: 0.9547 - val_loss: 0.1583 - val_acc: 0.9617\n",
            "Epoch 273/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1314 - acc: 0.9638\n",
            "Epoch 00273: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1296 - acc: 0.9647 - val_loss: 0.1242 - val_acc: 0.9700\n",
            "Epoch 274/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1495 - acc: 0.9506\n",
            "Epoch 00274: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1520 - acc: 0.9492 - val_loss: 0.1958 - val_acc: 0.9433\n",
            "Epoch 275/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1335 - acc: 0.9633\n",
            "Epoch 00275: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1330 - acc: 0.9642 - val_loss: 0.1460 - val_acc: 0.9650\n",
            "Epoch 276/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9640\n",
            "Epoch 00276: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1247 - acc: 0.9636 - val_loss: 0.1289 - val_acc: 0.9683\n",
            "Epoch 277/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1270 - acc: 0.9644\n",
            "Epoch 00277: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1294 - acc: 0.9636 - val_loss: 0.1511 - val_acc: 0.9617\n",
            "Epoch 278/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1418 - acc: 0.9576\n",
            "Epoch 00278: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1393 - acc: 0.9583 - val_loss: 0.1430 - val_acc: 0.9625\n",
            "Epoch 279/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1273 - acc: 0.9641\n",
            "Epoch 00279: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1328 - acc: 0.9625 - val_loss: 0.1416 - val_acc: 0.9608\n",
            "Epoch 280/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1355 - acc: 0.9570\n",
            "Epoch 00280: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1335 - acc: 0.9589 - val_loss: 0.1262 - val_acc: 0.9658\n",
            "Epoch 281/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9620\n",
            "Epoch 00281: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1326 - acc: 0.9614 - val_loss: 0.1339 - val_acc: 0.9683\n",
            "Epoch 282/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1409 - acc: 0.9577\n",
            "Epoch 00282: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1416 - acc: 0.9572 - val_loss: 0.1417 - val_acc: 0.9642\n",
            "Epoch 283/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1515 - acc: 0.9521\n",
            "Epoch 00283: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1543 - acc: 0.9508 - val_loss: 0.1420 - val_acc: 0.9575\n",
            "Epoch 284/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1295 - acc: 0.9600\n",
            "Epoch 00284: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1308 - acc: 0.9597 - val_loss: 0.1172 - val_acc: 0.9700\n",
            "Epoch 285/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9614\n",
            "Epoch 00285: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1325 - acc: 0.9608 - val_loss: 0.1511 - val_acc: 0.9625\n",
            "Epoch 286/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1294 - acc: 0.9650\n",
            "Epoch 00286: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1307 - acc: 0.9647 - val_loss: 0.1248 - val_acc: 0.9717\n",
            "Epoch 287/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1392 - acc: 0.9612\n",
            "Epoch 00287: val_loss did not improve from 0.11630\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1396 - acc: 0.9603 - val_loss: 0.1657 - val_acc: 0.9567\n",
            "Epoch 288/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1554 - acc: 0.9521\n",
            "Epoch 00288: val_loss improved from 0.11630 to 0.11201, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.1604 - acc: 0.9486 - val_loss: 0.1120 - val_acc: 0.9725\n",
            "Epoch 289/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.9554\n",
            "Epoch 00289: val_loss did not improve from 0.11201\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1439 - acc: 0.9553 - val_loss: 0.1351 - val_acc: 0.9650\n",
            "Epoch 290/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9569\n",
            "Epoch 00290: val_loss did not improve from 0.11201\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1467 - acc: 0.9561 - val_loss: 0.1321 - val_acc: 0.9675\n",
            "Epoch 291/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1508 - acc: 0.9576\n",
            "Epoch 00291: val_loss did not improve from 0.11201\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1499 - acc: 0.9578 - val_loss: 0.1139 - val_acc: 0.9742\n",
            "Epoch 292/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1366 - acc: 0.9591\n",
            "Epoch 00292: val_loss did not improve from 0.11201\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1390 - acc: 0.9581 - val_loss: 0.1192 - val_acc: 0.9700\n",
            "Epoch 293/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1266 - acc: 0.9638\n",
            "Epoch 00293: val_loss did not improve from 0.11201\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1265 - acc: 0.9642 - val_loss: 0.1373 - val_acc: 0.9642\n",
            "Epoch 294/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9623\n",
            "Epoch 00294: val_loss improved from 0.11201 to 0.09664, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.1232 - acc: 0.9625 - val_loss: 0.0966 - val_acc: 0.9775\n",
            "Epoch 295/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1179 - acc: 0.9659\n",
            "Epoch 00295: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1189 - acc: 0.9650 - val_loss: 0.1163 - val_acc: 0.9775\n",
            "Epoch 296/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1263 - acc: 0.9635\n",
            "Epoch 00296: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1263 - acc: 0.9633 - val_loss: 0.1059 - val_acc: 0.9717\n",
            "Epoch 297/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1353 - acc: 0.9556\n",
            "Epoch 00297: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1363 - acc: 0.9547 - val_loss: 0.1158 - val_acc: 0.9708\n",
            "Epoch 298/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1368 - acc: 0.9575\n",
            "Epoch 00298: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1337 - acc: 0.9581 - val_loss: 0.1231 - val_acc: 0.9683\n",
            "Epoch 299/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1224 - acc: 0.9624\n",
            "Epoch 00299: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1232 - acc: 0.9625 - val_loss: 0.1270 - val_acc: 0.9642\n",
            "Epoch 300/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1363 - acc: 0.9569\n",
            "Epoch 00300: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1373 - acc: 0.9569 - val_loss: 0.1153 - val_acc: 0.9708\n",
            "Epoch 301/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1298 - acc: 0.9600\n",
            "Epoch 00301: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1310 - acc: 0.9606 - val_loss: 0.1031 - val_acc: 0.9775\n",
            "Epoch 302/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9626\n",
            "Epoch 00302: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1266 - acc: 0.9622 - val_loss: 0.1262 - val_acc: 0.9675\n",
            "Epoch 303/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1284 - acc: 0.9650\n",
            "Epoch 00303: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1325 - acc: 0.9639 - val_loss: 0.1225 - val_acc: 0.9717\n",
            "Epoch 304/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1295 - acc: 0.9613\n",
            "Epoch 00304: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1274 - acc: 0.9633 - val_loss: 0.1334 - val_acc: 0.9675\n",
            "Epoch 305/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1174 - acc: 0.9629\n",
            "Epoch 00305: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1212 - acc: 0.9614 - val_loss: 0.1205 - val_acc: 0.9700\n",
            "Epoch 306/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9574\n",
            "Epoch 00306: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1344 - acc: 0.9575 - val_loss: 0.1231 - val_acc: 0.9708\n",
            "Epoch 307/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1320 - acc: 0.9609\n",
            "Epoch 00307: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1314 - acc: 0.9611 - val_loss: 0.1129 - val_acc: 0.9717\n",
            "Epoch 308/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1244 - acc: 0.9645\n",
            "Epoch 00308: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1268 - acc: 0.9628 - val_loss: 0.1105 - val_acc: 0.9758\n",
            "Epoch 309/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1367 - acc: 0.9553\n",
            "Epoch 00309: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1357 - acc: 0.9561 - val_loss: 0.1302 - val_acc: 0.9667\n",
            "Epoch 310/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1330 - acc: 0.9623\n",
            "Epoch 00310: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1348 - acc: 0.9614 - val_loss: 0.1051 - val_acc: 0.9742\n",
            "Epoch 311/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1270 - acc: 0.9636\n",
            "Epoch 00311: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1246 - acc: 0.9642 - val_loss: 0.1072 - val_acc: 0.9758\n",
            "Epoch 312/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9603\n",
            "Epoch 00312: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1272 - acc: 0.9600 - val_loss: 0.1032 - val_acc: 0.9758\n",
            "Epoch 313/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1320 - acc: 0.9609\n",
            "Epoch 00313: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1301 - acc: 0.9619 - val_loss: 0.1019 - val_acc: 0.9733\n",
            "Epoch 314/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1283 - acc: 0.9641\n",
            "Epoch 00314: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1276 - acc: 0.9644 - val_loss: 0.1309 - val_acc: 0.9650\n",
            "Epoch 315/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9571\n",
            "Epoch 00315: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1337 - acc: 0.9578 - val_loss: 0.1428 - val_acc: 0.9633\n",
            "Epoch 316/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9597\n",
            "Epoch 00316: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1392 - acc: 0.9600 - val_loss: 0.1199 - val_acc: 0.9733\n",
            "Epoch 317/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1283 - acc: 0.9630\n",
            "Epoch 00317: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1304 - acc: 0.9617 - val_loss: 0.1251 - val_acc: 0.9758\n",
            "Epoch 318/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1157 - acc: 0.9644\n",
            "Epoch 00318: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1184 - acc: 0.9642 - val_loss: 0.1563 - val_acc: 0.9592\n",
            "Epoch 319/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1139 - acc: 0.9636\n",
            "Epoch 00319: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1119 - acc: 0.9650 - val_loss: 0.1088 - val_acc: 0.9750\n",
            "Epoch 320/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9634\n",
            "Epoch 00320: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1212 - acc: 0.9631 - val_loss: 0.1339 - val_acc: 0.9650\n",
            "Epoch 321/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9649\n",
            "Epoch 00321: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1209 - acc: 0.9642 - val_loss: 0.1037 - val_acc: 0.9758\n",
            "Epoch 322/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9660\n",
            "Epoch 00322: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1252 - acc: 0.9664 - val_loss: 0.1168 - val_acc: 0.9708\n",
            "Epoch 323/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1301 - acc: 0.9634\n",
            "Epoch 00323: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1308 - acc: 0.9631 - val_loss: 0.1059 - val_acc: 0.9700\n",
            "Epoch 324/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1392 - acc: 0.9606\n",
            "Epoch 00324: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1388 - acc: 0.9600 - val_loss: 0.1162 - val_acc: 0.9758\n",
            "Epoch 325/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1160 - acc: 0.9691\n",
            "Epoch 00325: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1158 - acc: 0.9706 - val_loss: 0.1197 - val_acc: 0.9675\n",
            "Epoch 326/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1142 - acc: 0.9672\n",
            "Epoch 00326: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1172 - acc: 0.9664 - val_loss: 0.1064 - val_acc: 0.9758\n",
            "Epoch 327/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1243 - acc: 0.9624\n",
            "Epoch 00327: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1257 - acc: 0.9619 - val_loss: 0.1322 - val_acc: 0.9675\n",
            "Epoch 328/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1191 - acc: 0.9645\n",
            "Epoch 00328: val_loss did not improve from 0.09664\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1200 - acc: 0.9650 - val_loss: 0.1243 - val_acc: 0.9658\n",
            "Epoch 329/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1248 - acc: 0.9624\n",
            "Epoch 00329: val_loss improved from 0.09664 to 0.09624, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1255 - acc: 0.9628 - val_loss: 0.0962 - val_acc: 0.9842\n",
            "Epoch 330/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1108 - acc: 0.9703\n",
            "Epoch 00330: val_loss did not improve from 0.09624\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1107 - acc: 0.9694 - val_loss: 0.1109 - val_acc: 0.9775\n",
            "Epoch 331/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9611\n",
            "Epoch 00331: val_loss did not improve from 0.09624\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1280 - acc: 0.9611 - val_loss: 0.1146 - val_acc: 0.9717\n",
            "Epoch 332/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1215 - acc: 0.9671\n",
            "Epoch 00332: val_loss improved from 0.09624 to 0.09400, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1223 - acc: 0.9672 - val_loss: 0.0940 - val_acc: 0.9767\n",
            "Epoch 333/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9657\n",
            "Epoch 00333: val_loss improved from 0.09400 to 0.09292, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.1137 - acc: 0.9664 - val_loss: 0.0929 - val_acc: 0.9783\n",
            "Epoch 334/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9609\n",
            "Epoch 00334: val_loss did not improve from 0.09292\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1257 - acc: 0.9600 - val_loss: 0.1027 - val_acc: 0.9700\n",
            "Epoch 335/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1199 - acc: 0.9644\n",
            "Epoch 00335: val_loss did not improve from 0.09292\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1226 - acc: 0.9636 - val_loss: 0.1055 - val_acc: 0.9758\n",
            "Epoch 336/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1103 - acc: 0.9712\n",
            "Epoch 00336: val_loss did not improve from 0.09292\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1104 - acc: 0.9708 - val_loss: 0.1059 - val_acc: 0.9783\n",
            "Epoch 337/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9614\n",
            "Epoch 00337: val_loss improved from 0.09292 to 0.09122, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1203 - acc: 0.9625 - val_loss: 0.0912 - val_acc: 0.9800\n",
            "Epoch 338/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1155 - acc: 0.9662\n",
            "Epoch 00338: val_loss did not improve from 0.09122\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1185 - acc: 0.9644 - val_loss: 0.1027 - val_acc: 0.9725\n",
            "Epoch 339/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1436 - acc: 0.9553\n",
            "Epoch 00339: val_loss did not improve from 0.09122\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1399 - acc: 0.9569 - val_loss: 0.1027 - val_acc: 0.9767\n",
            "Epoch 340/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1249 - acc: 0.9628\n",
            "Epoch 00340: val_loss did not improve from 0.09122\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1269 - acc: 0.9603 - val_loss: 0.1060 - val_acc: 0.9767\n",
            "Epoch 341/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9677\n",
            "Epoch 00341: val_loss did not improve from 0.09122\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1135 - acc: 0.9683 - val_loss: 0.1069 - val_acc: 0.9725\n",
            "Epoch 342/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1302 - acc: 0.9618\n",
            "Epoch 00342: val_loss improved from 0.09122 to 0.08080, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1325 - acc: 0.9611 - val_loss: 0.0808 - val_acc: 0.9867\n",
            "Epoch 343/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1240 - acc: 0.9575\n",
            "Epoch 00343: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1262 - acc: 0.9572 - val_loss: 0.1015 - val_acc: 0.9792\n",
            "Epoch 344/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9614\n",
            "Epoch 00344: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1222 - acc: 0.9614 - val_loss: 0.1123 - val_acc: 0.9717\n",
            "Epoch 345/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9689\n",
            "Epoch 00345: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1116 - acc: 0.9681 - val_loss: 0.0985 - val_acc: 0.9725\n",
            "Epoch 346/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1382 - acc: 0.9571\n",
            "Epoch 00346: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1383 - acc: 0.9575 - val_loss: 0.1155 - val_acc: 0.9683\n",
            "Epoch 347/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1235 - acc: 0.9640\n",
            "Epoch 00347: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1245 - acc: 0.9636 - val_loss: 0.1101 - val_acc: 0.9700\n",
            "Epoch 348/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1209 - acc: 0.9658\n",
            "Epoch 00348: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1212 - acc: 0.9658 - val_loss: 0.1158 - val_acc: 0.9683\n",
            "Epoch 349/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9637\n",
            "Epoch 00349: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1207 - acc: 0.9622 - val_loss: 0.0954 - val_acc: 0.9783\n",
            "Epoch 350/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1253 - acc: 0.9636\n",
            "Epoch 00350: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1243 - acc: 0.9644 - val_loss: 0.1106 - val_acc: 0.9717\n",
            "Epoch 351/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1305 - acc: 0.9591\n",
            "Epoch 00351: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1308 - acc: 0.9594 - val_loss: 0.0915 - val_acc: 0.9808\n",
            "Epoch 352/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1306 - acc: 0.9597\n",
            "Epoch 00352: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1283 - acc: 0.9611 - val_loss: 0.1189 - val_acc: 0.9675\n",
            "Epoch 353/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1255 - acc: 0.9606\n",
            "Epoch 00353: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1272 - acc: 0.9600 - val_loss: 0.0945 - val_acc: 0.9783\n",
            "Epoch 354/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1132 - acc: 0.9679\n",
            "Epoch 00354: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1132 - acc: 0.9678 - val_loss: 0.1158 - val_acc: 0.9725\n",
            "Epoch 355/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1113 - acc: 0.9678\n",
            "Epoch 00355: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1108 - acc: 0.9686 - val_loss: 0.1141 - val_acc: 0.9725\n",
            "Epoch 356/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9617\n",
            "Epoch 00356: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1222 - acc: 0.9600 - val_loss: 0.0988 - val_acc: 0.9758\n",
            "Epoch 357/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1282 - acc: 0.9612\n",
            "Epoch 00357: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1317 - acc: 0.9592 - val_loss: 0.1016 - val_acc: 0.9733\n",
            "Epoch 358/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1230 - acc: 0.9650\n",
            "Epoch 00358: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1233 - acc: 0.9642 - val_loss: 0.1116 - val_acc: 0.9733\n",
            "Epoch 359/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1167 - acc: 0.9647\n",
            "Epoch 00359: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1217 - acc: 0.9631 - val_loss: 0.1102 - val_acc: 0.9742\n",
            "Epoch 360/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1086 - acc: 0.9676\n",
            "Epoch 00360: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1080 - acc: 0.9683 - val_loss: 0.1040 - val_acc: 0.9792\n",
            "Epoch 361/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0984 - acc: 0.9752\n",
            "Epoch 00361: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0999 - acc: 0.9739 - val_loss: 0.1151 - val_acc: 0.9700\n",
            "Epoch 362/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1183 - acc: 0.9624\n",
            "Epoch 00362: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1199 - acc: 0.9617 - val_loss: 0.1100 - val_acc: 0.9708\n",
            "Epoch 363/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1291 - acc: 0.9586\n",
            "Epoch 00363: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1296 - acc: 0.9586 - val_loss: 0.0973 - val_acc: 0.9808\n",
            "Epoch 364/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1215 - acc: 0.9618\n",
            "Epoch 00364: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1217 - acc: 0.9611 - val_loss: 0.1274 - val_acc: 0.9667\n",
            "Epoch 365/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1213 - acc: 0.9609\n",
            "Epoch 00365: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1222 - acc: 0.9614 - val_loss: 0.1156 - val_acc: 0.9700\n",
            "Epoch 366/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1189 - acc: 0.9627\n",
            "Epoch 00366: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1223 - acc: 0.9614 - val_loss: 0.0940 - val_acc: 0.9775\n",
            "Epoch 367/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1288 - acc: 0.9597\n",
            "Epoch 00367: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1272 - acc: 0.9617 - val_loss: 0.1012 - val_acc: 0.9775\n",
            "Epoch 368/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1134 - acc: 0.9624\n",
            "Epoch 00368: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1153 - acc: 0.9631 - val_loss: 0.1081 - val_acc: 0.9725\n",
            "Epoch 369/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1188 - acc: 0.9624\n",
            "Epoch 00369: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1170 - acc: 0.9631 - val_loss: 0.0928 - val_acc: 0.9767\n",
            "Epoch 370/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1184 - acc: 0.9617\n",
            "Epoch 00370: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1177 - acc: 0.9619 - val_loss: 0.1002 - val_acc: 0.9758\n",
            "Epoch 371/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1159 - acc: 0.9633\n",
            "Epoch 00371: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1150 - acc: 0.9636 - val_loss: 0.1242 - val_acc: 0.9667\n",
            "Epoch 372/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1133 - acc: 0.9647\n",
            "Epoch 00372: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1141 - acc: 0.9644 - val_loss: 0.1383 - val_acc: 0.9575\n",
            "Epoch 373/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1199 - acc: 0.9633\n",
            "Epoch 00373: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1189 - acc: 0.9631 - val_loss: 0.1188 - val_acc: 0.9650\n",
            "Epoch 374/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1132 - acc: 0.9615\n",
            "Epoch 00374: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1138 - acc: 0.9608 - val_loss: 0.0896 - val_acc: 0.9792\n",
            "Epoch 375/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9649\n",
            "Epoch 00375: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1227 - acc: 0.9642 - val_loss: 0.1093 - val_acc: 0.9717\n",
            "Epoch 376/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1077 - acc: 0.9724\n",
            "Epoch 00376: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1084 - acc: 0.9719 - val_loss: 0.1020 - val_acc: 0.9733\n",
            "Epoch 377/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9606\n",
            "Epoch 00377: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1288 - acc: 0.9603 - val_loss: 0.1124 - val_acc: 0.9708\n",
            "Epoch 378/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1315 - acc: 0.9600\n",
            "Epoch 00378: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1339 - acc: 0.9589 - val_loss: 0.1058 - val_acc: 0.9750\n",
            "Epoch 379/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1184 - acc: 0.9673\n",
            "Epoch 00379: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1193 - acc: 0.9672 - val_loss: 0.0976 - val_acc: 0.9775\n",
            "Epoch 380/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1217 - acc: 0.9626\n",
            "Epoch 00380: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1218 - acc: 0.9631 - val_loss: 0.1076 - val_acc: 0.9683\n",
            "Epoch 381/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1101 - acc: 0.9662\n",
            "Epoch 00381: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1116 - acc: 0.9656 - val_loss: 0.1176 - val_acc: 0.9725\n",
            "Epoch 382/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1251 - acc: 0.9606\n",
            "Epoch 00382: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1232 - acc: 0.9614 - val_loss: 0.1237 - val_acc: 0.9683\n",
            "Epoch 383/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0941 - acc: 0.9753\n",
            "Epoch 00383: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0949 - acc: 0.9747 - val_loss: 0.0913 - val_acc: 0.9783\n",
            "Epoch 384/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9617\n",
            "Epoch 00384: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1187 - acc: 0.9617 - val_loss: 0.1004 - val_acc: 0.9775\n",
            "Epoch 385/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1161 - acc: 0.9656\n",
            "Epoch 00385: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1140 - acc: 0.9661 - val_loss: 0.0924 - val_acc: 0.9783\n",
            "Epoch 386/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9717\n",
            "Epoch 00386: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1056 - acc: 0.9717 - val_loss: 0.1074 - val_acc: 0.9700\n",
            "Epoch 387/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1025 - acc: 0.9694\n",
            "Epoch 00387: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1030 - acc: 0.9697 - val_loss: 0.1145 - val_acc: 0.9667\n",
            "Epoch 388/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1241 - acc: 0.9615\n",
            "Epoch 00388: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1230 - acc: 0.9628 - val_loss: 0.1054 - val_acc: 0.9767\n",
            "Epoch 389/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1204 - acc: 0.9615\n",
            "Epoch 00389: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1222 - acc: 0.9614 - val_loss: 0.1223 - val_acc: 0.9658\n",
            "Epoch 390/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1265 - acc: 0.9585\n",
            "Epoch 00390: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1240 - acc: 0.9597 - val_loss: 0.0872 - val_acc: 0.9817\n",
            "Epoch 391/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1098 - acc: 0.9669\n",
            "Epoch 00391: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.1124 - acc: 0.9661 - val_loss: 0.1012 - val_acc: 0.9742\n",
            "Epoch 392/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1093 - acc: 0.9685\n",
            "Epoch 00392: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1087 - acc: 0.9686 - val_loss: 0.0930 - val_acc: 0.9792\n",
            "Epoch 393/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1101 - acc: 0.9670\n",
            "Epoch 00393: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1136 - acc: 0.9656 - val_loss: 0.1027 - val_acc: 0.9742\n",
            "Epoch 394/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1146 - acc: 0.9659\n",
            "Epoch 00394: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1129 - acc: 0.9661 - val_loss: 0.1062 - val_acc: 0.9725\n",
            "Epoch 395/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1035 - acc: 0.9676\n",
            "Epoch 00395: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1028 - acc: 0.9683 - val_loss: 0.0874 - val_acc: 0.9783\n",
            "Epoch 396/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0991 - acc: 0.9725\n",
            "Epoch 00396: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1017 - acc: 0.9722 - val_loss: 0.0811 - val_acc: 0.9825\n",
            "Epoch 397/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1050 - acc: 0.9682\n",
            "Epoch 00397: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1058 - acc: 0.9681 - val_loss: 0.1300 - val_acc: 0.9625\n",
            "Epoch 398/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1091 - acc: 0.9691\n",
            "Epoch 00398: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1092 - acc: 0.9697 - val_loss: 0.0933 - val_acc: 0.9792\n",
            "Epoch 399/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1161 - acc: 0.9650\n",
            "Epoch 00399: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1196 - acc: 0.9622 - val_loss: 0.1068 - val_acc: 0.9692\n",
            "Epoch 400/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1028 - acc: 0.9709\n",
            "Epoch 00400: val_loss did not improve from 0.08080\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1050 - acc: 0.9694 - val_loss: 0.0984 - val_acc: 0.9708\n",
            "1200/1200 [==============================] - 0s 119us/sample - loss: 0.0984 - acc: 0.9708\n",
            "[0.09836206500728925, 0.97083336]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJbWVautrduQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "110ab532-d3b7-432e-9d2c-3ee9009440e7"
      },
      "source": [
        "for i in range(6, 7): # Итерација низ секој испитен примерок\n",
        "  print(f\"====================== Примерок ({i}) ======================\")\n",
        "  print(\"Вчитување тест податоци од испитниот примерок \" + str(i) + \"...\")\n",
        "  \n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_test_data = np.empty(0)\n",
        "  temp_test_events = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_test_set = 'S' + format(j, '02') + '/Test'\n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testData.mat\"\n",
        "    temp = loadmat(full_path)['testData']\n",
        "    if temp_test_data.size != 0:\n",
        "      temp_test_data = np.concatenate((temp_test_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_test_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_test_events.size != 0:\n",
        "        temp_test_events = np.append(temp_test_events, temp)\n",
        "      else:\n",
        "        temp_test_events = np.array(temp)\n",
        "\n",
        "    # Вчитување на бројот на runs \n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/runs_per_block.txt\"\n",
        "    with open(full_path, \"r\") as runs_per_block:\n",
        "      test_runs_per_block[i-1][j-1] = int(runs_per_block.read())\n",
        "\n",
        "    print(\"\\t - Тест податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "  # Зачувај ги тест податоците вчитани од испитниот примерок во низа\n",
        "  test_data.append(temp_test_data)\n",
        "  test_events.append(temp_test_events)\n",
        "  print(\"Тест податоците од испитниот примерок \" + str(i) + \" се вчитани.\\n\")\n",
        "\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_data: \" + str(test_data[i-1].shape)) # test_data to predict\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_events: \" + str(len(test_events[i-1]))) # test_events\n",
        "  for j in range (1,4):\n",
        "    print(\"SBJ\" + str(format(i-1, '02')) + \" / S\" + str(format(j-1, '02')) + \"| Runs per block: \" + str(test_runs_per_block[i-1][j-1])) # runs per block in SJB01, SJ00 \n",
        "\n",
        "  to_predict_data = reshape_data_to_mne_format(test_data[i-1])\n",
        "  predictions = model6.predict(to_predict_data)\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Predictions: \" + str(len(predictions)))\n",
        "  # np.savetxt(\"predictions.csv\", predictions, delimiter=\",\")\n",
        "\n",
        "\n",
        "  # ========= FALI USTE DA SE ISPARSIRA PREDICTIONOT... NE E SREDEN OVOJ KOD DOLE =======\n",
        "\n",
        "  int_pred = np.argmax(predictions, axis=1)\n",
        "  int_ytest = np.argmax(y_test, axis=1)\n",
        "\n",
        "  session_start = 0\n",
        "  start_prediction_index = 0\n",
        "  end_prediction_index = 0\n",
        "  for session in range(0, 3):\n",
        "    print(f\"============== Сесија ({session}) ==============\")\n",
        "    for block in range(0, 50):    \n",
        "      events_per_block = test_runs_per_block[i-1][session]\n",
        "\n",
        "      start_prediction_index = session_start + (block*events_per_block)*8\n",
        "      end_prediction_index = session_start + ((block+1)*events_per_block)*8\n",
        "\n",
        "      block_prediction = int_pred[start_prediction_index:end_prediction_index]\n",
        "      prediction = np.bincount(block_prediction).argmax()\n",
        "      df.iat[session+15,block+2] = prediction+1\n",
        "      # UNCOMMENT ZA PODOBAR PRIKAZ :)\n",
        "      # print(f\"Session {session} | Block: {block} | Prediction: {prediction} | Address: {end_prediction_index}\")\n",
        "\n",
        "      print(str(prediction+1) + \",\", end=\"\")\n",
        "    session_start = end_prediction_index\n",
        "    print(\"\")\n",
        "  print(\"Stigna li do kraj: \" + str(session_start == len(predictions)))\n",
        "  print(f\"====================== Примерок ({i}) ======================\\n\\n\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Примерок (6) ======================\n",
            "Вчитување тест податоци од испитниот примерок 6...\n",
            "\t - Тест податоците од сесија 1 се вчитани.\n",
            "\t - Тест податоците од сесија 2 се вчитани.\n",
            "\t - Тест податоците од сесија 3 се вчитани.\n",
            "Тест податоците од испитниот примерок 6 се вчитани.\n",
            "\n",
            "SBJ05| Test_data: (8, 350, 8000)\n",
            "SBJ05| Test_events: 8000\n",
            "SBJ05 / S00| Runs per block: 4\n",
            "SBJ05 / S01| Runs per block: 7\n",
            "SBJ05 / S02| Runs per block: 9\n",
            "SBJ05| Predictions: 8000\n",
            "============== Сесија (0) ==============\n",
            "1,3,1,8,3,3,3,4,7,2,5,3,5,3,3,2,4,3,8,5,4,2,4,3,2,8,2,4,3,3,3,8,3,3,3,3,3,7,8,5,3,4,4,3,7,2,7,2,5,8,\n",
            "============== Сесија (1) ==============\n",
            "3,4,1,7,1,1,1,1,4,1,3,3,1,5,4,8,3,2,8,3,4,3,4,3,8,4,3,3,3,3,2,3,4,8,3,4,3,5,5,4,2,5,5,5,5,5,5,5,5,5,\n",
            "============== Сесија (2) ==============\n",
            "2,3,2,5,3,3,3,5,2,3,2,1,3,3,3,3,2,3,3,5,1,3,3,1,1,3,3,3,3,3,5,3,1,1,3,3,3,3,3,1,1,3,2,3,3,3,1,1,3,3,\n",
            "Stigna li do kraj: True\n",
            "====================== Примерок (6) ======================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODINphQprl3X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "898b46d6-ef49-41e9-a942-8a4c492d2d57"
      },
      "source": [
        "df"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUBJECT</th>\n",
              "      <th>SESSION</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>Unnamed: 52</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    SUBJECT  SESSION  1  2  3  4  5  6  7  ... 43 44 45 46 47 48 49 50 Unnamed: 52\n",
              "0         1        1  6  6  3  6  6  3  6  ...  6  4  8  3  4  5  5  7         NaN\n",
              "1         1        2  6  3  2  1  2  1  3  ...  6  2  2  2  3  6  6  2         NaN\n",
              "2         1        3  3  3  3  3  3  3  3  ...  3  3  6  3  6  7  1  6         NaN\n",
              "3         2        1  8  8  8  8  8  8  8  ...  8  4  8  4  8  7  8  8         NaN\n",
              "4         2        2  2  7  6  6  6  6  7  ...  2  6  6  2  6  6  2  2         NaN\n",
              "5         2        3  2  7  2  6  7  4  2  ...  2  6  2  2  7  2  2  2         NaN\n",
              "6         3        1  3  3  4  4  4  3  6  ...  4  6  3  3  4  3  4  4         NaN\n",
              "7         3        2  6  1  7  7  7  7  7  ...  6  7  1  6  6  6  6  6         NaN\n",
              "8         3        3  6  4  8  8  5  7  8  ...  4  2  8  2  2  2  2  8         NaN\n",
              "9         4        1  1  1  2  1  5  5  6  ...  1  1  7  1  6  6  6  6         NaN\n",
              "10        4        2  7  7  7  7  6  6  7  ...  1  5  5  5  5  5  2  6         NaN\n",
              "11        4        3  7  3  7  3  6  6  7  ...  6  1  4  4  4  6  6  6         NaN\n",
              "12        5        1  5  4  4  4  6  4  5  ...  3  1  4  4  1  3  3  5         NaN\n",
              "13        5        2  3  6  3  5  2  6  7  ...  6  6  6  6  6  6  6  4         NaN\n",
              "14        5        3  4  3  3  6  5  7  6  ...  7  2  2  7  5  6  4  2         NaN\n",
              "15        6        1  1  3  1  8  3  3  3  ...  4  3  7  2  7  2  5  8         NaN\n",
              "16        6        2  3  4  1  7  1  1  1  ...  5  5  5  5  5  5  5  5         NaN\n",
              "17        6        3  2  3  2  5  3  3  3  ...  2  3  3  3  1  1  3  3         NaN\n",
              "18        7        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "19        7        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "20        7        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "21        8        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "22        8        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "23        8        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "24        9        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "25        9        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "26        9        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "27       10        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "28       10        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "29       10        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "30       11        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "31       11        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "32       11        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "33       12        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "34       12        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "35       12        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "36       13        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "37       13        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "38       13        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "39       14        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "40       14        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "41       14        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "42       15        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "43       15        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "44       15        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "\n",
              "[45 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRiGj782sJvK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1716e906-701b-4f4c-a205-3a5b31d8bbf3"
      },
      "source": [
        "for i in range(7, 8): # Итерација низ секој испитен примерок\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_data = np.empty(0)\n",
        "  temp_labels = np.empty(0)\n",
        "  temp_events = np.empty(0)\n",
        "  temp_targets = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainData.mat\"\n",
        "    temp = loadmat(full_path)['trainData']\n",
        "    if temp_data.size != 0:\n",
        "      temp_data = np.concatenate((temp_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на label-ите\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainLabels.txt\"\n",
        "    with open(full_path, \"r\") as file_labels:\n",
        "      temp = file_labels.read().splitlines()\n",
        "      temp = np.repeat(temp, 8*10)\n",
        "      if temp_labels.size != 0:\n",
        "        temp_labels = np.concatenate((temp_labels, temp))\n",
        "      else:\n",
        "        temp_labels = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_events.size != 0:\n",
        "        temp_events = np.append(temp_events, temp)\n",
        "      else:\n",
        "        temp_events = np.array(temp)\n",
        "      \n",
        "\n",
        "    # Вчитување на редоследот на објекти кои се target\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainTargets.txt\"\n",
        "    with open(full_path, \"r\") as file_targets:\n",
        "      temp = file_targets.read().splitlines()\n",
        "      if temp_targets.size != 0:\n",
        "        temp_targets = np.concatenate((temp_targets, temp))\n",
        "      else:\n",
        "        temp_targets = np.array(temp)\n",
        "    print(\"\\t - Податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "\n",
        "  for j in range(4, 8): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "      \n",
        "  # Зачувај ги податоците вчитани од испитниот примерок во низа\n",
        "  data.append(temp_data)\n",
        "  labels.append(temp_labels)\n",
        "  events.append(temp_events)\n",
        "  targets.append(temp_targets)\n",
        "\n",
        "  \n",
        "  print(\"Податоците од испитниот примерок \" + str(i) + \" се вчитани.\")\n",
        "\n",
        "\n",
        "  #data = target_events_data_scaled\n",
        "  mne_array = np.swapaxes(data[i-1], 0, 2) # (епохa, канал, настан). \n",
        "  mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "  mne_array = mne_array.reshape(mne_array.shape[0],1, 8,350)\n",
        "  print(mne_array.shape)\n",
        "\n",
        "  events_arr = events[i-1].astype(np.int)\n",
        "  labels_arr = labels[i-1].astype(np.int)\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(mne_array, labels_arr-1, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "  model7 = DeepConvNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "  model7.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = Adam(0.0009))\n",
        "  checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                                verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "  #clf = RandomForestClassifier(max_depth=5)\n",
        "  #clf.fit(X_train, y_train)\n",
        "  #score = clf.score(X_test, y_test)\n",
        "  # print(score)\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  num_batch_size=100\n",
        "  num_epochs=400\n",
        "  model7.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "            validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "  score = model7.evaluate(X_test, y_test, verbose=1)\n",
        "  print(score)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t - Податоците од сесија 1 се вчитани.\n",
            "\t - Податоците од сесија 2 се вчитани.\n",
            "\t - Податоците од сесија 3 се вчитани.\n",
            "Податоците од испитниот примерок 7 се вчитани.\n",
            "(4800, 1, 8, 350)\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Train on 3600 samples, validate on 1200 samples\n",
            "Epoch 1/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.4373 - acc: 0.1386\n",
            "Epoch 00001: val_loss improved from inf to 2.23719, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 6s 2ms/sample - loss: 2.4329 - acc: 0.1381 - val_loss: 2.2372 - val_acc: 0.1475\n",
            "Epoch 2/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.2315 - acc: 0.1576\n",
            "Epoch 00002: val_loss did not improve from 2.23719\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 2.2405 - acc: 0.1542 - val_loss: 2.4563 - val_acc: 0.1125\n",
            "Epoch 3/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 2.1692 - acc: 0.1759\n",
            "Epoch 00003: val_loss did not improve from 2.23719\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 2.1759 - acc: 0.1731 - val_loss: 2.2952 - val_acc: 0.1242\n",
            "Epoch 4/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.1285 - acc: 0.1945\n",
            "Epoch 00004: val_loss improved from 2.23719 to 2.14342, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 2.1279 - acc: 0.1953 - val_loss: 2.1434 - val_acc: 0.2025\n",
            "Epoch 5/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.0879 - acc: 0.2182\n",
            "Epoch 00005: val_loss did not improve from 2.14342\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 2.0884 - acc: 0.2181 - val_loss: 2.1511 - val_acc: 0.1675\n",
            "Epoch 6/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.0003 - acc: 0.2443\n",
            "Epoch 00006: val_loss did not improve from 2.14342\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.9990 - acc: 0.2439 - val_loss: 2.2175 - val_acc: 0.1942\n",
            "Epoch 7/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.9442 - acc: 0.2665\n",
            "Epoch 00007: val_loss improved from 2.14342 to 1.96524, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 1.9394 - acc: 0.2683 - val_loss: 1.9652 - val_acc: 0.2575\n",
            "Epoch 8/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.8288 - acc: 0.3023\n",
            "Epoch 00008: val_loss did not improve from 1.96524\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.8334 - acc: 0.3008 - val_loss: 2.2013 - val_acc: 0.2600\n",
            "Epoch 9/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.7346 - acc: 0.3380\n",
            "Epoch 00009: val_loss improved from 1.96524 to 1.89800, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 1.7331 - acc: 0.3369 - val_loss: 1.8980 - val_acc: 0.2758\n",
            "Epoch 10/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.6730 - acc: 0.3627\n",
            "Epoch 00010: val_loss did not improve from 1.89800\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.6804 - acc: 0.3578 - val_loss: 1.9483 - val_acc: 0.2867\n",
            "Epoch 11/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.5818 - acc: 0.3903\n",
            "Epoch 00011: val_loss improved from 1.89800 to 1.68605, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 1.5829 - acc: 0.3914 - val_loss: 1.6861 - val_acc: 0.3308\n",
            "Epoch 12/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.5278 - acc: 0.4155\n",
            "Epoch 00012: val_loss did not improve from 1.68605\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.5324 - acc: 0.4131 - val_loss: 1.8687 - val_acc: 0.3167\n",
            "Epoch 13/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4745 - acc: 0.4409\n",
            "Epoch 00013: val_loss did not improve from 1.68605\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.4713 - acc: 0.4406 - val_loss: 2.3877 - val_acc: 0.2417\n",
            "Epoch 14/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4350 - acc: 0.4647\n",
            "Epoch 00014: val_loss did not improve from 1.68605\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.4448 - acc: 0.4606 - val_loss: 1.7614 - val_acc: 0.3383\n",
            "Epoch 15/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.3900 - acc: 0.4688\n",
            "Epoch 00015: val_loss did not improve from 1.68605\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 1.3986 - acc: 0.4661 - val_loss: 1.7541 - val_acc: 0.3858\n",
            "Epoch 16/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.3635 - acc: 0.4912\n",
            "Epoch 00016: val_loss improved from 1.68605 to 1.59222, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 1.3618 - acc: 0.4919 - val_loss: 1.5922 - val_acc: 0.3900\n",
            "Epoch 17/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.2667 - acc: 0.5171\n",
            "Epoch 00017: val_loss improved from 1.59222 to 1.44047, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 1.2667 - acc: 0.5172 - val_loss: 1.4405 - val_acc: 0.4550\n",
            "Epoch 18/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.2547 - acc: 0.5254\n",
            "Epoch 00018: val_loss did not improve from 1.44047\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.2548 - acc: 0.5239 - val_loss: 1.6674 - val_acc: 0.3867\n",
            "Epoch 19/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.1855 - acc: 0.5611\n",
            "Epoch 00019: val_loss improved from 1.44047 to 1.41014, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 1.1871 - acc: 0.5594 - val_loss: 1.4101 - val_acc: 0.4933\n",
            "Epoch 20/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.1469 - acc: 0.5666\n",
            "Epoch 00020: val_loss improved from 1.41014 to 1.40135, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 1.1476 - acc: 0.5658 - val_loss: 1.4014 - val_acc: 0.4800\n",
            "Epoch 21/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0901 - acc: 0.5918\n",
            "Epoch 00021: val_loss improved from 1.40135 to 1.27207, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 1.0991 - acc: 0.5864 - val_loss: 1.2721 - val_acc: 0.5100\n",
            "Epoch 22/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.0595 - acc: 0.5984\n",
            "Epoch 00022: val_loss improved from 1.27207 to 1.23416, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 1.0568 - acc: 0.5992 - val_loss: 1.2342 - val_acc: 0.5342\n",
            "Epoch 23/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0247 - acc: 0.6162\n",
            "Epoch 00023: val_loss did not improve from 1.23416\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.0269 - acc: 0.6136 - val_loss: 1.3744 - val_acc: 0.4992\n",
            "Epoch 24/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.0088 - acc: 0.6191\n",
            "Epoch 00024: val_loss improved from 1.23416 to 1.16555, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 1.0154 - acc: 0.6169 - val_loss: 1.1655 - val_acc: 0.5617\n",
            "Epoch 25/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9935 - acc: 0.6317\n",
            "Epoch 00025: val_loss did not improve from 1.16555\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.9921 - acc: 0.6311 - val_loss: 1.2130 - val_acc: 0.5575\n",
            "Epoch 26/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9418 - acc: 0.6567\n",
            "Epoch 00026: val_loss improved from 1.16555 to 1.06311, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.9429 - acc: 0.6550 - val_loss: 1.0631 - val_acc: 0.6217\n",
            "Epoch 27/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9060 - acc: 0.6658\n",
            "Epoch 00027: val_loss did not improve from 1.06311\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.9110 - acc: 0.6639 - val_loss: 1.1470 - val_acc: 0.5608\n",
            "Epoch 28/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.8864 - acc: 0.6794\n",
            "Epoch 00028: val_loss did not improve from 1.06311\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.8813 - acc: 0.6814 - val_loss: 1.1319 - val_acc: 0.5758\n",
            "Epoch 29/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.8535 - acc: 0.6843\n",
            "Epoch 00029: val_loss did not improve from 1.06311\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.8502 - acc: 0.6847 - val_loss: 1.1303 - val_acc: 0.5817\n",
            "Epoch 30/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8174 - acc: 0.7015\n",
            "Epoch 00030: val_loss improved from 1.06311 to 0.99083, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.8224 - acc: 0.7006 - val_loss: 0.9908 - val_acc: 0.6367\n",
            "Epoch 31/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8234 - acc: 0.6906\n",
            "Epoch 00031: val_loss did not improve from 0.99083\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.8230 - acc: 0.6911 - val_loss: 1.0971 - val_acc: 0.5775\n",
            "Epoch 32/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7775 - acc: 0.7166\n",
            "Epoch 00032: val_loss improved from 0.99083 to 0.90906, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.7822 - acc: 0.7158 - val_loss: 0.9091 - val_acc: 0.6592\n",
            "Epoch 33/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7459 - acc: 0.7403\n",
            "Epoch 00033: val_loss improved from 0.90906 to 0.87398, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.7512 - acc: 0.7375 - val_loss: 0.8740 - val_acc: 0.6858\n",
            "Epoch 34/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7475 - acc: 0.7373\n",
            "Epoch 00034: val_loss did not improve from 0.87398\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.7430 - acc: 0.7392 - val_loss: 0.8933 - val_acc: 0.6708\n",
            "Epoch 35/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7226 - acc: 0.7426\n",
            "Epoch 00035: val_loss improved from 0.87398 to 0.83258, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.7260 - acc: 0.7403 - val_loss: 0.8326 - val_acc: 0.7083\n",
            "Epoch 36/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6920 - acc: 0.7526\n",
            "Epoch 00036: val_loss improved from 0.83258 to 0.80732, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.6957 - acc: 0.7525 - val_loss: 0.8073 - val_acc: 0.7075\n",
            "Epoch 37/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6805 - acc: 0.7565\n",
            "Epoch 00037: val_loss did not improve from 0.80732\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.6793 - acc: 0.7575 - val_loss: 0.8318 - val_acc: 0.6892\n",
            "Epoch 38/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.6637 - acc: 0.7570\n",
            "Epoch 00038: val_loss improved from 0.80732 to 0.78778, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.6641 - acc: 0.7572 - val_loss: 0.7878 - val_acc: 0.7092\n",
            "Epoch 39/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6427 - acc: 0.7744\n",
            "Epoch 00039: val_loss improved from 0.78778 to 0.77504, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.6446 - acc: 0.7756 - val_loss: 0.7750 - val_acc: 0.7058\n",
            "Epoch 40/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.6171 - acc: 0.7891\n",
            "Epoch 00040: val_loss did not improve from 0.77504\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.6192 - acc: 0.7867 - val_loss: 0.8345 - val_acc: 0.6883\n",
            "Epoch 41/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5960 - acc: 0.7921\n",
            "Epoch 00041: val_loss did not improve from 0.77504\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.5962 - acc: 0.7933 - val_loss: 0.8217 - val_acc: 0.6742\n",
            "Epoch 42/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5707 - acc: 0.7948\n",
            "Epoch 00042: val_loss improved from 0.77504 to 0.76159, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.5794 - acc: 0.7908 - val_loss: 0.7616 - val_acc: 0.7100\n",
            "Epoch 43/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5588 - acc: 0.8073\n",
            "Epoch 00043: val_loss improved from 0.76159 to 0.67455, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.5563 - acc: 0.8083 - val_loss: 0.6745 - val_acc: 0.7708\n",
            "Epoch 44/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5470 - acc: 0.8058\n",
            "Epoch 00044: val_loss did not improve from 0.67455\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.5524 - acc: 0.8022 - val_loss: 0.7346 - val_acc: 0.7342\n",
            "Epoch 45/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5228 - acc: 0.8203\n",
            "Epoch 00045: val_loss did not improve from 0.67455\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.5315 - acc: 0.8161 - val_loss: 0.7496 - val_acc: 0.7092\n",
            "Epoch 46/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5065 - acc: 0.8336\n",
            "Epoch 00046: val_loss did not improve from 0.67455\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.5099 - acc: 0.8322 - val_loss: 0.6958 - val_acc: 0.7583\n",
            "Epoch 47/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5176 - acc: 0.8203\n",
            "Epoch 00047: val_loss improved from 0.67455 to 0.61654, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.5191 - acc: 0.8200 - val_loss: 0.6165 - val_acc: 0.7842\n",
            "Epoch 48/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.5063 - acc: 0.8238\n",
            "Epoch 00048: val_loss did not improve from 0.61654\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.5058 - acc: 0.8247 - val_loss: 0.6217 - val_acc: 0.7808\n",
            "Epoch 49/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4905 - acc: 0.8358\n",
            "Epoch 00049: val_loss did not improve from 0.61654\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.4981 - acc: 0.8331 - val_loss: 0.6257 - val_acc: 0.7750\n",
            "Epoch 50/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4809 - acc: 0.8374\n",
            "Epoch 00050: val_loss improved from 0.61654 to 0.56832, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.4832 - acc: 0.8367 - val_loss: 0.5683 - val_acc: 0.8000\n",
            "Epoch 51/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4249 - acc: 0.8585\n",
            "Epoch 00051: val_loss improved from 0.56832 to 0.55341, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.4286 - acc: 0.8561 - val_loss: 0.5534 - val_acc: 0.7992\n",
            "Epoch 52/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4346 - acc: 0.8582\n",
            "Epoch 00052: val_loss did not improve from 0.55341\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.4381 - acc: 0.8572 - val_loss: 0.5830 - val_acc: 0.7933\n",
            "Epoch 53/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4295 - acc: 0.8579\n",
            "Epoch 00053: val_loss improved from 0.55341 to 0.53649, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.4304 - acc: 0.8583 - val_loss: 0.5365 - val_acc: 0.8125\n",
            "Epoch 54/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4110 - acc: 0.8668\n",
            "Epoch 00054: val_loss improved from 0.53649 to 0.47394, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.4151 - acc: 0.8642 - val_loss: 0.4739 - val_acc: 0.8400\n",
            "Epoch 55/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.4031 - acc: 0.8697\n",
            "Epoch 00055: val_loss did not improve from 0.47394\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.4060 - acc: 0.8694 - val_loss: 0.5072 - val_acc: 0.8217\n",
            "Epoch 56/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3837 - acc: 0.8757\n",
            "Epoch 00056: val_loss did not improve from 0.47394\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.3837 - acc: 0.8761 - val_loss: 0.5060 - val_acc: 0.8242\n",
            "Epoch 57/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3906 - acc: 0.8715\n",
            "Epoch 00057: val_loss improved from 0.47394 to 0.46911, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.3934 - acc: 0.8700 - val_loss: 0.4691 - val_acc: 0.8433\n",
            "Epoch 58/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3740 - acc: 0.8785\n",
            "Epoch 00058: val_loss did not improve from 0.46911\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.3775 - acc: 0.8769 - val_loss: 0.5262 - val_acc: 0.8192\n",
            "Epoch 59/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3683 - acc: 0.8803\n",
            "Epoch 00059: val_loss did not improve from 0.46911\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.3664 - acc: 0.8811 - val_loss: 0.5281 - val_acc: 0.8142\n",
            "Epoch 60/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3798 - acc: 0.8697\n",
            "Epoch 00060: val_loss improved from 0.46911 to 0.43324, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.3786 - acc: 0.8731 - val_loss: 0.4332 - val_acc: 0.8558\n",
            "Epoch 61/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3465 - acc: 0.8863\n",
            "Epoch 00061: val_loss did not improve from 0.43324\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.3520 - acc: 0.8839 - val_loss: 0.4768 - val_acc: 0.8367\n",
            "Epoch 62/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3344 - acc: 0.8994\n",
            "Epoch 00062: val_loss improved from 0.43324 to 0.40795, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.3330 - acc: 0.9000 - val_loss: 0.4080 - val_acc: 0.8675\n",
            "Epoch 63/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3245 - acc: 0.9021\n",
            "Epoch 00063: val_loss did not improve from 0.40795\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.3257 - acc: 0.9031 - val_loss: 0.4167 - val_acc: 0.8650\n",
            "Epoch 64/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2874 - acc: 0.9162\n",
            "Epoch 00064: val_loss did not improve from 0.40795\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2912 - acc: 0.9139 - val_loss: 0.4104 - val_acc: 0.8542\n",
            "Epoch 65/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3456 - acc: 0.8853\n",
            "Epoch 00065: val_loss did not improve from 0.40795\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.3423 - acc: 0.8869 - val_loss: 0.4230 - val_acc: 0.8625\n",
            "Epoch 66/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3060 - acc: 0.9053\n",
            "Epoch 00066: val_loss improved from 0.40795 to 0.40134, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.3079 - acc: 0.9033 - val_loss: 0.4013 - val_acc: 0.8667\n",
            "Epoch 67/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2895 - acc: 0.9154\n",
            "Epoch 00067: val_loss did not improve from 0.40134\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2891 - acc: 0.9150 - val_loss: 0.4112 - val_acc: 0.8633\n",
            "Epoch 68/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9100\n",
            "Epoch 00068: val_loss improved from 0.40134 to 0.38887, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.2881 - acc: 0.9083 - val_loss: 0.3889 - val_acc: 0.8700\n",
            "Epoch 69/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2816 - acc: 0.9158\n",
            "Epoch 00069: val_loss improved from 0.38887 to 0.36938, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.2819 - acc: 0.9139 - val_loss: 0.3694 - val_acc: 0.8800\n",
            "Epoch 70/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2740 - acc: 0.9179\n",
            "Epoch 00070: val_loss improved from 0.36938 to 0.34399, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.2758 - acc: 0.9175 - val_loss: 0.3440 - val_acc: 0.8908\n",
            "Epoch 71/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2705 - acc: 0.9169\n",
            "Epoch 00071: val_loss did not improve from 0.34399\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2705 - acc: 0.9167 - val_loss: 0.3458 - val_acc: 0.8858\n",
            "Epoch 72/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.9206\n",
            "Epoch 00072: val_loss improved from 0.34399 to 0.33254, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.2714 - acc: 0.9194 - val_loss: 0.3325 - val_acc: 0.8925\n",
            "Epoch 73/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2650 - acc: 0.9223\n",
            "Epoch 00073: val_loss improved from 0.33254 to 0.31326, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.2649 - acc: 0.9217 - val_loss: 0.3133 - val_acc: 0.9058\n",
            "Epoch 74/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2333 - acc: 0.9347\n",
            "Epoch 00074: val_loss improved from 0.31326 to 0.31226, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.2399 - acc: 0.9317 - val_loss: 0.3123 - val_acc: 0.8942\n",
            "Epoch 75/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2508 - acc: 0.9226\n",
            "Epoch 00075: val_loss did not improve from 0.31226\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2488 - acc: 0.9239 - val_loss: 0.4353 - val_acc: 0.8542\n",
            "Epoch 76/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2533 - acc: 0.9209\n",
            "Epoch 00076: val_loss improved from 0.31226 to 0.30152, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.2547 - acc: 0.9211 - val_loss: 0.3015 - val_acc: 0.9083\n",
            "Epoch 77/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2379 - acc: 0.9288\n",
            "Epoch 00077: val_loss did not improve from 0.30152\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2423 - acc: 0.9286 - val_loss: 0.3144 - val_acc: 0.9017\n",
            "Epoch 78/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2377 - acc: 0.9253\n",
            "Epoch 00078: val_loss did not improve from 0.30152\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2428 - acc: 0.9222 - val_loss: 0.3650 - val_acc: 0.8825\n",
            "Epoch 79/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2476 - acc: 0.9252\n",
            "Epoch 00079: val_loss did not improve from 0.30152\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2516 - acc: 0.9239 - val_loss: 0.3679 - val_acc: 0.8767\n",
            "Epoch 80/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2395 - acc: 0.9236\n",
            "Epoch 00080: val_loss did not improve from 0.30152\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2413 - acc: 0.9239 - val_loss: 0.3562 - val_acc: 0.8808\n",
            "Epoch 81/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2272 - acc: 0.9321\n",
            "Epoch 00081: val_loss did not improve from 0.30152\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2295 - acc: 0.9303 - val_loss: 0.4010 - val_acc: 0.8667\n",
            "Epoch 82/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2404 - acc: 0.9250\n",
            "Epoch 00082: val_loss did not improve from 0.30152\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2388 - acc: 0.9281 - val_loss: 0.3851 - val_acc: 0.8683\n",
            "Epoch 83/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2279 - acc: 0.9356\n",
            "Epoch 00083: val_loss improved from 0.30152 to 0.27127, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.2301 - acc: 0.9347 - val_loss: 0.2713 - val_acc: 0.9183\n",
            "Epoch 84/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2336 - acc: 0.9303\n",
            "Epoch 00084: val_loss did not improve from 0.27127\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2380 - acc: 0.9289 - val_loss: 0.3016 - val_acc: 0.9175\n",
            "Epoch 85/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2182 - acc: 0.9372\n",
            "Epoch 00085: val_loss improved from 0.27127 to 0.26522, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.2235 - acc: 0.9336 - val_loss: 0.2652 - val_acc: 0.9250\n",
            "Epoch 86/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1888 - acc: 0.9442\n",
            "Epoch 00086: val_loss improved from 0.26522 to 0.22865, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1887 - acc: 0.9447 - val_loss: 0.2287 - val_acc: 0.9375\n",
            "Epoch 87/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1973 - acc: 0.9428\n",
            "Epoch 00087: val_loss did not improve from 0.22865\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2006 - acc: 0.9411 - val_loss: 0.2572 - val_acc: 0.9250\n",
            "Epoch 88/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1940 - acc: 0.9438\n",
            "Epoch 00088: val_loss did not improve from 0.22865\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1954 - acc: 0.9436 - val_loss: 0.2588 - val_acc: 0.9242\n",
            "Epoch 89/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9463\n",
            "Epoch 00089: val_loss did not improve from 0.22865\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1913 - acc: 0.9464 - val_loss: 0.2390 - val_acc: 0.9325\n",
            "Epoch 90/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2095 - acc: 0.9397\n",
            "Epoch 00090: val_loss did not improve from 0.22865\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2073 - acc: 0.9417 - val_loss: 0.2649 - val_acc: 0.9167\n",
            "Epoch 91/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2105 - acc: 0.9424\n",
            "Epoch 00091: val_loss did not improve from 0.22865\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2087 - acc: 0.9436 - val_loss: 0.2312 - val_acc: 0.9400\n",
            "Epoch 92/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1882 - acc: 0.9470\n",
            "Epoch 00092: val_loss did not improve from 0.22865\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1878 - acc: 0.9478 - val_loss: 0.2744 - val_acc: 0.9225\n",
            "Epoch 93/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2015 - acc: 0.9403\n",
            "Epoch 00093: val_loss did not improve from 0.22865\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1990 - acc: 0.9406 - val_loss: 0.2452 - val_acc: 0.9225\n",
            "Epoch 94/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1879 - acc: 0.9485\n",
            "Epoch 00094: val_loss did not improve from 0.22865\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1894 - acc: 0.9483 - val_loss: 0.2357 - val_acc: 0.9292\n",
            "Epoch 95/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1928 - acc: 0.9420\n",
            "Epoch 00095: val_loss did not improve from 0.22865\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1922 - acc: 0.9425 - val_loss: 0.2451 - val_acc: 0.9250\n",
            "Epoch 96/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9486\n",
            "Epoch 00096: val_loss did not improve from 0.22865\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1833 - acc: 0.9492 - val_loss: 0.2413 - val_acc: 0.9150\n",
            "Epoch 97/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2095 - acc: 0.9337\n",
            "Epoch 00097: val_loss improved from 0.22865 to 0.22785, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.2088 - acc: 0.9339 - val_loss: 0.2278 - val_acc: 0.9325\n",
            "Epoch 98/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1819 - acc: 0.9479\n",
            "Epoch 00098: val_loss did not improve from 0.22785\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1809 - acc: 0.9472 - val_loss: 0.2970 - val_acc: 0.8975\n",
            "Epoch 99/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1790 - acc: 0.9453\n",
            "Epoch 00099: val_loss did not improve from 0.22785\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1797 - acc: 0.9439 - val_loss: 0.2378 - val_acc: 0.9250\n",
            "Epoch 100/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1669 - acc: 0.9521\n",
            "Epoch 00100: val_loss did not improve from 0.22785\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1652 - acc: 0.9531 - val_loss: 0.2671 - val_acc: 0.9192\n",
            "Epoch 101/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1705 - acc: 0.9503\n",
            "Epoch 00101: val_loss did not improve from 0.22785\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1708 - acc: 0.9497 - val_loss: 0.2360 - val_acc: 0.9250\n",
            "Epoch 102/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1646 - acc: 0.9509\n",
            "Epoch 00102: val_loss improved from 0.22785 to 0.21129, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.1654 - acc: 0.9503 - val_loss: 0.2113 - val_acc: 0.9350\n",
            "Epoch 103/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1830 - acc: 0.9458\n",
            "Epoch 00103: val_loss did not improve from 0.21129\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1840 - acc: 0.9458 - val_loss: 0.2344 - val_acc: 0.9333\n",
            "Epoch 104/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1741 - acc: 0.9497\n",
            "Epoch 00104: val_loss improved from 0.21129 to 0.20820, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1744 - acc: 0.9503 - val_loss: 0.2082 - val_acc: 0.9375\n",
            "Epoch 105/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1941 - acc: 0.9411\n",
            "Epoch 00105: val_loss did not improve from 0.20820\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1943 - acc: 0.9408 - val_loss: 0.2144 - val_acc: 0.9333\n",
            "Epoch 106/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1842 - acc: 0.9424\n",
            "Epoch 00106: val_loss did not improve from 0.20820\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1843 - acc: 0.9431 - val_loss: 0.2363 - val_acc: 0.9225\n",
            "Epoch 107/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1769 - acc: 0.9438\n",
            "Epoch 00107: val_loss did not improve from 0.20820\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1747 - acc: 0.9453 - val_loss: 0.2743 - val_acc: 0.9108\n",
            "Epoch 108/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1779 - acc: 0.9470\n",
            "Epoch 00108: val_loss did not improve from 0.20820\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1780 - acc: 0.9469 - val_loss: 0.2906 - val_acc: 0.8950\n",
            "Epoch 109/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.9580\n",
            "Epoch 00109: val_loss did not improve from 0.20820\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1588 - acc: 0.9569 - val_loss: 0.2155 - val_acc: 0.9308\n",
            "Epoch 110/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1686 - acc: 0.9532\n",
            "Epoch 00110: val_loss did not improve from 0.20820\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1710 - acc: 0.9517 - val_loss: 0.2250 - val_acc: 0.9350\n",
            "Epoch 111/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1614 - acc: 0.9550\n",
            "Epoch 00111: val_loss improved from 0.20820 to 0.20688, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1631 - acc: 0.9544 - val_loss: 0.2069 - val_acc: 0.9433\n",
            "Epoch 112/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1782 - acc: 0.9439\n",
            "Epoch 00112: val_loss improved from 0.20688 to 0.19661, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1805 - acc: 0.9433 - val_loss: 0.1966 - val_acc: 0.9400\n",
            "Epoch 113/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1540 - acc: 0.9584\n",
            "Epoch 00113: val_loss improved from 0.19661 to 0.19496, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1567 - acc: 0.9564 - val_loss: 0.1950 - val_acc: 0.9342\n",
            "Epoch 114/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1581 - acc: 0.9558\n",
            "Epoch 00114: val_loss did not improve from 0.19496\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1633 - acc: 0.9528 - val_loss: 0.2016 - val_acc: 0.9367\n",
            "Epoch 115/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1575 - acc: 0.9542\n",
            "Epoch 00115: val_loss did not improve from 0.19496\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1565 - acc: 0.9544 - val_loss: 0.2119 - val_acc: 0.9292\n",
            "Epoch 116/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1764 - acc: 0.9500\n",
            "Epoch 00116: val_loss did not improve from 0.19496\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1748 - acc: 0.9508 - val_loss: 0.2124 - val_acc: 0.9375\n",
            "Epoch 117/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1482 - acc: 0.9603\n",
            "Epoch 00117: val_loss did not improve from 0.19496\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1475 - acc: 0.9611 - val_loss: 0.2214 - val_acc: 0.9275\n",
            "Epoch 118/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1437 - acc: 0.9579\n",
            "Epoch 00118: val_loss did not improve from 0.19496\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1448 - acc: 0.9572 - val_loss: 0.2330 - val_acc: 0.9242\n",
            "Epoch 119/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1575 - acc: 0.9524\n",
            "Epoch 00119: val_loss did not improve from 0.19496\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1578 - acc: 0.9517 - val_loss: 0.2222 - val_acc: 0.9283\n",
            "Epoch 120/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1383 - acc: 0.9626\n",
            "Epoch 00120: val_loss improved from 0.19496 to 0.16373, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1447 - acc: 0.9600 - val_loss: 0.1637 - val_acc: 0.9508\n",
            "Epoch 121/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9583\n",
            "Epoch 00121: val_loss did not improve from 0.16373\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1375 - acc: 0.9578 - val_loss: 0.2839 - val_acc: 0.9067\n",
            "Epoch 122/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1488 - acc: 0.9550\n",
            "Epoch 00122: val_loss did not improve from 0.16373\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1460 - acc: 0.9558 - val_loss: 0.1871 - val_acc: 0.9408\n",
            "Epoch 123/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1426 - acc: 0.9555\n",
            "Epoch 00123: val_loss did not improve from 0.16373\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1414 - acc: 0.9567 - val_loss: 0.2191 - val_acc: 0.9292\n",
            "Epoch 124/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1422 - acc: 0.9597\n",
            "Epoch 00124: val_loss improved from 0.16373 to 0.15804, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.1411 - acc: 0.9597 - val_loss: 0.1580 - val_acc: 0.9617\n",
            "Epoch 125/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1252 - acc: 0.9652\n",
            "Epoch 00125: val_loss improved from 0.15804 to 0.15707, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.1247 - acc: 0.9664 - val_loss: 0.1571 - val_acc: 0.9517\n",
            "Epoch 126/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9551\n",
            "Epoch 00126: val_loss did not improve from 0.15707\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1463 - acc: 0.9553 - val_loss: 0.1962 - val_acc: 0.9433\n",
            "Epoch 127/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1380 - acc: 0.9591\n",
            "Epoch 00127: val_loss did not improve from 0.15707\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1443 - acc: 0.9567 - val_loss: 0.1849 - val_acc: 0.9425\n",
            "Epoch 128/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9486\n",
            "Epoch 00128: val_loss did not improve from 0.15707\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1646 - acc: 0.9494 - val_loss: 0.2021 - val_acc: 0.9417\n",
            "Epoch 129/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9609\n",
            "Epoch 00129: val_loss did not improve from 0.15707\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1359 - acc: 0.9603 - val_loss: 0.1913 - val_acc: 0.9392\n",
            "Epoch 130/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1376 - acc: 0.9573\n",
            "Epoch 00130: val_loss did not improve from 0.15707\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1324 - acc: 0.9597 - val_loss: 0.1854 - val_acc: 0.9433\n",
            "Epoch 131/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1345 - acc: 0.9603\n",
            "Epoch 00131: val_loss did not improve from 0.15707\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1357 - acc: 0.9589 - val_loss: 0.1830 - val_acc: 0.9475\n",
            "Epoch 132/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1285 - acc: 0.9639\n",
            "Epoch 00132: val_loss did not improve from 0.15707\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1303 - acc: 0.9642 - val_loss: 0.1739 - val_acc: 0.9483\n",
            "Epoch 133/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1311 - acc: 0.9609\n",
            "Epoch 00133: val_loss did not improve from 0.15707\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1337 - acc: 0.9600 - val_loss: 0.1755 - val_acc: 0.9517\n",
            "Epoch 134/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1222 - acc: 0.9659\n",
            "Epoch 00134: val_loss did not improve from 0.15707\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1240 - acc: 0.9650 - val_loss: 0.1958 - val_acc: 0.9392\n",
            "Epoch 135/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1497 - acc: 0.9536\n",
            "Epoch 00135: val_loss did not improve from 0.15707\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1460 - acc: 0.9558 - val_loss: 0.1875 - val_acc: 0.9350\n",
            "Epoch 136/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1174 - acc: 0.9647\n",
            "Epoch 00136: val_loss did not improve from 0.15707\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1189 - acc: 0.9644 - val_loss: 0.1758 - val_acc: 0.9475\n",
            "Epoch 137/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9686\n",
            "Epoch 00137: val_loss did not improve from 0.15707\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1226 - acc: 0.9683 - val_loss: 0.1792 - val_acc: 0.9483\n",
            "Epoch 138/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1265 - acc: 0.9614\n",
            "Epoch 00138: val_loss improved from 0.15707 to 0.13333, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1275 - acc: 0.9606 - val_loss: 0.1333 - val_acc: 0.9692\n",
            "Epoch 139/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9631\n",
            "Epoch 00139: val_loss did not improve from 0.13333\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1345 - acc: 0.9628 - val_loss: 0.1472 - val_acc: 0.9592\n",
            "Epoch 140/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1268 - acc: 0.9645\n",
            "Epoch 00140: val_loss did not improve from 0.13333\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1254 - acc: 0.9650 - val_loss: 0.1665 - val_acc: 0.9508\n",
            "Epoch 141/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1035 - acc: 0.9747\n",
            "Epoch 00141: val_loss did not improve from 0.13333\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1092 - acc: 0.9719 - val_loss: 0.1978 - val_acc: 0.9392\n",
            "Epoch 142/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1341 - acc: 0.9545\n",
            "Epoch 00142: val_loss did not improve from 0.13333\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1362 - acc: 0.9550 - val_loss: 0.1922 - val_acc: 0.9383\n",
            "Epoch 143/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9617\n",
            "Epoch 00143: val_loss did not improve from 0.13333\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1299 - acc: 0.9619 - val_loss: 0.1506 - val_acc: 0.9575\n",
            "Epoch 144/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1396 - acc: 0.9561\n",
            "Epoch 00144: val_loss did not improve from 0.13333\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1392 - acc: 0.9553 - val_loss: 0.1972 - val_acc: 0.9442\n",
            "Epoch 145/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1148 - acc: 0.9676\n",
            "Epoch 00145: val_loss did not improve from 0.13333\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1142 - acc: 0.9669 - val_loss: 0.1521 - val_acc: 0.9608\n",
            "Epoch 146/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1211 - acc: 0.9653\n",
            "Epoch 00146: val_loss did not improve from 0.13333\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1227 - acc: 0.9644 - val_loss: 0.1773 - val_acc: 0.9450\n",
            "Epoch 147/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1243 - acc: 0.9661\n",
            "Epoch 00147: val_loss did not improve from 0.13333\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1257 - acc: 0.9653 - val_loss: 0.1553 - val_acc: 0.9525\n",
            "Epoch 148/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9560\n",
            "Epoch 00148: val_loss did not improve from 0.13333\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1362 - acc: 0.9561 - val_loss: 0.1457 - val_acc: 0.9583\n",
            "Epoch 149/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9589\n",
            "Epoch 00149: val_loss did not improve from 0.13333\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1331 - acc: 0.9592 - val_loss: 0.1872 - val_acc: 0.9400\n",
            "Epoch 150/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1154 - acc: 0.9694\n",
            "Epoch 00150: val_loss did not improve from 0.13333\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1198 - acc: 0.9667 - val_loss: 0.1559 - val_acc: 0.9550\n",
            "Epoch 151/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9677\n",
            "Epoch 00151: val_loss did not improve from 0.13333\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1187 - acc: 0.9675 - val_loss: 0.1660 - val_acc: 0.9525\n",
            "Epoch 152/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9554\n",
            "Epoch 00152: val_loss did not improve from 0.13333\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1476 - acc: 0.9553 - val_loss: 0.1956 - val_acc: 0.9433\n",
            "Epoch 153/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9623\n",
            "Epoch 00153: val_loss did not improve from 0.13333\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1309 - acc: 0.9622 - val_loss: 0.1601 - val_acc: 0.9558\n",
            "Epoch 154/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1107 - acc: 0.9715\n",
            "Epoch 00154: val_loss did not improve from 0.13333\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1146 - acc: 0.9697 - val_loss: 0.1539 - val_acc: 0.9608\n",
            "Epoch 155/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1200 - acc: 0.9618\n",
            "Epoch 00155: val_loss did not improve from 0.13333\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1229 - acc: 0.9606 - val_loss: 0.1601 - val_acc: 0.9450\n",
            "Epoch 156/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1201 - acc: 0.9636\n",
            "Epoch 00156: val_loss improved from 0.13333 to 0.13162, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1186 - acc: 0.9636 - val_loss: 0.1316 - val_acc: 0.9625\n",
            "Epoch 157/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9743\n",
            "Epoch 00157: val_loss did not improve from 0.13162\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1034 - acc: 0.9742 - val_loss: 0.1798 - val_acc: 0.9467\n",
            "Epoch 158/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9683\n",
            "Epoch 00158: val_loss did not improve from 0.13162\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1082 - acc: 0.9686 - val_loss: 0.1690 - val_acc: 0.9533\n",
            "Epoch 159/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9657\n",
            "Epoch 00159: val_loss did not improve from 0.13162\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1142 - acc: 0.9658 - val_loss: 0.1943 - val_acc: 0.9392\n",
            "Epoch 160/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9617\n",
            "Epoch 00160: val_loss did not improve from 0.13162\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1252 - acc: 0.9622 - val_loss: 0.1659 - val_acc: 0.9542\n",
            "Epoch 161/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1134 - acc: 0.9691\n",
            "Epoch 00161: val_loss did not improve from 0.13162\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1131 - acc: 0.9689 - val_loss: 0.1403 - val_acc: 0.9542\n",
            "Epoch 162/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9674\n",
            "Epoch 00162: val_loss did not improve from 0.13162\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1195 - acc: 0.9667 - val_loss: 0.1602 - val_acc: 0.9558\n",
            "Epoch 163/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1306 - acc: 0.9597\n",
            "Epoch 00163: val_loss did not improve from 0.13162\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1305 - acc: 0.9594 - val_loss: 0.1521 - val_acc: 0.9600\n",
            "Epoch 164/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1229 - acc: 0.9609\n",
            "Epoch 00164: val_loss did not improve from 0.13162\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1221 - acc: 0.9622 - val_loss: 0.1653 - val_acc: 0.9525\n",
            "Epoch 165/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1128 - acc: 0.9697\n",
            "Epoch 00165: val_loss did not improve from 0.13162\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1205 - acc: 0.9650 - val_loss: 0.1537 - val_acc: 0.9508\n",
            "Epoch 166/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1370 - acc: 0.9565\n",
            "Epoch 00166: val_loss did not improve from 0.13162\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1366 - acc: 0.9572 - val_loss: 0.1722 - val_acc: 0.9450\n",
            "Epoch 167/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1168 - acc: 0.9641\n",
            "Epoch 00167: val_loss did not improve from 0.13162\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1200 - acc: 0.9625 - val_loss: 0.1360 - val_acc: 0.9575\n",
            "Epoch 168/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1159 - acc: 0.9678\n",
            "Epoch 00168: val_loss did not improve from 0.13162\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1165 - acc: 0.9658 - val_loss: 0.1383 - val_acc: 0.9608\n",
            "Epoch 169/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1316 - acc: 0.9603\n",
            "Epoch 00169: val_loss did not improve from 0.13162\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1284 - acc: 0.9614 - val_loss: 0.1462 - val_acc: 0.9608\n",
            "Epoch 170/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1165 - acc: 0.9650\n",
            "Epoch 00170: val_loss did not improve from 0.13162\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1159 - acc: 0.9658 - val_loss: 0.1493 - val_acc: 0.9567\n",
            "Epoch 171/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1040 - acc: 0.9678\n",
            "Epoch 00171: val_loss did not improve from 0.13162\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1045 - acc: 0.9675 - val_loss: 0.1621 - val_acc: 0.9450\n",
            "Epoch 172/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1065 - acc: 0.9715\n",
            "Epoch 00172: val_loss did not improve from 0.13162\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1051 - acc: 0.9719 - val_loss: 0.1563 - val_acc: 0.9525\n",
            "Epoch 173/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9714\n",
            "Epoch 00173: val_loss did not improve from 0.13162\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1036 - acc: 0.9717 - val_loss: 0.1485 - val_acc: 0.9508\n",
            "Epoch 174/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1053 - acc: 0.9682\n",
            "Epoch 00174: val_loss improved from 0.13162 to 0.12661, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1058 - acc: 0.9681 - val_loss: 0.1266 - val_acc: 0.9667\n",
            "Epoch 175/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1027 - acc: 0.9709\n",
            "Epoch 00175: val_loss did not improve from 0.12661\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1037 - acc: 0.9697 - val_loss: 0.1328 - val_acc: 0.9617\n",
            "Epoch 176/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1007 - acc: 0.9728\n",
            "Epoch 00176: val_loss improved from 0.12661 to 0.09911, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1004 - acc: 0.9728 - val_loss: 0.0991 - val_acc: 0.9725\n",
            "Epoch 177/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1086 - acc: 0.9647\n",
            "Epoch 00177: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1103 - acc: 0.9644 - val_loss: 0.1442 - val_acc: 0.9533\n",
            "Epoch 178/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1074 - acc: 0.9706\n",
            "Epoch 00178: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1076 - acc: 0.9708 - val_loss: 0.1523 - val_acc: 0.9533\n",
            "Epoch 179/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1159 - acc: 0.9641\n",
            "Epoch 00179: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1173 - acc: 0.9628 - val_loss: 0.1257 - val_acc: 0.9642\n",
            "Epoch 180/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1284 - acc: 0.9619\n",
            "Epoch 00180: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1267 - acc: 0.9625 - val_loss: 0.1508 - val_acc: 0.9533\n",
            "Epoch 181/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1269 - acc: 0.9627\n",
            "Epoch 00181: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1246 - acc: 0.9639 - val_loss: 0.1431 - val_acc: 0.9583\n",
            "Epoch 182/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1130 - acc: 0.9664\n",
            "Epoch 00182: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1121 - acc: 0.9672 - val_loss: 0.1410 - val_acc: 0.9617\n",
            "Epoch 183/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1100 - acc: 0.9697\n",
            "Epoch 00183: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1103 - acc: 0.9694 - val_loss: 0.2065 - val_acc: 0.9367\n",
            "Epoch 184/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1250 - acc: 0.9615\n",
            "Epoch 00184: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1273 - acc: 0.9614 - val_loss: 0.1396 - val_acc: 0.9550\n",
            "Epoch 185/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9609\n",
            "Epoch 00185: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1305 - acc: 0.9606 - val_loss: 0.1357 - val_acc: 0.9650\n",
            "Epoch 186/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1114 - acc: 0.9685\n",
            "Epoch 00186: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1154 - acc: 0.9664 - val_loss: 0.1385 - val_acc: 0.9558\n",
            "Epoch 187/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1081 - acc: 0.9700\n",
            "Epoch 00187: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1086 - acc: 0.9689 - val_loss: 0.1437 - val_acc: 0.9567\n",
            "Epoch 188/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1180 - acc: 0.9618\n",
            "Epoch 00188: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1198 - acc: 0.9611 - val_loss: 0.1460 - val_acc: 0.9575\n",
            "Epoch 189/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1067 - acc: 0.9659\n",
            "Epoch 00189: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1075 - acc: 0.9653 - val_loss: 0.1314 - val_acc: 0.9592\n",
            "Epoch 190/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1072 - acc: 0.9688\n",
            "Epoch 00190: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1067 - acc: 0.9692 - val_loss: 0.1583 - val_acc: 0.9500\n",
            "Epoch 191/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9666\n",
            "Epoch 00191: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1071 - acc: 0.9669 - val_loss: 0.1549 - val_acc: 0.9492\n",
            "Epoch 192/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9697\n",
            "Epoch 00192: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1094 - acc: 0.9697 - val_loss: 0.1491 - val_acc: 0.9508\n",
            "Epoch 193/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9731\n",
            "Epoch 00193: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1056 - acc: 0.9733 - val_loss: 0.1237 - val_acc: 0.9683\n",
            "Epoch 194/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9769\n",
            "Epoch 00194: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0905 - acc: 0.9769 - val_loss: 0.1427 - val_acc: 0.9525\n",
            "Epoch 195/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1062 - acc: 0.9667\n",
            "Epoch 00195: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1064 - acc: 0.9658 - val_loss: 0.1016 - val_acc: 0.9717\n",
            "Epoch 196/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1044 - acc: 0.9685\n",
            "Epoch 00196: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1079 - acc: 0.9681 - val_loss: 0.1235 - val_acc: 0.9625\n",
            "Epoch 197/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9691\n",
            "Epoch 00197: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1129 - acc: 0.9692 - val_loss: 0.1123 - val_acc: 0.9708\n",
            "Epoch 198/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1132 - acc: 0.9700\n",
            "Epoch 00198: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1115 - acc: 0.9706 - val_loss: 0.1137 - val_acc: 0.9692\n",
            "Epoch 199/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9686\n",
            "Epoch 00199: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1007 - acc: 0.9686 - val_loss: 0.1143 - val_acc: 0.9683\n",
            "Epoch 200/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1101 - acc: 0.9697\n",
            "Epoch 00200: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1107 - acc: 0.9694 - val_loss: 0.1108 - val_acc: 0.9700\n",
            "Epoch 201/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0923 - acc: 0.9748\n",
            "Epoch 00201: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0930 - acc: 0.9744 - val_loss: 0.1091 - val_acc: 0.9692\n",
            "Epoch 202/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9666\n",
            "Epoch 00202: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1067 - acc: 0.9664 - val_loss: 0.1085 - val_acc: 0.9658\n",
            "Epoch 203/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1472 - acc: 0.9481\n",
            "Epoch 00203: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1473 - acc: 0.9494 - val_loss: 0.2034 - val_acc: 0.9275\n",
            "Epoch 204/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1143 - acc: 0.9653\n",
            "Epoch 00204: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1192 - acc: 0.9633 - val_loss: 0.1260 - val_acc: 0.9650\n",
            "Epoch 205/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9620\n",
            "Epoch 00205: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1191 - acc: 0.9625 - val_loss: 0.1368 - val_acc: 0.9575\n",
            "Epoch 206/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9714\n",
            "Epoch 00206: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0997 - acc: 0.9711 - val_loss: 0.1551 - val_acc: 0.9517\n",
            "Epoch 207/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9723\n",
            "Epoch 00207: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1038 - acc: 0.9722 - val_loss: 0.1224 - val_acc: 0.9658\n",
            "Epoch 208/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1008 - acc: 0.9706\n",
            "Epoch 00208: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1015 - acc: 0.9703 - val_loss: 0.1121 - val_acc: 0.9750\n",
            "Epoch 209/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1013 - acc: 0.9676\n",
            "Epoch 00209: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1004 - acc: 0.9686 - val_loss: 0.1633 - val_acc: 0.9425\n",
            "Epoch 210/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1124 - acc: 0.9662\n",
            "Epoch 00210: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1134 - acc: 0.9650 - val_loss: 0.1176 - val_acc: 0.9642\n",
            "Epoch 211/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1176 - acc: 0.9638\n",
            "Epoch 00211: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1167 - acc: 0.9633 - val_loss: 0.1302 - val_acc: 0.9583\n",
            "Epoch 212/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1179 - acc: 0.9638\n",
            "Epoch 00212: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1148 - acc: 0.9661 - val_loss: 0.1249 - val_acc: 0.9633\n",
            "Epoch 213/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1090 - acc: 0.9679\n",
            "Epoch 00213: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1101 - acc: 0.9681 - val_loss: 0.1186 - val_acc: 0.9700\n",
            "Epoch 214/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1068 - acc: 0.9674\n",
            "Epoch 00214: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1051 - acc: 0.9678 - val_loss: 0.1427 - val_acc: 0.9550\n",
            "Epoch 215/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9757\n",
            "Epoch 00215: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0909 - acc: 0.9753 - val_loss: 0.1245 - val_acc: 0.9625\n",
            "Epoch 216/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9666\n",
            "Epoch 00216: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1061 - acc: 0.9658 - val_loss: 0.1712 - val_acc: 0.9392\n",
            "Epoch 217/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9717\n",
            "Epoch 00217: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0947 - acc: 0.9719 - val_loss: 0.1177 - val_acc: 0.9625\n",
            "Epoch 218/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0905 - acc: 0.9737\n",
            "Epoch 00218: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0896 - acc: 0.9750 - val_loss: 0.1154 - val_acc: 0.9650\n",
            "Epoch 219/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0967 - acc: 0.9750\n",
            "Epoch 00219: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0988 - acc: 0.9731 - val_loss: 0.1087 - val_acc: 0.9658\n",
            "Epoch 220/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0984 - acc: 0.9724\n",
            "Epoch 00220: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0980 - acc: 0.9722 - val_loss: 0.1416 - val_acc: 0.9608\n",
            "Epoch 221/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1075 - acc: 0.9650\n",
            "Epoch 00221: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.1067 - acc: 0.9653 - val_loss: 0.1194 - val_acc: 0.9625\n",
            "Epoch 222/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1048 - acc: 0.9691\n",
            "Epoch 00222: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1057 - acc: 0.9686 - val_loss: 0.1164 - val_acc: 0.9650\n",
            "Epoch 223/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9706\n",
            "Epoch 00223: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1021 - acc: 0.9706 - val_loss: 0.1078 - val_acc: 0.9642\n",
            "Epoch 224/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0946 - acc: 0.9739\n",
            "Epoch 00224: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0973 - acc: 0.9725 - val_loss: 0.1111 - val_acc: 0.9658\n",
            "Epoch 225/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0910 - acc: 0.9759\n",
            "Epoch 00225: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0917 - acc: 0.9756 - val_loss: 0.1090 - val_acc: 0.9725\n",
            "Epoch 226/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0946 - acc: 0.9756\n",
            "Epoch 00226: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0959 - acc: 0.9744 - val_loss: 0.1298 - val_acc: 0.9550\n",
            "Epoch 227/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0886 - acc: 0.9750\n",
            "Epoch 00227: val_loss did not improve from 0.09911\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0870 - acc: 0.9756 - val_loss: 0.1275 - val_acc: 0.9617\n",
            "Epoch 228/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0878 - acc: 0.9750\n",
            "Epoch 00228: val_loss improved from 0.09911 to 0.09861, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.0871 - acc: 0.9750 - val_loss: 0.0986 - val_acc: 0.9717\n",
            "Epoch 229/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9777\n",
            "Epoch 00229: val_loss did not improve from 0.09861\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0868 - acc: 0.9778 - val_loss: 0.1012 - val_acc: 0.9733\n",
            "Epoch 230/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0958 - acc: 0.9741\n",
            "Epoch 00230: val_loss did not improve from 0.09861\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0957 - acc: 0.9742 - val_loss: 0.1043 - val_acc: 0.9700\n",
            "Epoch 231/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1050 - acc: 0.9694\n",
            "Epoch 00231: val_loss did not improve from 0.09861\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1055 - acc: 0.9697 - val_loss: 0.1093 - val_acc: 0.9633\n",
            "Epoch 232/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9703\n",
            "Epoch 00232: val_loss improved from 0.09861 to 0.09503, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.1019 - acc: 0.9700 - val_loss: 0.0950 - val_acc: 0.9725\n",
            "Epoch 233/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0907 - acc: 0.9724\n",
            "Epoch 00233: val_loss did not improve from 0.09503\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0907 - acc: 0.9717 - val_loss: 0.1201 - val_acc: 0.9625\n",
            "Epoch 234/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0845 - acc: 0.9776\n",
            "Epoch 00234: val_loss improved from 0.09503 to 0.08422, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.0860 - acc: 0.9767 - val_loss: 0.0842 - val_acc: 0.9792\n",
            "Epoch 235/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0848 - acc: 0.9750\n",
            "Epoch 00235: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0842 - acc: 0.9750 - val_loss: 0.1498 - val_acc: 0.9592\n",
            "Epoch 236/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1081 - acc: 0.9671\n",
            "Epoch 00236: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1090 - acc: 0.9664 - val_loss: 0.1493 - val_acc: 0.9483\n",
            "Epoch 237/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.9657\n",
            "Epoch 00237: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1075 - acc: 0.9656 - val_loss: 0.1757 - val_acc: 0.9408\n",
            "Epoch 238/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0989 - acc: 0.9726\n",
            "Epoch 00238: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0992 - acc: 0.9725 - val_loss: 0.1002 - val_acc: 0.9775\n",
            "Epoch 239/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9720\n",
            "Epoch 00239: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0892 - acc: 0.9725 - val_loss: 0.1025 - val_acc: 0.9717\n",
            "Epoch 240/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0936 - acc: 0.9737\n",
            "Epoch 00240: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0989 - acc: 0.9719 - val_loss: 0.1589 - val_acc: 0.9500\n",
            "Epoch 241/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0928 - acc: 0.9731\n",
            "Epoch 00241: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0933 - acc: 0.9714 - val_loss: 0.1106 - val_acc: 0.9683\n",
            "Epoch 242/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1037 - acc: 0.9662\n",
            "Epoch 00242: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1028 - acc: 0.9667 - val_loss: 0.1008 - val_acc: 0.9700\n",
            "Epoch 243/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0985 - acc: 0.9725\n",
            "Epoch 00243: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0980 - acc: 0.9731 - val_loss: 0.1005 - val_acc: 0.9700\n",
            "Epoch 244/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9694\n",
            "Epoch 00244: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1033 - acc: 0.9697 - val_loss: 0.0972 - val_acc: 0.9700\n",
            "Epoch 245/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0825 - acc: 0.9782\n",
            "Epoch 00245: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0825 - acc: 0.9772 - val_loss: 0.1273 - val_acc: 0.9583\n",
            "Epoch 246/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0840 - acc: 0.9761\n",
            "Epoch 00246: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0854 - acc: 0.9750 - val_loss: 0.1119 - val_acc: 0.9675\n",
            "Epoch 247/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9760\n",
            "Epoch 00247: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0872 - acc: 0.9764 - val_loss: 0.1041 - val_acc: 0.9692\n",
            "Epoch 248/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0884 - acc: 0.9725\n",
            "Epoch 00248: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0875 - acc: 0.9722 - val_loss: 0.1114 - val_acc: 0.9667\n",
            "Epoch 249/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0878 - acc: 0.9742\n",
            "Epoch 00249: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0870 - acc: 0.9744 - val_loss: 0.1004 - val_acc: 0.9700\n",
            "Epoch 250/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0773 - acc: 0.9797\n",
            "Epoch 00250: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0793 - acc: 0.9786 - val_loss: 0.1336 - val_acc: 0.9608\n",
            "Epoch 251/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0816 - acc: 0.9768\n",
            "Epoch 00251: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0849 - acc: 0.9742 - val_loss: 0.1261 - val_acc: 0.9608\n",
            "Epoch 252/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0822 - acc: 0.9744\n",
            "Epoch 00252: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0812 - acc: 0.9753 - val_loss: 0.1053 - val_acc: 0.9700\n",
            "Epoch 253/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0865 - acc: 0.9779\n",
            "Epoch 00253: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0868 - acc: 0.9772 - val_loss: 0.0856 - val_acc: 0.9758\n",
            "Epoch 254/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1085 - acc: 0.9647\n",
            "Epoch 00254: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1090 - acc: 0.9639 - val_loss: 0.1272 - val_acc: 0.9542\n",
            "Epoch 255/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9691\n",
            "Epoch 00255: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0987 - acc: 0.9692 - val_loss: 0.0990 - val_acc: 0.9708\n",
            "Epoch 256/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9720\n",
            "Epoch 00256: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0887 - acc: 0.9714 - val_loss: 0.1039 - val_acc: 0.9692\n",
            "Epoch 257/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9763\n",
            "Epoch 00257: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0858 - acc: 0.9753 - val_loss: 0.1355 - val_acc: 0.9592\n",
            "Epoch 258/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0907 - acc: 0.9724\n",
            "Epoch 00258: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0897 - acc: 0.9722 - val_loss: 0.1025 - val_acc: 0.9683\n",
            "Epoch 259/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0873 - acc: 0.9750\n",
            "Epoch 00259: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0867 - acc: 0.9756 - val_loss: 0.1212 - val_acc: 0.9642\n",
            "Epoch 260/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0820 - acc: 0.9765\n",
            "Epoch 00260: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0853 - acc: 0.9750 - val_loss: 0.0984 - val_acc: 0.9725\n",
            "Epoch 261/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1027 - acc: 0.9662\n",
            "Epoch 00261: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1037 - acc: 0.9661 - val_loss: 0.1195 - val_acc: 0.9592\n",
            "Epoch 262/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0980 - acc: 0.9719\n",
            "Epoch 00262: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1006 - acc: 0.9711 - val_loss: 0.1040 - val_acc: 0.9667\n",
            "Epoch 263/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1082 - acc: 0.9697\n",
            "Epoch 00263: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1076 - acc: 0.9694 - val_loss: 0.1556 - val_acc: 0.9458\n",
            "Epoch 264/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9703\n",
            "Epoch 00264: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1011 - acc: 0.9700 - val_loss: 0.1297 - val_acc: 0.9583\n",
            "Epoch 265/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0961 - acc: 0.9724\n",
            "Epoch 00265: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0956 - acc: 0.9731 - val_loss: 0.1012 - val_acc: 0.9658\n",
            "Epoch 266/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9774\n",
            "Epoch 00266: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0852 - acc: 0.9758 - val_loss: 0.1318 - val_acc: 0.9575\n",
            "Epoch 267/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0967 - acc: 0.9739\n",
            "Epoch 00267: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0973 - acc: 0.9736 - val_loss: 0.0909 - val_acc: 0.9700\n",
            "Epoch 268/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1092 - acc: 0.9642\n",
            "Epoch 00268: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1099 - acc: 0.9644 - val_loss: 0.1062 - val_acc: 0.9650\n",
            "Epoch 269/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0944 - acc: 0.9741\n",
            "Epoch 00269: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0969 - acc: 0.9739 - val_loss: 0.1209 - val_acc: 0.9625\n",
            "Epoch 270/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0918 - acc: 0.9764\n",
            "Epoch 00270: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0931 - acc: 0.9756 - val_loss: 0.1220 - val_acc: 0.9642\n",
            "Epoch 271/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0867 - acc: 0.9747\n",
            "Epoch 00271: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0854 - acc: 0.9753 - val_loss: 0.0854 - val_acc: 0.9775\n",
            "Epoch 272/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1051 - acc: 0.9688\n",
            "Epoch 00272: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1056 - acc: 0.9683 - val_loss: 0.1323 - val_acc: 0.9550\n",
            "Epoch 273/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0935 - acc: 0.9706\n",
            "Epoch 00273: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0932 - acc: 0.9711 - val_loss: 0.1068 - val_acc: 0.9758\n",
            "Epoch 274/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0941 - acc: 0.9712\n",
            "Epoch 00274: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0964 - acc: 0.9703 - val_loss: 0.1257 - val_acc: 0.9608\n",
            "Epoch 275/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9723\n",
            "Epoch 00275: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0983 - acc: 0.9728 - val_loss: 0.1057 - val_acc: 0.9650\n",
            "Epoch 276/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1042 - acc: 0.9712\n",
            "Epoch 00276: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1066 - acc: 0.9706 - val_loss: 0.1126 - val_acc: 0.9650\n",
            "Epoch 277/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9700\n",
            "Epoch 00277: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0947 - acc: 0.9703 - val_loss: 0.1004 - val_acc: 0.9725\n",
            "Epoch 278/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0850 - acc: 0.9753\n",
            "Epoch 00278: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0864 - acc: 0.9742 - val_loss: 0.0934 - val_acc: 0.9758\n",
            "Epoch 279/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0856 - acc: 0.9748\n",
            "Epoch 00279: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0850 - acc: 0.9753 - val_loss: 0.1174 - val_acc: 0.9633\n",
            "Epoch 280/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9726\n",
            "Epoch 00280: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0927 - acc: 0.9719 - val_loss: 0.1017 - val_acc: 0.9758\n",
            "Epoch 281/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0959 - acc: 0.9706\n",
            "Epoch 00281: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0973 - acc: 0.9703 - val_loss: 0.1293 - val_acc: 0.9592\n",
            "Epoch 282/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0828 - acc: 0.9748\n",
            "Epoch 00282: val_loss did not improve from 0.08422\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0808 - acc: 0.9758 - val_loss: 0.1081 - val_acc: 0.9683\n",
            "Epoch 283/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0699 - acc: 0.9803\n",
            "Epoch 00283: val_loss improved from 0.08422 to 0.08354, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.0728 - acc: 0.9789 - val_loss: 0.0835 - val_acc: 0.9767\n",
            "Epoch 284/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9811\n",
            "Epoch 00284: val_loss did not improve from 0.08354\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0794 - acc: 0.9811 - val_loss: 0.1224 - val_acc: 0.9642\n",
            "Epoch 285/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9717\n",
            "Epoch 00285: val_loss did not improve from 0.08354\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0895 - acc: 0.9719 - val_loss: 0.0872 - val_acc: 0.9783\n",
            "Epoch 286/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9731\n",
            "Epoch 00286: val_loss improved from 0.08354 to 0.07782, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.0913 - acc: 0.9736 - val_loss: 0.0778 - val_acc: 0.9800\n",
            "Epoch 287/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0746 - acc: 0.9787\n",
            "Epoch 00287: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0794 - acc: 0.9775 - val_loss: 0.1102 - val_acc: 0.9658\n",
            "Epoch 288/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0835 - acc: 0.9759\n",
            "Epoch 00288: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0828 - acc: 0.9769 - val_loss: 0.1164 - val_acc: 0.9633\n",
            "Epoch 289/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9780\n",
            "Epoch 00289: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0766 - acc: 0.9778 - val_loss: 0.0994 - val_acc: 0.9675\n",
            "Epoch 290/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9769\n",
            "Epoch 00290: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0855 - acc: 0.9772 - val_loss: 0.0954 - val_acc: 0.9733\n",
            "Epoch 291/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9711\n",
            "Epoch 00291: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0860 - acc: 0.9717 - val_loss: 0.1370 - val_acc: 0.9575\n",
            "Epoch 292/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1005 - acc: 0.9700\n",
            "Epoch 00292: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0993 - acc: 0.9700 - val_loss: 0.1190 - val_acc: 0.9583\n",
            "Epoch 293/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9734\n",
            "Epoch 00293: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0897 - acc: 0.9733 - val_loss: 0.1247 - val_acc: 0.9583\n",
            "Epoch 294/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9754\n",
            "Epoch 00294: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0870 - acc: 0.9758 - val_loss: 0.0907 - val_acc: 0.9733\n",
            "Epoch 295/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9683\n",
            "Epoch 00295: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1031 - acc: 0.9683 - val_loss: 0.1127 - val_acc: 0.9658\n",
            "Epoch 296/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1173 - acc: 0.9621\n",
            "Epoch 00296: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1166 - acc: 0.9619 - val_loss: 0.1195 - val_acc: 0.9650\n",
            "Epoch 297/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9711\n",
            "Epoch 00297: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0970 - acc: 0.9708 - val_loss: 0.1143 - val_acc: 0.9658\n",
            "Epoch 298/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9791\n",
            "Epoch 00298: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0793 - acc: 0.9789 - val_loss: 0.1012 - val_acc: 0.9700\n",
            "Epoch 299/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1101 - acc: 0.9656\n",
            "Epoch 00299: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1094 - acc: 0.9667 - val_loss: 0.0891 - val_acc: 0.9750\n",
            "Epoch 300/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0873 - acc: 0.9756\n",
            "Epoch 00300: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0902 - acc: 0.9744 - val_loss: 0.1006 - val_acc: 0.9758\n",
            "Epoch 301/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9714\n",
            "Epoch 00301: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0851 - acc: 0.9708 - val_loss: 0.1246 - val_acc: 0.9642\n",
            "Epoch 302/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0761 - acc: 0.9772\n",
            "Epoch 00302: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0785 - acc: 0.9761 - val_loss: 0.0833 - val_acc: 0.9800\n",
            "Epoch 303/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0870 - acc: 0.9726\n",
            "Epoch 00303: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0869 - acc: 0.9722 - val_loss: 0.1060 - val_acc: 0.9725\n",
            "Epoch 304/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0844 - acc: 0.9756\n",
            "Epoch 00304: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0857 - acc: 0.9750 - val_loss: 0.1211 - val_acc: 0.9667\n",
            "Epoch 305/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0808 - acc: 0.9750\n",
            "Epoch 00305: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0807 - acc: 0.9747 - val_loss: 0.0955 - val_acc: 0.9692\n",
            "Epoch 306/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0836 - acc: 0.9742\n",
            "Epoch 00306: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0818 - acc: 0.9753 - val_loss: 0.1033 - val_acc: 0.9708\n",
            "Epoch 307/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9766\n",
            "Epoch 00307: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0809 - acc: 0.9767 - val_loss: 0.1076 - val_acc: 0.9667\n",
            "Epoch 308/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0793 - acc: 0.9773\n",
            "Epoch 00308: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0802 - acc: 0.9761 - val_loss: 0.0848 - val_acc: 0.9758\n",
            "Epoch 309/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9769\n",
            "Epoch 00309: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0776 - acc: 0.9767 - val_loss: 0.1062 - val_acc: 0.9733\n",
            "Epoch 310/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0833 - acc: 0.9764\n",
            "Epoch 00310: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0829 - acc: 0.9772 - val_loss: 0.0986 - val_acc: 0.9742\n",
            "Epoch 311/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9774\n",
            "Epoch 00311: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0736 - acc: 0.9781 - val_loss: 0.0951 - val_acc: 0.9750\n",
            "Epoch 312/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0869 - acc: 0.9712\n",
            "Epoch 00312: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0873 - acc: 0.9714 - val_loss: 0.0951 - val_acc: 0.9758\n",
            "Epoch 313/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0873 - acc: 0.9729\n",
            "Epoch 00313: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0881 - acc: 0.9722 - val_loss: 0.0930 - val_acc: 0.9758\n",
            "Epoch 314/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0721 - acc: 0.9785\n",
            "Epoch 00314: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0716 - acc: 0.9786 - val_loss: 0.0971 - val_acc: 0.9717\n",
            "Epoch 315/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0692 - acc: 0.9809\n",
            "Epoch 00315: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0712 - acc: 0.9806 - val_loss: 0.1099 - val_acc: 0.9650\n",
            "Epoch 316/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0721 - acc: 0.9803\n",
            "Epoch 00316: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0718 - acc: 0.9803 - val_loss: 0.0824 - val_acc: 0.9783\n",
            "Epoch 317/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0701 - acc: 0.9821\n",
            "Epoch 00317: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0694 - acc: 0.9825 - val_loss: 0.1111 - val_acc: 0.9642\n",
            "Epoch 318/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0679 - acc: 0.9800\n",
            "Epoch 00318: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0693 - acc: 0.9797 - val_loss: 0.0862 - val_acc: 0.9717\n",
            "Epoch 319/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0751 - acc: 0.9782\n",
            "Epoch 00319: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0753 - acc: 0.9781 - val_loss: 0.1129 - val_acc: 0.9650\n",
            "Epoch 320/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0839 - acc: 0.9727\n",
            "Epoch 00320: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0841 - acc: 0.9722 - val_loss: 0.1018 - val_acc: 0.9733\n",
            "Epoch 321/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0782 - acc: 0.9761\n",
            "Epoch 00321: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0796 - acc: 0.9758 - val_loss: 0.0974 - val_acc: 0.9667\n",
            "Epoch 322/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0905 - acc: 0.9742\n",
            "Epoch 00322: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0921 - acc: 0.9731 - val_loss: 0.0967 - val_acc: 0.9700\n",
            "Epoch 323/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0864 - acc: 0.9752\n",
            "Epoch 00323: val_loss did not improve from 0.07782\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0851 - acc: 0.9761 - val_loss: 0.1010 - val_acc: 0.9667\n",
            "Epoch 324/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9734\n",
            "Epoch 00324: val_loss improved from 0.07782 to 0.07597, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.0905 - acc: 0.9736 - val_loss: 0.0760 - val_acc: 0.9792\n",
            "Epoch 325/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0879 - acc: 0.9745\n",
            "Epoch 00325: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0898 - acc: 0.9728 - val_loss: 0.1056 - val_acc: 0.9683\n",
            "Epoch 326/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0998 - acc: 0.9721\n",
            "Epoch 00326: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0999 - acc: 0.9719 - val_loss: 0.1340 - val_acc: 0.9525\n",
            "Epoch 327/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0836 - acc: 0.9758\n",
            "Epoch 00327: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0854 - acc: 0.9747 - val_loss: 0.1064 - val_acc: 0.9692\n",
            "Epoch 328/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0876 - acc: 0.9744\n",
            "Epoch 00328: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0843 - acc: 0.9758 - val_loss: 0.0770 - val_acc: 0.9825\n",
            "Epoch 329/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0715 - acc: 0.9779\n",
            "Epoch 00329: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0736 - acc: 0.9775 - val_loss: 0.1037 - val_acc: 0.9650\n",
            "Epoch 330/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9771\n",
            "Epoch 00330: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0788 - acc: 0.9775 - val_loss: 0.0938 - val_acc: 0.9742\n",
            "Epoch 331/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9697\n",
            "Epoch 00331: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1030 - acc: 0.9694 - val_loss: 0.0837 - val_acc: 0.9758\n",
            "Epoch 332/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0969 - acc: 0.9694\n",
            "Epoch 00332: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0956 - acc: 0.9706 - val_loss: 0.0901 - val_acc: 0.9775\n",
            "Epoch 333/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0782 - acc: 0.9788\n",
            "Epoch 00333: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0778 - acc: 0.9792 - val_loss: 0.0854 - val_acc: 0.9767\n",
            "Epoch 334/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9774\n",
            "Epoch 00334: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0766 - acc: 0.9775 - val_loss: 0.0972 - val_acc: 0.9725\n",
            "Epoch 335/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0716 - acc: 0.9791\n",
            "Epoch 00335: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0697 - acc: 0.9803 - val_loss: 0.0904 - val_acc: 0.9683\n",
            "Epoch 336/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0807 - acc: 0.9755\n",
            "Epoch 00336: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0796 - acc: 0.9761 - val_loss: 0.0940 - val_acc: 0.9758\n",
            "Epoch 337/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9774\n",
            "Epoch 00337: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0781 - acc: 0.9778 - val_loss: 0.1032 - val_acc: 0.9650\n",
            "Epoch 338/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0758 - acc: 0.9794\n",
            "Epoch 00338: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0762 - acc: 0.9783 - val_loss: 0.1217 - val_acc: 0.9608\n",
            "Epoch 339/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0903 - acc: 0.9718\n",
            "Epoch 00339: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0880 - acc: 0.9731 - val_loss: 0.1318 - val_acc: 0.9600\n",
            "Epoch 340/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9831\n",
            "Epoch 00340: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0642 - acc: 0.9825 - val_loss: 0.0812 - val_acc: 0.9750\n",
            "Epoch 341/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9777\n",
            "Epoch 00341: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0760 - acc: 0.9775 - val_loss: 0.1037 - val_acc: 0.9683\n",
            "Epoch 342/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9769\n",
            "Epoch 00342: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0764 - acc: 0.9764 - val_loss: 0.1171 - val_acc: 0.9633\n",
            "Epoch 343/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0878 - acc: 0.9721\n",
            "Epoch 00343: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0889 - acc: 0.9717 - val_loss: 0.0999 - val_acc: 0.9708\n",
            "Epoch 344/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1098 - acc: 0.9648\n",
            "Epoch 00344: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1075 - acc: 0.9656 - val_loss: 0.1170 - val_acc: 0.9633\n",
            "Epoch 345/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0841 - acc: 0.9739\n",
            "Epoch 00345: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0848 - acc: 0.9739 - val_loss: 0.0962 - val_acc: 0.9692\n",
            "Epoch 346/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9711\n",
            "Epoch 00346: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0956 - acc: 0.9711 - val_loss: 0.1228 - val_acc: 0.9608\n",
            "Epoch 347/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9711\n",
            "Epoch 00347: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0929 - acc: 0.9711 - val_loss: 0.1000 - val_acc: 0.9692\n",
            "Epoch 348/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0851 - acc: 0.9730\n",
            "Epoch 00348: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0848 - acc: 0.9739 - val_loss: 0.1123 - val_acc: 0.9692\n",
            "Epoch 349/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0759 - acc: 0.9767\n",
            "Epoch 00349: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0759 - acc: 0.9767 - val_loss: 0.1283 - val_acc: 0.9575\n",
            "Epoch 350/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0756 - acc: 0.9791\n",
            "Epoch 00350: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0759 - acc: 0.9786 - val_loss: 0.0804 - val_acc: 0.9808\n",
            "Epoch 351/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0676 - acc: 0.9824\n",
            "Epoch 00351: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0659 - acc: 0.9833 - val_loss: 0.0891 - val_acc: 0.9708\n",
            "Epoch 352/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9786\n",
            "Epoch 00352: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0750 - acc: 0.9775 - val_loss: 0.1229 - val_acc: 0.9650\n",
            "Epoch 353/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0897 - acc: 0.9694\n",
            "Epoch 00353: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0887 - acc: 0.9697 - val_loss: 0.0973 - val_acc: 0.9725\n",
            "Epoch 354/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0816 - acc: 0.9739\n",
            "Epoch 00354: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0808 - acc: 0.9744 - val_loss: 0.1300 - val_acc: 0.9675\n",
            "Epoch 355/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9671\n",
            "Epoch 00355: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0993 - acc: 0.9678 - val_loss: 0.1111 - val_acc: 0.9625\n",
            "Epoch 356/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9709\n",
            "Epoch 00356: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0955 - acc: 0.9708 - val_loss: 0.0990 - val_acc: 0.9708\n",
            "Epoch 357/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0785 - acc: 0.9762\n",
            "Epoch 00357: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0800 - acc: 0.9764 - val_loss: 0.0837 - val_acc: 0.9758\n",
            "Epoch 358/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0730 - acc: 0.9791\n",
            "Epoch 00358: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0748 - acc: 0.9783 - val_loss: 0.0985 - val_acc: 0.9692\n",
            "Epoch 359/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0834 - acc: 0.9727\n",
            "Epoch 00359: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0825 - acc: 0.9731 - val_loss: 0.1243 - val_acc: 0.9633\n",
            "Epoch 360/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0828 - acc: 0.9741\n",
            "Epoch 00360: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0849 - acc: 0.9736 - val_loss: 0.0797 - val_acc: 0.9767\n",
            "Epoch 361/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9743\n",
            "Epoch 00361: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0852 - acc: 0.9744 - val_loss: 0.1070 - val_acc: 0.9608\n",
            "Epoch 362/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0767 - acc: 0.9766\n",
            "Epoch 00362: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0789 - acc: 0.9764 - val_loss: 0.1006 - val_acc: 0.9675\n",
            "Epoch 363/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0716 - acc: 0.9788\n",
            "Epoch 00363: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0740 - acc: 0.9772 - val_loss: 0.0821 - val_acc: 0.9742\n",
            "Epoch 364/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9686\n",
            "Epoch 00364: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0958 - acc: 0.9692 - val_loss: 0.1500 - val_acc: 0.9492\n",
            "Epoch 365/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9706\n",
            "Epoch 00365: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0974 - acc: 0.9714 - val_loss: 0.0949 - val_acc: 0.9733\n",
            "Epoch 366/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0808 - acc: 0.9733\n",
            "Epoch 00366: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0803 - acc: 0.9736 - val_loss: 0.0808 - val_acc: 0.9800\n",
            "Epoch 367/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9769\n",
            "Epoch 00367: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0772 - acc: 0.9764 - val_loss: 0.1073 - val_acc: 0.9675\n",
            "Epoch 368/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0707 - acc: 0.9803\n",
            "Epoch 00368: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0704 - acc: 0.9806 - val_loss: 0.1080 - val_acc: 0.9692\n",
            "Epoch 369/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9754\n",
            "Epoch 00369: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0776 - acc: 0.9761 - val_loss: 0.0927 - val_acc: 0.9725\n",
            "Epoch 370/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9789\n",
            "Epoch 00370: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0728 - acc: 0.9789 - val_loss: 0.0880 - val_acc: 0.9708\n",
            "Epoch 371/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0869 - acc: 0.9718\n",
            "Epoch 00371: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0867 - acc: 0.9717 - val_loss: 0.0869 - val_acc: 0.9700\n",
            "Epoch 372/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0851 - acc: 0.9737\n",
            "Epoch 00372: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0857 - acc: 0.9731 - val_loss: 0.1142 - val_acc: 0.9675\n",
            "Epoch 373/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0784 - acc: 0.9791\n",
            "Epoch 00373: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0801 - acc: 0.9783 - val_loss: 0.1122 - val_acc: 0.9683\n",
            "Epoch 374/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9797\n",
            "Epoch 00374: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0724 - acc: 0.9794 - val_loss: 0.1167 - val_acc: 0.9683\n",
            "Epoch 375/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0736 - acc: 0.9785\n",
            "Epoch 00375: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0732 - acc: 0.9789 - val_loss: 0.0932 - val_acc: 0.9750\n",
            "Epoch 376/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9763\n",
            "Epoch 00376: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0764 - acc: 0.9769 - val_loss: 0.1038 - val_acc: 0.9700\n",
            "Epoch 377/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9771\n",
            "Epoch 00377: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0788 - acc: 0.9775 - val_loss: 0.0947 - val_acc: 0.9692\n",
            "Epoch 378/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0846 - acc: 0.9738\n",
            "Epoch 00378: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0853 - acc: 0.9731 - val_loss: 0.0915 - val_acc: 0.9742\n",
            "Epoch 379/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9751\n",
            "Epoch 00379: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0777 - acc: 0.9758 - val_loss: 0.0951 - val_acc: 0.9700\n",
            "Epoch 380/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0715 - acc: 0.9781\n",
            "Epoch 00380: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0716 - acc: 0.9772 - val_loss: 0.0877 - val_acc: 0.9775\n",
            "Epoch 381/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0682 - acc: 0.9806\n",
            "Epoch 00381: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0670 - acc: 0.9814 - val_loss: 0.0999 - val_acc: 0.9700\n",
            "Epoch 382/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0648 - acc: 0.9829\n",
            "Epoch 00382: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0655 - acc: 0.9825 - val_loss: 0.1060 - val_acc: 0.9700\n",
            "Epoch 383/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0645 - acc: 0.9835\n",
            "Epoch 00383: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0641 - acc: 0.9831 - val_loss: 0.0844 - val_acc: 0.9742\n",
            "Epoch 384/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9820\n",
            "Epoch 00384: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0638 - acc: 0.9825 - val_loss: 0.0971 - val_acc: 0.9717\n",
            "Epoch 385/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9814\n",
            "Epoch 00385: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0657 - acc: 0.9806 - val_loss: 0.1056 - val_acc: 0.9692\n",
            "Epoch 386/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0751 - acc: 0.9778\n",
            "Epoch 00386: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0764 - acc: 0.9781 - val_loss: 0.1029 - val_acc: 0.9717\n",
            "Epoch 387/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9780\n",
            "Epoch 00387: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0820 - acc: 0.9778 - val_loss: 0.0940 - val_acc: 0.9683\n",
            "Epoch 388/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0770 - acc: 0.9765\n",
            "Epoch 00388: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0806 - acc: 0.9747 - val_loss: 0.1245 - val_acc: 0.9567\n",
            "Epoch 389/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0908 - acc: 0.9733\n",
            "Epoch 00389: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0869 - acc: 0.9753 - val_loss: 0.1142 - val_acc: 0.9675\n",
            "Epoch 390/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9723\n",
            "Epoch 00390: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0899 - acc: 0.9731 - val_loss: 0.1112 - val_acc: 0.9667\n",
            "Epoch 391/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0948 - acc: 0.9700\n",
            "Epoch 00391: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0930 - acc: 0.9708 - val_loss: 0.0791 - val_acc: 0.9767\n",
            "Epoch 392/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0798 - acc: 0.9774\n",
            "Epoch 00392: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0779 - acc: 0.9781 - val_loss: 0.0939 - val_acc: 0.9683\n",
            "Epoch 393/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0687 - acc: 0.9806\n",
            "Epoch 00393: val_loss did not improve from 0.07597\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0713 - acc: 0.9792 - val_loss: 0.0996 - val_acc: 0.9700\n",
            "Epoch 394/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0713 - acc: 0.9803\n",
            "Epoch 00394: val_loss improved from 0.07597 to 0.07036, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.0695 - acc: 0.9811 - val_loss: 0.0704 - val_acc: 0.9817\n",
            "Epoch 395/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9809\n",
            "Epoch 00395: val_loss did not improve from 0.07036\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0687 - acc: 0.9808 - val_loss: 0.0869 - val_acc: 0.9767\n",
            "Epoch 396/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9731\n",
            "Epoch 00396: val_loss did not improve from 0.07036\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0847 - acc: 0.9731 - val_loss: 0.1154 - val_acc: 0.9658\n",
            "Epoch 397/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0734 - acc: 0.9791\n",
            "Epoch 00397: val_loss did not improve from 0.07036\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0768 - acc: 0.9767 - val_loss: 0.0864 - val_acc: 0.9758\n",
            "Epoch 398/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9751\n",
            "Epoch 00398: val_loss did not improve from 0.07036\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0823 - acc: 0.9756 - val_loss: 0.1025 - val_acc: 0.9675\n",
            "Epoch 399/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0712 - acc: 0.9815\n",
            "Epoch 00399: val_loss did not improve from 0.07036\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0730 - acc: 0.9808 - val_loss: 0.0892 - val_acc: 0.9742\n",
            "Epoch 400/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0698 - acc: 0.9782\n",
            "Epoch 00400: val_loss did not improve from 0.07036\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0684 - acc: 0.9789 - val_loss: 0.0995 - val_acc: 0.9683\n",
            "1200/1200 [==============================] - 0s 118us/sample - loss: 0.0995 - acc: 0.9683\n",
            "[0.09950840981056293, 0.9683333]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2n5jWdesYdV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "6e3166c0-3ba3-42c7-f03d-09af204da392"
      },
      "source": [
        "for i in range(7, 8): # Итерација низ секој испитен примерок\n",
        "  print(f\"====================== Примерок ({i}) ======================\")\n",
        "  print(\"Вчитување тест податоци од испитниот примерок \" + str(i) + \"...\")\n",
        "  \n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_test_data = np.empty(0)\n",
        "  temp_test_events = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_test_set = 'S' + format(j, '02') + '/Test'\n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testData.mat\"\n",
        "    temp = loadmat(full_path)['testData']\n",
        "    if temp_test_data.size != 0:\n",
        "      temp_test_data = np.concatenate((temp_test_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_test_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_test_events.size != 0:\n",
        "        temp_test_events = np.append(temp_test_events, temp)\n",
        "      else:\n",
        "        temp_test_events = np.array(temp)\n",
        "\n",
        "    # Вчитување на бројот на runs \n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/runs_per_block.txt\"\n",
        "    with open(full_path, \"r\") as runs_per_block:\n",
        "      test_runs_per_block[i-1][j-1] = int(runs_per_block.read())\n",
        "\n",
        "    print(\"\\t - Тест податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "  # Зачувај ги тест податоците вчитани од испитниот примерок во низа\n",
        "  test_data.append(temp_test_data)\n",
        "  test_events.append(temp_test_events)\n",
        "  print(\"Тест податоците од испитниот примерок \" + str(i) + \" се вчитани.\\n\")\n",
        "\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_data: \" + str(test_data[i-1].shape)) # test_data to predict\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_events: \" + str(len(test_events[i-1]))) # test_events\n",
        "  for j in range (1,4):\n",
        "    print(\"SBJ\" + str(format(i-1, '02')) + \" / S\" + str(format(j-1, '02')) + \"| Runs per block: \" + str(test_runs_per_block[i-1][j-1])) # runs per block in SJB01, SJ00 \n",
        "\n",
        "  to_predict_data = reshape_data_to_mne_format(test_data[i-1])\n",
        "  predictions = model7.predict(to_predict_data)\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Predictions: \" + str(len(predictions)))\n",
        "  # np.savetxt(\"predictions.csv\", predictions, delimiter=\",\")\n",
        "\n",
        "\n",
        "  # ========= FALI USTE DA SE ISPARSIRA PREDICTIONOT... NE E SREDEN OVOJ KOD DOLE =======\n",
        "\n",
        "  int_pred = np.argmax(predictions, axis=1)\n",
        "  int_ytest = np.argmax(y_test, axis=1)\n",
        "\n",
        "  session_start = 0\n",
        "  start_prediction_index = 0\n",
        "  end_prediction_index = 0\n",
        "  for session in range(0, 3):\n",
        "    print(f\"============== Сесија ({session}) ==============\")\n",
        "    for block in range(0, 50):    \n",
        "      events_per_block = test_runs_per_block[i-1][session]\n",
        "\n",
        "      start_prediction_index = session_start + (block*events_per_block)*8\n",
        "      end_prediction_index = session_start + ((block+1)*events_per_block)*8\n",
        "\n",
        "      block_prediction = int_pred[start_prediction_index:end_prediction_index]\n",
        "      prediction = np.bincount(block_prediction).argmax()\n",
        "      df.iat[session+18,block+2] = prediction+1\n",
        "      # UNCOMMENT ZA PODOBAR PRIKAZ :)\n",
        "      # print(f\"Session {session} | Block: {block} | Prediction: {prediction} | Address: {end_prediction_index}\")\n",
        "\n",
        "      print(str(prediction+1) + \",\", end=\"\")\n",
        "    session_start = end_prediction_index\n",
        "    print(\"\")\n",
        "  print(\"Stigna li do kraj: \" + str(session_start == len(predictions)))\n",
        "  print(f\"====================== Примерок ({i}) ======================\\n\\n\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Примерок (7) ======================\n",
            "Вчитување тест податоци од испитниот примерок 7...\n",
            "\t - Тест податоците од сесија 1 се вчитани.\n",
            "\t - Тест податоците од сесија 2 се вчитани.\n",
            "\t - Тест податоците од сесија 3 се вчитани.\n",
            "Тест податоците од испитниот примерок 7 се вчитани.\n",
            "\n",
            "SBJ06| Test_data: (8, 350, 9600)\n",
            "SBJ06| Test_events: 9600\n",
            "SBJ06 / S00| Runs per block: 9\n",
            "SBJ06 / S01| Runs per block: 5\n",
            "SBJ06 / S02| Runs per block: 10\n",
            "SBJ06| Predictions: 9600\n",
            "============== Сесија (0) ==============\n",
            "4,7,5,7,4,5,4,1,1,1,1,6,2,1,5,6,2,1,7,5,5,5,5,4,1,5,2,8,8,4,5,4,5,1,5,4,4,5,1,4,5,5,5,1,1,4,4,4,1,4,\n",
            "============== Сесија (1) ==============\n",
            "2,2,2,8,2,5,2,8,2,5,8,5,1,1,8,2,1,1,1,7,1,7,3,1,5,1,8,2,1,1,7,5,8,1,2,2,2,5,2,1,2,1,5,5,5,2,2,2,2,2,\n",
            "============== Сесија (2) ==============\n",
            "2,7,7,5,7,5,4,3,3,3,5,5,5,5,5,5,3,8,5,5,1,7,3,4,4,2,5,2,5,3,5,5,5,3,1,5,5,1,3,3,8,5,1,3,5,5,3,5,5,1,\n",
            "Stigna li do kraj: True\n",
            "====================== Примерок (7) ======================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv2F009csjvt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a37fa08c-dee4-4b7e-96eb-0484d763ff26"
      },
      "source": [
        "df"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUBJECT</th>\n",
              "      <th>SESSION</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>Unnamed: 52</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    SUBJECT  SESSION  1  2  3  4  5  6  7  ... 43 44 45 46 47 48 49 50 Unnamed: 52\n",
              "0         1        1  6  6  3  6  6  3  6  ...  6  4  8  3  4  5  5  7         NaN\n",
              "1         1        2  6  3  2  1  2  1  3  ...  6  2  2  2  3  6  6  2         NaN\n",
              "2         1        3  3  3  3  3  3  3  3  ...  3  3  6  3  6  7  1  6         NaN\n",
              "3         2        1  8  8  8  8  8  8  8  ...  8  4  8  4  8  7  8  8         NaN\n",
              "4         2        2  2  7  6  6  6  6  7  ...  2  6  6  2  6  6  2  2         NaN\n",
              "5         2        3  2  7  2  6  7  4  2  ...  2  6  2  2  7  2  2  2         NaN\n",
              "6         3        1  3  3  4  4  4  3  6  ...  4  6  3  3  4  3  4  4         NaN\n",
              "7         3        2  6  1  7  7  7  7  7  ...  6  7  1  6  6  6  6  6         NaN\n",
              "8         3        3  6  4  8  8  5  7  8  ...  4  2  8  2  2  2  2  8         NaN\n",
              "9         4        1  1  1  2  1  5  5  6  ...  1  1  7  1  6  6  6  6         NaN\n",
              "10        4        2  7  7  7  7  6  6  7  ...  1  5  5  5  5  5  2  6         NaN\n",
              "11        4        3  7  3  7  3  6  6  7  ...  6  1  4  4  4  6  6  6         NaN\n",
              "12        5        1  5  4  4  4  6  4  5  ...  3  1  4  4  1  3  3  5         NaN\n",
              "13        5        2  3  6  3  5  2  6  7  ...  6  6  6  6  6  6  6  4         NaN\n",
              "14        5        3  4  3  3  6  5  7  6  ...  7  2  2  7  5  6  4  2         NaN\n",
              "15        6        1  1  3  1  8  3  3  3  ...  4  3  7  2  7  2  5  8         NaN\n",
              "16        6        2  3  4  1  7  1  1  1  ...  5  5  5  5  5  5  5  5         NaN\n",
              "17        6        3  2  3  2  5  3  3  3  ...  2  3  3  3  1  1  3  3         NaN\n",
              "18        7        1  4  7  5  7  4  5  4  ...  5  1  1  4  4  4  1  4         NaN\n",
              "19        7        2  2  2  2  8  2  5  2  ...  5  5  5  2  2  2  2  2         NaN\n",
              "20        7        3  2  7  7  5  7  5  4  ...  1  3  5  5  3  5  5  1         NaN\n",
              "21        8        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "22        8        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "23        8        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "24        9        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "25        9        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "26        9        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "27       10        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "28       10        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "29       10        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "30       11        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "31       11        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "32       11        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "33       12        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "34       12        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "35       12        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "36       13        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "37       13        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "38       13        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "39       14        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "40       14        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "41       14        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "42       15        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "43       15        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "44       15        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "\n",
              "[45 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgXgYTaRte6J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "81797654-9e58-4157-fef9-7059a230976c"
      },
      "source": [
        "for i in range(8, 9): # Итерација низ секој испитен примерок\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_data = np.empty(0)\n",
        "  temp_labels = np.empty(0)\n",
        "  temp_events = np.empty(0)\n",
        "  temp_targets = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainData.mat\"\n",
        "    temp = loadmat(full_path)['trainData']\n",
        "    if temp_data.size != 0:\n",
        "      temp_data = np.concatenate((temp_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на label-ите\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainLabels.txt\"\n",
        "    with open(full_path, \"r\") as file_labels:\n",
        "      temp = file_labels.read().splitlines()\n",
        "      temp = np.repeat(temp, 8*10)\n",
        "      if temp_labels.size != 0:\n",
        "        temp_labels = np.concatenate((temp_labels, temp))\n",
        "      else:\n",
        "        temp_labels = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_events.size != 0:\n",
        "        temp_events = np.append(temp_events, temp)\n",
        "      else:\n",
        "        temp_events = np.array(temp)\n",
        "      \n",
        "\n",
        "    # Вчитување на редоследот на објекти кои се target\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainTargets.txt\"\n",
        "    with open(full_path, \"r\") as file_targets:\n",
        "      temp = file_targets.read().splitlines()\n",
        "      if temp_targets.size != 0:\n",
        "        temp_targets = np.concatenate((temp_targets, temp))\n",
        "      else:\n",
        "        temp_targets = np.array(temp)\n",
        "    print(\"\\t - Податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "\n",
        "  for j in range(4, 8): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "      \n",
        "  # Зачувај ги податоците вчитани од испитниот примерок во низа\n",
        "  data.append(temp_data)\n",
        "  labels.append(temp_labels)\n",
        "  events.append(temp_events)\n",
        "  targets.append(temp_targets)\n",
        "\n",
        "  \n",
        "  print(\"Податоците од испитниот примерок \" + str(i) + \" се вчитани.\")\n",
        "\n",
        "\n",
        "  #data = target_events_data_scaled\n",
        "  mne_array = np.swapaxes(data[i-1], 0, 2) # (епохa, канал, настан). \n",
        "  mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "  mne_array = mne_array.reshape(mne_array.shape[0],1, 8,350)\n",
        "  print(mne_array.shape)\n",
        "\n",
        "  events_arr = events[i-1].astype(np.int)\n",
        "  labels_arr = labels[i-1].astype(np.int)\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(mne_array, labels_arr-1, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "  model8 = DeepConvNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "  model8.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = Adam(0.0009))\n",
        "  checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                                verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "  #clf = RandomForestClassifier(max_depth=5)\n",
        "  #clf.fit(X_train, y_train)\n",
        "  #score = clf.score(X_test, y_test)\n",
        "  # print(score)\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  num_batch_size=100\n",
        "  num_epochs=400\n",
        "  model8.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "            validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "  score = model8.evaluate(X_test, y_test, verbose=1)\n",
        "  print(score)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t - Податоците од сесија 1 се вчитани.\n",
            "\t - Податоците од сесија 2 се вчитани.\n",
            "\t - Податоците од сесија 3 се вчитани.\n",
            "Податоците од испитниот примерок 8 се вчитани.\n",
            "(4800, 1, 8, 350)\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Train on 3600 samples, validate on 1200 samples\n",
            "Epoch 1/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.2870 - acc: 0.1900\n",
            "Epoch 00001: val_loss improved from inf to 2.23248, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 6s 2ms/sample - loss: 2.2769 - acc: 0.1931 - val_loss: 2.2325 - val_acc: 0.2108\n",
            "Epoch 2/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.0936 - acc: 0.2427\n",
            "Epoch 00002: val_loss did not improve from 2.23248\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 2.1054 - acc: 0.2408 - val_loss: 3.0164 - val_acc: 0.0958\n",
            "Epoch 3/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.0408 - acc: 0.2797\n",
            "Epoch 00003: val_loss did not improve from 2.23248\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 2.0454 - acc: 0.2781 - val_loss: 2.4897 - val_acc: 0.1975\n",
            "Epoch 4/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.9405 - acc: 0.3016\n",
            "Epoch 00004: val_loss did not improve from 2.23248\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.9416 - acc: 0.3028 - val_loss: 2.4233 - val_acc: 0.2233\n",
            "Epoch 5/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.8697 - acc: 0.3217\n",
            "Epoch 00005: val_loss improved from 2.23248 to 1.94054, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 1.8705 - acc: 0.3222 - val_loss: 1.9405 - val_acc: 0.2475\n",
            "Epoch 6/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.7799 - acc: 0.3497\n",
            "Epoch 00006: val_loss did not improve from 1.94054\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 1.7845 - acc: 0.3486 - val_loss: 2.0611 - val_acc: 0.2567\n",
            "Epoch 7/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.7036 - acc: 0.3723\n",
            "Epoch 00007: val_loss did not improve from 1.94054\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.6959 - acc: 0.3736 - val_loss: 1.9594 - val_acc: 0.2642\n",
            "Epoch 8/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.6622 - acc: 0.3794\n",
            "Epoch 00008: val_loss improved from 1.94054 to 1.58754, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 1.6557 - acc: 0.3825 - val_loss: 1.5875 - val_acc: 0.3533\n",
            "Epoch 9/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.5959 - acc: 0.3991\n",
            "Epoch 00009: val_loss did not improve from 1.58754\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 1.5952 - acc: 0.3989 - val_loss: 1.7828 - val_acc: 0.2567\n",
            "Epoch 10/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.5184 - acc: 0.4244\n",
            "Epoch 00010: val_loss did not improve from 1.58754\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.5144 - acc: 0.4250 - val_loss: 2.0952 - val_acc: 0.2617\n",
            "Epoch 11/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.5003 - acc: 0.4291\n",
            "Epoch 00011: val_loss did not improve from 1.58754\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.5042 - acc: 0.4272 - val_loss: 2.1973 - val_acc: 0.2517\n",
            "Epoch 12/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.4640 - acc: 0.4569\n",
            "Epoch 00012: val_loss did not improve from 1.58754\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.4649 - acc: 0.4556 - val_loss: 2.2251 - val_acc: 0.2817\n",
            "Epoch 13/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.3853 - acc: 0.4843\n",
            "Epoch 00013: val_loss did not improve from 1.58754\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.3825 - acc: 0.4864 - val_loss: 1.6314 - val_acc: 0.3942\n",
            "Epoch 14/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.3677 - acc: 0.4712\n",
            "Epoch 00014: val_loss did not improve from 1.58754\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.3642 - acc: 0.4717 - val_loss: 1.7093 - val_acc: 0.3700\n",
            "Epoch 15/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.3269 - acc: 0.4891\n",
            "Epoch 00015: val_loss improved from 1.58754 to 1.51362, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 1.3266 - acc: 0.4911 - val_loss: 1.5136 - val_acc: 0.4450\n",
            "Epoch 16/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.2372 - acc: 0.5316\n",
            "Epoch 00016: val_loss did not improve from 1.51362\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 1.2402 - acc: 0.5278 - val_loss: 1.5145 - val_acc: 0.4042\n",
            "Epoch 17/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2333 - acc: 0.5276\n",
            "Epoch 00017: val_loss improved from 1.51362 to 1.38999, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 1.2333 - acc: 0.5300 - val_loss: 1.3900 - val_acc: 0.4483\n",
            "Epoch 18/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2087 - acc: 0.5379\n",
            "Epoch 00018: val_loss did not improve from 1.38999\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 1.2088 - acc: 0.5381 - val_loss: 1.4470 - val_acc: 0.4658\n",
            "Epoch 19/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.1409 - acc: 0.5678\n",
            "Epoch 00019: val_loss improved from 1.38999 to 1.35237, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 1.1399 - acc: 0.5653 - val_loss: 1.3524 - val_acc: 0.5050\n",
            "Epoch 20/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1153 - acc: 0.5609\n",
            "Epoch 00020: val_loss did not improve from 1.35237\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.1174 - acc: 0.5592 - val_loss: 1.4182 - val_acc: 0.4517\n",
            "Epoch 21/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0596 - acc: 0.6082\n",
            "Epoch 00021: val_loss did not improve from 1.35237\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.0579 - acc: 0.6083 - val_loss: 1.3950 - val_acc: 0.4642\n",
            "Epoch 22/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.0421 - acc: 0.6029\n",
            "Epoch 00022: val_loss improved from 1.35237 to 1.19592, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 1.0423 - acc: 0.6022 - val_loss: 1.1959 - val_acc: 0.5225\n",
            "Epoch 23/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.0209 - acc: 0.6089\n",
            "Epoch 00023: val_loss did not improve from 1.19592\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.0238 - acc: 0.6072 - val_loss: 1.2226 - val_acc: 0.5125\n",
            "Epoch 24/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9924 - acc: 0.6240\n",
            "Epoch 00024: val_loss did not improve from 1.19592\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.9946 - acc: 0.6225 - val_loss: 1.2648 - val_acc: 0.5008\n",
            "Epoch 25/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9824 - acc: 0.6344\n",
            "Epoch 00025: val_loss improved from 1.19592 to 1.12182, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.9740 - acc: 0.6397 - val_loss: 1.1218 - val_acc: 0.5458\n",
            "Epoch 26/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9199 - acc: 0.6476\n",
            "Epoch 00026: val_loss improved from 1.12182 to 1.08193, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 221us/sample - loss: 0.9216 - acc: 0.6472 - val_loss: 1.0819 - val_acc: 0.5742\n",
            "Epoch 27/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9084 - acc: 0.6559\n",
            "Epoch 00027: val_loss did not improve from 1.08193\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.9067 - acc: 0.6558 - val_loss: 1.1977 - val_acc: 0.5300\n",
            "Epoch 28/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.8574 - acc: 0.6840\n",
            "Epoch 00028: val_loss improved from 1.08193 to 1.05239, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.8582 - acc: 0.6833 - val_loss: 1.0524 - val_acc: 0.5775\n",
            "Epoch 29/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.8539 - acc: 0.6751\n",
            "Epoch 00029: val_loss did not improve from 1.05239\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.8524 - acc: 0.6764 - val_loss: 1.1348 - val_acc: 0.5517\n",
            "Epoch 30/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.8277 - acc: 0.6937\n",
            "Epoch 00030: val_loss did not improve from 1.05239\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.8282 - acc: 0.6936 - val_loss: 1.0901 - val_acc: 0.5550\n",
            "Epoch 31/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.8180 - acc: 0.6949\n",
            "Epoch 00031: val_loss improved from 1.05239 to 1.01549, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.8206 - acc: 0.6925 - val_loss: 1.0155 - val_acc: 0.5967\n",
            "Epoch 32/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7893 - acc: 0.7036\n",
            "Epoch 00032: val_loss improved from 1.01549 to 0.97430, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.7833 - acc: 0.7072 - val_loss: 0.9743 - val_acc: 0.6133\n",
            "Epoch 33/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7679 - acc: 0.7222\n",
            "Epoch 00033: val_loss improved from 0.97430 to 0.91946, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.7677 - acc: 0.7225 - val_loss: 0.9195 - val_acc: 0.6458\n",
            "Epoch 34/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7277 - acc: 0.7369\n",
            "Epoch 00034: val_loss did not improve from 0.91946\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.7295 - acc: 0.7361 - val_loss: 1.0181 - val_acc: 0.6183\n",
            "Epoch 35/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7244 - acc: 0.7329\n",
            "Epoch 00035: val_loss did not improve from 0.91946\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.7259 - acc: 0.7322 - val_loss: 0.9296 - val_acc: 0.6475\n",
            "Epoch 36/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6928 - acc: 0.7457\n",
            "Epoch 00036: val_loss did not improve from 0.91946\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.6910 - acc: 0.7469 - val_loss: 0.9199 - val_acc: 0.6408\n",
            "Epoch 37/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6707 - acc: 0.7594\n",
            "Epoch 00037: val_loss did not improve from 0.91946\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.6741 - acc: 0.7567 - val_loss: 0.9324 - val_acc: 0.6392\n",
            "Epoch 38/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.6586 - acc: 0.7585\n",
            "Epoch 00038: val_loss improved from 0.91946 to 0.91136, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.6642 - acc: 0.7569 - val_loss: 0.9114 - val_acc: 0.6308\n",
            "Epoch 39/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6306 - acc: 0.7724\n",
            "Epoch 00039: val_loss improved from 0.91136 to 0.90609, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.6296 - acc: 0.7714 - val_loss: 0.9061 - val_acc: 0.6475\n",
            "Epoch 40/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6413 - acc: 0.7594\n",
            "Epoch 00040: val_loss improved from 0.90609 to 0.78976, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.6428 - acc: 0.7586 - val_loss: 0.7898 - val_acc: 0.7008\n",
            "Epoch 41/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6161 - acc: 0.7824\n",
            "Epoch 00041: val_loss did not improve from 0.78976\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.6169 - acc: 0.7822 - val_loss: 0.8522 - val_acc: 0.6667\n",
            "Epoch 42/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5918 - acc: 0.7920\n",
            "Epoch 00042: val_loss improved from 0.78976 to 0.76695, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.5910 - acc: 0.7933 - val_loss: 0.7669 - val_acc: 0.7025\n",
            "Epoch 43/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5537 - acc: 0.8065\n",
            "Epoch 00043: val_loss did not improve from 0.76695\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.5587 - acc: 0.8050 - val_loss: 0.8162 - val_acc: 0.6833\n",
            "Epoch 44/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5549 - acc: 0.8014\n",
            "Epoch 00044: val_loss did not improve from 0.76695\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.5575 - acc: 0.7997 - val_loss: 0.7877 - val_acc: 0.6875\n",
            "Epoch 45/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5315 - acc: 0.8091\n",
            "Epoch 00045: val_loss did not improve from 0.76695\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.5307 - acc: 0.8097 - val_loss: 0.8096 - val_acc: 0.6817\n",
            "Epoch 46/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5166 - acc: 0.8262\n",
            "Epoch 00046: val_loss improved from 0.76695 to 0.68817, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.5174 - acc: 0.8250 - val_loss: 0.6882 - val_acc: 0.7483\n",
            "Epoch 47/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5014 - acc: 0.8259\n",
            "Epoch 00047: val_loss did not improve from 0.68817\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.4996 - acc: 0.8264 - val_loss: 0.6886 - val_acc: 0.7517\n",
            "Epoch 48/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4888 - acc: 0.8300\n",
            "Epoch 00048: val_loss did not improve from 0.68817\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.4950 - acc: 0.8283 - val_loss: 0.7754 - val_acc: 0.6925\n",
            "Epoch 49/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4793 - acc: 0.8331\n",
            "Epoch 00049: val_loss did not improve from 0.68817\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.4831 - acc: 0.8311 - val_loss: 0.7083 - val_acc: 0.7208\n",
            "Epoch 50/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4758 - acc: 0.8397\n",
            "Epoch 00050: val_loss improved from 0.68817 to 0.66140, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.4746 - acc: 0.8400 - val_loss: 0.6614 - val_acc: 0.7625\n",
            "Epoch 51/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4442 - acc: 0.8460\n",
            "Epoch 00051: val_loss did not improve from 0.66140\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.4441 - acc: 0.8456 - val_loss: 0.6825 - val_acc: 0.7375\n",
            "Epoch 52/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4287 - acc: 0.8565\n",
            "Epoch 00052: val_loss improved from 0.66140 to 0.61447, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.4279 - acc: 0.8575 - val_loss: 0.6145 - val_acc: 0.7700\n",
            "Epoch 53/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4253 - acc: 0.8566\n",
            "Epoch 00053: val_loss improved from 0.61447 to 0.61350, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.4274 - acc: 0.8556 - val_loss: 0.6135 - val_acc: 0.7725\n",
            "Epoch 54/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4284 - acc: 0.8470\n",
            "Epoch 00054: val_loss improved from 0.61350 to 0.59260, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.4309 - acc: 0.8461 - val_loss: 0.5926 - val_acc: 0.7725\n",
            "Epoch 55/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4297 - acc: 0.8494\n",
            "Epoch 00055: val_loss improved from 0.59260 to 0.52175, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.4296 - acc: 0.8506 - val_loss: 0.5218 - val_acc: 0.8142\n",
            "Epoch 56/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.4016 - acc: 0.8731\n",
            "Epoch 00056: val_loss did not improve from 0.52175\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.4010 - acc: 0.8725 - val_loss: 0.6105 - val_acc: 0.7808\n",
            "Epoch 57/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3720 - acc: 0.8766\n",
            "Epoch 00057: val_loss improved from 0.52175 to 0.52090, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.3779 - acc: 0.8717 - val_loss: 0.5209 - val_acc: 0.8050\n",
            "Epoch 58/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3748 - acc: 0.8783\n",
            "Epoch 00058: val_loss improved from 0.52090 to 0.50488, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.3746 - acc: 0.8781 - val_loss: 0.5049 - val_acc: 0.8183\n",
            "Epoch 59/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3478 - acc: 0.8832\n",
            "Epoch 00059: val_loss did not improve from 0.50488\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.3507 - acc: 0.8825 - val_loss: 0.5742 - val_acc: 0.7717\n",
            "Epoch 60/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3724 - acc: 0.8789\n",
            "Epoch 00060: val_loss improved from 0.50488 to 0.50088, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.3715 - acc: 0.8786 - val_loss: 0.5009 - val_acc: 0.8225\n",
            "Epoch 61/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3395 - acc: 0.8869\n",
            "Epoch 00061: val_loss improved from 0.50088 to 0.45511, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.3408 - acc: 0.8864 - val_loss: 0.4551 - val_acc: 0.8408\n",
            "Epoch 62/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3289 - acc: 0.9003\n",
            "Epoch 00062: val_loss improved from 0.45511 to 0.42953, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.3332 - acc: 0.8986 - val_loss: 0.4295 - val_acc: 0.8542\n",
            "Epoch 63/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3093 - acc: 0.9036\n",
            "Epoch 00063: val_loss improved from 0.42953 to 0.40914, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.3065 - acc: 0.9050 - val_loss: 0.4091 - val_acc: 0.8517\n",
            "Epoch 64/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3065 - acc: 0.9094\n",
            "Epoch 00064: val_loss did not improve from 0.40914\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.3073 - acc: 0.9086 - val_loss: 0.4605 - val_acc: 0.8375\n",
            "Epoch 65/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3059 - acc: 0.9062\n",
            "Epoch 00065: val_loss did not improve from 0.40914\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.3090 - acc: 0.9039 - val_loss: 0.4187 - val_acc: 0.8533\n",
            "Epoch 66/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3099 - acc: 0.9000\n",
            "Epoch 00066: val_loss did not improve from 0.40914\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.3109 - acc: 0.9011 - val_loss: 0.4378 - val_acc: 0.8408\n",
            "Epoch 67/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9040\n",
            "Epoch 00067: val_loss did not improve from 0.40914\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.2935 - acc: 0.9042 - val_loss: 0.4300 - val_acc: 0.8483\n",
            "Epoch 68/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3002 - acc: 0.9048\n",
            "Epoch 00068: val_loss did not improve from 0.40914\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.3041 - acc: 0.9036 - val_loss: 0.4358 - val_acc: 0.8575\n",
            "Epoch 69/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2900 - acc: 0.9043\n",
            "Epoch 00069: val_loss improved from 0.40914 to 0.36743, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.2918 - acc: 0.9031 - val_loss: 0.3674 - val_acc: 0.8767\n",
            "Epoch 70/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2795 - acc: 0.9140\n",
            "Epoch 00070: val_loss did not improve from 0.36743\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2802 - acc: 0.9136 - val_loss: 0.4745 - val_acc: 0.8283\n",
            "Epoch 71/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3044 - acc: 0.8977\n",
            "Epoch 00071: val_loss did not improve from 0.36743\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.3021 - acc: 0.8992 - val_loss: 0.4786 - val_acc: 0.8292\n",
            "Epoch 72/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2764 - acc: 0.9163\n",
            "Epoch 00072: val_loss improved from 0.36743 to 0.34836, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.2786 - acc: 0.9147 - val_loss: 0.3484 - val_acc: 0.8817\n",
            "Epoch 73/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.9214\n",
            "Epoch 00073: val_loss did not improve from 0.34836\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2533 - acc: 0.9228 - val_loss: 0.4082 - val_acc: 0.8650\n",
            "Epoch 74/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2574 - acc: 0.9171\n",
            "Epoch 00074: val_loss did not improve from 0.34836\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2596 - acc: 0.9169 - val_loss: 0.3699 - val_acc: 0.8700\n",
            "Epoch 75/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2645 - acc: 0.9097\n",
            "Epoch 00075: val_loss improved from 0.34836 to 0.34378, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.2641 - acc: 0.9100 - val_loss: 0.3438 - val_acc: 0.8758\n",
            "Epoch 76/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.9131\n",
            "Epoch 00076: val_loss did not improve from 0.34378\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2701 - acc: 0.9133 - val_loss: 0.3531 - val_acc: 0.8783\n",
            "Epoch 77/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2590 - acc: 0.9215\n",
            "Epoch 00077: val_loss improved from 0.34378 to 0.30123, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.2591 - acc: 0.9214 - val_loss: 0.3012 - val_acc: 0.9058\n",
            "Epoch 78/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2421 - acc: 0.9261\n",
            "Epoch 00078: val_loss did not improve from 0.30123\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2437 - acc: 0.9253 - val_loss: 0.3086 - val_acc: 0.8908\n",
            "Epoch 79/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2374 - acc: 0.9284\n",
            "Epoch 00079: val_loss did not improve from 0.30123\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2392 - acc: 0.9269 - val_loss: 0.3081 - val_acc: 0.9100\n",
            "Epoch 80/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2323 - acc: 0.9303\n",
            "Epoch 00080: val_loss did not improve from 0.30123\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2324 - acc: 0.9297 - val_loss: 0.3039 - val_acc: 0.9108\n",
            "Epoch 81/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2225 - acc: 0.9349\n",
            "Epoch 00081: val_loss improved from 0.30123 to 0.29736, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.2221 - acc: 0.9350 - val_loss: 0.2974 - val_acc: 0.9075\n",
            "Epoch 82/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2121 - acc: 0.9364\n",
            "Epoch 00082: val_loss did not improve from 0.29736\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2128 - acc: 0.9364 - val_loss: 0.3240 - val_acc: 0.8967\n",
            "Epoch 83/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9306\n",
            "Epoch 00083: val_loss did not improve from 0.29736\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2311 - acc: 0.9311 - val_loss: 0.3775 - val_acc: 0.8758\n",
            "Epoch 84/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2338 - acc: 0.9250\n",
            "Epoch 00084: val_loss did not improve from 0.29736\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2344 - acc: 0.9250 - val_loss: 0.3302 - val_acc: 0.8933\n",
            "Epoch 85/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2173 - acc: 0.9288\n",
            "Epoch 00085: val_loss improved from 0.29736 to 0.27297, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.2190 - acc: 0.9281 - val_loss: 0.2730 - val_acc: 0.9183\n",
            "Epoch 86/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1962 - acc: 0.9411\n",
            "Epoch 00086: val_loss did not improve from 0.27297\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1954 - acc: 0.9425 - val_loss: 0.2776 - val_acc: 0.9150\n",
            "Epoch 87/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9394\n",
            "Epoch 00087: val_loss did not improve from 0.27297\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.2031 - acc: 0.9397 - val_loss: 0.3460 - val_acc: 0.8867\n",
            "Epoch 88/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1960 - acc: 0.9421\n",
            "Epoch 00088: val_loss improved from 0.27297 to 0.25262, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1994 - acc: 0.9400 - val_loss: 0.2526 - val_acc: 0.9242\n",
            "Epoch 89/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1856 - acc: 0.9475\n",
            "Epoch 00089: val_loss did not improve from 0.25262\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1921 - acc: 0.9458 - val_loss: 0.4042 - val_acc: 0.8483\n",
            "Epoch 90/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2027 - acc: 0.9356\n",
            "Epoch 00090: val_loss did not improve from 0.25262\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.2016 - acc: 0.9367 - val_loss: 0.3193 - val_acc: 0.8967\n",
            "Epoch 91/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1964 - acc: 0.9431\n",
            "Epoch 00091: val_loss did not improve from 0.25262\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1956 - acc: 0.9439 - val_loss: 0.3235 - val_acc: 0.8908\n",
            "Epoch 92/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2207 - acc: 0.9303\n",
            "Epoch 00092: val_loss did not improve from 0.25262\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2200 - acc: 0.9294 - val_loss: 0.3094 - val_acc: 0.8933\n",
            "Epoch 93/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2122 - acc: 0.9306\n",
            "Epoch 00093: val_loss did not improve from 0.25262\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2099 - acc: 0.9328 - val_loss: 0.3231 - val_acc: 0.8867\n",
            "Epoch 94/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9403\n",
            "Epoch 00094: val_loss improved from 0.25262 to 0.20956, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.2036 - acc: 0.9394 - val_loss: 0.2096 - val_acc: 0.9358\n",
            "Epoch 95/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2038 - acc: 0.9349\n",
            "Epoch 00095: val_loss did not improve from 0.20956\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2056 - acc: 0.9347 - val_loss: 0.3279 - val_acc: 0.8917\n",
            "Epoch 96/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1822 - acc: 0.9463\n",
            "Epoch 00096: val_loss did not improve from 0.20956\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1826 - acc: 0.9461 - val_loss: 0.2468 - val_acc: 0.9067\n",
            "Epoch 97/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1707 - acc: 0.9465\n",
            "Epoch 00097: val_loss did not improve from 0.20956\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1711 - acc: 0.9467 - val_loss: 0.3621 - val_acc: 0.8892\n",
            "Epoch 98/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2176 - acc: 0.9303\n",
            "Epoch 00098: val_loss did not improve from 0.20956\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2161 - acc: 0.9311 - val_loss: 0.2378 - val_acc: 0.9292\n",
            "Epoch 99/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1770 - acc: 0.9535\n",
            "Epoch 00099: val_loss did not improve from 0.20956\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1764 - acc: 0.9533 - val_loss: 0.2946 - val_acc: 0.9117\n",
            "Epoch 100/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1887 - acc: 0.9445\n",
            "Epoch 00100: val_loss improved from 0.20956 to 0.20910, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.1883 - acc: 0.9442 - val_loss: 0.2091 - val_acc: 0.9392\n",
            "Epoch 101/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1937 - acc: 0.9397\n",
            "Epoch 00101: val_loss did not improve from 0.20910\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1938 - acc: 0.9400 - val_loss: 0.2688 - val_acc: 0.9192\n",
            "Epoch 102/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1747 - acc: 0.9452\n",
            "Epoch 00102: val_loss did not improve from 0.20910\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1771 - acc: 0.9436 - val_loss: 0.2522 - val_acc: 0.9267\n",
            "Epoch 103/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1737 - acc: 0.9484\n",
            "Epoch 00103: val_loss did not improve from 0.20910\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1704 - acc: 0.9494 - val_loss: 0.2743 - val_acc: 0.9133\n",
            "Epoch 104/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1684 - acc: 0.9491\n",
            "Epoch 00104: val_loss did not improve from 0.20910\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1677 - acc: 0.9494 - val_loss: 0.2099 - val_acc: 0.9442\n",
            "Epoch 105/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1513 - acc: 0.9518\n",
            "Epoch 00105: val_loss did not improve from 0.20910\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1515 - acc: 0.9528 - val_loss: 0.2248 - val_acc: 0.9350\n",
            "Epoch 106/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1673 - acc: 0.9525\n",
            "Epoch 00106: val_loss did not improve from 0.20910\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1720 - acc: 0.9503 - val_loss: 0.2177 - val_acc: 0.9333\n",
            "Epoch 107/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1778 - acc: 0.9427\n",
            "Epoch 00107: val_loss improved from 0.20910 to 0.20839, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1738 - acc: 0.9447 - val_loss: 0.2084 - val_acc: 0.9283\n",
            "Epoch 108/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9534\n",
            "Epoch 00108: val_loss did not improve from 0.20839\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1581 - acc: 0.9544 - val_loss: 0.2521 - val_acc: 0.9125\n",
            "Epoch 109/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1505 - acc: 0.9574\n",
            "Epoch 00109: val_loss did not improve from 0.20839\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1493 - acc: 0.9583 - val_loss: 0.2468 - val_acc: 0.9133\n",
            "Epoch 110/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1570 - acc: 0.9539\n",
            "Epoch 00110: val_loss did not improve from 0.20839\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1565 - acc: 0.9539 - val_loss: 0.2365 - val_acc: 0.9183\n",
            "Epoch 111/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1599 - acc: 0.9525\n",
            "Epoch 00111: val_loss did not improve from 0.20839\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1610 - acc: 0.9503 - val_loss: 0.2368 - val_acc: 0.9325\n",
            "Epoch 112/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1596 - acc: 0.9527\n",
            "Epoch 00112: val_loss did not improve from 0.20839\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1583 - acc: 0.9533 - val_loss: 0.2167 - val_acc: 0.9333\n",
            "Epoch 113/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1480 - acc: 0.9561\n",
            "Epoch 00113: val_loss improved from 0.20839 to 0.20725, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1480 - acc: 0.9558 - val_loss: 0.2072 - val_acc: 0.9342\n",
            "Epoch 114/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9626\n",
            "Epoch 00114: val_loss did not improve from 0.20725\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1358 - acc: 0.9622 - val_loss: 0.2297 - val_acc: 0.9258\n",
            "Epoch 115/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9614\n",
            "Epoch 00115: val_loss did not improve from 0.20725\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1365 - acc: 0.9606 - val_loss: 0.2421 - val_acc: 0.9175\n",
            "Epoch 116/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1510 - acc: 0.9551\n",
            "Epoch 00116: val_loss improved from 0.20725 to 0.18977, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1496 - acc: 0.9558 - val_loss: 0.1898 - val_acc: 0.9433\n",
            "Epoch 117/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1444 - acc: 0.9536\n",
            "Epoch 00117: val_loss did not improve from 0.18977\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1432 - acc: 0.9550 - val_loss: 0.2074 - val_acc: 0.9367\n",
            "Epoch 118/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1335 - acc: 0.9656\n",
            "Epoch 00118: val_loss improved from 0.18977 to 0.18740, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1383 - acc: 0.9639 - val_loss: 0.1874 - val_acc: 0.9442\n",
            "Epoch 119/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.9526\n",
            "Epoch 00119: val_loss did not improve from 0.18740\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1535 - acc: 0.9528 - val_loss: 0.2512 - val_acc: 0.9150\n",
            "Epoch 120/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.9586\n",
            "Epoch 00120: val_loss did not improve from 0.18740\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1423 - acc: 0.9586 - val_loss: 0.2115 - val_acc: 0.9300\n",
            "Epoch 121/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9591\n",
            "Epoch 00121: val_loss did not improve from 0.18740\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1479 - acc: 0.9594 - val_loss: 0.1997 - val_acc: 0.9425\n",
            "Epoch 122/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9577\n",
            "Epoch 00122: val_loss did not improve from 0.18740\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1446 - acc: 0.9575 - val_loss: 0.2403 - val_acc: 0.9175\n",
            "Epoch 123/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1304 - acc: 0.9606\n",
            "Epoch 00123: val_loss did not improve from 0.18740\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1286 - acc: 0.9622 - val_loss: 0.1887 - val_acc: 0.9375\n",
            "Epoch 124/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1313 - acc: 0.9648\n",
            "Epoch 00124: val_loss improved from 0.18740 to 0.18665, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.1316 - acc: 0.9644 - val_loss: 0.1866 - val_acc: 0.9383\n",
            "Epoch 125/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1233 - acc: 0.9662\n",
            "Epoch 00125: val_loss did not improve from 0.18665\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1238 - acc: 0.9661 - val_loss: 0.2051 - val_acc: 0.9333\n",
            "Epoch 126/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1386 - acc: 0.9609\n",
            "Epoch 00126: val_loss improved from 0.18665 to 0.18378, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1397 - acc: 0.9606 - val_loss: 0.1838 - val_acc: 0.9475\n",
            "Epoch 127/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1528 - acc: 0.9550\n",
            "Epoch 00127: val_loss did not improve from 0.18378\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1579 - acc: 0.9531 - val_loss: 0.1911 - val_acc: 0.9442\n",
            "Epoch 128/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1434 - acc: 0.9581\n",
            "Epoch 00128: val_loss improved from 0.18378 to 0.15564, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1443 - acc: 0.9583 - val_loss: 0.1556 - val_acc: 0.9525\n",
            "Epoch 129/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1225 - acc: 0.9676\n",
            "Epoch 00129: val_loss did not improve from 0.15564\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1221 - acc: 0.9683 - val_loss: 0.1889 - val_acc: 0.9358\n",
            "Epoch 130/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9600\n",
            "Epoch 00130: val_loss did not improve from 0.15564\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1344 - acc: 0.9597 - val_loss: 0.1813 - val_acc: 0.9508\n",
            "Epoch 131/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1335 - acc: 0.9597\n",
            "Epoch 00131: val_loss did not improve from 0.15564\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1335 - acc: 0.9597 - val_loss: 0.1815 - val_acc: 0.9508\n",
            "Epoch 132/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1189 - acc: 0.9644\n",
            "Epoch 00132: val_loss did not improve from 0.15564\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1209 - acc: 0.9631 - val_loss: 0.1577 - val_acc: 0.9525\n",
            "Epoch 133/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1261 - acc: 0.9645\n",
            "Epoch 00133: val_loss did not improve from 0.15564\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1272 - acc: 0.9639 - val_loss: 0.1839 - val_acc: 0.9400\n",
            "Epoch 134/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1286 - acc: 0.9644\n",
            "Epoch 00134: val_loss did not improve from 0.15564\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1285 - acc: 0.9639 - val_loss: 0.2178 - val_acc: 0.9283\n",
            "Epoch 135/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1307 - acc: 0.9579\n",
            "Epoch 00135: val_loss did not improve from 0.15564\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1298 - acc: 0.9581 - val_loss: 0.2064 - val_acc: 0.9233\n",
            "Epoch 136/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1276 - acc: 0.9606\n",
            "Epoch 00136: val_loss did not improve from 0.15564\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1299 - acc: 0.9603 - val_loss: 0.1864 - val_acc: 0.9425\n",
            "Epoch 137/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9651\n",
            "Epoch 00137: val_loss did not improve from 0.15564\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1189 - acc: 0.9633 - val_loss: 0.1885 - val_acc: 0.9433\n",
            "Epoch 138/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9614\n",
            "Epoch 00138: val_loss improved from 0.15564 to 0.14509, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.1363 - acc: 0.9619 - val_loss: 0.1451 - val_acc: 0.9617\n",
            "Epoch 139/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1316 - acc: 0.9591\n",
            "Epoch 00139: val_loss did not improve from 0.14509\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1312 - acc: 0.9592 - val_loss: 0.1826 - val_acc: 0.9417\n",
            "Epoch 140/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9503\n",
            "Epoch 00140: val_loss did not improve from 0.14509\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1508 - acc: 0.9494 - val_loss: 0.1853 - val_acc: 0.9400\n",
            "Epoch 141/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9614\n",
            "Epoch 00141: val_loss did not improve from 0.14509\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1344 - acc: 0.9617 - val_loss: 0.1550 - val_acc: 0.9567\n",
            "Epoch 142/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1434 - acc: 0.9568\n",
            "Epoch 00142: val_loss did not improve from 0.14509\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1422 - acc: 0.9575 - val_loss: 0.1980 - val_acc: 0.9367\n",
            "Epoch 143/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1309 - acc: 0.9638\n",
            "Epoch 00143: val_loss did not improve from 0.14509\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1323 - acc: 0.9633 - val_loss: 0.1951 - val_acc: 0.9400\n",
            "Epoch 144/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1251 - acc: 0.9618\n",
            "Epoch 00144: val_loss did not improve from 0.14509\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1264 - acc: 0.9614 - val_loss: 0.2076 - val_acc: 0.9325\n",
            "Epoch 145/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1293 - acc: 0.9644\n",
            "Epoch 00145: val_loss improved from 0.14509 to 0.14098, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.1266 - acc: 0.9656 - val_loss: 0.1410 - val_acc: 0.9600\n",
            "Epoch 146/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1171 - acc: 0.9644\n",
            "Epoch 00146: val_loss did not improve from 0.14098\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1152 - acc: 0.9653 - val_loss: 0.1581 - val_acc: 0.9525\n",
            "Epoch 147/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9663\n",
            "Epoch 00147: val_loss did not improve from 0.14098\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1207 - acc: 0.9656 - val_loss: 0.2229 - val_acc: 0.9217\n",
            "Epoch 148/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1281 - acc: 0.9626\n",
            "Epoch 00148: val_loss did not improve from 0.14098\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1281 - acc: 0.9625 - val_loss: 0.1568 - val_acc: 0.9558\n",
            "Epoch 149/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1086 - acc: 0.9706\n",
            "Epoch 00149: val_loss did not improve from 0.14098\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1089 - acc: 0.9706 - val_loss: 0.1668 - val_acc: 0.9425\n",
            "Epoch 150/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9686\n",
            "Epoch 00150: val_loss did not improve from 0.14098\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1086 - acc: 0.9689 - val_loss: 0.1450 - val_acc: 0.9550\n",
            "Epoch 151/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1213 - acc: 0.9626\n",
            "Epoch 00151: val_loss did not improve from 0.14098\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1236 - acc: 0.9614 - val_loss: 0.1501 - val_acc: 0.9583\n",
            "Epoch 152/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1345 - acc: 0.9600\n",
            "Epoch 00152: val_loss did not improve from 0.14098\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1348 - acc: 0.9594 - val_loss: 0.1899 - val_acc: 0.9350\n",
            "Epoch 153/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1074 - acc: 0.9712\n",
            "Epoch 00153: val_loss improved from 0.14098 to 0.13182, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1077 - acc: 0.9700 - val_loss: 0.1318 - val_acc: 0.9667\n",
            "Epoch 154/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9677\n",
            "Epoch 00154: val_loss did not improve from 0.13182\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1119 - acc: 0.9675 - val_loss: 0.1401 - val_acc: 0.9625\n",
            "Epoch 155/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1051 - acc: 0.9709\n",
            "Epoch 00155: val_loss did not improve from 0.13182\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1067 - acc: 0.9697 - val_loss: 0.1680 - val_acc: 0.9525\n",
            "Epoch 156/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1142 - acc: 0.9658\n",
            "Epoch 00156: val_loss did not improve from 0.13182\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1131 - acc: 0.9669 - val_loss: 0.1612 - val_acc: 0.9558\n",
            "Epoch 157/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1114 - acc: 0.9691\n",
            "Epoch 00157: val_loss did not improve from 0.13182\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1133 - acc: 0.9683 - val_loss: 0.1449 - val_acc: 0.9575\n",
            "Epoch 158/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1027 - acc: 0.9715\n",
            "Epoch 00158: val_loss did not improve from 0.13182\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1034 - acc: 0.9708 - val_loss: 0.1410 - val_acc: 0.9542\n",
            "Epoch 159/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9720\n",
            "Epoch 00159: val_loss did not improve from 0.13182\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0991 - acc: 0.9714 - val_loss: 0.1870 - val_acc: 0.9408\n",
            "Epoch 160/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0976 - acc: 0.9732\n",
            "Epoch 00160: val_loss improved from 0.13182 to 0.12873, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.0981 - acc: 0.9733 - val_loss: 0.1287 - val_acc: 0.9658\n",
            "Epoch 161/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1024 - acc: 0.9706\n",
            "Epoch 00161: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1009 - acc: 0.9714 - val_loss: 0.1432 - val_acc: 0.9600\n",
            "Epoch 162/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1159 - acc: 0.9644\n",
            "Epoch 00162: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1153 - acc: 0.9642 - val_loss: 0.1325 - val_acc: 0.9608\n",
            "Epoch 163/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1165 - acc: 0.9624\n",
            "Epoch 00163: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1185 - acc: 0.9625 - val_loss: 0.1775 - val_acc: 0.9467\n",
            "Epoch 164/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1087 - acc: 0.9706\n",
            "Epoch 00164: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1116 - acc: 0.9694 - val_loss: 0.1602 - val_acc: 0.9492\n",
            "Epoch 165/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1185 - acc: 0.9671\n",
            "Epoch 00165: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1183 - acc: 0.9675 - val_loss: 0.1597 - val_acc: 0.9525\n",
            "Epoch 166/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1096 - acc: 0.9669\n",
            "Epoch 00166: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1124 - acc: 0.9658 - val_loss: 0.1646 - val_acc: 0.9508\n",
            "Epoch 167/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1027 - acc: 0.9753\n",
            "Epoch 00167: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1058 - acc: 0.9733 - val_loss: 0.1593 - val_acc: 0.9458\n",
            "Epoch 168/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0974 - acc: 0.9724\n",
            "Epoch 00168: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1014 - acc: 0.9708 - val_loss: 0.1300 - val_acc: 0.9658\n",
            "Epoch 169/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1020 - acc: 0.9697\n",
            "Epoch 00169: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1031 - acc: 0.9689 - val_loss: 0.1475 - val_acc: 0.9533\n",
            "Epoch 170/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1040 - acc: 0.9742\n",
            "Epoch 00170: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1029 - acc: 0.9747 - val_loss: 0.1956 - val_acc: 0.9342\n",
            "Epoch 171/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1148 - acc: 0.9612\n",
            "Epoch 00171: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1142 - acc: 0.9619 - val_loss: 0.1519 - val_acc: 0.9458\n",
            "Epoch 172/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1179 - acc: 0.9665\n",
            "Epoch 00172: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1175 - acc: 0.9667 - val_loss: 0.1417 - val_acc: 0.9617\n",
            "Epoch 173/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1133 - acc: 0.9658\n",
            "Epoch 00173: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1146 - acc: 0.9644 - val_loss: 0.2131 - val_acc: 0.9250\n",
            "Epoch 174/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1168 - acc: 0.9676\n",
            "Epoch 00174: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1182 - acc: 0.9664 - val_loss: 0.1892 - val_acc: 0.9375\n",
            "Epoch 175/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1029 - acc: 0.9718\n",
            "Epoch 00175: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1044 - acc: 0.9714 - val_loss: 0.1389 - val_acc: 0.9533\n",
            "Epoch 176/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9680\n",
            "Epoch 00176: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1039 - acc: 0.9678 - val_loss: 0.1519 - val_acc: 0.9567\n",
            "Epoch 177/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0970 - acc: 0.9753\n",
            "Epoch 00177: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0975 - acc: 0.9750 - val_loss: 0.1764 - val_acc: 0.9442\n",
            "Epoch 178/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1127 - acc: 0.9653\n",
            "Epoch 00178: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1141 - acc: 0.9647 - val_loss: 0.1448 - val_acc: 0.9583\n",
            "Epoch 179/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1178 - acc: 0.9653\n",
            "Epoch 00179: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1181 - acc: 0.9650 - val_loss: 0.1606 - val_acc: 0.9433\n",
            "Epoch 180/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1062 - acc: 0.9716\n",
            "Epoch 00180: val_loss did not improve from 0.12873\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1025 - acc: 0.9728 - val_loss: 0.3006 - val_acc: 0.8950\n",
            "Epoch 181/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9626\n",
            "Epoch 00181: val_loss improved from 0.12873 to 0.12424, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1151 - acc: 0.9625 - val_loss: 0.1242 - val_acc: 0.9600\n",
            "Epoch 182/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1062 - acc: 0.9729\n",
            "Epoch 00182: val_loss did not improve from 0.12424\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1067 - acc: 0.9725 - val_loss: 0.1686 - val_acc: 0.9500\n",
            "Epoch 183/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1009 - acc: 0.9721\n",
            "Epoch 00183: val_loss improved from 0.12424 to 0.11593, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.1002 - acc: 0.9728 - val_loss: 0.1159 - val_acc: 0.9642\n",
            "Epoch 184/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9729\n",
            "Epoch 00184: val_loss did not improve from 0.11593\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1009 - acc: 0.9731 - val_loss: 0.1354 - val_acc: 0.9575\n",
            "Epoch 185/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0939 - acc: 0.9722\n",
            "Epoch 00185: val_loss did not improve from 0.11593\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0941 - acc: 0.9719 - val_loss: 0.1487 - val_acc: 0.9550\n",
            "Epoch 186/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9680\n",
            "Epoch 00186: val_loss did not improve from 0.11593\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1052 - acc: 0.9681 - val_loss: 0.1186 - val_acc: 0.9592\n",
            "Epoch 187/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1134 - acc: 0.9647\n",
            "Epoch 00187: val_loss did not improve from 0.11593\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1152 - acc: 0.9639 - val_loss: 0.1345 - val_acc: 0.9592\n",
            "Epoch 188/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0988 - acc: 0.9735\n",
            "Epoch 00188: val_loss did not improve from 0.11593\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0980 - acc: 0.9739 - val_loss: 0.1181 - val_acc: 0.9617\n",
            "Epoch 189/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1030 - acc: 0.9653\n",
            "Epoch 00189: val_loss improved from 0.11593 to 0.11178, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.1058 - acc: 0.9636 - val_loss: 0.1118 - val_acc: 0.9650\n",
            "Epoch 190/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0984 - acc: 0.9688\n",
            "Epoch 00190: val_loss improved from 0.11178 to 0.10637, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.0975 - acc: 0.9692 - val_loss: 0.1064 - val_acc: 0.9700\n",
            "Epoch 191/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0915 - acc: 0.9737\n",
            "Epoch 00191: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0971 - acc: 0.9717 - val_loss: 0.1326 - val_acc: 0.9625\n",
            "Epoch 192/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1014 - acc: 0.9674\n",
            "Epoch 00192: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1048 - acc: 0.9672 - val_loss: 0.1250 - val_acc: 0.9633\n",
            "Epoch 193/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9714\n",
            "Epoch 00193: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1084 - acc: 0.9711 - val_loss: 0.1636 - val_acc: 0.9467\n",
            "Epoch 194/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0905 - acc: 0.9762\n",
            "Epoch 00194: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0909 - acc: 0.9758 - val_loss: 0.1148 - val_acc: 0.9692\n",
            "Epoch 195/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0909 - acc: 0.9750\n",
            "Epoch 00195: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0914 - acc: 0.9744 - val_loss: 0.1279 - val_acc: 0.9625\n",
            "Epoch 196/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0926 - acc: 0.9721\n",
            "Epoch 00196: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0911 - acc: 0.9728 - val_loss: 0.1321 - val_acc: 0.9583\n",
            "Epoch 197/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0994 - acc: 0.9700\n",
            "Epoch 00197: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1068 - acc: 0.9664 - val_loss: 0.1186 - val_acc: 0.9667\n",
            "Epoch 198/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1414 - acc: 0.9569\n",
            "Epoch 00198: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1399 - acc: 0.9578 - val_loss: 0.1636 - val_acc: 0.9425\n",
            "Epoch 199/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1262 - acc: 0.9597\n",
            "Epoch 00199: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1242 - acc: 0.9606 - val_loss: 0.1307 - val_acc: 0.9658\n",
            "Epoch 200/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0930 - acc: 0.9735\n",
            "Epoch 00200: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0937 - acc: 0.9728 - val_loss: 0.1337 - val_acc: 0.9558\n",
            "Epoch 201/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9763\n",
            "Epoch 00201: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0853 - acc: 0.9764 - val_loss: 0.1300 - val_acc: 0.9625\n",
            "Epoch 202/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0984 - acc: 0.9703\n",
            "Epoch 00202: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0991 - acc: 0.9706 - val_loss: 0.1486 - val_acc: 0.9517\n",
            "Epoch 203/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9677\n",
            "Epoch 00203: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1054 - acc: 0.9675 - val_loss: 0.1441 - val_acc: 0.9550\n",
            "Epoch 204/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0900 - acc: 0.9764\n",
            "Epoch 00204: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0898 - acc: 0.9764 - val_loss: 0.1406 - val_acc: 0.9608\n",
            "Epoch 205/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0973 - acc: 0.9700\n",
            "Epoch 00205: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0962 - acc: 0.9703 - val_loss: 0.1077 - val_acc: 0.9633\n",
            "Epoch 206/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0873 - acc: 0.9738\n",
            "Epoch 00206: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0869 - acc: 0.9744 - val_loss: 0.1111 - val_acc: 0.9625\n",
            "Epoch 207/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1000 - acc: 0.9732\n",
            "Epoch 00207: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0994 - acc: 0.9733 - val_loss: 0.1172 - val_acc: 0.9658\n",
            "Epoch 208/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9754\n",
            "Epoch 00208: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0865 - acc: 0.9750 - val_loss: 0.1080 - val_acc: 0.9675\n",
            "Epoch 209/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0982 - acc: 0.9700\n",
            "Epoch 00209: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0982 - acc: 0.9703 - val_loss: 0.1090 - val_acc: 0.9700\n",
            "Epoch 210/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9737\n",
            "Epoch 00210: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0878 - acc: 0.9742 - val_loss: 0.1185 - val_acc: 0.9633\n",
            "Epoch 211/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9751\n",
            "Epoch 00211: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0874 - acc: 0.9756 - val_loss: 0.1474 - val_acc: 0.9492\n",
            "Epoch 212/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9729\n",
            "Epoch 00212: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0934 - acc: 0.9728 - val_loss: 0.1440 - val_acc: 0.9558\n",
            "Epoch 213/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0883 - acc: 0.9741\n",
            "Epoch 00213: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0919 - acc: 0.9722 - val_loss: 0.1251 - val_acc: 0.9592\n",
            "Epoch 214/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1025 - acc: 0.9694\n",
            "Epoch 00214: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1017 - acc: 0.9703 - val_loss: 0.1601 - val_acc: 0.9433\n",
            "Epoch 215/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1024 - acc: 0.9685\n",
            "Epoch 00215: val_loss did not improve from 0.10637\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1014 - acc: 0.9689 - val_loss: 0.1748 - val_acc: 0.9408\n",
            "Epoch 216/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9700\n",
            "Epoch 00216: val_loss improved from 0.10637 to 0.09346, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.0992 - acc: 0.9703 - val_loss: 0.0935 - val_acc: 0.9733\n",
            "Epoch 217/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0794 - acc: 0.9824\n",
            "Epoch 00217: val_loss did not improve from 0.09346\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0811 - acc: 0.9819 - val_loss: 0.1025 - val_acc: 0.9692\n",
            "Epoch 218/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0911 - acc: 0.9726\n",
            "Epoch 00218: val_loss did not improve from 0.09346\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0905 - acc: 0.9731 - val_loss: 0.1376 - val_acc: 0.9567\n",
            "Epoch 219/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0935 - acc: 0.9715\n",
            "Epoch 00219: val_loss did not improve from 0.09346\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0975 - acc: 0.9703 - val_loss: 0.1237 - val_acc: 0.9633\n",
            "Epoch 220/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0991 - acc: 0.9739\n",
            "Epoch 00220: val_loss did not improve from 0.09346\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1000 - acc: 0.9731 - val_loss: 0.1288 - val_acc: 0.9633\n",
            "Epoch 221/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0749 - acc: 0.9821\n",
            "Epoch 00221: val_loss did not improve from 0.09346\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0770 - acc: 0.9811 - val_loss: 0.1072 - val_acc: 0.9708\n",
            "Epoch 222/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0967 - acc: 0.9718\n",
            "Epoch 00222: val_loss did not improve from 0.09346\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0976 - acc: 0.9714 - val_loss: 0.1307 - val_acc: 0.9617\n",
            "Epoch 223/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9754\n",
            "Epoch 00223: val_loss did not improve from 0.09346\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0821 - acc: 0.9758 - val_loss: 0.1080 - val_acc: 0.9700\n",
            "Epoch 224/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0923 - acc: 0.9712\n",
            "Epoch 00224: val_loss did not improve from 0.09346\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0946 - acc: 0.9700 - val_loss: 0.1252 - val_acc: 0.9600\n",
            "Epoch 225/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0972 - acc: 0.9724\n",
            "Epoch 00225: val_loss did not improve from 0.09346\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0973 - acc: 0.9733 - val_loss: 0.1136 - val_acc: 0.9683\n",
            "Epoch 226/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0934 - acc: 0.9729\n",
            "Epoch 00226: val_loss did not improve from 0.09346\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0935 - acc: 0.9725 - val_loss: 0.1162 - val_acc: 0.9617\n",
            "Epoch 227/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0933 - acc: 0.9719\n",
            "Epoch 00227: val_loss did not improve from 0.09346\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.0906 - acc: 0.9739 - val_loss: 0.1062 - val_acc: 0.9742\n",
            "Epoch 228/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0807 - acc: 0.9759\n",
            "Epoch 00228: val_loss improved from 0.09346 to 0.09124, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.0835 - acc: 0.9744 - val_loss: 0.0912 - val_acc: 0.9725\n",
            "Epoch 229/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9757\n",
            "Epoch 00229: val_loss did not improve from 0.09124\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0881 - acc: 0.9742 - val_loss: 0.1037 - val_acc: 0.9675\n",
            "Epoch 230/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9726\n",
            "Epoch 00230: val_loss did not improve from 0.09124\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0983 - acc: 0.9728 - val_loss: 0.1118 - val_acc: 0.9658\n",
            "Epoch 231/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1007 - acc: 0.9682\n",
            "Epoch 00231: val_loss did not improve from 0.09124\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1033 - acc: 0.9672 - val_loss: 0.1726 - val_acc: 0.9392\n",
            "Epoch 232/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1069 - acc: 0.9684\n",
            "Epoch 00232: val_loss did not improve from 0.09124\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1066 - acc: 0.9686 - val_loss: 0.1216 - val_acc: 0.9633\n",
            "Epoch 233/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0992 - acc: 0.9712\n",
            "Epoch 00233: val_loss did not improve from 0.09124\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0991 - acc: 0.9708 - val_loss: 0.1158 - val_acc: 0.9675\n",
            "Epoch 234/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0929 - acc: 0.9747\n",
            "Epoch 00234: val_loss did not improve from 0.09124\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0938 - acc: 0.9744 - val_loss: 0.0975 - val_acc: 0.9708\n",
            "Epoch 235/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1042 - acc: 0.9681\n",
            "Epoch 00235: val_loss did not improve from 0.09124\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1013 - acc: 0.9694 - val_loss: 0.1619 - val_acc: 0.9433\n",
            "Epoch 236/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1042 - acc: 0.9658\n",
            "Epoch 00236: val_loss did not improve from 0.09124\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1065 - acc: 0.9658 - val_loss: 0.1291 - val_acc: 0.9550\n",
            "Epoch 237/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9606\n",
            "Epoch 00237: val_loss did not improve from 0.09124\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1205 - acc: 0.9608 - val_loss: 0.1004 - val_acc: 0.9742\n",
            "Epoch 238/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9626\n",
            "Epoch 00238: val_loss did not improve from 0.09124\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1217 - acc: 0.9625 - val_loss: 0.1167 - val_acc: 0.9675\n",
            "Epoch 239/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1067 - acc: 0.9644\n",
            "Epoch 00239: val_loss did not improve from 0.09124\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1110 - acc: 0.9625 - val_loss: 0.1268 - val_acc: 0.9642\n",
            "Epoch 240/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9763\n",
            "Epoch 00240: val_loss did not improve from 0.09124\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0906 - acc: 0.9769 - val_loss: 0.1756 - val_acc: 0.9350\n",
            "Epoch 241/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1029 - acc: 0.9716\n",
            "Epoch 00241: val_loss did not improve from 0.09124\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1025 - acc: 0.9711 - val_loss: 0.1114 - val_acc: 0.9683\n",
            "Epoch 242/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1106 - acc: 0.9636\n",
            "Epoch 00242: val_loss did not improve from 0.09124\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1113 - acc: 0.9639 - val_loss: 0.0934 - val_acc: 0.9708\n",
            "Epoch 243/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0832 - acc: 0.9737\n",
            "Epoch 00243: val_loss did not improve from 0.09124\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0841 - acc: 0.9733 - val_loss: 0.1547 - val_acc: 0.9500\n",
            "Epoch 244/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0749 - acc: 0.9809\n",
            "Epoch 00244: val_loss did not improve from 0.09124\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0760 - acc: 0.9814 - val_loss: 0.1238 - val_acc: 0.9633\n",
            "Epoch 245/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0867 - acc: 0.9726\n",
            "Epoch 00245: val_loss improved from 0.09124 to 0.08187, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.0852 - acc: 0.9733 - val_loss: 0.0819 - val_acc: 0.9800\n",
            "Epoch 246/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9783\n",
            "Epoch 00246: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0794 - acc: 0.9786 - val_loss: 0.1069 - val_acc: 0.9675\n",
            "Epoch 247/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0940 - acc: 0.9759\n",
            "Epoch 00247: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0932 - acc: 0.9764 - val_loss: 0.1359 - val_acc: 0.9542\n",
            "Epoch 248/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0920 - acc: 0.9732\n",
            "Epoch 00248: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0942 - acc: 0.9717 - val_loss: 0.0913 - val_acc: 0.9733\n",
            "Epoch 249/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9737\n",
            "Epoch 00249: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0912 - acc: 0.9733 - val_loss: 0.0928 - val_acc: 0.9775\n",
            "Epoch 250/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9714\n",
            "Epoch 00250: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0927 - acc: 0.9714 - val_loss: 0.1726 - val_acc: 0.9383\n",
            "Epoch 251/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9766\n",
            "Epoch 00251: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0863 - acc: 0.9764 - val_loss: 0.1357 - val_acc: 0.9525\n",
            "Epoch 252/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0830 - acc: 0.9785\n",
            "Epoch 00252: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0835 - acc: 0.9783 - val_loss: 0.0876 - val_acc: 0.9725\n",
            "Epoch 253/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0878 - acc: 0.9788\n",
            "Epoch 00253: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0899 - acc: 0.9772 - val_loss: 0.0877 - val_acc: 0.9767\n",
            "Epoch 254/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0806 - acc: 0.9788\n",
            "Epoch 00254: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0798 - acc: 0.9786 - val_loss: 0.0909 - val_acc: 0.9750\n",
            "Epoch 255/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0805 - acc: 0.9755\n",
            "Epoch 00255: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0803 - acc: 0.9761 - val_loss: 0.1133 - val_acc: 0.9642\n",
            "Epoch 256/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0967 - acc: 0.9691\n",
            "Epoch 00256: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0960 - acc: 0.9689 - val_loss: 0.0889 - val_acc: 0.9742\n",
            "Epoch 257/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9797\n",
            "Epoch 00257: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0788 - acc: 0.9800 - val_loss: 0.1366 - val_acc: 0.9550\n",
            "Epoch 258/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0894 - acc: 0.9769\n",
            "Epoch 00258: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0912 - acc: 0.9747 - val_loss: 0.1219 - val_acc: 0.9625\n",
            "Epoch 259/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0817 - acc: 0.9776\n",
            "Epoch 00259: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0822 - acc: 0.9764 - val_loss: 0.0882 - val_acc: 0.9783\n",
            "Epoch 260/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0916 - acc: 0.9700\n",
            "Epoch 00260: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0943 - acc: 0.9697 - val_loss: 0.1127 - val_acc: 0.9683\n",
            "Epoch 261/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0802 - acc: 0.9762\n",
            "Epoch 00261: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0824 - acc: 0.9753 - val_loss: 0.0958 - val_acc: 0.9725\n",
            "Epoch 262/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9729\n",
            "Epoch 00262: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0827 - acc: 0.9725 - val_loss: 0.1305 - val_acc: 0.9625\n",
            "Epoch 263/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9746\n",
            "Epoch 00263: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0861 - acc: 0.9744 - val_loss: 0.0882 - val_acc: 0.9750\n",
            "Epoch 264/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0836 - acc: 0.9747\n",
            "Epoch 00264: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0823 - acc: 0.9750 - val_loss: 0.0861 - val_acc: 0.9725\n",
            "Epoch 265/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0694 - acc: 0.9825\n",
            "Epoch 00265: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0713 - acc: 0.9817 - val_loss: 0.0822 - val_acc: 0.9758\n",
            "Epoch 266/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0691 - acc: 0.9829\n",
            "Epoch 00266: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0677 - acc: 0.9836 - val_loss: 0.0945 - val_acc: 0.9733\n",
            "Epoch 267/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0936 - acc: 0.9688\n",
            "Epoch 00267: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0945 - acc: 0.9686 - val_loss: 0.1690 - val_acc: 0.9408\n",
            "Epoch 268/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9757\n",
            "Epoch 00268: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0810 - acc: 0.9756 - val_loss: 0.0926 - val_acc: 0.9692\n",
            "Epoch 269/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9746\n",
            "Epoch 00269: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0853 - acc: 0.9747 - val_loss: 0.1484 - val_acc: 0.9500\n",
            "Epoch 270/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0854 - acc: 0.9703\n",
            "Epoch 00270: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0853 - acc: 0.9714 - val_loss: 0.1124 - val_acc: 0.9708\n",
            "Epoch 271/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9697\n",
            "Epoch 00271: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1031 - acc: 0.9703 - val_loss: 0.1284 - val_acc: 0.9667\n",
            "Epoch 272/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0865 - acc: 0.9730\n",
            "Epoch 00272: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0862 - acc: 0.9736 - val_loss: 0.1093 - val_acc: 0.9692\n",
            "Epoch 273/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0875 - acc: 0.9721\n",
            "Epoch 00273: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0882 - acc: 0.9722 - val_loss: 0.1292 - val_acc: 0.9558\n",
            "Epoch 274/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1099 - acc: 0.9661\n",
            "Epoch 00274: val_loss did not improve from 0.08187\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1094 - acc: 0.9667 - val_loss: 0.1203 - val_acc: 0.9667\n",
            "Epoch 275/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9714\n",
            "Epoch 00275: val_loss improved from 0.08187 to 0.07562, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.1014 - acc: 0.9719 - val_loss: 0.0756 - val_acc: 0.9808\n",
            "Epoch 276/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0747 - acc: 0.9770\n",
            "Epoch 00276: val_loss did not improve from 0.07562\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0761 - acc: 0.9778 - val_loss: 0.1243 - val_acc: 0.9608\n",
            "Epoch 277/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0730 - acc: 0.9797\n",
            "Epoch 00277: val_loss did not improve from 0.07562\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0728 - acc: 0.9803 - val_loss: 0.0852 - val_acc: 0.9742\n",
            "Epoch 278/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0799 - acc: 0.9788\n",
            "Epoch 00278: val_loss did not improve from 0.07562\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0826 - acc: 0.9769 - val_loss: 0.0936 - val_acc: 0.9683\n",
            "Epoch 279/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0885 - acc: 0.9756\n",
            "Epoch 00279: val_loss did not improve from 0.07562\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0897 - acc: 0.9750 - val_loss: 0.0946 - val_acc: 0.9750\n",
            "Epoch 280/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9769\n",
            "Epoch 00280: val_loss did not improve from 0.07562\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0784 - acc: 0.9769 - val_loss: 0.1224 - val_acc: 0.9558\n",
            "Epoch 281/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0794 - acc: 0.9756\n",
            "Epoch 00281: val_loss did not improve from 0.07562\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0795 - acc: 0.9758 - val_loss: 0.0909 - val_acc: 0.9733\n",
            "Epoch 282/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9731\n",
            "Epoch 00282: val_loss did not improve from 0.07562\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0844 - acc: 0.9736 - val_loss: 0.0946 - val_acc: 0.9725\n",
            "Epoch 283/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0724 - acc: 0.9778\n",
            "Epoch 00283: val_loss did not improve from 0.07562\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0717 - acc: 0.9775 - val_loss: 0.0890 - val_acc: 0.9725\n",
            "Epoch 284/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0652 - acc: 0.9803\n",
            "Epoch 00284: val_loss did not improve from 0.07562\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0657 - acc: 0.9803 - val_loss: 0.0892 - val_acc: 0.9700\n",
            "Epoch 285/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9831\n",
            "Epoch 00285: val_loss did not improve from 0.07562\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0650 - acc: 0.9828 - val_loss: 0.0956 - val_acc: 0.9733\n",
            "Epoch 286/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0872 - acc: 0.9744\n",
            "Epoch 00286: val_loss did not improve from 0.07562\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0859 - acc: 0.9756 - val_loss: 0.1076 - val_acc: 0.9692\n",
            "Epoch 287/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0844 - acc: 0.9724\n",
            "Epoch 00287: val_loss improved from 0.07562 to 0.07156, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.0838 - acc: 0.9725 - val_loss: 0.0716 - val_acc: 0.9808\n",
            "Epoch 288/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9800\n",
            "Epoch 00288: val_loss did not improve from 0.07156\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0694 - acc: 0.9800 - val_loss: 0.1131 - val_acc: 0.9633\n",
            "Epoch 289/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9757\n",
            "Epoch 00289: val_loss did not improve from 0.07156\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0811 - acc: 0.9753 - val_loss: 0.1411 - val_acc: 0.9558\n",
            "Epoch 290/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0840 - acc: 0.9762\n",
            "Epoch 00290: val_loss did not improve from 0.07156\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0843 - acc: 0.9761 - val_loss: 0.1029 - val_acc: 0.9708\n",
            "Epoch 291/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0848 - acc: 0.9758\n",
            "Epoch 00291: val_loss did not improve from 0.07156\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0861 - acc: 0.9756 - val_loss: 0.1005 - val_acc: 0.9733\n",
            "Epoch 292/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0758 - acc: 0.9782\n",
            "Epoch 00292: val_loss did not improve from 0.07156\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0752 - acc: 0.9781 - val_loss: 0.1078 - val_acc: 0.9650\n",
            "Epoch 293/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0698 - acc: 0.9809\n",
            "Epoch 00293: val_loss did not improve from 0.07156\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0699 - acc: 0.9803 - val_loss: 0.1139 - val_acc: 0.9625\n",
            "Epoch 294/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0845 - acc: 0.9741\n",
            "Epoch 00294: val_loss did not improve from 0.07156\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0838 - acc: 0.9742 - val_loss: 0.1201 - val_acc: 0.9575\n",
            "Epoch 295/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0836 - acc: 0.9728\n",
            "Epoch 00295: val_loss did not improve from 0.07156\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0857 - acc: 0.9728 - val_loss: 0.0985 - val_acc: 0.9700\n",
            "Epoch 296/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9691\n",
            "Epoch 00296: val_loss did not improve from 0.07156\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1022 - acc: 0.9692 - val_loss: 0.0980 - val_acc: 0.9717\n",
            "Epoch 297/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0790 - acc: 0.9762\n",
            "Epoch 00297: val_loss did not improve from 0.07156\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0785 - acc: 0.9764 - val_loss: 0.0839 - val_acc: 0.9742\n",
            "Epoch 298/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0866 - acc: 0.9724\n",
            "Epoch 00298: val_loss did not improve from 0.07156\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0877 - acc: 0.9722 - val_loss: 0.1069 - val_acc: 0.9675\n",
            "Epoch 299/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0774 - acc: 0.9771\n",
            "Epoch 00299: val_loss did not improve from 0.07156\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0807 - acc: 0.9758 - val_loss: 0.0999 - val_acc: 0.9700\n",
            "Epoch 300/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0798 - acc: 0.9765\n",
            "Epoch 00300: val_loss improved from 0.07156 to 0.06221, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.0792 - acc: 0.9769 - val_loss: 0.0622 - val_acc: 0.9858\n",
            "Epoch 301/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9791\n",
            "Epoch 00301: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0727 - acc: 0.9786 - val_loss: 0.1025 - val_acc: 0.9708\n",
            "Epoch 302/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0747 - acc: 0.9774\n",
            "Epoch 00302: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0744 - acc: 0.9772 - val_loss: 0.0782 - val_acc: 0.9783\n",
            "Epoch 303/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0825 - acc: 0.9732\n",
            "Epoch 00303: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0811 - acc: 0.9739 - val_loss: 0.0820 - val_acc: 0.9783\n",
            "Epoch 304/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0751 - acc: 0.9776\n",
            "Epoch 00304: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0767 - acc: 0.9775 - val_loss: 0.0911 - val_acc: 0.9742\n",
            "Epoch 305/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0854 - acc: 0.9762\n",
            "Epoch 00305: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0873 - acc: 0.9756 - val_loss: 0.1110 - val_acc: 0.9667\n",
            "Epoch 306/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0908 - acc: 0.9744\n",
            "Epoch 00306: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0897 - acc: 0.9744 - val_loss: 0.1055 - val_acc: 0.9692\n",
            "Epoch 307/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9697\n",
            "Epoch 00307: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0957 - acc: 0.9700 - val_loss: 0.1209 - val_acc: 0.9575\n",
            "Epoch 308/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0823 - acc: 0.9738\n",
            "Epoch 00308: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0827 - acc: 0.9736 - val_loss: 0.0767 - val_acc: 0.9733\n",
            "Epoch 309/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0716 - acc: 0.9791\n",
            "Epoch 00309: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0719 - acc: 0.9789 - val_loss: 0.0703 - val_acc: 0.9808\n",
            "Epoch 310/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9757\n",
            "Epoch 00310: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0746 - acc: 0.9761 - val_loss: 0.0978 - val_acc: 0.9700\n",
            "Epoch 311/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0770 - acc: 0.9758\n",
            "Epoch 00311: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0754 - acc: 0.9769 - val_loss: 0.0866 - val_acc: 0.9758\n",
            "Epoch 312/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0674 - acc: 0.9821\n",
            "Epoch 00312: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0685 - acc: 0.9814 - val_loss: 0.0783 - val_acc: 0.9742\n",
            "Epoch 313/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9771\n",
            "Epoch 00313: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0778 - acc: 0.9775 - val_loss: 0.1175 - val_acc: 0.9600\n",
            "Epoch 314/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0904 - acc: 0.9732\n",
            "Epoch 00314: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0914 - acc: 0.9725 - val_loss: 0.1452 - val_acc: 0.9575\n",
            "Epoch 315/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0910 - acc: 0.9719\n",
            "Epoch 00315: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0895 - acc: 0.9725 - val_loss: 0.1234 - val_acc: 0.9642\n",
            "Epoch 316/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0833 - acc: 0.9732\n",
            "Epoch 00316: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0843 - acc: 0.9733 - val_loss: 0.0984 - val_acc: 0.9667\n",
            "Epoch 317/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0833 - acc: 0.9733\n",
            "Epoch 00317: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0830 - acc: 0.9736 - val_loss: 0.1245 - val_acc: 0.9617\n",
            "Epoch 318/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0843 - acc: 0.9745\n",
            "Epoch 00318: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0860 - acc: 0.9744 - val_loss: 0.1185 - val_acc: 0.9575\n",
            "Epoch 319/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0881 - acc: 0.9739\n",
            "Epoch 00319: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0869 - acc: 0.9744 - val_loss: 0.0897 - val_acc: 0.9717\n",
            "Epoch 320/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0810 - acc: 0.9759\n",
            "Epoch 00320: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0791 - acc: 0.9769 - val_loss: 0.0739 - val_acc: 0.9792\n",
            "Epoch 321/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0908 - acc: 0.9758\n",
            "Epoch 00321: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0900 - acc: 0.9747 - val_loss: 0.1183 - val_acc: 0.9583\n",
            "Epoch 322/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9771\n",
            "Epoch 00322: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0823 - acc: 0.9769 - val_loss: 0.1234 - val_acc: 0.9625\n",
            "Epoch 323/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9789\n",
            "Epoch 00323: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0718 - acc: 0.9789 - val_loss: 0.0931 - val_acc: 0.9725\n",
            "Epoch 324/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0892 - acc: 0.9718\n",
            "Epoch 00324: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0871 - acc: 0.9728 - val_loss: 0.0877 - val_acc: 0.9750\n",
            "Epoch 325/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9714\n",
            "Epoch 00325: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0784 - acc: 0.9714 - val_loss: 0.1085 - val_acc: 0.9600\n",
            "Epoch 326/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0720 - acc: 0.9797\n",
            "Epoch 00326: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0714 - acc: 0.9800 - val_loss: 0.1143 - val_acc: 0.9667\n",
            "Epoch 327/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0652 - acc: 0.9827\n",
            "Epoch 00327: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0638 - acc: 0.9839 - val_loss: 0.1033 - val_acc: 0.9650\n",
            "Epoch 328/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0686 - acc: 0.9803\n",
            "Epoch 00328: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0696 - acc: 0.9797 - val_loss: 0.0834 - val_acc: 0.9767\n",
            "Epoch 329/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0740 - acc: 0.9785\n",
            "Epoch 00329: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0726 - acc: 0.9792 - val_loss: 0.0724 - val_acc: 0.9775\n",
            "Epoch 330/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0658 - acc: 0.9826\n",
            "Epoch 00330: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0658 - acc: 0.9822 - val_loss: 0.0833 - val_acc: 0.9758\n",
            "Epoch 331/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0759 - acc: 0.9773\n",
            "Epoch 00331: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0746 - acc: 0.9778 - val_loss: 0.1101 - val_acc: 0.9658\n",
            "Epoch 332/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0683 - acc: 0.9797\n",
            "Epoch 00332: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0677 - acc: 0.9800 - val_loss: 0.0650 - val_acc: 0.9792\n",
            "Epoch 333/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0671 - acc: 0.9815\n",
            "Epoch 00333: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0677 - acc: 0.9814 - val_loss: 0.0855 - val_acc: 0.9717\n",
            "Epoch 334/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0744 - acc: 0.9782\n",
            "Epoch 00334: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0723 - acc: 0.9789 - val_loss: 0.0718 - val_acc: 0.9792\n",
            "Epoch 335/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9809\n",
            "Epoch 00335: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0730 - acc: 0.9800 - val_loss: 0.1028 - val_acc: 0.9700\n",
            "Epoch 336/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0705 - acc: 0.9785\n",
            "Epoch 00336: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0743 - acc: 0.9769 - val_loss: 0.1042 - val_acc: 0.9633\n",
            "Epoch 337/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0770 - acc: 0.9776\n",
            "Epoch 00337: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0755 - acc: 0.9783 - val_loss: 0.0859 - val_acc: 0.9717\n",
            "Epoch 338/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0894 - acc: 0.9747\n",
            "Epoch 00338: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0899 - acc: 0.9742 - val_loss: 0.0751 - val_acc: 0.9758\n",
            "Epoch 339/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0912 - acc: 0.9716\n",
            "Epoch 00339: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0897 - acc: 0.9717 - val_loss: 0.0725 - val_acc: 0.9717\n",
            "Epoch 340/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0888 - acc: 0.9735\n",
            "Epoch 00340: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0892 - acc: 0.9733 - val_loss: 0.1104 - val_acc: 0.9667\n",
            "Epoch 341/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0796 - acc: 0.9741\n",
            "Epoch 00341: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0788 - acc: 0.9747 - val_loss: 0.0814 - val_acc: 0.9758\n",
            "Epoch 342/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0799 - acc: 0.9744\n",
            "Epoch 00342: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0779 - acc: 0.9756 - val_loss: 0.0999 - val_acc: 0.9633\n",
            "Epoch 343/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0759 - acc: 0.9762\n",
            "Epoch 00343: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0767 - acc: 0.9761 - val_loss: 0.1122 - val_acc: 0.9650\n",
            "Epoch 344/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0866 - acc: 0.9716\n",
            "Epoch 00344: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0885 - acc: 0.9708 - val_loss: 0.1231 - val_acc: 0.9633\n",
            "Epoch 345/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0895 - acc: 0.9739\n",
            "Epoch 00345: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0880 - acc: 0.9747 - val_loss: 0.1260 - val_acc: 0.9625\n",
            "Epoch 346/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9780\n",
            "Epoch 00346: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0800 - acc: 0.9764 - val_loss: 0.1266 - val_acc: 0.9550\n",
            "Epoch 347/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9766\n",
            "Epoch 00347: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0801 - acc: 0.9764 - val_loss: 0.0897 - val_acc: 0.9717\n",
            "Epoch 348/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9800\n",
            "Epoch 00348: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0711 - acc: 0.9806 - val_loss: 0.0961 - val_acc: 0.9708\n",
            "Epoch 349/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9823\n",
            "Epoch 00349: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0657 - acc: 0.9814 - val_loss: 0.1001 - val_acc: 0.9692\n",
            "Epoch 350/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0783 - acc: 0.9771\n",
            "Epoch 00350: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0772 - acc: 0.9775 - val_loss: 0.1031 - val_acc: 0.9658\n",
            "Epoch 351/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9714\n",
            "Epoch 00351: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0847 - acc: 0.9717 - val_loss: 0.0920 - val_acc: 0.9700\n",
            "Epoch 352/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9814\n",
            "Epoch 00352: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0674 - acc: 0.9806 - val_loss: 0.0724 - val_acc: 0.9833\n",
            "Epoch 353/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0715 - acc: 0.9794\n",
            "Epoch 00353: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0715 - acc: 0.9797 - val_loss: 0.1001 - val_acc: 0.9692\n",
            "Epoch 354/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9869\n",
            "Epoch 00354: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0567 - acc: 0.9867 - val_loss: 0.0686 - val_acc: 0.9833\n",
            "Epoch 355/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0733 - acc: 0.9785\n",
            "Epoch 00355: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0738 - acc: 0.9781 - val_loss: 0.0889 - val_acc: 0.9742\n",
            "Epoch 356/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0637 - acc: 0.9803\n",
            "Epoch 00356: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0630 - acc: 0.9803 - val_loss: 0.1242 - val_acc: 0.9625\n",
            "Epoch 357/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9837\n",
            "Epoch 00357: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0590 - acc: 0.9842 - val_loss: 0.0900 - val_acc: 0.9767\n",
            "Epoch 358/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0718 - acc: 0.9770\n",
            "Epoch 00358: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0713 - acc: 0.9772 - val_loss: 0.0925 - val_acc: 0.9692\n",
            "Epoch 359/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9809\n",
            "Epoch 00359: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.0621 - acc: 0.9806 - val_loss: 0.0720 - val_acc: 0.9758\n",
            "Epoch 360/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0747 - acc: 0.9779\n",
            "Epoch 00360: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0743 - acc: 0.9783 - val_loss: 0.0995 - val_acc: 0.9667\n",
            "Epoch 361/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0906 - acc: 0.9721\n",
            "Epoch 00361: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0890 - acc: 0.9728 - val_loss: 0.0909 - val_acc: 0.9683\n",
            "Epoch 362/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0874 - acc: 0.9731\n",
            "Epoch 00362: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0861 - acc: 0.9739 - val_loss: 0.0854 - val_acc: 0.9742\n",
            "Epoch 363/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0797 - acc: 0.9748\n",
            "Epoch 00363: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0795 - acc: 0.9753 - val_loss: 0.0857 - val_acc: 0.9742\n",
            "Epoch 364/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0811 - acc: 0.9774\n",
            "Epoch 00364: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0809 - acc: 0.9772 - val_loss: 0.1114 - val_acc: 0.9600\n",
            "Epoch 365/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0789 - acc: 0.9747\n",
            "Epoch 00365: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0782 - acc: 0.9744 - val_loss: 0.0741 - val_acc: 0.9800\n",
            "Epoch 366/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0639 - acc: 0.9825\n",
            "Epoch 00366: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0632 - acc: 0.9828 - val_loss: 0.1380 - val_acc: 0.9517\n",
            "Epoch 367/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9837\n",
            "Epoch 00367: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0597 - acc: 0.9836 - val_loss: 0.1261 - val_acc: 0.9600\n",
            "Epoch 368/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0661 - acc: 0.9787\n",
            "Epoch 00368: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0661 - acc: 0.9786 - val_loss: 0.1586 - val_acc: 0.9517\n",
            "Epoch 369/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0915 - acc: 0.9731\n",
            "Epoch 00369: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0934 - acc: 0.9725 - val_loss: 0.1235 - val_acc: 0.9575\n",
            "Epoch 370/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0755 - acc: 0.9766\n",
            "Epoch 00370: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0748 - acc: 0.9764 - val_loss: 0.0966 - val_acc: 0.9717\n",
            "Epoch 371/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0715 - acc: 0.9794\n",
            "Epoch 00371: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0760 - acc: 0.9772 - val_loss: 0.1399 - val_acc: 0.9517\n",
            "Epoch 372/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0972 - acc: 0.9681\n",
            "Epoch 00372: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0985 - acc: 0.9675 - val_loss: 0.0995 - val_acc: 0.9650\n",
            "Epoch 373/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0815 - acc: 0.9755\n",
            "Epoch 00373: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0809 - acc: 0.9756 - val_loss: 0.1060 - val_acc: 0.9667\n",
            "Epoch 374/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0651 - acc: 0.9803\n",
            "Epoch 00374: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0648 - acc: 0.9811 - val_loss: 0.0952 - val_acc: 0.9700\n",
            "Epoch 375/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0760 - acc: 0.9785\n",
            "Epoch 00375: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0777 - acc: 0.9783 - val_loss: 0.1010 - val_acc: 0.9667\n",
            "Epoch 376/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9740\n",
            "Epoch 00376: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0778 - acc: 0.9744 - val_loss: 0.0749 - val_acc: 0.9800\n",
            "Epoch 377/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9823\n",
            "Epoch 00377: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0634 - acc: 0.9822 - val_loss: 0.1245 - val_acc: 0.9642\n",
            "Epoch 378/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0754 - acc: 0.9784\n",
            "Epoch 00378: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0736 - acc: 0.9783 - val_loss: 0.0937 - val_acc: 0.9708\n",
            "Epoch 379/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0640 - acc: 0.9815\n",
            "Epoch 00379: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0632 - acc: 0.9817 - val_loss: 0.0743 - val_acc: 0.9800\n",
            "Epoch 380/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0708 - acc: 0.9803\n",
            "Epoch 00380: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0695 - acc: 0.9808 - val_loss: 0.0805 - val_acc: 0.9750\n",
            "Epoch 381/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0615 - acc: 0.9800\n",
            "Epoch 00381: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0624 - acc: 0.9794 - val_loss: 0.1187 - val_acc: 0.9600\n",
            "Epoch 382/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0536 - acc: 0.9844\n",
            "Epoch 00382: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0559 - acc: 0.9836 - val_loss: 0.0810 - val_acc: 0.9675\n",
            "Epoch 383/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0585 - acc: 0.9821\n",
            "Epoch 00383: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0597 - acc: 0.9822 - val_loss: 0.0883 - val_acc: 0.9742\n",
            "Epoch 384/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0559 - acc: 0.9842\n",
            "Epoch 00384: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0559 - acc: 0.9844 - val_loss: 0.1146 - val_acc: 0.9633\n",
            "Epoch 385/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0582 - acc: 0.9829\n",
            "Epoch 00385: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0590 - acc: 0.9825 - val_loss: 0.0986 - val_acc: 0.9717\n",
            "Epoch 386/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0616 - acc: 0.9848\n",
            "Epoch 00386: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0605 - acc: 0.9853 - val_loss: 0.1031 - val_acc: 0.9675\n",
            "Epoch 387/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0888 - acc: 0.9724\n",
            "Epoch 00387: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0871 - acc: 0.9728 - val_loss: 0.0772 - val_acc: 0.9767\n",
            "Epoch 388/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0816 - acc: 0.9726\n",
            "Epoch 00388: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0817 - acc: 0.9722 - val_loss: 0.1029 - val_acc: 0.9683\n",
            "Epoch 389/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0876 - acc: 0.9742\n",
            "Epoch 00389: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0879 - acc: 0.9739 - val_loss: 0.0958 - val_acc: 0.9733\n",
            "Epoch 390/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9763\n",
            "Epoch 00390: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0765 - acc: 0.9769 - val_loss: 0.1076 - val_acc: 0.9608\n",
            "Epoch 391/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0775 - acc: 0.9771\n",
            "Epoch 00391: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0774 - acc: 0.9769 - val_loss: 0.0759 - val_acc: 0.9750\n",
            "Epoch 392/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9826\n",
            "Epoch 00392: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0592 - acc: 0.9825 - val_loss: 0.1004 - val_acc: 0.9675\n",
            "Epoch 393/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0662 - acc: 0.9791\n",
            "Epoch 00393: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0692 - acc: 0.9781 - val_loss: 0.0915 - val_acc: 0.9742\n",
            "Epoch 394/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0825 - acc: 0.9769\n",
            "Epoch 00394: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0794 - acc: 0.9781 - val_loss: 0.0977 - val_acc: 0.9692\n",
            "Epoch 395/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0792 - acc: 0.9744\n",
            "Epoch 00395: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0784 - acc: 0.9747 - val_loss: 0.0836 - val_acc: 0.9783\n",
            "Epoch 396/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0898 - acc: 0.9697\n",
            "Epoch 00396: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0901 - acc: 0.9697 - val_loss: 0.1131 - val_acc: 0.9633\n",
            "Epoch 397/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0993 - acc: 0.9656\n",
            "Epoch 00397: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0979 - acc: 0.9658 - val_loss: 0.1110 - val_acc: 0.9658\n",
            "Epoch 398/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9706\n",
            "Epoch 00398: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1008 - acc: 0.9700 - val_loss: 0.1482 - val_acc: 0.9483\n",
            "Epoch 399/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9757\n",
            "Epoch 00399: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0879 - acc: 0.9761 - val_loss: 0.0795 - val_acc: 0.9767\n",
            "Epoch 400/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0847 - acc: 0.9735\n",
            "Epoch 00400: val_loss did not improve from 0.06221\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0831 - acc: 0.9744 - val_loss: 0.0868 - val_acc: 0.9700\n",
            "1200/1200 [==============================] - 0s 117us/sample - loss: 0.0868 - acc: 0.9700\n",
            "[0.0868143937488397, 0.97]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8yZ6w48twzs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "c9a28b8f-0be6-4bba-b864-d5928588c457"
      },
      "source": [
        "for i in range(8, 9): # Итерација низ секој испитен примерок\n",
        "  print(f\"====================== Примерок ({i}) ======================\")\n",
        "  print(\"Вчитување тест податоци од испитниот примерок \" + str(i) + \"...\")\n",
        "  \n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_test_data = np.empty(0)\n",
        "  temp_test_events = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_test_set = 'S' + format(j, '02') + '/Test'\n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testData.mat\"\n",
        "    temp = loadmat(full_path)['testData']\n",
        "    if temp_test_data.size != 0:\n",
        "      temp_test_data = np.concatenate((temp_test_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_test_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_test_events.size != 0:\n",
        "        temp_test_events = np.append(temp_test_events, temp)\n",
        "      else:\n",
        "        temp_test_events = np.array(temp)\n",
        "\n",
        "    # Вчитување на бројот на runs \n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/runs_per_block.txt\"\n",
        "    with open(full_path, \"r\") as runs_per_block:\n",
        "      test_runs_per_block[i-1][j-1] = int(runs_per_block.read())\n",
        "\n",
        "    print(\"\\t - Тест податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "  # Зачувај ги тест податоците вчитани од испитниот примерок во низа\n",
        "  test_data.append(temp_test_data)\n",
        "  test_events.append(temp_test_events)\n",
        "  print(\"Тест податоците од испитниот примерок \" + str(i) + \" се вчитани.\\n\")\n",
        "\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_data: \" + str(test_data[i-1].shape)) # test_data to predict\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_events: \" + str(len(test_events[i-1]))) # test_events\n",
        "  for j in range (1,4):\n",
        "    print(\"SBJ\" + str(format(i-1, '02')) + \" / S\" + str(format(j-1, '02')) + \"| Runs per block: \" + str(test_runs_per_block[i-1][j-1])) # runs per block in SJB01, SJ00 \n",
        "\n",
        "  to_predict_data = reshape_data_to_mne_format(test_data[i-1])\n",
        "  predictions = model8.predict(to_predict_data)\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Predictions: \" + str(len(predictions)))\n",
        "  # np.savetxt(\"predictions.csv\", predictions, delimiter=\",\")\n",
        "\n",
        "\n",
        "  # ========= FALI USTE DA SE ISPARSIRA PREDICTIONOT... NE E SREDEN OVOJ KOD DOLE =======\n",
        "\n",
        "  int_pred = np.argmax(predictions, axis=1)\n",
        "  int_ytest = np.argmax(y_test, axis=1)\n",
        "\n",
        "  session_start = 0\n",
        "  start_prediction_index = 0\n",
        "  end_prediction_index = 0\n",
        "  for session in range(0, 3):\n",
        "    print(f\"============== Сесија ({session}) ==============\")\n",
        "    for block in range(0, 50):    \n",
        "      events_per_block = test_runs_per_block[i-1][session]\n",
        "\n",
        "      start_prediction_index = session_start + (block*events_per_block)*8\n",
        "      end_prediction_index = session_start + ((block+1)*events_per_block)*8\n",
        "\n",
        "      block_prediction = int_pred[start_prediction_index:end_prediction_index]\n",
        "      prediction = np.bincount(block_prediction).argmax()\n",
        "      df.iat[session+21,block+2] = prediction+1\n",
        "      # UNCOMMENT ZA PODOBAR PRIKAZ :)\n",
        "      # print(f\"Session {session} | Block: {block} | Prediction: {prediction} | Address: {end_prediction_index}\")\n",
        "\n",
        "      print(str(prediction+1) + \",\", end=\"\")\n",
        "    session_start = end_prediction_index\n",
        "    print(\"\")\n",
        "  print(\"Stigna li do kraj: \" + str(session_start == len(predictions)))\n",
        "  print(f\"====================== Примерок ({i}) ======================\\n\\n\")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Примерок (8) ======================\n",
            "Вчитување тест податоци од испитниот примерок 8...\n",
            "\t - Тест податоците од сесија 1 се вчитани.\n",
            "\t - Тест податоците од сесија 2 се вчитани.\n",
            "\t - Тест податоците од сесија 3 се вчитани.\n",
            "Тест податоците од испитниот примерок 8 се вчитани.\n",
            "\n",
            "SBJ07| Test_data: (8, 350, 7600)\n",
            "SBJ07| Test_events: 7600\n",
            "SBJ07 / S00| Runs per block: 8\n",
            "SBJ07 / S01| Runs per block: 7\n",
            "SBJ07 / S02| Runs per block: 4\n",
            "SBJ07| Predictions: 7600\n",
            "============== Сесија (0) ==============\n",
            "5,8,2,8,2,2,1,1,1,8,5,2,5,2,1,2,5,7,1,8,1,1,1,3,3,7,1,2,5,8,7,7,1,1,2,5,2,4,1,5,7,5,1,8,2,2,5,5,2,8,\n",
            "============== Сесија (1) ==============\n",
            "2,7,1,5,1,2,1,1,2,7,7,7,1,7,7,7,7,7,7,3,1,2,2,2,7,7,1,6,1,8,2,1,5,2,7,6,1,8,1,7,6,5,7,1,7,8,2,1,7,7,\n",
            "============== Сесија (2) ==============\n",
            "4,1,6,6,1,2,1,1,1,1,1,1,1,1,5,3,3,1,1,1,1,5,3,3,1,1,1,1,1,1,1,7,7,1,1,1,7,7,1,1,1,1,1,8,1,7,1,1,7,1,\n",
            "Stigna li do kraj: True\n",
            "====================== Примерок (8) ======================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_5uAf97uDs8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "70d88758-3535-4ac7-fda9-ecf2c04802ee"
      },
      "source": [
        "df"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUBJECT</th>\n",
              "      <th>SESSION</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>Unnamed: 52</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    SUBJECT  SESSION  1  2  3  4  5  6  7  ... 43 44 45 46 47 48 49 50 Unnamed: 52\n",
              "0         1        1  6  6  3  6  6  3  6  ...  6  4  8  3  4  5  5  7         NaN\n",
              "1         1        2  6  3  2  1  2  1  3  ...  6  2  2  2  3  6  6  2         NaN\n",
              "2         1        3  3  3  3  3  3  3  3  ...  3  3  6  3  6  7  1  6         NaN\n",
              "3         2        1  8  8  8  8  8  8  8  ...  8  4  8  4  8  7  8  8         NaN\n",
              "4         2        2  2  7  6  6  6  6  7  ...  2  6  6  2  6  6  2  2         NaN\n",
              "5         2        3  2  7  2  6  7  4  2  ...  2  6  2  2  7  2  2  2         NaN\n",
              "6         3        1  3  3  4  4  4  3  6  ...  4  6  3  3  4  3  4  4         NaN\n",
              "7         3        2  6  1  7  7  7  7  7  ...  6  7  1  6  6  6  6  6         NaN\n",
              "8         3        3  6  4  8  8  5  7  8  ...  4  2  8  2  2  2  2  8         NaN\n",
              "9         4        1  1  1  2  1  5  5  6  ...  1  1  7  1  6  6  6  6         NaN\n",
              "10        4        2  7  7  7  7  6  6  7  ...  1  5  5  5  5  5  2  6         NaN\n",
              "11        4        3  7  3  7  3  6  6  7  ...  6  1  4  4  4  6  6  6         NaN\n",
              "12        5        1  5  4  4  4  6  4  5  ...  3  1  4  4  1  3  3  5         NaN\n",
              "13        5        2  3  6  3  5  2  6  7  ...  6  6  6  6  6  6  6  4         NaN\n",
              "14        5        3  4  3  3  6  5  7  6  ...  7  2  2  7  5  6  4  2         NaN\n",
              "15        6        1  1  3  1  8  3  3  3  ...  4  3  7  2  7  2  5  8         NaN\n",
              "16        6        2  3  4  1  7  1  1  1  ...  5  5  5  5  5  5  5  5         NaN\n",
              "17        6        3  2  3  2  5  3  3  3  ...  2  3  3  3  1  1  3  3         NaN\n",
              "18        7        1  4  7  5  7  4  5  4  ...  5  1  1  4  4  4  1  4         NaN\n",
              "19        7        2  2  2  2  8  2  5  2  ...  5  5  5  2  2  2  2  2         NaN\n",
              "20        7        3  2  7  7  5  7  5  4  ...  1  3  5  5  3  5  5  1         NaN\n",
              "21        8        1  5  8  2  8  2  2  1  ...  1  8  2  2  5  5  2  8         NaN\n",
              "22        8        2  2  7  1  5  1  2  1  ...  7  1  7  8  2  1  7  7         NaN\n",
              "23        8        3  4  1  6  6  1  2  1  ...  1  8  1  7  1  1  7  1         NaN\n",
              "24        9        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "25        9        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "26        9        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "27       10        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "28       10        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "29       10        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "30       11        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "31       11        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "32       11        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "33       12        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "34       12        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "35       12        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "36       13        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "37       13        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "38       13        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "39       14        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "40       14        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "41       14        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "42       15        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "43       15        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "44       15        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "\n",
              "[45 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUE_aL2Quda0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6d093d13-db95-407a-edd2-6dda5cfb13d0"
      },
      "source": [
        "for i in range(9, 10): # Итерација низ секој испитен примерок\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_data = np.empty(0)\n",
        "  temp_labels = np.empty(0)\n",
        "  temp_events = np.empty(0)\n",
        "  temp_targets = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainData.mat\"\n",
        "    temp = loadmat(full_path)['trainData']\n",
        "    if temp_data.size != 0:\n",
        "      temp_data = np.concatenate((temp_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на label-ите\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainLabels.txt\"\n",
        "    with open(full_path, \"r\") as file_labels:\n",
        "      temp = file_labels.read().splitlines()\n",
        "      temp = np.repeat(temp, 8*10)\n",
        "      if temp_labels.size != 0:\n",
        "        temp_labels = np.concatenate((temp_labels, temp))\n",
        "      else:\n",
        "        temp_labels = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_events.size != 0:\n",
        "        temp_events = np.append(temp_events, temp)\n",
        "      else:\n",
        "        temp_events = np.array(temp)\n",
        "      \n",
        "\n",
        "    # Вчитување на редоследот на објекти кои се target\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainTargets.txt\"\n",
        "    with open(full_path, \"r\") as file_targets:\n",
        "      temp = file_targets.read().splitlines()\n",
        "      if temp_targets.size != 0:\n",
        "        temp_targets = np.concatenate((temp_targets, temp))\n",
        "      else:\n",
        "        temp_targets = np.array(temp)\n",
        "    print(\"\\t - Податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "\n",
        "  for j in range(4, 8): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "      \n",
        "  # Зачувај ги податоците вчитани од испитниот примерок во низа\n",
        "  data.append(temp_data)\n",
        "  labels.append(temp_labels)\n",
        "  events.append(temp_events)\n",
        "  targets.append(temp_targets)\n",
        "\n",
        "  \n",
        "  print(\"Податоците од испитниот примерок \" + str(i) + \" се вчитани.\")\n",
        "\n",
        "\n",
        "  #data = target_events_data_scaled\n",
        "  mne_array = np.swapaxes(data[i-1], 0, 2) # (епохa, канал, настан). \n",
        "  mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "  mne_array = mne_array.reshape(mne_array.shape[0],1, 8,350)\n",
        "  print(mne_array.shape)\n",
        "\n",
        "  events_arr = events[i-1].astype(np.int)\n",
        "  labels_arr = labels[i-1].astype(np.int)\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(mne_array, labels_arr-1, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "  model9 = DeepConvNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "  model9.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = Adam(0.0009))\n",
        "  checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                                verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "  #clf = RandomForestClassifier(max_depth=5)\n",
        "  #clf.fit(X_train, y_train)\n",
        "  #score = clf.score(X_test, y_test)\n",
        "  # print(score)\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  num_batch_size=100\n",
        "  num_epochs=400\n",
        "  model9.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "            validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "  score = model9.evaluate(X_test, y_test, verbose=1)\n",
        "  print(score)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t - Податоците од сесија 1 се вчитани.\n",
            "\t - Податоците од сесија 2 се вчитани.\n",
            "\t - Податоците од сесија 3 се вчитани.\n",
            "Податоците од испитниот примерок 9 се вчитани.\n",
            "(4800, 1, 8, 350)\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Train on 3600 samples, validate on 1200 samples\n",
            "Epoch 1/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.1180 - acc: 0.2318\n",
            "Epoch 00001: val_loss improved from inf to 2.16939, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 7s 2ms/sample - loss: 2.1121 - acc: 0.2297 - val_loss: 2.1694 - val_acc: 0.2000\n",
            "Epoch 2/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.8936 - acc: 0.2657\n",
            "Epoch 00002: val_loss improved from 2.16939 to 1.84940, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 1.8887 - acc: 0.2653 - val_loss: 1.8494 - val_acc: 0.2392\n",
            "Epoch 3/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.7952 - acc: 0.3000\n",
            "Epoch 00003: val_loss did not improve from 1.84940\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 1.7942 - acc: 0.3031 - val_loss: 1.9306 - val_acc: 0.2575\n",
            "Epoch 4/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.7691 - acc: 0.3262\n",
            "Epoch 00004: val_loss improved from 1.84940 to 1.84117, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 1.7678 - acc: 0.3261 - val_loss: 1.8412 - val_acc: 0.2908\n",
            "Epoch 5/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.7160 - acc: 0.3455\n",
            "Epoch 00005: val_loss improved from 1.84117 to 1.78816, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 1.7196 - acc: 0.3481 - val_loss: 1.7882 - val_acc: 0.3267\n",
            "Epoch 6/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.7318 - acc: 0.3584\n",
            "Epoch 00006: val_loss improved from 1.78816 to 1.70067, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 1.7248 - acc: 0.3528 - val_loss: 1.7007 - val_acc: 0.3717\n",
            "Epoch 7/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.5939 - acc: 0.3919\n",
            "Epoch 00007: val_loss did not improve from 1.70067\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.5906 - acc: 0.3981 - val_loss: 1.8279 - val_acc: 0.3517\n",
            "Epoch 8/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.4954 - acc: 0.4270\n",
            "Epoch 00008: val_loss improved from 1.70067 to 1.66606, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 1.4916 - acc: 0.4264 - val_loss: 1.6661 - val_acc: 0.3658\n",
            "Epoch 9/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.3795 - acc: 0.4661\n",
            "Epoch 00009: val_loss improved from 1.66606 to 1.57385, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 1.3859 - acc: 0.4614 - val_loss: 1.5738 - val_acc: 0.4150\n",
            "Epoch 10/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.3178 - acc: 0.4853\n",
            "Epoch 00010: val_loss improved from 1.57385 to 1.41848, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 1.3284 - acc: 0.4831 - val_loss: 1.4185 - val_acc: 0.4358\n",
            "Epoch 11/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.2649 - acc: 0.5085\n",
            "Epoch 00011: val_loss did not improve from 1.41848\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 1.2625 - acc: 0.5064 - val_loss: 1.4819 - val_acc: 0.4325\n",
            "Epoch 12/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.2176 - acc: 0.5221\n",
            "Epoch 00012: val_loss did not improve from 1.41848\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 1.2258 - acc: 0.5217 - val_loss: 1.4676 - val_acc: 0.4583\n",
            "Epoch 13/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.2138 - acc: 0.5226\n",
            "Epoch 00013: val_loss improved from 1.41848 to 1.34138, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 1.2115 - acc: 0.5222 - val_loss: 1.3414 - val_acc: 0.4792\n",
            "Epoch 14/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.1426 - acc: 0.5418\n",
            "Epoch 00014: val_loss improved from 1.34138 to 1.27034, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 1.1380 - acc: 0.5436 - val_loss: 1.2703 - val_acc: 0.4992\n",
            "Epoch 15/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.1243 - acc: 0.5588\n",
            "Epoch 00015: val_loss did not improve from 1.27034\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 1.1266 - acc: 0.5567 - val_loss: 1.2808 - val_acc: 0.5108\n",
            "Epoch 16/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.1032 - acc: 0.5650\n",
            "Epoch 00016: val_loss improved from 1.27034 to 1.15857, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 1.1010 - acc: 0.5661 - val_loss: 1.1586 - val_acc: 0.5283\n",
            "Epoch 17/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.0557 - acc: 0.5926\n",
            "Epoch 00017: val_loss did not improve from 1.15857\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.0498 - acc: 0.5950 - val_loss: 1.1832 - val_acc: 0.5433\n",
            "Epoch 18/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9981 - acc: 0.6103\n",
            "Epoch 00018: val_loss improved from 1.15857 to 1.05144, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.9974 - acc: 0.6119 - val_loss: 1.0514 - val_acc: 0.5808\n",
            "Epoch 19/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9679 - acc: 0.6209\n",
            "Epoch 00019: val_loss did not improve from 1.05144\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.9679 - acc: 0.6189 - val_loss: 1.1123 - val_acc: 0.5725\n",
            "Epoch 20/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9239 - acc: 0.6491\n",
            "Epoch 00020: val_loss did not improve from 1.05144\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.9334 - acc: 0.6436 - val_loss: 1.0575 - val_acc: 0.5850\n",
            "Epoch 21/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8838 - acc: 0.6612\n",
            "Epoch 00021: val_loss improved from 1.05144 to 0.94388, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.8870 - acc: 0.6575 - val_loss: 0.9439 - val_acc: 0.6242\n",
            "Epoch 22/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8553 - acc: 0.6774\n",
            "Epoch 00022: val_loss did not improve from 0.94388\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.8535 - acc: 0.6783 - val_loss: 1.0824 - val_acc: 0.5708\n",
            "Epoch 23/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.8000 - acc: 0.6909\n",
            "Epoch 00023: val_loss did not improve from 0.94388\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.7978 - acc: 0.6917 - val_loss: 1.0083 - val_acc: 0.6008\n",
            "Epoch 24/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7809 - acc: 0.7044\n",
            "Epoch 00024: val_loss did not improve from 0.94388\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.7808 - acc: 0.7031 - val_loss: 1.0613 - val_acc: 0.5825\n",
            "Epoch 25/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7470 - acc: 0.7163\n",
            "Epoch 00025: val_loss did not improve from 0.94388\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.7460 - acc: 0.7161 - val_loss: 0.9711 - val_acc: 0.6242\n",
            "Epoch 26/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7508 - acc: 0.7115\n",
            "Epoch 00026: val_loss did not improve from 0.94388\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.7507 - acc: 0.7125 - val_loss: 1.0574 - val_acc: 0.5925\n",
            "Epoch 27/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7424 - acc: 0.7094\n",
            "Epoch 00027: val_loss improved from 0.94388 to 0.87709, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.7406 - acc: 0.7100 - val_loss: 0.8771 - val_acc: 0.6542\n",
            "Epoch 28/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6764 - acc: 0.7385\n",
            "Epoch 00028: val_loss improved from 0.87709 to 0.83827, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.6748 - acc: 0.7400 - val_loss: 0.8383 - val_acc: 0.6808\n",
            "Epoch 29/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.6698 - acc: 0.7506\n",
            "Epoch 00029: val_loss did not improve from 0.83827\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.6673 - acc: 0.7506 - val_loss: 0.8989 - val_acc: 0.6675\n",
            "Epoch 30/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6248 - acc: 0.7709\n",
            "Epoch 00030: val_loss improved from 0.83827 to 0.80669, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.6314 - acc: 0.7667 - val_loss: 0.8067 - val_acc: 0.7025\n",
            "Epoch 31/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.6233 - acc: 0.7731\n",
            "Epoch 00031: val_loss did not improve from 0.80669\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.6335 - acc: 0.7692 - val_loss: 0.8857 - val_acc: 0.6592\n",
            "Epoch 32/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5908 - acc: 0.7871\n",
            "Epoch 00032: val_loss did not improve from 0.80669\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.5919 - acc: 0.7875 - val_loss: 0.8298 - val_acc: 0.6617\n",
            "Epoch 33/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5676 - acc: 0.7894\n",
            "Epoch 00033: val_loss did not improve from 0.80669\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.5694 - acc: 0.7894 - val_loss: 0.9368 - val_acc: 0.6375\n",
            "Epoch 34/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5392 - acc: 0.8058\n",
            "Epoch 00034: val_loss improved from 0.80669 to 0.73473, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.5413 - acc: 0.8019 - val_loss: 0.7347 - val_acc: 0.7075\n",
            "Epoch 35/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5603 - acc: 0.7966\n",
            "Epoch 00035: val_loss did not improve from 0.73473\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.5596 - acc: 0.7967 - val_loss: 0.7840 - val_acc: 0.7008\n",
            "Epoch 36/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5473 - acc: 0.7976\n",
            "Epoch 00036: val_loss did not improve from 0.73473\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.5498 - acc: 0.7958 - val_loss: 0.7864 - val_acc: 0.6958\n",
            "Epoch 37/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5042 - acc: 0.8145\n",
            "Epoch 00037: val_loss did not improve from 0.73473\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.5083 - acc: 0.8128 - val_loss: 0.7871 - val_acc: 0.7192\n",
            "Epoch 38/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5131 - acc: 0.8155\n",
            "Epoch 00038: val_loss improved from 0.73473 to 0.66818, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.5136 - acc: 0.8144 - val_loss: 0.6682 - val_acc: 0.7425\n",
            "Epoch 39/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.4983 - acc: 0.8259\n",
            "Epoch 00039: val_loss did not improve from 0.66818\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.4919 - acc: 0.8286 - val_loss: 0.6798 - val_acc: 0.7383\n",
            "Epoch 40/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4771 - acc: 0.8303\n",
            "Epoch 00040: val_loss improved from 0.66818 to 0.58452, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.4791 - acc: 0.8292 - val_loss: 0.5845 - val_acc: 0.7825\n",
            "Epoch 41/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4762 - acc: 0.8279\n",
            "Epoch 00041: val_loss did not improve from 0.58452\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.4735 - acc: 0.8294 - val_loss: 0.6339 - val_acc: 0.7708\n",
            "Epoch 42/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4389 - acc: 0.8434\n",
            "Epoch 00042: val_loss improved from 0.58452 to 0.54045, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.4405 - acc: 0.8431 - val_loss: 0.5404 - val_acc: 0.8033\n",
            "Epoch 43/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4056 - acc: 0.8639\n",
            "Epoch 00043: val_loss did not improve from 0.54045\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.4028 - acc: 0.8636 - val_loss: 0.6994 - val_acc: 0.7392\n",
            "Epoch 44/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4357 - acc: 0.8503\n",
            "Epoch 00044: val_loss did not improve from 0.54045\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.4321 - acc: 0.8511 - val_loss: 0.6204 - val_acc: 0.7725\n",
            "Epoch 45/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4125 - acc: 0.8688\n",
            "Epoch 00045: val_loss did not improve from 0.54045\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.4084 - acc: 0.8697 - val_loss: 0.5664 - val_acc: 0.7908\n",
            "Epoch 46/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3904 - acc: 0.8706\n",
            "Epoch 00046: val_loss did not improve from 0.54045\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.3968 - acc: 0.8669 - val_loss: 0.6010 - val_acc: 0.7883\n",
            "Epoch 47/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3756 - acc: 0.8720\n",
            "Epoch 00047: val_loss improved from 0.54045 to 0.46445, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.3768 - acc: 0.8717 - val_loss: 0.4645 - val_acc: 0.8392\n",
            "Epoch 48/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3597 - acc: 0.8759\n",
            "Epoch 00048: val_loss did not improve from 0.46445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.3644 - acc: 0.8744 - val_loss: 0.5129 - val_acc: 0.8100\n",
            "Epoch 49/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3693 - acc: 0.8724\n",
            "Epoch 00049: val_loss did not improve from 0.46445\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.3753 - acc: 0.8689 - val_loss: 0.4927 - val_acc: 0.8225\n",
            "Epoch 50/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3401 - acc: 0.8850\n",
            "Epoch 00050: val_loss did not improve from 0.46445\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.3482 - acc: 0.8811 - val_loss: 0.4972 - val_acc: 0.8150\n",
            "Epoch 51/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3455 - acc: 0.8834\n",
            "Epoch 00051: val_loss did not improve from 0.46445\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.3424 - acc: 0.8847 - val_loss: 0.4896 - val_acc: 0.8125\n",
            "Epoch 52/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3207 - acc: 0.9012\n",
            "Epoch 00052: val_loss did not improve from 0.46445\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.3259 - acc: 0.8994 - val_loss: 0.5405 - val_acc: 0.8125\n",
            "Epoch 53/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3329 - acc: 0.8949\n",
            "Epoch 00053: val_loss did not improve from 0.46445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.3313 - acc: 0.8956 - val_loss: 0.4970 - val_acc: 0.8250\n",
            "Epoch 54/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3164 - acc: 0.8918\n",
            "Epoch 00054: val_loss did not improve from 0.46445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.3213 - acc: 0.8900 - val_loss: 0.4856 - val_acc: 0.8308\n",
            "Epoch 55/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3414 - acc: 0.8772\n",
            "Epoch 00055: val_loss improved from 0.46445 to 0.41820, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.3345 - acc: 0.8822 - val_loss: 0.4182 - val_acc: 0.8483\n",
            "Epoch 56/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3005 - acc: 0.9034\n",
            "Epoch 00056: val_loss did not improve from 0.41820\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.3002 - acc: 0.9036 - val_loss: 0.4985 - val_acc: 0.8125\n",
            "Epoch 57/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3299 - acc: 0.8851\n",
            "Epoch 00057: val_loss did not improve from 0.41820\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.3279 - acc: 0.8867 - val_loss: 0.4695 - val_acc: 0.8233\n",
            "Epoch 58/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2983 - acc: 0.8977\n",
            "Epoch 00058: val_loss did not improve from 0.41820\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2977 - acc: 0.8981 - val_loss: 0.4748 - val_acc: 0.8192\n",
            "Epoch 59/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2922 - acc: 0.9017\n",
            "Epoch 00059: val_loss did not improve from 0.41820\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2930 - acc: 0.9014 - val_loss: 0.5155 - val_acc: 0.8258\n",
            "Epoch 60/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2921 - acc: 0.9034\n",
            "Epoch 00060: val_loss did not improve from 0.41820\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2967 - acc: 0.9006 - val_loss: 0.4421 - val_acc: 0.8400\n",
            "Epoch 61/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2587 - acc: 0.9206\n",
            "Epoch 00061: val_loss did not improve from 0.41820\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2592 - acc: 0.9208 - val_loss: 0.4899 - val_acc: 0.8325\n",
            "Epoch 62/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2672 - acc: 0.9103\n",
            "Epoch 00062: val_loss improved from 0.41820 to 0.40807, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.2677 - acc: 0.9086 - val_loss: 0.4081 - val_acc: 0.8492\n",
            "Epoch 63/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2374 - acc: 0.9291\n",
            "Epoch 00063: val_loss did not improve from 0.40807\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2449 - acc: 0.9256 - val_loss: 0.4135 - val_acc: 0.8508\n",
            "Epoch 64/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2495 - acc: 0.9249\n",
            "Epoch 00064: val_loss did not improve from 0.40807\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2473 - acc: 0.9258 - val_loss: 0.4088 - val_acc: 0.8658\n",
            "Epoch 65/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9229\n",
            "Epoch 00065: val_loss did not improve from 0.40807\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2506 - acc: 0.9217 - val_loss: 0.4417 - val_acc: 0.8408\n",
            "Epoch 66/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.9214\n",
            "Epoch 00066: val_loss improved from 0.40807 to 0.38767, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.2439 - acc: 0.9217 - val_loss: 0.3877 - val_acc: 0.8642\n",
            "Epoch 67/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9360\n",
            "Epoch 00067: val_loss improved from 0.38767 to 0.38666, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.2115 - acc: 0.9358 - val_loss: 0.3867 - val_acc: 0.8667\n",
            "Epoch 68/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2252 - acc: 0.9252\n",
            "Epoch 00068: val_loss improved from 0.38666 to 0.36050, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.2239 - acc: 0.9275 - val_loss: 0.3605 - val_acc: 0.8883\n",
            "Epoch 69/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2281 - acc: 0.9241\n",
            "Epoch 00069: val_loss did not improve from 0.36050\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2293 - acc: 0.9242 - val_loss: 0.3969 - val_acc: 0.8608\n",
            "Epoch 70/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2167 - acc: 0.9379\n",
            "Epoch 00070: val_loss improved from 0.36050 to 0.32545, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.2228 - acc: 0.9361 - val_loss: 0.3254 - val_acc: 0.8800\n",
            "Epoch 71/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2139 - acc: 0.9362\n",
            "Epoch 00071: val_loss did not improve from 0.32545\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2177 - acc: 0.9344 - val_loss: 0.4024 - val_acc: 0.8567\n",
            "Epoch 72/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2303 - acc: 0.9269\n",
            "Epoch 00072: val_loss did not improve from 0.32545\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2267 - acc: 0.9283 - val_loss: 0.3629 - val_acc: 0.8758\n",
            "Epoch 73/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2015 - acc: 0.9409\n",
            "Epoch 00073: val_loss did not improve from 0.32545\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2004 - acc: 0.9422 - val_loss: 0.3373 - val_acc: 0.8800\n",
            "Epoch 74/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1922 - acc: 0.9415\n",
            "Epoch 00074: val_loss improved from 0.32545 to 0.32442, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1924 - acc: 0.9422 - val_loss: 0.3244 - val_acc: 0.8917\n",
            "Epoch 75/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1822 - acc: 0.9451\n",
            "Epoch 00075: val_loss did not improve from 0.32442\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1821 - acc: 0.9453 - val_loss: 0.3304 - val_acc: 0.8792\n",
            "Epoch 76/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9371\n",
            "Epoch 00076: val_loss did not improve from 0.32442\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1939 - acc: 0.9378 - val_loss: 0.3262 - val_acc: 0.8942\n",
            "Epoch 77/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2041 - acc: 0.9367\n",
            "Epoch 00077: val_loss did not improve from 0.32442\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2054 - acc: 0.9347 - val_loss: 0.3790 - val_acc: 0.8758\n",
            "Epoch 78/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1889 - acc: 0.9412\n",
            "Epoch 00078: val_loss improved from 0.32442 to 0.29893, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.1890 - acc: 0.9414 - val_loss: 0.2989 - val_acc: 0.8875\n",
            "Epoch 79/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1779 - acc: 0.9425\n",
            "Epoch 00079: val_loss improved from 0.29893 to 0.29112, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1747 - acc: 0.9447 - val_loss: 0.2911 - val_acc: 0.9017\n",
            "Epoch 80/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1701 - acc: 0.9473\n",
            "Epoch 00080: val_loss improved from 0.29112 to 0.26382, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1719 - acc: 0.9472 - val_loss: 0.2638 - val_acc: 0.9208\n",
            "Epoch 81/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1686 - acc: 0.9497\n",
            "Epoch 00081: val_loss did not improve from 0.26382\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1686 - acc: 0.9492 - val_loss: 0.3720 - val_acc: 0.8800\n",
            "Epoch 82/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2083 - acc: 0.9369\n",
            "Epoch 00082: val_loss did not improve from 0.26382\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2110 - acc: 0.9361 - val_loss: 0.3245 - val_acc: 0.8917\n",
            "Epoch 83/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1959 - acc: 0.9382\n",
            "Epoch 00083: val_loss did not improve from 0.26382\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1969 - acc: 0.9378 - val_loss: 0.2972 - val_acc: 0.8992\n",
            "Epoch 84/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1559 - acc: 0.9584\n",
            "Epoch 00084: val_loss did not improve from 0.26382\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1568 - acc: 0.9578 - val_loss: 0.3229 - val_acc: 0.8925\n",
            "Epoch 85/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9571\n",
            "Epoch 00085: val_loss did not improve from 0.26382\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1547 - acc: 0.9572 - val_loss: 0.2781 - val_acc: 0.9083\n",
            "Epoch 86/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1580 - acc: 0.9566\n",
            "Epoch 00086: val_loss did not improve from 0.26382\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1571 - acc: 0.9569 - val_loss: 0.3288 - val_acc: 0.8858\n",
            "Epoch 87/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9457\n",
            "Epoch 00087: val_loss did not improve from 0.26382\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1704 - acc: 0.9464 - val_loss: 0.4132 - val_acc: 0.8608\n",
            "Epoch 88/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1739 - acc: 0.9426\n",
            "Epoch 00088: val_loss improved from 0.26382 to 0.26116, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1713 - acc: 0.9433 - val_loss: 0.2612 - val_acc: 0.9150\n",
            "Epoch 89/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9589\n",
            "Epoch 00089: val_loss improved from 0.26116 to 0.24449, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.1459 - acc: 0.9589 - val_loss: 0.2445 - val_acc: 0.9242\n",
            "Epoch 90/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1385 - acc: 0.9606\n",
            "Epoch 00090: val_loss improved from 0.24449 to 0.23942, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.1394 - acc: 0.9606 - val_loss: 0.2394 - val_acc: 0.9175\n",
            "Epoch 91/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9583\n",
            "Epoch 00091: val_loss improved from 0.23942 to 0.23635, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1489 - acc: 0.9572 - val_loss: 0.2363 - val_acc: 0.9258\n",
            "Epoch 92/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1526 - acc: 0.9564\n",
            "Epoch 00092: val_loss did not improve from 0.23635\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1527 - acc: 0.9564 - val_loss: 0.2875 - val_acc: 0.9025\n",
            "Epoch 93/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9569\n",
            "Epoch 00093: val_loss did not improve from 0.23635\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1507 - acc: 0.9553 - val_loss: 0.3102 - val_acc: 0.8975\n",
            "Epoch 94/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1455 - acc: 0.9582\n",
            "Epoch 00094: val_loss did not improve from 0.23635\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1472 - acc: 0.9578 - val_loss: 0.3358 - val_acc: 0.8825\n",
            "Epoch 95/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1358 - acc: 0.9624\n",
            "Epoch 00095: val_loss did not improve from 0.23635\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1353 - acc: 0.9628 - val_loss: 0.2948 - val_acc: 0.9083\n",
            "Epoch 96/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9620\n",
            "Epoch 00096: val_loss improved from 0.23635 to 0.21214, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.1342 - acc: 0.9617 - val_loss: 0.2121 - val_acc: 0.9250\n",
            "Epoch 97/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1353 - acc: 0.9600\n",
            "Epoch 00097: val_loss did not improve from 0.21214\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1327 - acc: 0.9611 - val_loss: 0.2555 - val_acc: 0.9125\n",
            "Epoch 98/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1332 - acc: 0.9582\n",
            "Epoch 00098: val_loss did not improve from 0.21214\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1321 - acc: 0.9600 - val_loss: 0.2780 - val_acc: 0.9125\n",
            "Epoch 99/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9549\n",
            "Epoch 00099: val_loss did not improve from 0.21214\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1559 - acc: 0.9544 - val_loss: 0.2753 - val_acc: 0.9083\n",
            "Epoch 100/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1317 - acc: 0.9636\n",
            "Epoch 00100: val_loss did not improve from 0.21214\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1378 - acc: 0.9614 - val_loss: 0.2176 - val_acc: 0.9250\n",
            "Epoch 101/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1295 - acc: 0.9668\n",
            "Epoch 00101: val_loss improved from 0.21214 to 0.18051, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.1290 - acc: 0.9667 - val_loss: 0.1805 - val_acc: 0.9417\n",
            "Epoch 102/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1260 - acc: 0.9618\n",
            "Epoch 00102: val_loss did not improve from 0.18051\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1261 - acc: 0.9617 - val_loss: 0.2370 - val_acc: 0.9175\n",
            "Epoch 103/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1342 - acc: 0.9638\n",
            "Epoch 00103: val_loss did not improve from 0.18051\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1344 - acc: 0.9639 - val_loss: 0.2106 - val_acc: 0.9325\n",
            "Epoch 104/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1468 - acc: 0.9578\n",
            "Epoch 00104: val_loss did not improve from 0.18051\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1479 - acc: 0.9575 - val_loss: 0.2279 - val_acc: 0.9300\n",
            "Epoch 105/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9569\n",
            "Epoch 00105: val_loss did not improve from 0.18051\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1478 - acc: 0.9561 - val_loss: 0.2050 - val_acc: 0.9367\n",
            "Epoch 106/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1086 - acc: 0.9733\n",
            "Epoch 00106: val_loss did not improve from 0.18051\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1091 - acc: 0.9731 - val_loss: 0.2481 - val_acc: 0.9175\n",
            "Epoch 107/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9646\n",
            "Epoch 00107: val_loss did not improve from 0.18051\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.1299 - acc: 0.9642 - val_loss: 0.2343 - val_acc: 0.9175\n",
            "Epoch 108/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1304 - acc: 0.9612\n",
            "Epoch 00108: val_loss did not improve from 0.18051\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1305 - acc: 0.9614 - val_loss: 0.2280 - val_acc: 0.9183\n",
            "Epoch 109/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1408 - acc: 0.9565\n",
            "Epoch 00109: val_loss did not improve from 0.18051\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1445 - acc: 0.9542 - val_loss: 0.2461 - val_acc: 0.9233\n",
            "Epoch 110/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9643\n",
            "Epoch 00110: val_loss did not improve from 0.18051\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1251 - acc: 0.9625 - val_loss: 0.2431 - val_acc: 0.9175\n",
            "Epoch 111/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1412 - acc: 0.9494\n",
            "Epoch 00111: val_loss did not improve from 0.18051\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1414 - acc: 0.9500 - val_loss: 0.2114 - val_acc: 0.9375\n",
            "Epoch 112/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9760\n",
            "Epoch 00112: val_loss did not improve from 0.18051\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1010 - acc: 0.9756 - val_loss: 0.1822 - val_acc: 0.9458\n",
            "Epoch 113/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9714\n",
            "Epoch 00113: val_loss did not improve from 0.18051\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1086 - acc: 0.9711 - val_loss: 0.2263 - val_acc: 0.9242\n",
            "Epoch 114/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1089 - acc: 0.9688\n",
            "Epoch 00114: val_loss did not improve from 0.18051\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1103 - acc: 0.9689 - val_loss: 0.2011 - val_acc: 0.9342\n",
            "Epoch 115/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9711\n",
            "Epoch 00115: val_loss improved from 0.18051 to 0.17471, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.1027 - acc: 0.9708 - val_loss: 0.1747 - val_acc: 0.9475\n",
            "Epoch 116/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0930 - acc: 0.9733\n",
            "Epoch 00116: val_loss did not improve from 0.17471\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0926 - acc: 0.9742 - val_loss: 0.2944 - val_acc: 0.9008\n",
            "Epoch 117/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9700\n",
            "Epoch 00117: val_loss did not improve from 0.17471\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.1042 - acc: 0.9700 - val_loss: 0.1995 - val_acc: 0.9258\n",
            "Epoch 118/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0946 - acc: 0.9767\n",
            "Epoch 00118: val_loss improved from 0.17471 to 0.16494, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.0969 - acc: 0.9753 - val_loss: 0.1649 - val_acc: 0.9525\n",
            "Epoch 119/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1098 - acc: 0.9703\n",
            "Epoch 00119: val_loss did not improve from 0.16494\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1105 - acc: 0.9697 - val_loss: 0.1793 - val_acc: 0.9483\n",
            "Epoch 120/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9671\n",
            "Epoch 00120: val_loss did not improve from 0.16494\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1182 - acc: 0.9667 - val_loss: 0.2345 - val_acc: 0.9192\n",
            "Epoch 121/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0952 - acc: 0.9765\n",
            "Epoch 00121: val_loss improved from 0.16494 to 0.16410, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.0943 - acc: 0.9772 - val_loss: 0.1641 - val_acc: 0.9508\n",
            "Epoch 122/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1077 - acc: 0.9673\n",
            "Epoch 00122: val_loss did not improve from 0.16410\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1092 - acc: 0.9667 - val_loss: 0.1803 - val_acc: 0.9358\n",
            "Epoch 123/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1044 - acc: 0.9732\n",
            "Epoch 00123: val_loss did not improve from 0.16410\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1048 - acc: 0.9725 - val_loss: 0.1983 - val_acc: 0.9350\n",
            "Epoch 124/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0992 - acc: 0.9727\n",
            "Epoch 00124: val_loss improved from 0.16410 to 0.15954, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.0973 - acc: 0.9736 - val_loss: 0.1595 - val_acc: 0.9483\n",
            "Epoch 125/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1042 - acc: 0.9684\n",
            "Epoch 00125: val_loss did not improve from 0.15954\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1050 - acc: 0.9692 - val_loss: 0.1640 - val_acc: 0.9533\n",
            "Epoch 126/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9697\n",
            "Epoch 00126: val_loss did not improve from 0.15954\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1028 - acc: 0.9703 - val_loss: 0.1704 - val_acc: 0.9425\n",
            "Epoch 127/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1042 - acc: 0.9694\n",
            "Epoch 00127: val_loss did not improve from 0.15954\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1063 - acc: 0.9686 - val_loss: 0.2208 - val_acc: 0.9242\n",
            "Epoch 128/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1029 - acc: 0.9700\n",
            "Epoch 00128: val_loss did not improve from 0.15954\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1029 - acc: 0.9703 - val_loss: 0.1957 - val_acc: 0.9333\n",
            "Epoch 129/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0954 - acc: 0.9727\n",
            "Epoch 00129: val_loss did not improve from 0.15954\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0982 - acc: 0.9711 - val_loss: 0.2016 - val_acc: 0.9325\n",
            "Epoch 130/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1191 - acc: 0.9659\n",
            "Epoch 00130: val_loss did not improve from 0.15954\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1167 - acc: 0.9675 - val_loss: 0.1631 - val_acc: 0.9508\n",
            "Epoch 131/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0886 - acc: 0.9779\n",
            "Epoch 00131: val_loss did not improve from 0.15954\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0920 - acc: 0.9764 - val_loss: 0.1720 - val_acc: 0.9425\n",
            "Epoch 132/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1010 - acc: 0.9700\n",
            "Epoch 00132: val_loss did not improve from 0.15954\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0995 - acc: 0.9708 - val_loss: 0.1790 - val_acc: 0.9417\n",
            "Epoch 133/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9760\n",
            "Epoch 00133: val_loss did not improve from 0.15954\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0903 - acc: 0.9753 - val_loss: 0.1904 - val_acc: 0.9300\n",
            "Epoch 134/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9737\n",
            "Epoch 00134: val_loss improved from 0.15954 to 0.14473, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.0949 - acc: 0.9733 - val_loss: 0.1447 - val_acc: 0.9558\n",
            "Epoch 135/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9700\n",
            "Epoch 00135: val_loss did not improve from 0.14473\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1040 - acc: 0.9700 - val_loss: 0.2829 - val_acc: 0.9167\n",
            "Epoch 136/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1057 - acc: 0.9679\n",
            "Epoch 00136: val_loss did not improve from 0.14473\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1055 - acc: 0.9681 - val_loss: 0.1783 - val_acc: 0.9508\n",
            "Epoch 137/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9729\n",
            "Epoch 00137: val_loss did not improve from 0.14473\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0929 - acc: 0.9722 - val_loss: 0.1956 - val_acc: 0.9333\n",
            "Epoch 138/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9671\n",
            "Epoch 00138: val_loss improved from 0.14473 to 0.13964, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1100 - acc: 0.9675 - val_loss: 0.1396 - val_acc: 0.9608\n",
            "Epoch 139/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1005 - acc: 0.9725\n",
            "Epoch 00139: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0999 - acc: 0.9722 - val_loss: 0.2050 - val_acc: 0.9325\n",
            "Epoch 140/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9774\n",
            "Epoch 00140: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0925 - acc: 0.9778 - val_loss: 0.1877 - val_acc: 0.9417\n",
            "Epoch 141/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0892 - acc: 0.9762\n",
            "Epoch 00141: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0888 - acc: 0.9761 - val_loss: 0.1725 - val_acc: 0.9375\n",
            "Epoch 142/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0888 - acc: 0.9764\n",
            "Epoch 00142: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0925 - acc: 0.9742 - val_loss: 0.1769 - val_acc: 0.9492\n",
            "Epoch 143/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9711\n",
            "Epoch 00143: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0989 - acc: 0.9714 - val_loss: 0.2563 - val_acc: 0.9133\n",
            "Epoch 144/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0978 - acc: 0.9709\n",
            "Epoch 00144: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0993 - acc: 0.9714 - val_loss: 0.1701 - val_acc: 0.9400\n",
            "Epoch 145/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1048 - acc: 0.9688\n",
            "Epoch 00145: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1037 - acc: 0.9697 - val_loss: 0.1487 - val_acc: 0.9533\n",
            "Epoch 146/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1112 - acc: 0.9676\n",
            "Epoch 00146: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1132 - acc: 0.9664 - val_loss: 0.1568 - val_acc: 0.9500\n",
            "Epoch 147/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1231 - acc: 0.9629\n",
            "Epoch 00147: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1245 - acc: 0.9628 - val_loss: 0.1943 - val_acc: 0.9283\n",
            "Epoch 148/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1281 - acc: 0.9597\n",
            "Epoch 00148: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1263 - acc: 0.9606 - val_loss: 0.1886 - val_acc: 0.9367\n",
            "Epoch 149/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1004 - acc: 0.9697\n",
            "Epoch 00149: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1022 - acc: 0.9689 - val_loss: 0.1891 - val_acc: 0.9367\n",
            "Epoch 150/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9757\n",
            "Epoch 00150: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0871 - acc: 0.9758 - val_loss: 0.1750 - val_acc: 0.9425\n",
            "Epoch 151/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0939 - acc: 0.9715\n",
            "Epoch 00151: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0921 - acc: 0.9722 - val_loss: 0.1976 - val_acc: 0.9375\n",
            "Epoch 152/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1012 - acc: 0.9730\n",
            "Epoch 00152: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1083 - acc: 0.9694 - val_loss: 0.1754 - val_acc: 0.9458\n",
            "Epoch 153/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1017 - acc: 0.9691\n",
            "Epoch 00153: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1020 - acc: 0.9686 - val_loss: 0.1456 - val_acc: 0.9542\n",
            "Epoch 154/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0903 - acc: 0.9742\n",
            "Epoch 00154: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0906 - acc: 0.9744 - val_loss: 0.1690 - val_acc: 0.9450\n",
            "Epoch 155/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0777 - acc: 0.9791\n",
            "Epoch 00155: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0781 - acc: 0.9792 - val_loss: 0.1689 - val_acc: 0.9408\n",
            "Epoch 156/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9817\n",
            "Epoch 00156: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0785 - acc: 0.9811 - val_loss: 0.1797 - val_acc: 0.9458\n",
            "Epoch 157/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1038 - acc: 0.9688\n",
            "Epoch 00157: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1026 - acc: 0.9694 - val_loss: 0.1798 - val_acc: 0.9417\n",
            "Epoch 158/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0854 - acc: 0.9768\n",
            "Epoch 00158: val_loss did not improve from 0.13964\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0883 - acc: 0.9753 - val_loss: 0.1425 - val_acc: 0.9517\n",
            "Epoch 159/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0782 - acc: 0.9785\n",
            "Epoch 00159: val_loss improved from 0.13964 to 0.13588, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.0786 - acc: 0.9781 - val_loss: 0.1359 - val_acc: 0.9558\n",
            "Epoch 160/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0871 - acc: 0.9730\n",
            "Epoch 00160: val_loss did not improve from 0.13588\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0880 - acc: 0.9731 - val_loss: 0.1614 - val_acc: 0.9458\n",
            "Epoch 161/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0838 - acc: 0.9755\n",
            "Epoch 00161: val_loss did not improve from 0.13588\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0839 - acc: 0.9753 - val_loss: 0.1954 - val_acc: 0.9267\n",
            "Epoch 162/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0889 - acc: 0.9756\n",
            "Epoch 00162: val_loss did not improve from 0.13588\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0887 - acc: 0.9753 - val_loss: 0.1966 - val_acc: 0.9342\n",
            "Epoch 163/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0949 - acc: 0.9697\n",
            "Epoch 00163: val_loss did not improve from 0.13588\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0933 - acc: 0.9708 - val_loss: 0.1631 - val_acc: 0.9442\n",
            "Epoch 164/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9617\n",
            "Epoch 00164: val_loss did not improve from 0.13588\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1229 - acc: 0.9608 - val_loss: 0.2576 - val_acc: 0.9108\n",
            "Epoch 165/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1117 - acc: 0.9662\n",
            "Epoch 00165: val_loss did not improve from 0.13588\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1134 - acc: 0.9658 - val_loss: 0.1679 - val_acc: 0.9483\n",
            "Epoch 166/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0871 - acc: 0.9770\n",
            "Epoch 00166: val_loss did not improve from 0.13588\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0892 - acc: 0.9750 - val_loss: 0.1406 - val_acc: 0.9533\n",
            "Epoch 167/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9791\n",
            "Epoch 00167: val_loss improved from 0.13588 to 0.12785, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.0789 - acc: 0.9797 - val_loss: 0.1278 - val_acc: 0.9642\n",
            "Epoch 168/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9749\n",
            "Epoch 00168: val_loss did not improve from 0.12785\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0933 - acc: 0.9736 - val_loss: 0.1607 - val_acc: 0.9467\n",
            "Epoch 169/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9697\n",
            "Epoch 00169: val_loss did not improve from 0.12785\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0972 - acc: 0.9703 - val_loss: 0.1756 - val_acc: 0.9417\n",
            "Epoch 170/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0810 - acc: 0.9800\n",
            "Epoch 00170: val_loss did not improve from 0.12785\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0832 - acc: 0.9775 - val_loss: 0.1629 - val_acc: 0.9500\n",
            "Epoch 171/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0817 - acc: 0.9770\n",
            "Epoch 00171: val_loss did not improve from 0.12785\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0826 - acc: 0.9767 - val_loss: 0.1735 - val_acc: 0.9358\n",
            "Epoch 172/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0809 - acc: 0.9784\n",
            "Epoch 00172: val_loss did not improve from 0.12785\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0801 - acc: 0.9789 - val_loss: 0.2657 - val_acc: 0.9042\n",
            "Epoch 173/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0786 - acc: 0.9758\n",
            "Epoch 00173: val_loss did not improve from 0.12785\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0793 - acc: 0.9753 - val_loss: 0.1376 - val_acc: 0.9600\n",
            "Epoch 174/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1081 - acc: 0.9671\n",
            "Epoch 00174: val_loss did not improve from 0.12785\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1093 - acc: 0.9672 - val_loss: 0.1649 - val_acc: 0.9483\n",
            "Epoch 175/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0856 - acc: 0.9771\n",
            "Epoch 00175: val_loss did not improve from 0.12785\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0890 - acc: 0.9753 - val_loss: 0.1410 - val_acc: 0.9625\n",
            "Epoch 176/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0748 - acc: 0.9797\n",
            "Epoch 00176: val_loss did not improve from 0.12785\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0765 - acc: 0.9786 - val_loss: 0.1444 - val_acc: 0.9525\n",
            "Epoch 177/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9763\n",
            "Epoch 00177: val_loss improved from 0.12785 to 0.12015, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.0805 - acc: 0.9767 - val_loss: 0.1202 - val_acc: 0.9617\n",
            "Epoch 178/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9734\n",
            "Epoch 00178: val_loss did not improve from 0.12015\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0868 - acc: 0.9728 - val_loss: 0.1375 - val_acc: 0.9600\n",
            "Epoch 179/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0898 - acc: 0.9739\n",
            "Epoch 00179: val_loss did not improve from 0.12015\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0917 - acc: 0.9739 - val_loss: 0.1322 - val_acc: 0.9575\n",
            "Epoch 180/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0903 - acc: 0.9716\n",
            "Epoch 00180: val_loss did not improve from 0.12015\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0887 - acc: 0.9733 - val_loss: 0.1241 - val_acc: 0.9625\n",
            "Epoch 181/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0774 - acc: 0.9785\n",
            "Epoch 00181: val_loss did not improve from 0.12015\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0775 - acc: 0.9786 - val_loss: 0.1730 - val_acc: 0.9467\n",
            "Epoch 182/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0938 - acc: 0.9726\n",
            "Epoch 00182: val_loss did not improve from 0.12015\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0940 - acc: 0.9722 - val_loss: 0.2793 - val_acc: 0.9042\n",
            "Epoch 183/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9714\n",
            "Epoch 00183: val_loss did not improve from 0.12015\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0972 - acc: 0.9711 - val_loss: 0.1321 - val_acc: 0.9567\n",
            "Epoch 184/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0874 - acc: 0.9756\n",
            "Epoch 00184: val_loss did not improve from 0.12015\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0855 - acc: 0.9767 - val_loss: 0.1607 - val_acc: 0.9492\n",
            "Epoch 185/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0897 - acc: 0.9719\n",
            "Epoch 00185: val_loss did not improve from 0.12015\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0902 - acc: 0.9717 - val_loss: 0.1387 - val_acc: 0.9550\n",
            "Epoch 186/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0784 - acc: 0.9775\n",
            "Epoch 00186: val_loss did not improve from 0.12015\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0793 - acc: 0.9767 - val_loss: 0.1677 - val_acc: 0.9467\n",
            "Epoch 187/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0655 - acc: 0.9821\n",
            "Epoch 00187: val_loss did not improve from 0.12015\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0665 - acc: 0.9817 - val_loss: 0.1277 - val_acc: 0.9617\n",
            "Epoch 188/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0686 - acc: 0.9806\n",
            "Epoch 00188: val_loss did not improve from 0.12015\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0741 - acc: 0.9775 - val_loss: 0.1454 - val_acc: 0.9467\n",
            "Epoch 189/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0932 - acc: 0.9727\n",
            "Epoch 00189: val_loss improved from 0.12015 to 0.11821, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.0921 - acc: 0.9733 - val_loss: 0.1182 - val_acc: 0.9642\n",
            "Epoch 190/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0822 - acc: 0.9762\n",
            "Epoch 00190: val_loss did not improve from 0.11821\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0822 - acc: 0.9753 - val_loss: 0.1970 - val_acc: 0.9283\n",
            "Epoch 191/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0901 - acc: 0.9741\n",
            "Epoch 00191: val_loss did not improve from 0.11821\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0906 - acc: 0.9733 - val_loss: 0.1344 - val_acc: 0.9533\n",
            "Epoch 192/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0917 - acc: 0.9703\n",
            "Epoch 00192: val_loss did not improve from 0.11821\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0898 - acc: 0.9708 - val_loss: 0.1404 - val_acc: 0.9558\n",
            "Epoch 193/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9749\n",
            "Epoch 00193: val_loss did not improve from 0.11821\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0840 - acc: 0.9744 - val_loss: 0.2916 - val_acc: 0.8983\n",
            "Epoch 194/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0895 - acc: 0.9739\n",
            "Epoch 00194: val_loss did not improve from 0.11821\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0886 - acc: 0.9742 - val_loss: 0.1324 - val_acc: 0.9592\n",
            "Epoch 195/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0699 - acc: 0.9797\n",
            "Epoch 00195: val_loss did not improve from 0.11821\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0713 - acc: 0.9786 - val_loss: 0.1280 - val_acc: 0.9633\n",
            "Epoch 196/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9734\n",
            "Epoch 00196: val_loss did not improve from 0.11821\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0900 - acc: 0.9725 - val_loss: 0.1399 - val_acc: 0.9542\n",
            "Epoch 197/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9731\n",
            "Epoch 00197: val_loss did not improve from 0.11821\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0887 - acc: 0.9733 - val_loss: 0.1650 - val_acc: 0.9483\n",
            "Epoch 198/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9751\n",
            "Epoch 00198: val_loss did not improve from 0.11821\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0894 - acc: 0.9756 - val_loss: 0.1784 - val_acc: 0.9358\n",
            "Epoch 199/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0834 - acc: 0.9748\n",
            "Epoch 00199: val_loss did not improve from 0.11821\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0833 - acc: 0.9742 - val_loss: 0.1347 - val_acc: 0.9600\n",
            "Epoch 200/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9766\n",
            "Epoch 00200: val_loss did not improve from 0.11821\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0727 - acc: 0.9772 - val_loss: 0.1685 - val_acc: 0.9392\n",
            "Epoch 201/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9797\n",
            "Epoch 00201: val_loss did not improve from 0.11821\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0682 - acc: 0.9803 - val_loss: 0.1435 - val_acc: 0.9508\n",
            "Epoch 202/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0650 - acc: 0.9830\n",
            "Epoch 00202: val_loss did not improve from 0.11821\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0675 - acc: 0.9817 - val_loss: 0.1334 - val_acc: 0.9600\n",
            "Epoch 203/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9806\n",
            "Epoch 00203: val_loss did not improve from 0.11821\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0680 - acc: 0.9806 - val_loss: 0.1459 - val_acc: 0.9475\n",
            "Epoch 204/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0608 - acc: 0.9841\n",
            "Epoch 00204: val_loss improved from 0.11821 to 0.09225, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.0620 - acc: 0.9842 - val_loss: 0.0922 - val_acc: 0.9725\n",
            "Epoch 205/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9731\n",
            "Epoch 00205: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0827 - acc: 0.9733 - val_loss: 0.1357 - val_acc: 0.9567\n",
            "Epoch 206/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9737\n",
            "Epoch 00206: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0863 - acc: 0.9731 - val_loss: 0.1336 - val_acc: 0.9650\n",
            "Epoch 207/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9717\n",
            "Epoch 00207: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0841 - acc: 0.9717 - val_loss: 0.1336 - val_acc: 0.9558\n",
            "Epoch 208/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0681 - acc: 0.9818\n",
            "Epoch 00208: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0703 - acc: 0.9808 - val_loss: 0.1405 - val_acc: 0.9575\n",
            "Epoch 209/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0873 - acc: 0.9722\n",
            "Epoch 00209: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0879 - acc: 0.9719 - val_loss: 0.1793 - val_acc: 0.9383\n",
            "Epoch 210/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0971 - acc: 0.9700\n",
            "Epoch 00210: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0975 - acc: 0.9700 - val_loss: 0.1338 - val_acc: 0.9583\n",
            "Epoch 211/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9689\n",
            "Epoch 00211: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0968 - acc: 0.9689 - val_loss: 0.1656 - val_acc: 0.9500\n",
            "Epoch 212/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0768 - acc: 0.9788\n",
            "Epoch 00212: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0768 - acc: 0.9789 - val_loss: 0.1213 - val_acc: 0.9658\n",
            "Epoch 213/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9743\n",
            "Epoch 00213: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0824 - acc: 0.9742 - val_loss: 0.1780 - val_acc: 0.9367\n",
            "Epoch 214/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0968 - acc: 0.9694\n",
            "Epoch 00214: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0963 - acc: 0.9692 - val_loss: 0.1018 - val_acc: 0.9700\n",
            "Epoch 215/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0772 - acc: 0.9800\n",
            "Epoch 00215: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0787 - acc: 0.9783 - val_loss: 0.1506 - val_acc: 0.9533\n",
            "Epoch 216/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9817\n",
            "Epoch 00216: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0685 - acc: 0.9814 - val_loss: 0.1462 - val_acc: 0.9567\n",
            "Epoch 217/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9854\n",
            "Epoch 00217: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0555 - acc: 0.9853 - val_loss: 0.1438 - val_acc: 0.9550\n",
            "Epoch 218/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0524 - acc: 0.9838\n",
            "Epoch 00218: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0528 - acc: 0.9842 - val_loss: 0.1376 - val_acc: 0.9550\n",
            "Epoch 219/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0657 - acc: 0.9794\n",
            "Epoch 00219: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0636 - acc: 0.9808 - val_loss: 0.1709 - val_acc: 0.9408\n",
            "Epoch 220/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9811\n",
            "Epoch 00220: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0617 - acc: 0.9811 - val_loss: 0.1596 - val_acc: 0.9475\n",
            "Epoch 221/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9823\n",
            "Epoch 00221: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0635 - acc: 0.9825 - val_loss: 0.1167 - val_acc: 0.9675\n",
            "Epoch 222/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1020 - acc: 0.9691\n",
            "Epoch 00222: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1024 - acc: 0.9689 - val_loss: 0.1404 - val_acc: 0.9500\n",
            "Epoch 223/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0842 - acc: 0.9741\n",
            "Epoch 00223: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0850 - acc: 0.9731 - val_loss: 0.1517 - val_acc: 0.9525\n",
            "Epoch 224/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9797\n",
            "Epoch 00224: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0735 - acc: 0.9794 - val_loss: 0.1250 - val_acc: 0.9617\n",
            "Epoch 225/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9806\n",
            "Epoch 00225: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0683 - acc: 0.9806 - val_loss: 0.1288 - val_acc: 0.9600\n",
            "Epoch 226/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9783\n",
            "Epoch 00226: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0699 - acc: 0.9789 - val_loss: 0.1161 - val_acc: 0.9608\n",
            "Epoch 227/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9857\n",
            "Epoch 00227: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0577 - acc: 0.9850 - val_loss: 0.1341 - val_acc: 0.9550\n",
            "Epoch 228/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0857 - acc: 0.9731\n",
            "Epoch 00228: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0896 - acc: 0.9722 - val_loss: 0.1277 - val_acc: 0.9642\n",
            "Epoch 229/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1011 - acc: 0.9655\n",
            "Epoch 00229: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1017 - acc: 0.9647 - val_loss: 0.1887 - val_acc: 0.9358\n",
            "Epoch 230/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0764 - acc: 0.9768\n",
            "Epoch 00230: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0763 - acc: 0.9769 - val_loss: 0.1356 - val_acc: 0.9583\n",
            "Epoch 231/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0734 - acc: 0.9776\n",
            "Epoch 00231: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0731 - acc: 0.9778 - val_loss: 0.1311 - val_acc: 0.9617\n",
            "Epoch 232/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1009 - acc: 0.9664\n",
            "Epoch 00232: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0991 - acc: 0.9669 - val_loss: 0.2297 - val_acc: 0.9333\n",
            "Epoch 233/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0851 - acc: 0.9764\n",
            "Epoch 00233: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0841 - acc: 0.9769 - val_loss: 0.1457 - val_acc: 0.9550\n",
            "Epoch 234/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9737\n",
            "Epoch 00234: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0866 - acc: 0.9739 - val_loss: 0.1192 - val_acc: 0.9575\n",
            "Epoch 235/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0751 - acc: 0.9776\n",
            "Epoch 00235: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0745 - acc: 0.9775 - val_loss: 0.1135 - val_acc: 0.9650\n",
            "Epoch 236/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0789 - acc: 0.9776\n",
            "Epoch 00236: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0775 - acc: 0.9786 - val_loss: 0.1324 - val_acc: 0.9567\n",
            "Epoch 237/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9786\n",
            "Epoch 00237: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0690 - acc: 0.9786 - val_loss: 0.1866 - val_acc: 0.9400\n",
            "Epoch 238/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0736 - acc: 0.9797\n",
            "Epoch 00238: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0738 - acc: 0.9794 - val_loss: 0.1308 - val_acc: 0.9583\n",
            "Epoch 239/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0699 - acc: 0.9797\n",
            "Epoch 00239: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0696 - acc: 0.9800 - val_loss: 0.1458 - val_acc: 0.9500\n",
            "Epoch 240/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9734\n",
            "Epoch 00240: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0817 - acc: 0.9739 - val_loss: 0.2210 - val_acc: 0.9317\n",
            "Epoch 241/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0931 - acc: 0.9691\n",
            "Epoch 00241: val_loss did not improve from 0.09225\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0947 - acc: 0.9686 - val_loss: 0.0984 - val_acc: 0.9700\n",
            "Epoch 242/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9689\n",
            "Epoch 00242: val_loss improved from 0.09225 to 0.09194, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.0946 - acc: 0.9678 - val_loss: 0.0919 - val_acc: 0.9708\n",
            "Epoch 243/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9806\n",
            "Epoch 00243: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0663 - acc: 0.9811 - val_loss: 0.1282 - val_acc: 0.9600\n",
            "Epoch 244/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9774\n",
            "Epoch 00244: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0710 - acc: 0.9778 - val_loss: 0.1185 - val_acc: 0.9675\n",
            "Epoch 245/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0586 - acc: 0.9830\n",
            "Epoch 00245: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0579 - acc: 0.9833 - val_loss: 0.1851 - val_acc: 0.9408\n",
            "Epoch 246/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0695 - acc: 0.9782\n",
            "Epoch 00246: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0689 - acc: 0.9783 - val_loss: 0.1622 - val_acc: 0.9433\n",
            "Epoch 247/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0699 - acc: 0.9797\n",
            "Epoch 00247: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0676 - acc: 0.9806 - val_loss: 0.1365 - val_acc: 0.9592\n",
            "Epoch 248/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0684 - acc: 0.9791\n",
            "Epoch 00248: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0683 - acc: 0.9794 - val_loss: 0.1123 - val_acc: 0.9642\n",
            "Epoch 249/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9797\n",
            "Epoch 00249: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0678 - acc: 0.9797 - val_loss: 0.1181 - val_acc: 0.9642\n",
            "Epoch 250/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9794\n",
            "Epoch 00250: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0689 - acc: 0.9800 - val_loss: 0.1428 - val_acc: 0.9567\n",
            "Epoch 251/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0739 - acc: 0.9782\n",
            "Epoch 00251: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0734 - acc: 0.9789 - val_loss: 0.1296 - val_acc: 0.9608\n",
            "Epoch 252/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0584 - acc: 0.9829\n",
            "Epoch 00252: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0595 - acc: 0.9825 - val_loss: 0.1414 - val_acc: 0.9558\n",
            "Epoch 253/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9831\n",
            "Epoch 00253: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0586 - acc: 0.9833 - val_loss: 0.1443 - val_acc: 0.9550\n",
            "Epoch 254/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9817\n",
            "Epoch 00254: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0649 - acc: 0.9811 - val_loss: 0.2143 - val_acc: 0.9350\n",
            "Epoch 255/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0611 - acc: 0.9841\n",
            "Epoch 00255: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0610 - acc: 0.9839 - val_loss: 0.1483 - val_acc: 0.9533\n",
            "Epoch 256/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0716 - acc: 0.9794\n",
            "Epoch 00256: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0729 - acc: 0.9792 - val_loss: 0.1408 - val_acc: 0.9525\n",
            "Epoch 257/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0639 - acc: 0.9809\n",
            "Epoch 00257: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0659 - acc: 0.9792 - val_loss: 0.1164 - val_acc: 0.9617\n",
            "Epoch 258/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0792 - acc: 0.9752\n",
            "Epoch 00258: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0770 - acc: 0.9758 - val_loss: 0.1094 - val_acc: 0.9633\n",
            "Epoch 259/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9786\n",
            "Epoch 00259: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0768 - acc: 0.9783 - val_loss: 0.1040 - val_acc: 0.9708\n",
            "Epoch 260/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9846\n",
            "Epoch 00260: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0611 - acc: 0.9847 - val_loss: 0.1535 - val_acc: 0.9558\n",
            "Epoch 261/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0655 - acc: 0.9850\n",
            "Epoch 00261: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0658 - acc: 0.9839 - val_loss: 0.1151 - val_acc: 0.9633\n",
            "Epoch 262/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9840\n",
            "Epoch 00262: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0605 - acc: 0.9842 - val_loss: 0.1279 - val_acc: 0.9583\n",
            "Epoch 263/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0681 - acc: 0.9800\n",
            "Epoch 00263: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.0686 - acc: 0.9794 - val_loss: 0.1377 - val_acc: 0.9525\n",
            "Epoch 264/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0741 - acc: 0.9806\n",
            "Epoch 00264: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0748 - acc: 0.9800 - val_loss: 0.1591 - val_acc: 0.9417\n",
            "Epoch 265/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0835 - acc: 0.9725\n",
            "Epoch 00265: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0832 - acc: 0.9733 - val_loss: 0.1101 - val_acc: 0.9667\n",
            "Epoch 266/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0636 - acc: 0.9809\n",
            "Epoch 00266: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0614 - acc: 0.9822 - val_loss: 0.1232 - val_acc: 0.9625\n",
            "Epoch 267/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0733 - acc: 0.9773\n",
            "Epoch 00267: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0742 - acc: 0.9775 - val_loss: 0.1346 - val_acc: 0.9608\n",
            "Epoch 268/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0689 - acc: 0.9794\n",
            "Epoch 00268: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0686 - acc: 0.9792 - val_loss: 0.0999 - val_acc: 0.9667\n",
            "Epoch 269/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9789\n",
            "Epoch 00269: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0661 - acc: 0.9789 - val_loss: 0.0962 - val_acc: 0.9708\n",
            "Epoch 270/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0527 - acc: 0.9841\n",
            "Epoch 00270: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0514 - acc: 0.9847 - val_loss: 0.1463 - val_acc: 0.9533\n",
            "Epoch 271/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9789\n",
            "Epoch 00271: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0748 - acc: 0.9789 - val_loss: 0.1166 - val_acc: 0.9608\n",
            "Epoch 272/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0612 - acc: 0.9818\n",
            "Epoch 00272: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0625 - acc: 0.9814 - val_loss: 0.1062 - val_acc: 0.9700\n",
            "Epoch 273/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0595 - acc: 0.9828\n",
            "Epoch 00273: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0612 - acc: 0.9825 - val_loss: 0.1023 - val_acc: 0.9700\n",
            "Epoch 274/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0537 - acc: 0.9871\n",
            "Epoch 00274: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0556 - acc: 0.9864 - val_loss: 0.1188 - val_acc: 0.9625\n",
            "Epoch 275/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0755 - acc: 0.9788\n",
            "Epoch 00275: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0746 - acc: 0.9794 - val_loss: 0.1501 - val_acc: 0.9492\n",
            "Epoch 276/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9814\n",
            "Epoch 00276: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0666 - acc: 0.9806 - val_loss: 0.1452 - val_acc: 0.9600\n",
            "Epoch 277/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0907 - acc: 0.9679\n",
            "Epoch 00277: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0962 - acc: 0.9658 - val_loss: 0.1202 - val_acc: 0.9650\n",
            "Epoch 278/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0799 - acc: 0.9759\n",
            "Epoch 00278: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0802 - acc: 0.9756 - val_loss: 0.1130 - val_acc: 0.9600\n",
            "Epoch 279/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0776 - acc: 0.9761\n",
            "Epoch 00279: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0785 - acc: 0.9756 - val_loss: 0.1252 - val_acc: 0.9600\n",
            "Epoch 280/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0981 - acc: 0.9676\n",
            "Epoch 00280: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0982 - acc: 0.9675 - val_loss: 0.1216 - val_acc: 0.9575\n",
            "Epoch 281/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0971 - acc: 0.9682\n",
            "Epoch 00281: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0993 - acc: 0.9675 - val_loss: 0.1790 - val_acc: 0.9408\n",
            "Epoch 282/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0897 - acc: 0.9735\n",
            "Epoch 00282: val_loss did not improve from 0.09194\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0927 - acc: 0.9722 - val_loss: 0.1506 - val_acc: 0.9533\n",
            "Epoch 283/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9797\n",
            "Epoch 00283: val_loss improved from 0.09194 to 0.07445, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.0686 - acc: 0.9803 - val_loss: 0.0744 - val_acc: 0.9800\n",
            "Epoch 284/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0513 - acc: 0.9868\n",
            "Epoch 00284: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0517 - acc: 0.9867 - val_loss: 0.0938 - val_acc: 0.9775\n",
            "Epoch 285/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0761 - acc: 0.9772\n",
            "Epoch 00285: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0747 - acc: 0.9772 - val_loss: 0.1363 - val_acc: 0.9567\n",
            "Epoch 286/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0518 - acc: 0.9891\n",
            "Epoch 00286: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0512 - acc: 0.9892 - val_loss: 0.1293 - val_acc: 0.9567\n",
            "Epoch 287/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0546 - acc: 0.9838\n",
            "Epoch 00287: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0532 - acc: 0.9844 - val_loss: 0.0960 - val_acc: 0.9700\n",
            "Epoch 288/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9843\n",
            "Epoch 00288: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0521 - acc: 0.9844 - val_loss: 0.1262 - val_acc: 0.9592\n",
            "Epoch 289/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0646 - acc: 0.9809\n",
            "Epoch 00289: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0652 - acc: 0.9808 - val_loss: 0.1446 - val_acc: 0.9517\n",
            "Epoch 290/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0620 - acc: 0.9818\n",
            "Epoch 00290: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0617 - acc: 0.9822 - val_loss: 0.1083 - val_acc: 0.9650\n",
            "Epoch 291/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0751 - acc: 0.9745\n",
            "Epoch 00291: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0771 - acc: 0.9742 - val_loss: 0.1586 - val_acc: 0.9558\n",
            "Epoch 292/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0554 - acc: 0.9859\n",
            "Epoch 00292: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0563 - acc: 0.9856 - val_loss: 0.1399 - val_acc: 0.9558\n",
            "Epoch 293/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9869\n",
            "Epoch 00293: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0504 - acc: 0.9872 - val_loss: 0.1124 - val_acc: 0.9675\n",
            "Epoch 294/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0500 - acc: 0.9878\n",
            "Epoch 00294: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0501 - acc: 0.9875 - val_loss: 0.0919 - val_acc: 0.9725\n",
            "Epoch 295/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9863\n",
            "Epoch 00295: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0455 - acc: 0.9864 - val_loss: 0.0857 - val_acc: 0.9775\n",
            "Epoch 296/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9840\n",
            "Epoch 00296: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0505 - acc: 0.9844 - val_loss: 0.1381 - val_acc: 0.9608\n",
            "Epoch 297/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0629 - acc: 0.9797\n",
            "Epoch 00297: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0612 - acc: 0.9808 - val_loss: 0.1212 - val_acc: 0.9575\n",
            "Epoch 298/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0530 - acc: 0.9852\n",
            "Epoch 00298: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0537 - acc: 0.9853 - val_loss: 0.1380 - val_acc: 0.9542\n",
            "Epoch 299/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0536 - acc: 0.9838\n",
            "Epoch 00299: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0554 - acc: 0.9831 - val_loss: 0.1076 - val_acc: 0.9675\n",
            "Epoch 300/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9806\n",
            "Epoch 00300: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0665 - acc: 0.9811 - val_loss: 0.1251 - val_acc: 0.9650\n",
            "Epoch 301/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9731\n",
            "Epoch 00301: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0737 - acc: 0.9731 - val_loss: 0.2238 - val_acc: 0.9283\n",
            "Epoch 302/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0787 - acc: 0.9721\n",
            "Epoch 00302: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0784 - acc: 0.9725 - val_loss: 0.1088 - val_acc: 0.9700\n",
            "Epoch 303/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9806\n",
            "Epoch 00303: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0610 - acc: 0.9803 - val_loss: 0.0805 - val_acc: 0.9808\n",
            "Epoch 304/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9851\n",
            "Epoch 00304: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0537 - acc: 0.9856 - val_loss: 0.1263 - val_acc: 0.9625\n",
            "Epoch 305/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0549 - acc: 0.9853\n",
            "Epoch 00305: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0569 - acc: 0.9844 - val_loss: 0.1489 - val_acc: 0.9533\n",
            "Epoch 306/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0765 - acc: 0.9768\n",
            "Epoch 00306: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0754 - acc: 0.9775 - val_loss: 0.0946 - val_acc: 0.9675\n",
            "Epoch 307/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9846\n",
            "Epoch 00307: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0605 - acc: 0.9844 - val_loss: 0.1051 - val_acc: 0.9700\n",
            "Epoch 308/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0591 - acc: 0.9821\n",
            "Epoch 00308: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0592 - acc: 0.9825 - val_loss: 0.1630 - val_acc: 0.9483\n",
            "Epoch 309/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9731\n",
            "Epoch 00309: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0847 - acc: 0.9733 - val_loss: 0.1054 - val_acc: 0.9658\n",
            "Epoch 310/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1169 - acc: 0.9626\n",
            "Epoch 00310: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1146 - acc: 0.9639 - val_loss: 0.1756 - val_acc: 0.9467\n",
            "Epoch 311/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9731\n",
            "Epoch 00311: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0888 - acc: 0.9725 - val_loss: 0.1353 - val_acc: 0.9542\n",
            "Epoch 312/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0888 - acc: 0.9694\n",
            "Epoch 00312: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0883 - acc: 0.9697 - val_loss: 0.1424 - val_acc: 0.9533\n",
            "Epoch 313/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0659 - acc: 0.9815\n",
            "Epoch 00313: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0648 - acc: 0.9819 - val_loss: 0.1753 - val_acc: 0.9442\n",
            "Epoch 314/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0754 - acc: 0.9759\n",
            "Epoch 00314: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0756 - acc: 0.9758 - val_loss: 0.1617 - val_acc: 0.9442\n",
            "Epoch 315/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0568 - acc: 0.9874\n",
            "Epoch 00315: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0565 - acc: 0.9875 - val_loss: 0.1068 - val_acc: 0.9675\n",
            "Epoch 316/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9811\n",
            "Epoch 00316: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0619 - acc: 0.9814 - val_loss: 0.1168 - val_acc: 0.9658\n",
            "Epoch 317/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0622 - acc: 0.9800\n",
            "Epoch 00317: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0623 - acc: 0.9797 - val_loss: 0.0759 - val_acc: 0.9775\n",
            "Epoch 318/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0508 - acc: 0.9862\n",
            "Epoch 00318: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0527 - acc: 0.9850 - val_loss: 0.0979 - val_acc: 0.9700\n",
            "Epoch 319/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0703 - acc: 0.9768\n",
            "Epoch 00319: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0703 - acc: 0.9767 - val_loss: 0.1270 - val_acc: 0.9592\n",
            "Epoch 320/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0801 - acc: 0.9735\n",
            "Epoch 00320: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0806 - acc: 0.9733 - val_loss: 0.0966 - val_acc: 0.9708\n",
            "Epoch 321/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0635 - acc: 0.9803\n",
            "Epoch 00321: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0630 - acc: 0.9803 - val_loss: 0.1496 - val_acc: 0.9442\n",
            "Epoch 322/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9780\n",
            "Epoch 00322: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0686 - acc: 0.9772 - val_loss: 0.1123 - val_acc: 0.9658\n",
            "Epoch 323/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0609 - acc: 0.9809\n",
            "Epoch 00323: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0610 - acc: 0.9806 - val_loss: 0.0931 - val_acc: 0.9717\n",
            "Epoch 324/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0710 - acc: 0.9776\n",
            "Epoch 00324: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0737 - acc: 0.9761 - val_loss: 0.0749 - val_acc: 0.9758\n",
            "Epoch 325/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0721 - acc: 0.9774\n",
            "Epoch 00325: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0718 - acc: 0.9769 - val_loss: 0.0919 - val_acc: 0.9742\n",
            "Epoch 326/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0489 - acc: 0.9868\n",
            "Epoch 00326: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0483 - acc: 0.9867 - val_loss: 0.1080 - val_acc: 0.9642\n",
            "Epoch 327/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0548 - acc: 0.9835\n",
            "Epoch 00327: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0546 - acc: 0.9833 - val_loss: 0.0867 - val_acc: 0.9792\n",
            "Epoch 328/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0714 - acc: 0.9771\n",
            "Epoch 00328: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0712 - acc: 0.9764 - val_loss: 0.1140 - val_acc: 0.9658\n",
            "Epoch 329/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0711 - acc: 0.9762\n",
            "Epoch 00329: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0721 - acc: 0.9769 - val_loss: 0.1146 - val_acc: 0.9683\n",
            "Epoch 330/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0715 - acc: 0.9800\n",
            "Epoch 00330: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0700 - acc: 0.9800 - val_loss: 0.0986 - val_acc: 0.9650\n",
            "Epoch 331/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9817\n",
            "Epoch 00331: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0614 - acc: 0.9814 - val_loss: 0.1337 - val_acc: 0.9583\n",
            "Epoch 332/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0480 - acc: 0.9882\n",
            "Epoch 00332: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0474 - acc: 0.9883 - val_loss: 0.1227 - val_acc: 0.9617\n",
            "Epoch 333/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9791\n",
            "Epoch 00333: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0627 - acc: 0.9789 - val_loss: 0.1862 - val_acc: 0.9400\n",
            "Epoch 334/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0405 - acc: 0.9891\n",
            "Epoch 00334: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0400 - acc: 0.9897 - val_loss: 0.1021 - val_acc: 0.9642\n",
            "Epoch 335/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0406 - acc: 0.9884\n",
            "Epoch 00335: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0424 - acc: 0.9878 - val_loss: 0.1318 - val_acc: 0.9542\n",
            "Epoch 336/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0446 - acc: 0.9870\n",
            "Epoch 00336: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0458 - acc: 0.9867 - val_loss: 0.0758 - val_acc: 0.9767\n",
            "Epoch 337/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0452 - acc: 0.9882\n",
            "Epoch 00337: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0459 - acc: 0.9881 - val_loss: 0.1066 - val_acc: 0.9700\n",
            "Epoch 338/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0495 - acc: 0.9856\n",
            "Epoch 00338: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0483 - acc: 0.9864 - val_loss: 0.0937 - val_acc: 0.9717\n",
            "Epoch 339/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9843\n",
            "Epoch 00339: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0555 - acc: 0.9839 - val_loss: 0.1567 - val_acc: 0.9542\n",
            "Epoch 340/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0544 - acc: 0.9838\n",
            "Epoch 00340: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0546 - acc: 0.9839 - val_loss: 0.1121 - val_acc: 0.9708\n",
            "Epoch 341/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0823 - acc: 0.9745\n",
            "Epoch 00341: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0821 - acc: 0.9747 - val_loss: 0.1176 - val_acc: 0.9600\n",
            "Epoch 342/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9869\n",
            "Epoch 00342: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0553 - acc: 0.9869 - val_loss: 0.1282 - val_acc: 0.9642\n",
            "Epoch 343/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0638 - acc: 0.9809\n",
            "Epoch 00343: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0624 - acc: 0.9814 - val_loss: 0.0974 - val_acc: 0.9725\n",
            "Epoch 344/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0407 - acc: 0.9915\n",
            "Epoch 00344: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0406 - acc: 0.9914 - val_loss: 0.0949 - val_acc: 0.9742\n",
            "Epoch 345/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0564 - acc: 0.9815\n",
            "Epoch 00345: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0592 - acc: 0.9797 - val_loss: 0.1206 - val_acc: 0.9658\n",
            "Epoch 346/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0660 - acc: 0.9767\n",
            "Epoch 00346: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0657 - acc: 0.9769 - val_loss: 0.0987 - val_acc: 0.9692\n",
            "Epoch 347/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0578 - acc: 0.9856\n",
            "Epoch 00347: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0579 - acc: 0.9853 - val_loss: 0.1006 - val_acc: 0.9700\n",
            "Epoch 348/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0542 - acc: 0.9847\n",
            "Epoch 00348: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0546 - acc: 0.9844 - val_loss: 0.1913 - val_acc: 0.9375\n",
            "Epoch 349/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0668 - acc: 0.9779\n",
            "Epoch 00349: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0677 - acc: 0.9769 - val_loss: 0.1042 - val_acc: 0.9675\n",
            "Epoch 350/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0828 - acc: 0.9768\n",
            "Epoch 00350: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0821 - acc: 0.9769 - val_loss: 0.0784 - val_acc: 0.9750\n",
            "Epoch 351/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9723\n",
            "Epoch 00351: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0783 - acc: 0.9725 - val_loss: 0.0875 - val_acc: 0.9750\n",
            "Epoch 352/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1014 - acc: 0.9671\n",
            "Epoch 00352: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1005 - acc: 0.9669 - val_loss: 0.0995 - val_acc: 0.9692\n",
            "Epoch 353/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0737 - acc: 0.9794\n",
            "Epoch 00353: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0743 - acc: 0.9794 - val_loss: 0.1204 - val_acc: 0.9592\n",
            "Epoch 354/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9820\n",
            "Epoch 00354: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0634 - acc: 0.9825 - val_loss: 0.0914 - val_acc: 0.9683\n",
            "Epoch 355/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9783\n",
            "Epoch 00355: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0697 - acc: 0.9789 - val_loss: 0.1049 - val_acc: 0.9667\n",
            "Epoch 356/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0575 - acc: 0.9838\n",
            "Epoch 00356: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0570 - acc: 0.9844 - val_loss: 0.1007 - val_acc: 0.9692\n",
            "Epoch 357/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0641 - acc: 0.9794\n",
            "Epoch 00357: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0632 - acc: 0.9800 - val_loss: 0.1364 - val_acc: 0.9583\n",
            "Epoch 358/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0493 - acc: 0.9848\n",
            "Epoch 00358: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0500 - acc: 0.9847 - val_loss: 0.0920 - val_acc: 0.9758\n",
            "Epoch 359/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9877\n",
            "Epoch 00359: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0488 - acc: 0.9881 - val_loss: 0.1127 - val_acc: 0.9642\n",
            "Epoch 360/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9871\n",
            "Epoch 00360: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0483 - acc: 0.9872 - val_loss: 0.0860 - val_acc: 0.9750\n",
            "Epoch 361/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0401 - acc: 0.9909\n",
            "Epoch 00361: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0396 - acc: 0.9911 - val_loss: 0.0812 - val_acc: 0.9767\n",
            "Epoch 362/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9769\n",
            "Epoch 00362: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0702 - acc: 0.9772 - val_loss: 0.1599 - val_acc: 0.9450\n",
            "Epoch 363/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0743 - acc: 0.9776\n",
            "Epoch 00363: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0725 - acc: 0.9783 - val_loss: 0.1322 - val_acc: 0.9592\n",
            "Epoch 364/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0775 - acc: 0.9772\n",
            "Epoch 00364: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0742 - acc: 0.9786 - val_loss: 0.1009 - val_acc: 0.9700\n",
            "Epoch 365/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9811\n",
            "Epoch 00365: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0591 - acc: 0.9811 - val_loss: 0.0996 - val_acc: 0.9692\n",
            "Epoch 366/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0624 - acc: 0.9803\n",
            "Epoch 00366: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0656 - acc: 0.9794 - val_loss: 0.1702 - val_acc: 0.9492\n",
            "Epoch 367/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0575 - acc: 0.9856\n",
            "Epoch 00367: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0578 - acc: 0.9856 - val_loss: 0.1590 - val_acc: 0.9483\n",
            "Epoch 368/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9794\n",
            "Epoch 00368: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0611 - acc: 0.9792 - val_loss: 0.1306 - val_acc: 0.9575\n",
            "Epoch 369/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9849\n",
            "Epoch 00369: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0560 - acc: 0.9847 - val_loss: 0.1150 - val_acc: 0.9600\n",
            "Epoch 370/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0651 - acc: 0.9809\n",
            "Epoch 00370: val_loss did not improve from 0.07445\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0667 - acc: 0.9800 - val_loss: 0.1182 - val_acc: 0.9625\n",
            "Epoch 371/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9791\n",
            "Epoch 00371: val_loss improved from 0.07445 to 0.07316, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.0636 - acc: 0.9789 - val_loss: 0.0732 - val_acc: 0.9767\n",
            "Epoch 372/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0491 - acc: 0.9861\n",
            "Epoch 00372: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0502 - acc: 0.9856 - val_loss: 0.0937 - val_acc: 0.9717\n",
            "Epoch 373/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9866\n",
            "Epoch 00373: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0459 - acc: 0.9869 - val_loss: 0.0971 - val_acc: 0.9692\n",
            "Epoch 374/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9834\n",
            "Epoch 00374: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0511 - acc: 0.9836 - val_loss: 0.1132 - val_acc: 0.9667\n",
            "Epoch 375/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9831\n",
            "Epoch 00375: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0506 - acc: 0.9836 - val_loss: 0.0908 - val_acc: 0.9733\n",
            "Epoch 376/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0701 - acc: 0.9772\n",
            "Epoch 00376: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0668 - acc: 0.9786 - val_loss: 0.1147 - val_acc: 0.9625\n",
            "Epoch 377/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9837\n",
            "Epoch 00377: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0532 - acc: 0.9836 - val_loss: 0.1257 - val_acc: 0.9575\n",
            "Epoch 378/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0483 - acc: 0.9867\n",
            "Epoch 00378: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0501 - acc: 0.9853 - val_loss: 0.0875 - val_acc: 0.9750\n",
            "Epoch 379/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0431 - acc: 0.9879\n",
            "Epoch 00379: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0446 - acc: 0.9875 - val_loss: 0.1386 - val_acc: 0.9567\n",
            "Epoch 380/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0537 - acc: 0.9836\n",
            "Epoch 00380: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0535 - acc: 0.9839 - val_loss: 0.1392 - val_acc: 0.9550\n",
            "Epoch 381/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0718 - acc: 0.9770\n",
            "Epoch 00381: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0726 - acc: 0.9761 - val_loss: 0.0811 - val_acc: 0.9750\n",
            "Epoch 382/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9789\n",
            "Epoch 00382: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0702 - acc: 0.9794 - val_loss: 0.0847 - val_acc: 0.9758\n",
            "Epoch 383/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0539 - acc: 0.9852\n",
            "Epoch 00383: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0545 - acc: 0.9847 - val_loss: 0.1403 - val_acc: 0.9558\n",
            "Epoch 384/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9766\n",
            "Epoch 00384: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0780 - acc: 0.9753 - val_loss: 0.1278 - val_acc: 0.9575\n",
            "Epoch 385/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0729 - acc: 0.9768\n",
            "Epoch 00385: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0719 - acc: 0.9778 - val_loss: 0.1976 - val_acc: 0.9417\n",
            "Epoch 386/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0654 - acc: 0.9778\n",
            "Epoch 00386: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0670 - acc: 0.9786 - val_loss: 0.1314 - val_acc: 0.9500\n",
            "Epoch 387/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0599 - acc: 0.9821\n",
            "Epoch 00387: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0594 - acc: 0.9831 - val_loss: 0.1046 - val_acc: 0.9675\n",
            "Epoch 388/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0557 - acc: 0.9858\n",
            "Epoch 00388: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0551 - acc: 0.9856 - val_loss: 0.0850 - val_acc: 0.9742\n",
            "Epoch 389/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0506 - acc: 0.9845\n",
            "Epoch 00389: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0513 - acc: 0.9844 - val_loss: 0.0832 - val_acc: 0.9758\n",
            "Epoch 390/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0442 - acc: 0.9879\n",
            "Epoch 00390: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0436 - acc: 0.9883 - val_loss: 0.0799 - val_acc: 0.9767\n",
            "Epoch 391/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9846\n",
            "Epoch 00391: val_loss did not improve from 0.07316\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0507 - acc: 0.9847 - val_loss: 0.0855 - val_acc: 0.9717\n",
            "Epoch 392/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0433 - acc: 0.9879\n",
            "Epoch 00392: val_loss improved from 0.07316 to 0.07005, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.0452 - acc: 0.9872 - val_loss: 0.0701 - val_acc: 0.9808\n",
            "Epoch 393/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0477 - acc: 0.9862\n",
            "Epoch 00393: val_loss did not improve from 0.07005\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0486 - acc: 0.9864 - val_loss: 0.1399 - val_acc: 0.9567\n",
            "Epoch 394/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0591 - acc: 0.9800\n",
            "Epoch 00394: val_loss did not improve from 0.07005\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0600 - acc: 0.9806 - val_loss: 0.1952 - val_acc: 0.9367\n",
            "Epoch 395/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0563 - acc: 0.9803\n",
            "Epoch 00395: val_loss did not improve from 0.07005\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0556 - acc: 0.9808 - val_loss: 0.1190 - val_acc: 0.9608\n",
            "Epoch 396/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0675 - acc: 0.9787\n",
            "Epoch 00396: val_loss improved from 0.07005 to 0.06892, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.0711 - acc: 0.9783 - val_loss: 0.0689 - val_acc: 0.9800\n",
            "Epoch 397/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0703 - acc: 0.9787\n",
            "Epoch 00397: val_loss did not improve from 0.06892\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0731 - acc: 0.9778 - val_loss: 0.1691 - val_acc: 0.9450\n",
            "Epoch 398/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0783 - acc: 0.9712\n",
            "Epoch 00398: val_loss did not improve from 0.06892\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0743 - acc: 0.9731 - val_loss: 0.0967 - val_acc: 0.9658\n",
            "Epoch 399/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0599 - acc: 0.9818\n",
            "Epoch 00399: val_loss did not improve from 0.06892\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0612 - acc: 0.9819 - val_loss: 0.1181 - val_acc: 0.9633\n",
            "Epoch 400/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0501 - acc: 0.9850\n",
            "Epoch 00400: val_loss did not improve from 0.06892\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0490 - acc: 0.9856 - val_loss: 0.0822 - val_acc: 0.9742\n",
            "1200/1200 [==============================] - 0s 126us/sample - loss: 0.0822 - acc: 0.9742\n",
            "[0.08218178878227869, 0.9741667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_9TthHHuwyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "ac69dff5-bbbb-4014-c366-3ae5a7f6ed98"
      },
      "source": [
        "for i in range(9, 10): # Итерација низ секој испитен примерок\n",
        "  print(f\"====================== Примерок ({i}) ======================\")\n",
        "  print(\"Вчитување тест податоци од испитниот примерок \" + str(i) + \"...\")\n",
        "  \n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_test_data = np.empty(0)\n",
        "  temp_test_events = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_test_set = 'S' + format(j, '02') + '/Test'\n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testData.mat\"\n",
        "    temp = loadmat(full_path)['testData']\n",
        "    if temp_test_data.size != 0:\n",
        "      temp_test_data = np.concatenate((temp_test_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_test_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_test_events.size != 0:\n",
        "        temp_test_events = np.append(temp_test_events, temp)\n",
        "      else:\n",
        "        temp_test_events = np.array(temp)\n",
        "\n",
        "    # Вчитување на бројот на runs \n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/runs_per_block.txt\"\n",
        "    with open(full_path, \"r\") as runs_per_block:\n",
        "      test_runs_per_block[i-1][j-1] = int(runs_per_block.read())\n",
        "\n",
        "    print(\"\\t - Тест податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "  # Зачувај ги тест податоците вчитани од испитниот примерок во низа\n",
        "  test_data.append(temp_test_data)\n",
        "  test_events.append(temp_test_events)\n",
        "  print(\"Тест податоците од испитниот примерок \" + str(i) + \" се вчитани.\\n\")\n",
        "\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_data: \" + str(test_data[i-1].shape)) # test_data to predict\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_events: \" + str(len(test_events[i-1]))) # test_events\n",
        "  for j in range (1,4):\n",
        "    print(\"SBJ\" + str(format(i-1, '02')) + \" / S\" + str(format(j-1, '02')) + \"| Runs per block: \" + str(test_runs_per_block[i-1][j-1])) # runs per block in SJB01, SJ00 \n",
        "\n",
        "  to_predict_data = reshape_data_to_mne_format(test_data[i-1])\n",
        "  predictions = model9.predict(to_predict_data)\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Predictions: \" + str(len(predictions)))\n",
        "  # np.savetxt(\"predictions.csv\", predictions, delimiter=\",\")\n",
        "\n",
        "\n",
        "  # ========= FALI USTE DA SE ISPARSIRA PREDICTIONOT... NE E SREDEN OVOJ KOD DOLE =======\n",
        "\n",
        "  int_pred = np.argmax(predictions, axis=1)\n",
        "  int_ytest = np.argmax(y_test, axis=1)\n",
        "\n",
        "  session_start = 0\n",
        "  start_prediction_index = 0\n",
        "  end_prediction_index = 0\n",
        "  for session in range(0, 3):\n",
        "    print(f\"============== Сесија ({session}) ==============\")\n",
        "    for block in range(0, 50):    \n",
        "      events_per_block = test_runs_per_block[i-1][session]\n",
        "\n",
        "      start_prediction_index = session_start + (block*events_per_block)*8\n",
        "      end_prediction_index = session_start + ((block+1)*events_per_block)*8\n",
        "\n",
        "      block_prediction = int_pred[start_prediction_index:end_prediction_index]\n",
        "      prediction = np.bincount(block_prediction).argmax()\n",
        "      df.iat[session+24,block+2] = prediction+1\n",
        "      # UNCOMMENT ZA PODOBAR PRIKAZ :)\n",
        "      # print(f\"Session {session} | Block: {block} | Prediction: {prediction} | Address: {end_prediction_index}\")\n",
        "\n",
        "      print(str(prediction+1) + \",\", end=\"\")\n",
        "    session_start = end_prediction_index\n",
        "    print(\"\")\n",
        "  print(\"Stigna li do kraj: \" + str(session_start == len(predictions)))\n",
        "  print(f\"====================== Примерок ({i}) ======================\\n\\n\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Примерок (9) ======================\n",
            "Вчитување тест податоци од испитниот примерок 9...\n",
            "\t - Тест податоците од сесија 1 се вчитани.\n",
            "\t - Тест податоците од сесија 2 се вчитани.\n",
            "\t - Тест податоците од сесија 3 се вчитани.\n",
            "Тест податоците од испитниот примерок 9 се вчитани.\n",
            "\n",
            "SBJ08| Test_data: (8, 350, 10000)\n",
            "SBJ08| Test_events: 10000\n",
            "SBJ08 / S00| Runs per block: 8\n",
            "SBJ08 / S01| Runs per block: 8\n",
            "SBJ08 / S02| Runs per block: 9\n",
            "SBJ08| Predictions: 10000\n",
            "============== Сесија (0) ==============\n",
            "6,5,6,6,7,6,8,6,8,6,4,8,6,6,5,6,5,8,6,8,8,8,6,6,6,6,6,6,6,5,5,5,6,6,5,6,8,5,5,8,6,6,6,6,6,5,5,5,3,5,\n",
            "============== Сесија (1) ==============\n",
            "3,4,8,4,1,6,1,6,3,3,6,3,3,6,6,1,1,1,1,1,1,2,1,4,2,1,2,1,1,3,1,4,8,1,1,1,1,4,1,4,1,4,1,1,1,1,1,3,6,6,\n",
            "============== Сесија (2) ==============\n",
            "5,2,2,2,2,3,2,3,2,2,3,3,2,2,2,2,3,3,2,2,6,4,2,2,4,6,2,1,1,3,1,2,3,3,4,2,3,2,3,2,3,3,2,2,2,3,2,2,4,2,\n",
            "Stigna li do kraj: True\n",
            "====================== Примерок (9) ======================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3ewhd_Mu79j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a70854a-28f0-46fb-f42a-a6bbd71d14ba"
      },
      "source": [
        "df"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUBJECT</th>\n",
              "      <th>SESSION</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>Unnamed: 52</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    SUBJECT  SESSION  1  2  3  4  5  6  7  ... 43 44 45 46 47 48 49 50 Unnamed: 52\n",
              "0         1        1  6  6  3  6  6  3  6  ...  6  4  8  3  4  5  5  7         NaN\n",
              "1         1        2  6  3  2  1  2  1  3  ...  6  2  2  2  3  6  6  2         NaN\n",
              "2         1        3  3  3  3  3  3  3  3  ...  3  3  6  3  6  7  1  6         NaN\n",
              "3         2        1  8  8  8  8  8  8  8  ...  8  4  8  4  8  7  8  8         NaN\n",
              "4         2        2  2  7  6  6  6  6  7  ...  2  6  6  2  6  6  2  2         NaN\n",
              "5         2        3  2  7  2  6  7  4  2  ...  2  6  2  2  7  2  2  2         NaN\n",
              "6         3        1  3  3  4  4  4  3  6  ...  4  6  3  3  4  3  4  4         NaN\n",
              "7         3        2  6  1  7  7  7  7  7  ...  6  7  1  6  6  6  6  6         NaN\n",
              "8         3        3  6  4  8  8  5  7  8  ...  4  2  8  2  2  2  2  8         NaN\n",
              "9         4        1  1  1  2  1  5  5  6  ...  1  1  7  1  6  6  6  6         NaN\n",
              "10        4        2  7  7  7  7  6  6  7  ...  1  5  5  5  5  5  2  6         NaN\n",
              "11        4        3  7  3  7  3  6  6  7  ...  6  1  4  4  4  6  6  6         NaN\n",
              "12        5        1  5  4  4  4  6  4  5  ...  3  1  4  4  1  3  3  5         NaN\n",
              "13        5        2  3  6  3  5  2  6  7  ...  6  6  6  6  6  6  6  4         NaN\n",
              "14        5        3  4  3  3  6  5  7  6  ...  7  2  2  7  5  6  4  2         NaN\n",
              "15        6        1  1  3  1  8  3  3  3  ...  4  3  7  2  7  2  5  8         NaN\n",
              "16        6        2  3  4  1  7  1  1  1  ...  5  5  5  5  5  5  5  5         NaN\n",
              "17        6        3  2  3  2  5  3  3  3  ...  2  3  3  3  1  1  3  3         NaN\n",
              "18        7        1  4  7  5  7  4  5  4  ...  5  1  1  4  4  4  1  4         NaN\n",
              "19        7        2  2  2  2  8  2  5  2  ...  5  5  5  2  2  2  2  2         NaN\n",
              "20        7        3  2  7  7  5  7  5  4  ...  1  3  5  5  3  5  5  1         NaN\n",
              "21        8        1  5  8  2  8  2  2  1  ...  1  8  2  2  5  5  2  8         NaN\n",
              "22        8        2  2  7  1  5  1  2  1  ...  7  1  7  8  2  1  7  7         NaN\n",
              "23        8        3  4  1  6  6  1  2  1  ...  1  8  1  7  1  1  7  1         NaN\n",
              "24        9        1  6  5  6  6  7  6  8  ...  6  6  6  5  5  5  3  5         NaN\n",
              "25        9        2  3  4  8  4  1  6  1  ...  1  1  1  1  1  3  6  6         NaN\n",
              "26        9        3  5  2  2  2  2  3  2  ...  2  2  2  3  2  2  4  2         NaN\n",
              "27       10        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "28       10        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "29       10        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "30       11        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "31       11        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "32       11        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "33       12        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "34       12        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "35       12        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "36       13        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "37       13        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "38       13        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "39       14        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "40       14        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "41       14        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "42       15        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "43       15        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "44       15        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "\n",
              "[45 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6f0sEeyvOzI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3a2c5c98-cccc-48c6-cbbe-e73d4bfa6cc1"
      },
      "source": [
        "for i in range(10, 11): # Итерација низ секој испитен примерок\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_data = np.empty(0)\n",
        "  temp_labels = np.empty(0)\n",
        "  temp_events = np.empty(0)\n",
        "  temp_targets = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainData.mat\"\n",
        "    temp = loadmat(full_path)['trainData']\n",
        "    if temp_data.size != 0:\n",
        "      temp_data = np.concatenate((temp_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на label-ите\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainLabels.txt\"\n",
        "    with open(full_path, \"r\") as file_labels:\n",
        "      temp = file_labels.read().splitlines()\n",
        "      temp = np.repeat(temp, 8*10)\n",
        "      if temp_labels.size != 0:\n",
        "        temp_labels = np.concatenate((temp_labels, temp))\n",
        "      else:\n",
        "        temp_labels = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_events.size != 0:\n",
        "        temp_events = np.append(temp_events, temp)\n",
        "      else:\n",
        "        temp_events = np.array(temp)\n",
        "      \n",
        "\n",
        "    # Вчитување на редоследот на објекти кои се target\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainTargets.txt\"\n",
        "    with open(full_path, \"r\") as file_targets:\n",
        "      temp = file_targets.read().splitlines()\n",
        "      if temp_targets.size != 0:\n",
        "        temp_targets = np.concatenate((temp_targets, temp))\n",
        "      else:\n",
        "        temp_targets = np.array(temp)\n",
        "    print(\"\\t - Податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "\n",
        "  for j in range(4, 8): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "      \n",
        "  # Зачувај ги податоците вчитани од испитниот примерок во низа\n",
        "  data.append(temp_data)\n",
        "  labels.append(temp_labels)\n",
        "  events.append(temp_events)\n",
        "  targets.append(temp_targets)\n",
        "\n",
        "  \n",
        "  print(\"Податоците од испитниот примерок \" + str(i) + \" се вчитани.\")\n",
        "\n",
        "\n",
        "  #data = target_events_data_scaled\n",
        "  mne_array = np.swapaxes(data[i-1], 0, 2) # (епохa, канал, настан). \n",
        "  mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "  mne_array = mne_array.reshape(mne_array.shape[0],1, 8,350)\n",
        "  print(mne_array.shape)\n",
        "\n",
        "  events_arr = events[i-1].astype(np.int)\n",
        "  labels_arr = labels[i-1].astype(np.int)\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(mne_array, labels_arr-1, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "  model10 = DeepConvNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "  model10.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = Adam(0.0009))\n",
        "  checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                                verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "  #clf = RandomForestClassifier(max_depth=5)\n",
        "  #clf.fit(X_train, y_train)\n",
        "  #score = clf.score(X_test, y_test)\n",
        "  # print(score)\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  num_batch_size=100\n",
        "  num_epochs=400\n",
        "  model10.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "            validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "  score = model10.evaluate(X_test, y_test, verbose=1)\n",
        "  print(score)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t - Податоците од сесија 1 се вчитани.\n",
            "\t - Податоците од сесија 2 се вчитани.\n",
            "\t - Податоците од сесија 3 се вчитани.\n",
            "Податоците од испитниот примерок 10 се вчитани.\n",
            "(4800, 1, 8, 350)\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Train on 3600 samples, validate on 1200 samples\n",
            "Epoch 1/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.3195 - acc: 0.1771\n",
            "Epoch 00001: val_loss improved from inf to 2.12739, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 7s 2ms/sample - loss: 2.3164 - acc: 0.1756 - val_loss: 2.1274 - val_acc: 0.1808\n",
            "Epoch 2/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.2226 - acc: 0.1951\n",
            "Epoch 00002: val_loss did not improve from 2.12739\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 2.2206 - acc: 0.1964 - val_loss: 2.2004 - val_acc: 0.2200\n",
            "Epoch 3/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 2.1261 - acc: 0.2284\n",
            "Epoch 00003: val_loss did not improve from 2.12739\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 2.1329 - acc: 0.2244 - val_loss: 2.2430 - val_acc: 0.1917\n",
            "Epoch 4/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.1326 - acc: 0.2183\n",
            "Epoch 00004: val_loss did not improve from 2.12739\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 2.1350 - acc: 0.2172 - val_loss: 2.1868 - val_acc: 0.2233\n",
            "Epoch 5/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.0540 - acc: 0.2470\n",
            "Epoch 00005: val_loss improved from 2.12739 to 2.11074, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 2.0547 - acc: 0.2467 - val_loss: 2.1107 - val_acc: 0.2583\n",
            "Epoch 6/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.0010 - acc: 0.2509\n",
            "Epoch 00006: val_loss improved from 2.11074 to 1.96788, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 2.0026 - acc: 0.2503 - val_loss: 1.9679 - val_acc: 0.2725\n",
            "Epoch 7/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.9337 - acc: 0.2882\n",
            "Epoch 00007: val_loss did not improve from 1.96788\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 1.9450 - acc: 0.2867 - val_loss: 2.1233 - val_acc: 0.2758\n",
            "Epoch 8/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.8956 - acc: 0.2916\n",
            "Epoch 00008: val_loss did not improve from 1.96788\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 1.8895 - acc: 0.2944 - val_loss: 2.0614 - val_acc: 0.2875\n",
            "Epoch 9/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.7693 - acc: 0.3418\n",
            "Epoch 00009: val_loss improved from 1.96788 to 1.79837, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 1.7677 - acc: 0.3411 - val_loss: 1.7984 - val_acc: 0.3400\n",
            "Epoch 10/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.6628 - acc: 0.3818\n",
            "Epoch 00010: val_loss did not improve from 1.79837\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 1.6619 - acc: 0.3800 - val_loss: 1.8115 - val_acc: 0.3167\n",
            "Epoch 11/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.5923 - acc: 0.4106\n",
            "Epoch 00011: val_loss improved from 1.79837 to 1.67836, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 1.5988 - acc: 0.4072 - val_loss: 1.6784 - val_acc: 0.3800\n",
            "Epoch 12/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.5323 - acc: 0.4236\n",
            "Epoch 00012: val_loss improved from 1.67836 to 1.63690, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 1.5345 - acc: 0.4225 - val_loss: 1.6369 - val_acc: 0.4000\n",
            "Epoch 13/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.4325 - acc: 0.4563\n",
            "Epoch 00013: val_loss improved from 1.63690 to 1.53995, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 1.4205 - acc: 0.4614 - val_loss: 1.5400 - val_acc: 0.4450\n",
            "Epoch 14/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.3760 - acc: 0.4731\n",
            "Epoch 00014: val_loss did not improve from 1.53995\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 1.3760 - acc: 0.4708 - val_loss: 1.6503 - val_acc: 0.4108\n",
            "Epoch 15/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.3196 - acc: 0.4985\n",
            "Epoch 00015: val_loss did not improve from 1.53995\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 1.3151 - acc: 0.4986 - val_loss: 1.5677 - val_acc: 0.4150\n",
            "Epoch 16/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2914 - acc: 0.4994\n",
            "Epoch 00016: val_loss improved from 1.53995 to 1.29356, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 1.2910 - acc: 0.4997 - val_loss: 1.2936 - val_acc: 0.5192\n",
            "Epoch 17/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2488 - acc: 0.5285\n",
            "Epoch 00017: val_loss did not improve from 1.29356\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 1.2517 - acc: 0.5256 - val_loss: 1.4141 - val_acc: 0.4733\n",
            "Epoch 18/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.1625 - acc: 0.5553\n",
            "Epoch 00018: val_loss did not improve from 1.29356\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.1597 - acc: 0.5586 - val_loss: 1.5644 - val_acc: 0.4392\n",
            "Epoch 19/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.1581 - acc: 0.5594\n",
            "Epoch 00019: val_loss did not improve from 1.29356\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.1597 - acc: 0.5583 - val_loss: 1.3232 - val_acc: 0.5100\n",
            "Epoch 20/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1064 - acc: 0.5779\n",
            "Epoch 00020: val_loss improved from 1.29356 to 1.28504, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 1.1026 - acc: 0.5803 - val_loss: 1.2850 - val_acc: 0.5008\n",
            "Epoch 21/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0642 - acc: 0.5915\n",
            "Epoch 00021: val_loss improved from 1.28504 to 1.19767, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 1.0661 - acc: 0.5922 - val_loss: 1.1977 - val_acc: 0.5425\n",
            "Epoch 22/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.0580 - acc: 0.6006\n",
            "Epoch 00022: val_loss improved from 1.19767 to 1.18432, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 1.0632 - acc: 0.6022 - val_loss: 1.1843 - val_acc: 0.5792\n",
            "Epoch 23/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0083 - acc: 0.6168\n",
            "Epoch 00023: val_loss improved from 1.18432 to 1.14879, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 1.0134 - acc: 0.6164 - val_loss: 1.1488 - val_acc: 0.5642\n",
            "Epoch 24/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9616 - acc: 0.6418\n",
            "Epoch 00024: val_loss did not improve from 1.14879\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.9644 - acc: 0.6431 - val_loss: 1.2293 - val_acc: 0.5458\n",
            "Epoch 25/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9365 - acc: 0.6453\n",
            "Epoch 00025: val_loss improved from 1.14879 to 1.05667, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.9422 - acc: 0.6411 - val_loss: 1.0567 - val_acc: 0.5992\n",
            "Epoch 26/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9176 - acc: 0.6682\n",
            "Epoch 00026: val_loss did not improve from 1.05667\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.9237 - acc: 0.6650 - val_loss: 1.1239 - val_acc: 0.5633\n",
            "Epoch 27/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.8721 - acc: 0.6786\n",
            "Epoch 00027: val_loss improved from 1.05667 to 0.93050, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.8731 - acc: 0.6794 - val_loss: 0.9305 - val_acc: 0.6408\n",
            "Epoch 28/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8529 - acc: 0.6785\n",
            "Epoch 00028: val_loss did not improve from 0.93050\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.8580 - acc: 0.6742 - val_loss: 1.0642 - val_acc: 0.5875\n",
            "Epoch 29/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8231 - acc: 0.7021\n",
            "Epoch 00029: val_loss did not improve from 0.93050\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.8321 - acc: 0.6972 - val_loss: 0.9690 - val_acc: 0.6367\n",
            "Epoch 30/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.8121 - acc: 0.6977\n",
            "Epoch 00030: val_loss did not improve from 0.93050\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.8115 - acc: 0.6975 - val_loss: 0.9722 - val_acc: 0.6292\n",
            "Epoch 31/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7643 - acc: 0.7168\n",
            "Epoch 00031: val_loss did not improve from 0.93050\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.7585 - acc: 0.7211 - val_loss: 0.9329 - val_acc: 0.6367\n",
            "Epoch 32/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7473 - acc: 0.7271\n",
            "Epoch 00032: val_loss did not improve from 0.93050\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.7470 - acc: 0.7269 - val_loss: 1.0270 - val_acc: 0.5950\n",
            "Epoch 33/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7643 - acc: 0.7209\n",
            "Epoch 00033: val_loss improved from 0.93050 to 0.85069, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.7662 - acc: 0.7192 - val_loss: 0.8507 - val_acc: 0.6717\n",
            "Epoch 34/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7073 - acc: 0.7363\n",
            "Epoch 00034: val_loss did not improve from 0.85069\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.7124 - acc: 0.7347 - val_loss: 0.8900 - val_acc: 0.6542\n",
            "Epoch 35/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6724 - acc: 0.7568\n",
            "Epoch 00035: val_loss improved from 0.85069 to 0.78387, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.6679 - acc: 0.7600 - val_loss: 0.7839 - val_acc: 0.7008\n",
            "Epoch 36/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.6605 - acc: 0.7639\n",
            "Epoch 00036: val_loss improved from 0.78387 to 0.76988, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.6640 - acc: 0.7625 - val_loss: 0.7699 - val_acc: 0.7242\n",
            "Epoch 37/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6481 - acc: 0.7629\n",
            "Epoch 00037: val_loss improved from 0.76988 to 0.74875, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.6488 - acc: 0.7633 - val_loss: 0.7487 - val_acc: 0.7225\n",
            "Epoch 38/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6198 - acc: 0.7817\n",
            "Epoch 00038: val_loss improved from 0.74875 to 0.69278, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.6174 - acc: 0.7828 - val_loss: 0.6928 - val_acc: 0.7417\n",
            "Epoch 39/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.5871 - acc: 0.8009\n",
            "Epoch 00039: val_loss did not improve from 0.69278\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.5870 - acc: 0.8011 - val_loss: 0.6934 - val_acc: 0.7450\n",
            "Epoch 40/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5710 - acc: 0.8014\n",
            "Epoch 00040: val_loss did not improve from 0.69278\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.5713 - acc: 0.8014 - val_loss: 0.7248 - val_acc: 0.7358\n",
            "Epoch 41/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5553 - acc: 0.8029\n",
            "Epoch 00041: val_loss improved from 0.69278 to 0.67086, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.5589 - acc: 0.8008 - val_loss: 0.6709 - val_acc: 0.7683\n",
            "Epoch 42/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5320 - acc: 0.8215\n",
            "Epoch 00042: val_loss did not improve from 0.67086\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.5335 - acc: 0.8211 - val_loss: 0.6900 - val_acc: 0.7458\n",
            "Epoch 43/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5409 - acc: 0.8114\n",
            "Epoch 00043: val_loss did not improve from 0.67086\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.5419 - acc: 0.8114 - val_loss: 0.6853 - val_acc: 0.7492\n",
            "Epoch 44/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5274 - acc: 0.8127\n",
            "Epoch 00044: val_loss improved from 0.67086 to 0.62158, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.5332 - acc: 0.8108 - val_loss: 0.6216 - val_acc: 0.7783\n",
            "Epoch 45/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.5126 - acc: 0.8194\n",
            "Epoch 00045: val_loss did not improve from 0.62158\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.5106 - acc: 0.8197 - val_loss: 0.6418 - val_acc: 0.7733\n",
            "Epoch 46/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5097 - acc: 0.8276\n",
            "Epoch 00046: val_loss improved from 0.62158 to 0.55938, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.5099 - acc: 0.8275 - val_loss: 0.5594 - val_acc: 0.7858\n",
            "Epoch 47/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4766 - acc: 0.8397\n",
            "Epoch 00047: val_loss improved from 0.55938 to 0.55872, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.4813 - acc: 0.8381 - val_loss: 0.5587 - val_acc: 0.7942\n",
            "Epoch 48/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4804 - acc: 0.8309\n",
            "Epoch 00048: val_loss improved from 0.55872 to 0.50102, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.4786 - acc: 0.8311 - val_loss: 0.5010 - val_acc: 0.8250\n",
            "Epoch 49/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4624 - acc: 0.8338\n",
            "Epoch 00049: val_loss did not improve from 0.50102\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.4636 - acc: 0.8342 - val_loss: 0.5331 - val_acc: 0.8150\n",
            "Epoch 50/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4510 - acc: 0.8412\n",
            "Epoch 00050: val_loss did not improve from 0.50102\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.4475 - acc: 0.8417 - val_loss: 0.5688 - val_acc: 0.7892\n",
            "Epoch 51/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4596 - acc: 0.8391\n",
            "Epoch 00051: val_loss did not improve from 0.50102\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.4645 - acc: 0.8361 - val_loss: 0.5417 - val_acc: 0.7958\n",
            "Epoch 52/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4165 - acc: 0.8606\n",
            "Epoch 00052: val_loss improved from 0.50102 to 0.46355, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.4183 - acc: 0.8594 - val_loss: 0.4636 - val_acc: 0.8583\n",
            "Epoch 53/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4235 - acc: 0.8582\n",
            "Epoch 00053: val_loss did not improve from 0.46355\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.4195 - acc: 0.8589 - val_loss: 0.4767 - val_acc: 0.8350\n",
            "Epoch 54/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3897 - acc: 0.8709\n",
            "Epoch 00054: val_loss did not improve from 0.46355\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.3860 - acc: 0.8731 - val_loss: 0.4866 - val_acc: 0.8408\n",
            "Epoch 55/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3762 - acc: 0.8750\n",
            "Epoch 00055: val_loss did not improve from 0.46355\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.3800 - acc: 0.8722 - val_loss: 0.5025 - val_acc: 0.8283\n",
            "Epoch 56/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3757 - acc: 0.8773\n",
            "Epoch 00056: val_loss improved from 0.46355 to 0.43036, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.3759 - acc: 0.8775 - val_loss: 0.4304 - val_acc: 0.8633\n",
            "Epoch 57/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3403 - acc: 0.8911\n",
            "Epoch 00057: val_loss did not improve from 0.43036\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.3412 - acc: 0.8908 - val_loss: 0.4435 - val_acc: 0.8500\n",
            "Epoch 58/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3702 - acc: 0.8844\n",
            "Epoch 00058: val_loss did not improve from 0.43036\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.3740 - acc: 0.8831 - val_loss: 0.4899 - val_acc: 0.8300\n",
            "Epoch 59/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3452 - acc: 0.8885\n",
            "Epoch 00059: val_loss improved from 0.43036 to 0.38861, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.3462 - acc: 0.8878 - val_loss: 0.3886 - val_acc: 0.8692\n",
            "Epoch 60/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3291 - acc: 0.8977\n",
            "Epoch 00060: val_loss did not improve from 0.38861\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.3287 - acc: 0.8986 - val_loss: 0.4064 - val_acc: 0.8650\n",
            "Epoch 61/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3335 - acc: 0.8927\n",
            "Epoch 00061: val_loss improved from 0.38861 to 0.38267, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.3313 - acc: 0.8925 - val_loss: 0.3827 - val_acc: 0.8817\n",
            "Epoch 62/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3214 - acc: 0.8982\n",
            "Epoch 00062: val_loss did not improve from 0.38267\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.3223 - acc: 0.8981 - val_loss: 0.4202 - val_acc: 0.8500\n",
            "Epoch 63/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3158 - acc: 0.8997\n",
            "Epoch 00063: val_loss improved from 0.38267 to 0.34068, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.3192 - acc: 0.8986 - val_loss: 0.3407 - val_acc: 0.8983\n",
            "Epoch 64/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3060 - acc: 0.9066\n",
            "Epoch 00064: val_loss did not improve from 0.34068\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.3070 - acc: 0.9069 - val_loss: 0.4674 - val_acc: 0.8475\n",
            "Epoch 65/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2871 - acc: 0.9117\n",
            "Epoch 00065: val_loss did not improve from 0.34068\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.2854 - acc: 0.9131 - val_loss: 0.3561 - val_acc: 0.8742\n",
            "Epoch 66/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2968 - acc: 0.9024\n",
            "Epoch 00066: val_loss did not improve from 0.34068\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2998 - acc: 0.9017 - val_loss: 0.3695 - val_acc: 0.8767\n",
            "Epoch 67/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2810 - acc: 0.9119\n",
            "Epoch 00067: val_loss did not improve from 0.34068\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2843 - acc: 0.9097 - val_loss: 0.4146 - val_acc: 0.8525\n",
            "Epoch 68/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2849 - acc: 0.9085\n",
            "Epoch 00068: val_loss did not improve from 0.34068\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2877 - acc: 0.9069 - val_loss: 0.3604 - val_acc: 0.8808\n",
            "Epoch 69/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9046\n",
            "Epoch 00069: val_loss did not improve from 0.34068\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.2902 - acc: 0.9044 - val_loss: 0.3480 - val_acc: 0.8850\n",
            "Epoch 70/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2853 - acc: 0.9086\n",
            "Epoch 00070: val_loss improved from 0.34068 to 0.29813, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.2871 - acc: 0.9083 - val_loss: 0.2981 - val_acc: 0.9100\n",
            "Epoch 71/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2505 - acc: 0.9253\n",
            "Epoch 00071: val_loss improved from 0.29813 to 0.27490, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.2492 - acc: 0.9267 - val_loss: 0.2749 - val_acc: 0.9150\n",
            "Epoch 72/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2625 - acc: 0.9172\n",
            "Epoch 00072: val_loss did not improve from 0.27490\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2573 - acc: 0.9197 - val_loss: 0.3083 - val_acc: 0.9033\n",
            "Epoch 73/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2434 - acc: 0.9297\n",
            "Epoch 00073: val_loss did not improve from 0.27490\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2432 - acc: 0.9303 - val_loss: 0.3388 - val_acc: 0.8958\n",
            "Epoch 74/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2529 - acc: 0.9261\n",
            "Epoch 00074: val_loss did not improve from 0.27490\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2536 - acc: 0.9258 - val_loss: 0.3095 - val_acc: 0.9000\n",
            "Epoch 75/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9303\n",
            "Epoch 00075: val_loss did not improve from 0.27490\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2353 - acc: 0.9311 - val_loss: 0.3036 - val_acc: 0.9092\n",
            "Epoch 76/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9251\n",
            "Epoch 00076: val_loss did not improve from 0.27490\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2430 - acc: 0.9244 - val_loss: 0.2986 - val_acc: 0.9042\n",
            "Epoch 77/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2397 - acc: 0.9276\n",
            "Epoch 00077: val_loss did not improve from 0.27490\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2391 - acc: 0.9272 - val_loss: 0.2814 - val_acc: 0.9067\n",
            "Epoch 78/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2489 - acc: 0.9209\n",
            "Epoch 00078: val_loss improved from 0.27490 to 0.26486, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.2454 - acc: 0.9222 - val_loss: 0.2649 - val_acc: 0.9158\n",
            "Epoch 79/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2297 - acc: 0.9311\n",
            "Epoch 00079: val_loss improved from 0.26486 to 0.25530, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.2277 - acc: 0.9325 - val_loss: 0.2553 - val_acc: 0.9267\n",
            "Epoch 80/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2147 - acc: 0.9339\n",
            "Epoch 00080: val_loss improved from 0.25530 to 0.23938, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.2143 - acc: 0.9350 - val_loss: 0.2394 - val_acc: 0.9292\n",
            "Epoch 81/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2012 - acc: 0.9380\n",
            "Epoch 00081: val_loss did not improve from 0.23938\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2004 - acc: 0.9386 - val_loss: 0.2437 - val_acc: 0.9250\n",
            "Epoch 82/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2045 - acc: 0.9412\n",
            "Epoch 00082: val_loss improved from 0.23938 to 0.23334, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.2036 - acc: 0.9428 - val_loss: 0.2333 - val_acc: 0.9275\n",
            "Epoch 83/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2084 - acc: 0.9362\n",
            "Epoch 00083: val_loss did not improve from 0.23334\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.2079 - acc: 0.9369 - val_loss: 0.2517 - val_acc: 0.9225\n",
            "Epoch 84/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2036 - acc: 0.9412\n",
            "Epoch 00084: val_loss did not improve from 0.23334\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2044 - acc: 0.9403 - val_loss: 0.2525 - val_acc: 0.9192\n",
            "Epoch 85/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2024 - acc: 0.9376\n",
            "Epoch 00085: val_loss did not improve from 0.23334\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2007 - acc: 0.9389 - val_loss: 0.2426 - val_acc: 0.9267\n",
            "Epoch 86/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2058 - acc: 0.9379\n",
            "Epoch 00086: val_loss did not improve from 0.23334\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2046 - acc: 0.9381 - val_loss: 0.2626 - val_acc: 0.9167\n",
            "Epoch 87/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1990 - acc: 0.9409\n",
            "Epoch 00087: val_loss did not improve from 0.23334\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1968 - acc: 0.9419 - val_loss: 0.2378 - val_acc: 0.9275\n",
            "Epoch 88/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1984 - acc: 0.9385\n",
            "Epoch 00088: val_loss improved from 0.23334 to 0.22595, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.2001 - acc: 0.9372 - val_loss: 0.2260 - val_acc: 0.9292\n",
            "Epoch 89/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1936 - acc: 0.9468\n",
            "Epoch 00089: val_loss improved from 0.22595 to 0.19030, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.1927 - acc: 0.9475 - val_loss: 0.1903 - val_acc: 0.9500\n",
            "Epoch 90/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9366\n",
            "Epoch 00090: val_loss did not improve from 0.19030\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1968 - acc: 0.9364 - val_loss: 0.2107 - val_acc: 0.9417\n",
            "Epoch 91/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2013 - acc: 0.9409\n",
            "Epoch 00091: val_loss did not improve from 0.19030\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2010 - acc: 0.9414 - val_loss: 0.2172 - val_acc: 0.9325\n",
            "Epoch 92/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1840 - acc: 0.9452\n",
            "Epoch 00092: val_loss did not improve from 0.19030\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1857 - acc: 0.9444 - val_loss: 0.2166 - val_acc: 0.9333\n",
            "Epoch 93/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1733 - acc: 0.9546\n",
            "Epoch 00093: val_loss did not improve from 0.19030\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1734 - acc: 0.9547 - val_loss: 0.2228 - val_acc: 0.9325\n",
            "Epoch 94/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1770 - acc: 0.9482\n",
            "Epoch 00094: val_loss did not improve from 0.19030\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1785 - acc: 0.9481 - val_loss: 0.1956 - val_acc: 0.9433\n",
            "Epoch 95/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1813 - acc: 0.9485\n",
            "Epoch 00095: val_loss did not improve from 0.19030\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1786 - acc: 0.9497 - val_loss: 0.1939 - val_acc: 0.9467\n",
            "Epoch 96/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1701 - acc: 0.9474\n",
            "Epoch 00096: val_loss did not improve from 0.19030\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1694 - acc: 0.9481 - val_loss: 0.2050 - val_acc: 0.9383\n",
            "Epoch 97/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.9586\n",
            "Epoch 00097: val_loss did not improve from 0.19030\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1585 - acc: 0.9578 - val_loss: 0.2344 - val_acc: 0.9250\n",
            "Epoch 98/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1494 - acc: 0.9591\n",
            "Epoch 00098: val_loss did not improve from 0.19030\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1522 - acc: 0.9575 - val_loss: 0.2072 - val_acc: 0.9358\n",
            "Epoch 99/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1590 - acc: 0.9528\n",
            "Epoch 00099: val_loss improved from 0.19030 to 0.18111, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.1623 - acc: 0.9519 - val_loss: 0.1811 - val_acc: 0.9517\n",
            "Epoch 100/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9514\n",
            "Epoch 00100: val_loss did not improve from 0.18111\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1636 - acc: 0.9517 - val_loss: 0.2149 - val_acc: 0.9392\n",
            "Epoch 101/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9586\n",
            "Epoch 00101: val_loss did not improve from 0.18111\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1536 - acc: 0.9581 - val_loss: 0.2069 - val_acc: 0.9408\n",
            "Epoch 102/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1597 - acc: 0.9535\n",
            "Epoch 00102: val_loss did not improve from 0.18111\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1613 - acc: 0.9531 - val_loss: 0.1823 - val_acc: 0.9475\n",
            "Epoch 103/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1702 - acc: 0.9464\n",
            "Epoch 00103: val_loss did not improve from 0.18111\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1699 - acc: 0.9461 - val_loss: 0.2234 - val_acc: 0.9317\n",
            "Epoch 104/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1650 - acc: 0.9526\n",
            "Epoch 00104: val_loss did not improve from 0.18111\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1633 - acc: 0.9539 - val_loss: 0.2044 - val_acc: 0.9408\n",
            "Epoch 105/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1531 - acc: 0.9565\n",
            "Epoch 00105: val_loss improved from 0.18111 to 0.16188, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.1540 - acc: 0.9556 - val_loss: 0.1619 - val_acc: 0.9550\n",
            "Epoch 106/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1616 - acc: 0.9494\n",
            "Epoch 00106: val_loss did not improve from 0.16188\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1628 - acc: 0.9494 - val_loss: 0.2200 - val_acc: 0.9325\n",
            "Epoch 107/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9571\n",
            "Epoch 00107: val_loss did not improve from 0.16188\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1527 - acc: 0.9569 - val_loss: 0.1797 - val_acc: 0.9492\n",
            "Epoch 108/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1466 - acc: 0.9589\n",
            "Epoch 00108: val_loss did not improve from 0.16188\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1471 - acc: 0.9581 - val_loss: 0.1941 - val_acc: 0.9308\n",
            "Epoch 109/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1439 - acc: 0.9553\n",
            "Epoch 00109: val_loss did not improve from 0.16188\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1451 - acc: 0.9550 - val_loss: 0.2044 - val_acc: 0.9317\n",
            "Epoch 110/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1544 - acc: 0.9521\n",
            "Epoch 00110: val_loss improved from 0.16188 to 0.15645, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.1527 - acc: 0.9536 - val_loss: 0.1564 - val_acc: 0.9600\n",
            "Epoch 111/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9551\n",
            "Epoch 00111: val_loss did not improve from 0.15645\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1486 - acc: 0.9544 - val_loss: 0.1799 - val_acc: 0.9475\n",
            "Epoch 112/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1574 - acc: 0.9494\n",
            "Epoch 00112: val_loss did not improve from 0.15645\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1566 - acc: 0.9497 - val_loss: 0.1640 - val_acc: 0.9567\n",
            "Epoch 113/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1412 - acc: 0.9582\n",
            "Epoch 00113: val_loss did not improve from 0.15645\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1421 - acc: 0.9575 - val_loss: 0.2107 - val_acc: 0.9325\n",
            "Epoch 114/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1477 - acc: 0.9600\n",
            "Epoch 00114: val_loss improved from 0.15645 to 0.14730, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.1474 - acc: 0.9597 - val_loss: 0.1473 - val_acc: 0.9658\n",
            "Epoch 115/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9637\n",
            "Epoch 00115: val_loss improved from 0.14730 to 0.13356, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.1347 - acc: 0.9644 - val_loss: 0.1336 - val_acc: 0.9658\n",
            "Epoch 116/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1261 - acc: 0.9653\n",
            "Epoch 00116: val_loss did not improve from 0.13356\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1267 - acc: 0.9653 - val_loss: 0.1515 - val_acc: 0.9608\n",
            "Epoch 117/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1310 - acc: 0.9624\n",
            "Epoch 00117: val_loss did not improve from 0.13356\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1322 - acc: 0.9619 - val_loss: 0.1438 - val_acc: 0.9575\n",
            "Epoch 118/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1286 - acc: 0.9636\n",
            "Epoch 00118: val_loss did not improve from 0.13356\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1328 - acc: 0.9619 - val_loss: 0.2181 - val_acc: 0.9300\n",
            "Epoch 119/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9549\n",
            "Epoch 00119: val_loss did not improve from 0.13356\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1498 - acc: 0.9544 - val_loss: 0.1902 - val_acc: 0.9383\n",
            "Epoch 120/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1325 - acc: 0.9625\n",
            "Epoch 00120: val_loss did not improve from 0.13356\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1340 - acc: 0.9611 - val_loss: 0.1431 - val_acc: 0.9633\n",
            "Epoch 121/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1332 - acc: 0.9626\n",
            "Epoch 00121: val_loss did not improve from 0.13356\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1349 - acc: 0.9625 - val_loss: 0.1630 - val_acc: 0.9575\n",
            "Epoch 122/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9583\n",
            "Epoch 00122: val_loss did not improve from 0.13356\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1437 - acc: 0.9575 - val_loss: 0.1403 - val_acc: 0.9625\n",
            "Epoch 123/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9529\n",
            "Epoch 00123: val_loss did not improve from 0.13356\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1507 - acc: 0.9528 - val_loss: 0.1744 - val_acc: 0.9508\n",
            "Epoch 124/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1344 - acc: 0.9609\n",
            "Epoch 00124: val_loss did not improve from 0.13356\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1327 - acc: 0.9614 - val_loss: 0.1549 - val_acc: 0.9558\n",
            "Epoch 125/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1325 - acc: 0.9609\n",
            "Epoch 00125: val_loss did not improve from 0.13356\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1328 - acc: 0.9611 - val_loss: 0.1641 - val_acc: 0.9542\n",
            "Epoch 126/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9643\n",
            "Epoch 00126: val_loss did not improve from 0.13356\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1276 - acc: 0.9647 - val_loss: 0.1721 - val_acc: 0.9450\n",
            "Epoch 127/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1316 - acc: 0.9636\n",
            "Epoch 00127: val_loss did not improve from 0.13356\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1307 - acc: 0.9647 - val_loss: 0.1550 - val_acc: 0.9550\n",
            "Epoch 128/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1288 - acc: 0.9597\n",
            "Epoch 00128: val_loss did not improve from 0.13356\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.1273 - acc: 0.9603 - val_loss: 0.1429 - val_acc: 0.9617\n",
            "Epoch 129/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1362 - acc: 0.9541\n",
            "Epoch 00129: val_loss did not improve from 0.13356\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1358 - acc: 0.9556 - val_loss: 0.1448 - val_acc: 0.9600\n",
            "Epoch 130/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1393 - acc: 0.9536\n",
            "Epoch 00130: val_loss improved from 0.13356 to 0.12662, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.1368 - acc: 0.9556 - val_loss: 0.1266 - val_acc: 0.9667\n",
            "Epoch 131/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1281 - acc: 0.9624\n",
            "Epoch 00131: val_loss did not improve from 0.12662\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1276 - acc: 0.9628 - val_loss: 0.1431 - val_acc: 0.9583\n",
            "Epoch 132/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1163 - acc: 0.9694\n",
            "Epoch 00132: val_loss did not improve from 0.12662\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1149 - acc: 0.9700 - val_loss: 0.1339 - val_acc: 0.9617\n",
            "Epoch 133/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9700\n",
            "Epoch 00133: val_loss improved from 0.12662 to 0.12127, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1165 - acc: 0.9703 - val_loss: 0.1213 - val_acc: 0.9650\n",
            "Epoch 134/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1099 - acc: 0.9691\n",
            "Epoch 00134: val_loss did not improve from 0.12127\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1116 - acc: 0.9683 - val_loss: 0.1327 - val_acc: 0.9625\n",
            "Epoch 135/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1352 - acc: 0.9591\n",
            "Epoch 00135: val_loss did not improve from 0.12127\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1323 - acc: 0.9608 - val_loss: 0.1382 - val_acc: 0.9617\n",
            "Epoch 136/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1191 - acc: 0.9664\n",
            "Epoch 00136: val_loss improved from 0.12127 to 0.11909, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.1227 - acc: 0.9647 - val_loss: 0.1191 - val_acc: 0.9717\n",
            "Epoch 137/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1313 - acc: 0.9603\n",
            "Epoch 00137: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1306 - acc: 0.9606 - val_loss: 0.1431 - val_acc: 0.9542\n",
            "Epoch 138/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0991 - acc: 0.9759\n",
            "Epoch 00138: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1006 - acc: 0.9756 - val_loss: 0.1252 - val_acc: 0.9608\n",
            "Epoch 139/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1055 - acc: 0.9709\n",
            "Epoch 00139: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1073 - acc: 0.9711 - val_loss: 0.1217 - val_acc: 0.9658\n",
            "Epoch 140/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1211 - acc: 0.9626\n",
            "Epoch 00140: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1224 - acc: 0.9614 - val_loss: 0.1684 - val_acc: 0.9525\n",
            "Epoch 141/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1317 - acc: 0.9609\n",
            "Epoch 00141: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1318 - acc: 0.9614 - val_loss: 0.1409 - val_acc: 0.9583\n",
            "Epoch 142/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1267 - acc: 0.9618\n",
            "Epoch 00142: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1257 - acc: 0.9617 - val_loss: 0.1659 - val_acc: 0.9525\n",
            "Epoch 143/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1501 - acc: 0.9541\n",
            "Epoch 00143: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1546 - acc: 0.9517 - val_loss: 0.1321 - val_acc: 0.9592\n",
            "Epoch 144/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1307 - acc: 0.9630\n",
            "Epoch 00144: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1298 - acc: 0.9631 - val_loss: 0.1233 - val_acc: 0.9675\n",
            "Epoch 145/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9663\n",
            "Epoch 00145: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1205 - acc: 0.9653 - val_loss: 0.1618 - val_acc: 0.9550\n",
            "Epoch 146/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9671\n",
            "Epoch 00146: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1158 - acc: 0.9664 - val_loss: 0.1519 - val_acc: 0.9617\n",
            "Epoch 147/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1234 - acc: 0.9636\n",
            "Epoch 00147: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1237 - acc: 0.9633 - val_loss: 0.1643 - val_acc: 0.9550\n",
            "Epoch 148/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1298 - acc: 0.9603\n",
            "Epoch 00148: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1277 - acc: 0.9608 - val_loss: 0.1192 - val_acc: 0.9717\n",
            "Epoch 149/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1178 - acc: 0.9672\n",
            "Epoch 00149: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1188 - acc: 0.9667 - val_loss: 0.1449 - val_acc: 0.9675\n",
            "Epoch 150/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1134 - acc: 0.9658\n",
            "Epoch 00150: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1147 - acc: 0.9656 - val_loss: 0.1256 - val_acc: 0.9600\n",
            "Epoch 151/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1199 - acc: 0.9664\n",
            "Epoch 00151: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1210 - acc: 0.9667 - val_loss: 0.1200 - val_acc: 0.9692\n",
            "Epoch 152/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1379 - acc: 0.9542\n",
            "Epoch 00152: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1361 - acc: 0.9561 - val_loss: 0.1262 - val_acc: 0.9642\n",
            "Epoch 153/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1350 - acc: 0.9585\n",
            "Epoch 00153: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1358 - acc: 0.9583 - val_loss: 0.1490 - val_acc: 0.9525\n",
            "Epoch 154/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1213 - acc: 0.9658\n",
            "Epoch 00154: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1211 - acc: 0.9650 - val_loss: 0.1380 - val_acc: 0.9583\n",
            "Epoch 155/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9614\n",
            "Epoch 00155: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1243 - acc: 0.9608 - val_loss: 0.1304 - val_acc: 0.9667\n",
            "Epoch 156/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9694\n",
            "Epoch 00156: val_loss did not improve from 0.11909\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1141 - acc: 0.9692 - val_loss: 0.1227 - val_acc: 0.9633\n",
            "Epoch 157/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1101 - acc: 0.9679\n",
            "Epoch 00157: val_loss improved from 0.11909 to 0.11384, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.1091 - acc: 0.9689 - val_loss: 0.1138 - val_acc: 0.9700\n",
            "Epoch 158/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9717\n",
            "Epoch 00158: val_loss did not improve from 0.11384\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1074 - acc: 0.9717 - val_loss: 0.1181 - val_acc: 0.9650\n",
            "Epoch 159/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0990 - acc: 0.9730\n",
            "Epoch 00159: val_loss did not improve from 0.11384\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1007 - acc: 0.9722 - val_loss: 0.1289 - val_acc: 0.9617\n",
            "Epoch 160/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9711\n",
            "Epoch 00160: val_loss improved from 0.11384 to 0.11035, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.1026 - acc: 0.9714 - val_loss: 0.1103 - val_acc: 0.9767\n",
            "Epoch 161/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1099 - acc: 0.9663\n",
            "Epoch 00161: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1090 - acc: 0.9675 - val_loss: 0.1419 - val_acc: 0.9558\n",
            "Epoch 162/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0940 - acc: 0.9715\n",
            "Epoch 00162: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0961 - acc: 0.9711 - val_loss: 0.1372 - val_acc: 0.9617\n",
            "Epoch 163/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9723\n",
            "Epoch 00163: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1020 - acc: 0.9722 - val_loss: 0.1134 - val_acc: 0.9708\n",
            "Epoch 164/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.9740\n",
            "Epoch 00164: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1008 - acc: 0.9739 - val_loss: 0.1401 - val_acc: 0.9592\n",
            "Epoch 165/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1072 - acc: 0.9703\n",
            "Epoch 00165: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1060 - acc: 0.9703 - val_loss: 0.1117 - val_acc: 0.9717\n",
            "Epoch 166/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1068 - acc: 0.9675\n",
            "Epoch 00166: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1066 - acc: 0.9675 - val_loss: 0.1380 - val_acc: 0.9575\n",
            "Epoch 167/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1102 - acc: 0.9661\n",
            "Epoch 00167: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1096 - acc: 0.9669 - val_loss: 0.1256 - val_acc: 0.9617\n",
            "Epoch 168/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9760\n",
            "Epoch 00168: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0928 - acc: 0.9758 - val_loss: 0.1378 - val_acc: 0.9575\n",
            "Epoch 169/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0967 - acc: 0.9697\n",
            "Epoch 00169: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0964 - acc: 0.9700 - val_loss: 0.1484 - val_acc: 0.9575\n",
            "Epoch 170/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9703\n",
            "Epoch 00170: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1043 - acc: 0.9706 - val_loss: 0.1175 - val_acc: 0.9675\n",
            "Epoch 171/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9666\n",
            "Epoch 00171: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1104 - acc: 0.9669 - val_loss: 0.1120 - val_acc: 0.9700\n",
            "Epoch 172/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0959 - acc: 0.9729\n",
            "Epoch 00172: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0973 - acc: 0.9728 - val_loss: 0.1219 - val_acc: 0.9692\n",
            "Epoch 173/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1136 - acc: 0.9667\n",
            "Epoch 00173: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1133 - acc: 0.9669 - val_loss: 0.1169 - val_acc: 0.9717\n",
            "Epoch 174/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1157 - acc: 0.9656\n",
            "Epoch 00174: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1155 - acc: 0.9658 - val_loss: 0.1330 - val_acc: 0.9617\n",
            "Epoch 175/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0976 - acc: 0.9712\n",
            "Epoch 00175: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0989 - acc: 0.9700 - val_loss: 0.1274 - val_acc: 0.9683\n",
            "Epoch 176/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1090 - acc: 0.9691\n",
            "Epoch 00176: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1063 - acc: 0.9700 - val_loss: 0.1129 - val_acc: 0.9692\n",
            "Epoch 177/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9711\n",
            "Epoch 00177: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0994 - acc: 0.9708 - val_loss: 0.1179 - val_acc: 0.9633\n",
            "Epoch 178/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1120 - acc: 0.9665\n",
            "Epoch 00178: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1111 - acc: 0.9658 - val_loss: 0.1212 - val_acc: 0.9642\n",
            "Epoch 179/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1156 - acc: 0.9656\n",
            "Epoch 00179: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1163 - acc: 0.9644 - val_loss: 0.1449 - val_acc: 0.9533\n",
            "Epoch 180/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1324 - acc: 0.9579\n",
            "Epoch 00180: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1315 - acc: 0.9583 - val_loss: 0.1435 - val_acc: 0.9558\n",
            "Epoch 181/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1343 - acc: 0.9548\n",
            "Epoch 00181: val_loss did not improve from 0.11035\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1303 - acc: 0.9567 - val_loss: 0.1226 - val_acc: 0.9658\n",
            "Epoch 182/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9671\n",
            "Epoch 00182: val_loss improved from 0.11035 to 0.10658, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.1143 - acc: 0.9672 - val_loss: 0.1066 - val_acc: 0.9708\n",
            "Epoch 183/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9754\n",
            "Epoch 00183: val_loss did not improve from 0.10658\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0937 - acc: 0.9733 - val_loss: 0.1286 - val_acc: 0.9617\n",
            "Epoch 184/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9646\n",
            "Epoch 00184: val_loss did not improve from 0.10658\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1119 - acc: 0.9653 - val_loss: 0.1330 - val_acc: 0.9567\n",
            "Epoch 185/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1014 - acc: 0.9674\n",
            "Epoch 00185: val_loss did not improve from 0.10658\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1031 - acc: 0.9669 - val_loss: 0.1468 - val_acc: 0.9500\n",
            "Epoch 186/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1085 - acc: 0.9682\n",
            "Epoch 00186: val_loss did not improve from 0.10658\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1078 - acc: 0.9689 - val_loss: 0.1368 - val_acc: 0.9575\n",
            "Epoch 187/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1080 - acc: 0.9661\n",
            "Epoch 00187: val_loss did not improve from 0.10658\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1079 - acc: 0.9656 - val_loss: 0.1254 - val_acc: 0.9658\n",
            "Epoch 188/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9631\n",
            "Epoch 00188: val_loss did not improve from 0.10658\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1086 - acc: 0.9636 - val_loss: 0.1295 - val_acc: 0.9625\n",
            "Epoch 189/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1056 - acc: 0.9659\n",
            "Epoch 00189: val_loss did not improve from 0.10658\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1060 - acc: 0.9658 - val_loss: 0.1206 - val_acc: 0.9700\n",
            "Epoch 190/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9711\n",
            "Epoch 00190: val_loss did not improve from 0.10658\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0936 - acc: 0.9719 - val_loss: 0.1239 - val_acc: 0.9633\n",
            "Epoch 191/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9717\n",
            "Epoch 00191: val_loss did not improve from 0.10658\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0943 - acc: 0.9719 - val_loss: 0.1163 - val_acc: 0.9667\n",
            "Epoch 192/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9706\n",
            "Epoch 00192: val_loss did not improve from 0.10658\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0970 - acc: 0.9714 - val_loss: 0.1099 - val_acc: 0.9658\n",
            "Epoch 193/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1024 - acc: 0.9706\n",
            "Epoch 00193: val_loss improved from 0.10658 to 0.10170, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1007 - acc: 0.9711 - val_loss: 0.1017 - val_acc: 0.9742\n",
            "Epoch 194/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9760\n",
            "Epoch 00194: val_loss did not improve from 0.10170\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0908 - acc: 0.9756 - val_loss: 0.1166 - val_acc: 0.9658\n",
            "Epoch 195/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9711\n",
            "Epoch 00195: val_loss did not improve from 0.10170\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1003 - acc: 0.9717 - val_loss: 0.1082 - val_acc: 0.9675\n",
            "Epoch 196/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1028 - acc: 0.9688\n",
            "Epoch 00196: val_loss did not improve from 0.10170\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1030 - acc: 0.9692 - val_loss: 0.1228 - val_acc: 0.9642\n",
            "Epoch 197/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9766\n",
            "Epoch 00197: val_loss did not improve from 0.10170\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0931 - acc: 0.9761 - val_loss: 0.1078 - val_acc: 0.9742\n",
            "Epoch 198/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9729\n",
            "Epoch 00198: val_loss did not improve from 0.10170\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0934 - acc: 0.9733 - val_loss: 0.1069 - val_acc: 0.9683\n",
            "Epoch 199/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9754\n",
            "Epoch 00199: val_loss did not improve from 0.10170\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0869 - acc: 0.9753 - val_loss: 0.1057 - val_acc: 0.9717\n",
            "Epoch 200/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9626\n",
            "Epoch 00200: val_loss did not improve from 0.10170\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1180 - acc: 0.9625 - val_loss: 0.1586 - val_acc: 0.9442\n",
            "Epoch 201/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0944 - acc: 0.9738\n",
            "Epoch 00201: val_loss improved from 0.10170 to 0.09212, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.0930 - acc: 0.9744 - val_loss: 0.0921 - val_acc: 0.9758\n",
            "Epoch 202/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1106 - acc: 0.9658\n",
            "Epoch 00202: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1121 - acc: 0.9658 - val_loss: 0.1309 - val_acc: 0.9592\n",
            "Epoch 203/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1074 - acc: 0.9674\n",
            "Epoch 00203: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1072 - acc: 0.9678 - val_loss: 0.1119 - val_acc: 0.9717\n",
            "Epoch 204/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9726\n",
            "Epoch 00204: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1011 - acc: 0.9733 - val_loss: 0.1104 - val_acc: 0.9650\n",
            "Epoch 205/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0931 - acc: 0.9755\n",
            "Epoch 00205: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0941 - acc: 0.9747 - val_loss: 0.0946 - val_acc: 0.9750\n",
            "Epoch 206/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0895 - acc: 0.9726\n",
            "Epoch 00206: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0887 - acc: 0.9731 - val_loss: 0.1044 - val_acc: 0.9708\n",
            "Epoch 207/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9751\n",
            "Epoch 00207: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0929 - acc: 0.9736 - val_loss: 0.1176 - val_acc: 0.9700\n",
            "Epoch 208/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9694\n",
            "Epoch 00208: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0951 - acc: 0.9694 - val_loss: 0.1037 - val_acc: 0.9675\n",
            "Epoch 209/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9717\n",
            "Epoch 00209: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0976 - acc: 0.9711 - val_loss: 0.1089 - val_acc: 0.9692\n",
            "Epoch 210/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0888 - acc: 0.9728\n",
            "Epoch 00210: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0938 - acc: 0.9708 - val_loss: 0.1060 - val_acc: 0.9717\n",
            "Epoch 211/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0959 - acc: 0.9721\n",
            "Epoch 00211: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0982 - acc: 0.9717 - val_loss: 0.1164 - val_acc: 0.9642\n",
            "Epoch 212/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9703\n",
            "Epoch 00212: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0989 - acc: 0.9692 - val_loss: 0.1250 - val_acc: 0.9658\n",
            "Epoch 213/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0984 - acc: 0.9724\n",
            "Epoch 00213: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0974 - acc: 0.9722 - val_loss: 0.1117 - val_acc: 0.9692\n",
            "Epoch 214/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9671\n",
            "Epoch 00214: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1138 - acc: 0.9678 - val_loss: 0.1085 - val_acc: 0.9708\n",
            "Epoch 215/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9654\n",
            "Epoch 00215: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1072 - acc: 0.9656 - val_loss: 0.1040 - val_acc: 0.9767\n",
            "Epoch 216/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1066 - acc: 0.9632\n",
            "Epoch 00216: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1084 - acc: 0.9622 - val_loss: 0.1131 - val_acc: 0.9675\n",
            "Epoch 217/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1041 - acc: 0.9665\n",
            "Epoch 00217: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1032 - acc: 0.9678 - val_loss: 0.1027 - val_acc: 0.9733\n",
            "Epoch 218/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0958 - acc: 0.9703\n",
            "Epoch 00218: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0975 - acc: 0.9686 - val_loss: 0.1084 - val_acc: 0.9642\n",
            "Epoch 219/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0959 - acc: 0.9724\n",
            "Epoch 00219: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0946 - acc: 0.9731 - val_loss: 0.1023 - val_acc: 0.9733\n",
            "Epoch 220/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0884 - acc: 0.9742\n",
            "Epoch 00220: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0884 - acc: 0.9739 - val_loss: 0.1195 - val_acc: 0.9667\n",
            "Epoch 221/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1078 - acc: 0.9674\n",
            "Epoch 00221: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1083 - acc: 0.9672 - val_loss: 0.1177 - val_acc: 0.9692\n",
            "Epoch 222/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0895 - acc: 0.9748\n",
            "Epoch 00222: val_loss did not improve from 0.09212\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0891 - acc: 0.9747 - val_loss: 0.1089 - val_acc: 0.9683\n",
            "Epoch 223/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0822 - acc: 0.9779\n",
            "Epoch 00223: val_loss improved from 0.09212 to 0.09037, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.0829 - acc: 0.9778 - val_loss: 0.0904 - val_acc: 0.9800\n",
            "Epoch 224/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0868 - acc: 0.9750\n",
            "Epoch 00224: val_loss did not improve from 0.09037\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0871 - acc: 0.9747 - val_loss: 0.1099 - val_acc: 0.9725\n",
            "Epoch 225/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0873 - acc: 0.9731\n",
            "Epoch 00225: val_loss did not improve from 0.09037\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0888 - acc: 0.9719 - val_loss: 0.1000 - val_acc: 0.9758\n",
            "Epoch 226/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0934 - acc: 0.9735\n",
            "Epoch 00226: val_loss did not improve from 0.09037\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0928 - acc: 0.9742 - val_loss: 0.0988 - val_acc: 0.9733\n",
            "Epoch 227/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9706\n",
            "Epoch 00227: val_loss did not improve from 0.09037\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0962 - acc: 0.9706 - val_loss: 0.1367 - val_acc: 0.9533\n",
            "Epoch 228/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1099 - acc: 0.9645\n",
            "Epoch 00228: val_loss did not improve from 0.09037\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1095 - acc: 0.9647 - val_loss: 0.0944 - val_acc: 0.9733\n",
            "Epoch 229/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9703\n",
            "Epoch 00229: val_loss improved from 0.09037 to 0.08370, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.0943 - acc: 0.9700 - val_loss: 0.0837 - val_acc: 0.9800\n",
            "Epoch 230/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0932 - acc: 0.9716\n",
            "Epoch 00230: val_loss did not improve from 0.08370\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0952 - acc: 0.9703 - val_loss: 0.1028 - val_acc: 0.9775\n",
            "Epoch 231/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0769 - acc: 0.9770\n",
            "Epoch 00231: val_loss did not improve from 0.08370\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0797 - acc: 0.9750 - val_loss: 0.0992 - val_acc: 0.9792\n",
            "Epoch 232/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0958 - acc: 0.9719\n",
            "Epoch 00232: val_loss did not improve from 0.08370\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0954 - acc: 0.9728 - val_loss: 0.1075 - val_acc: 0.9758\n",
            "Epoch 233/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0878 - acc: 0.9748\n",
            "Epoch 00233: val_loss improved from 0.08370 to 0.08059, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.0876 - acc: 0.9750 - val_loss: 0.0806 - val_acc: 0.9800\n",
            "Epoch 234/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1004 - acc: 0.9703\n",
            "Epoch 00234: val_loss did not improve from 0.08059\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0991 - acc: 0.9714 - val_loss: 0.0948 - val_acc: 0.9750\n",
            "Epoch 235/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9754\n",
            "Epoch 00235: val_loss did not improve from 0.08059\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0818 - acc: 0.9761 - val_loss: 0.0918 - val_acc: 0.9750\n",
            "Epoch 236/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0939 - acc: 0.9706\n",
            "Epoch 00236: val_loss did not improve from 0.08059\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0948 - acc: 0.9703 - val_loss: 0.0927 - val_acc: 0.9725\n",
            "Epoch 237/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0795 - acc: 0.9769\n",
            "Epoch 00237: val_loss did not improve from 0.08059\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0796 - acc: 0.9775 - val_loss: 0.0930 - val_acc: 0.9783\n",
            "Epoch 238/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0825 - acc: 0.9768\n",
            "Epoch 00238: val_loss did not improve from 0.08059\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0819 - acc: 0.9769 - val_loss: 0.0919 - val_acc: 0.9758\n",
            "Epoch 239/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0877 - acc: 0.9747\n",
            "Epoch 00239: val_loss did not improve from 0.08059\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0901 - acc: 0.9742 - val_loss: 0.1340 - val_acc: 0.9592\n",
            "Epoch 240/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9777\n",
            "Epoch 00240: val_loss did not improve from 0.08059\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0875 - acc: 0.9772 - val_loss: 0.1004 - val_acc: 0.9717\n",
            "Epoch 241/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9737\n",
            "Epoch 00241: val_loss did not improve from 0.08059\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0905 - acc: 0.9739 - val_loss: 0.0993 - val_acc: 0.9733\n",
            "Epoch 242/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9726\n",
            "Epoch 00242: val_loss did not improve from 0.08059\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0873 - acc: 0.9731 - val_loss: 0.1092 - val_acc: 0.9675\n",
            "Epoch 243/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9743\n",
            "Epoch 00243: val_loss improved from 0.08059 to 0.07689, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.0912 - acc: 0.9742 - val_loss: 0.0769 - val_acc: 0.9792\n",
            "Epoch 244/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9749\n",
            "Epoch 00244: val_loss did not improve from 0.07689\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0915 - acc: 0.9750 - val_loss: 0.0890 - val_acc: 0.9783\n",
            "Epoch 245/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9680\n",
            "Epoch 00245: val_loss did not improve from 0.07689\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1020 - acc: 0.9681 - val_loss: 0.1214 - val_acc: 0.9650\n",
            "Epoch 246/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0889 - acc: 0.9712\n",
            "Epoch 00246: val_loss did not improve from 0.07689\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0905 - acc: 0.9703 - val_loss: 0.0812 - val_acc: 0.9767\n",
            "Epoch 247/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0796 - acc: 0.9759\n",
            "Epoch 00247: val_loss did not improve from 0.07689\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0788 - acc: 0.9761 - val_loss: 0.0911 - val_acc: 0.9767\n",
            "Epoch 248/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0868 - acc: 0.9776\n",
            "Epoch 00248: val_loss did not improve from 0.07689\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0867 - acc: 0.9775 - val_loss: 0.0867 - val_acc: 0.9817\n",
            "Epoch 249/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9740\n",
            "Epoch 00249: val_loss did not improve from 0.07689\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0842 - acc: 0.9744 - val_loss: 0.0947 - val_acc: 0.9742\n",
            "Epoch 250/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9771\n",
            "Epoch 00250: val_loss did not improve from 0.07689\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0804 - acc: 0.9775 - val_loss: 0.0911 - val_acc: 0.9733\n",
            "Epoch 251/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0864 - acc: 0.9739\n",
            "Epoch 00251: val_loss did not improve from 0.07689\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0898 - acc: 0.9733 - val_loss: 0.0990 - val_acc: 0.9733\n",
            "Epoch 252/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0935 - acc: 0.9733\n",
            "Epoch 00252: val_loss did not improve from 0.07689\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0937 - acc: 0.9722 - val_loss: 0.0913 - val_acc: 0.9775\n",
            "Epoch 253/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0860 - acc: 0.9752\n",
            "Epoch 00253: val_loss did not improve from 0.07689\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0858 - acc: 0.9747 - val_loss: 0.0924 - val_acc: 0.9733\n",
            "Epoch 254/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0874 - acc: 0.9718\n",
            "Epoch 00254: val_loss did not improve from 0.07689\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0867 - acc: 0.9722 - val_loss: 0.1053 - val_acc: 0.9708\n",
            "Epoch 255/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0846 - acc: 0.9753\n",
            "Epoch 00255: val_loss did not improve from 0.07689\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0885 - acc: 0.9736 - val_loss: 0.0906 - val_acc: 0.9792\n",
            "Epoch 256/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0789 - acc: 0.9782\n",
            "Epoch 00256: val_loss did not improve from 0.07689\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0785 - acc: 0.9789 - val_loss: 0.0877 - val_acc: 0.9783\n",
            "Epoch 257/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1161 - acc: 0.9626\n",
            "Epoch 00257: val_loss did not improve from 0.07689\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1159 - acc: 0.9633 - val_loss: 0.0905 - val_acc: 0.9750\n",
            "Epoch 258/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1020 - acc: 0.9691\n",
            "Epoch 00258: val_loss did not improve from 0.07689\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1051 - acc: 0.9678 - val_loss: 0.0803 - val_acc: 0.9792\n",
            "Epoch 259/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0835 - acc: 0.9770\n",
            "Epoch 00259: val_loss improved from 0.07689 to 0.06824, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.0831 - acc: 0.9775 - val_loss: 0.0682 - val_acc: 0.9825\n",
            "Epoch 260/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1000 - acc: 0.9682\n",
            "Epoch 00260: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0986 - acc: 0.9689 - val_loss: 0.0941 - val_acc: 0.9733\n",
            "Epoch 261/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9760\n",
            "Epoch 00261: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0886 - acc: 0.9758 - val_loss: 0.0843 - val_acc: 0.9783\n",
            "Epoch 262/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9760\n",
            "Epoch 00262: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0790 - acc: 0.9764 - val_loss: 0.0874 - val_acc: 0.9750\n",
            "Epoch 263/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0929 - acc: 0.9730\n",
            "Epoch 00263: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0930 - acc: 0.9725 - val_loss: 0.0937 - val_acc: 0.9742\n",
            "Epoch 264/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0959 - acc: 0.9691\n",
            "Epoch 00264: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0952 - acc: 0.9700 - val_loss: 0.1628 - val_acc: 0.9475\n",
            "Epoch 265/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9697\n",
            "Epoch 00265: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1007 - acc: 0.9700 - val_loss: 0.1047 - val_acc: 0.9725\n",
            "Epoch 266/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0900 - acc: 0.9721\n",
            "Epoch 00266: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0899 - acc: 0.9717 - val_loss: 0.0869 - val_acc: 0.9758\n",
            "Epoch 267/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9734\n",
            "Epoch 00267: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0857 - acc: 0.9731 - val_loss: 0.0810 - val_acc: 0.9800\n",
            "Epoch 268/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0885 - acc: 0.9718\n",
            "Epoch 00268: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0888 - acc: 0.9722 - val_loss: 0.0931 - val_acc: 0.9758\n",
            "Epoch 269/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0738 - acc: 0.9782\n",
            "Epoch 00269: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0742 - acc: 0.9781 - val_loss: 0.0820 - val_acc: 0.9775\n",
            "Epoch 270/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0836 - acc: 0.9755\n",
            "Epoch 00270: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0824 - acc: 0.9756 - val_loss: 0.0891 - val_acc: 0.9775\n",
            "Epoch 271/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0911 - acc: 0.9706\n",
            "Epoch 00271: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0956 - acc: 0.9694 - val_loss: 0.1129 - val_acc: 0.9683\n",
            "Epoch 272/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0932 - acc: 0.9732\n",
            "Epoch 00272: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0925 - acc: 0.9742 - val_loss: 0.0803 - val_acc: 0.9808\n",
            "Epoch 273/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9757\n",
            "Epoch 00273: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0796 - acc: 0.9756 - val_loss: 0.0760 - val_acc: 0.9817\n",
            "Epoch 274/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0920 - acc: 0.9718\n",
            "Epoch 00274: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0928 - acc: 0.9722 - val_loss: 0.1055 - val_acc: 0.9733\n",
            "Epoch 275/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0929 - acc: 0.9703\n",
            "Epoch 00275: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0927 - acc: 0.9700 - val_loss: 0.0917 - val_acc: 0.9717\n",
            "Epoch 276/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0900 - acc: 0.9758\n",
            "Epoch 00276: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0880 - acc: 0.9761 - val_loss: 0.0891 - val_acc: 0.9783\n",
            "Epoch 277/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0813 - acc: 0.9764\n",
            "Epoch 00277: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0835 - acc: 0.9756 - val_loss: 0.1000 - val_acc: 0.9742\n",
            "Epoch 278/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0753 - acc: 0.9809\n",
            "Epoch 00278: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0780 - acc: 0.9789 - val_loss: 0.0986 - val_acc: 0.9733\n",
            "Epoch 279/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0857 - acc: 0.9748\n",
            "Epoch 00279: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0854 - acc: 0.9747 - val_loss: 0.0906 - val_acc: 0.9733\n",
            "Epoch 280/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0957 - acc: 0.9718\n",
            "Epoch 00280: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0980 - acc: 0.9703 - val_loss: 0.0757 - val_acc: 0.9783\n",
            "Epoch 281/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0815 - acc: 0.9773\n",
            "Epoch 00281: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0797 - acc: 0.9778 - val_loss: 0.0783 - val_acc: 0.9817\n",
            "Epoch 282/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0820 - acc: 0.9724\n",
            "Epoch 00282: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0812 - acc: 0.9728 - val_loss: 0.0828 - val_acc: 0.9775\n",
            "Epoch 283/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9749\n",
            "Epoch 00283: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0898 - acc: 0.9747 - val_loss: 0.0810 - val_acc: 0.9767\n",
            "Epoch 284/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9800\n",
            "Epoch 00284: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0748 - acc: 0.9789 - val_loss: 0.0801 - val_acc: 0.9783\n",
            "Epoch 285/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0755 - acc: 0.9782\n",
            "Epoch 00285: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0750 - acc: 0.9783 - val_loss: 0.0893 - val_acc: 0.9758\n",
            "Epoch 286/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9820\n",
            "Epoch 00286: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0642 - acc: 0.9825 - val_loss: 0.0704 - val_acc: 0.9800\n",
            "Epoch 287/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0699 - acc: 0.9809\n",
            "Epoch 00287: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0725 - acc: 0.9803 - val_loss: 0.0740 - val_acc: 0.9825\n",
            "Epoch 288/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0757 - acc: 0.9771\n",
            "Epoch 00288: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0775 - acc: 0.9767 - val_loss: 0.0743 - val_acc: 0.9800\n",
            "Epoch 289/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0774 - acc: 0.9753\n",
            "Epoch 00289: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0779 - acc: 0.9753 - val_loss: 0.1125 - val_acc: 0.9750\n",
            "Epoch 290/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0799 - acc: 0.9752\n",
            "Epoch 00290: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0820 - acc: 0.9750 - val_loss: 0.0791 - val_acc: 0.9758\n",
            "Epoch 291/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0720 - acc: 0.9800\n",
            "Epoch 00291: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0748 - acc: 0.9786 - val_loss: 0.0820 - val_acc: 0.9792\n",
            "Epoch 292/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0952 - acc: 0.9703\n",
            "Epoch 00292: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0955 - acc: 0.9706 - val_loss: 0.0904 - val_acc: 0.9767\n",
            "Epoch 293/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0752 - acc: 0.9771\n",
            "Epoch 00293: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0752 - acc: 0.9772 - val_loss: 0.0777 - val_acc: 0.9775\n",
            "Epoch 294/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9760\n",
            "Epoch 00294: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0847 - acc: 0.9753 - val_loss: 0.0979 - val_acc: 0.9692\n",
            "Epoch 295/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9780\n",
            "Epoch 00295: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0760 - acc: 0.9781 - val_loss: 0.0826 - val_acc: 0.9792\n",
            "Epoch 296/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0700 - acc: 0.9818\n",
            "Epoch 00296: val_loss did not improve from 0.06824\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0697 - acc: 0.9822 - val_loss: 0.0712 - val_acc: 0.9817\n",
            "Epoch 297/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0643 - acc: 0.9850\n",
            "Epoch 00297: val_loss improved from 0.06824 to 0.06035, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.0636 - acc: 0.9850 - val_loss: 0.0604 - val_acc: 0.9883\n",
            "Epoch 298/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0658 - acc: 0.9818\n",
            "Epoch 00298: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0662 - acc: 0.9822 - val_loss: 0.0811 - val_acc: 0.9808\n",
            "Epoch 299/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9771\n",
            "Epoch 00299: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0739 - acc: 0.9769 - val_loss: 0.0663 - val_acc: 0.9817\n",
            "Epoch 300/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9749\n",
            "Epoch 00300: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0792 - acc: 0.9753 - val_loss: 0.0709 - val_acc: 0.9800\n",
            "Epoch 301/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0893 - acc: 0.9694\n",
            "Epoch 00301: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0879 - acc: 0.9700 - val_loss: 0.0817 - val_acc: 0.9783\n",
            "Epoch 302/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0894 - acc: 0.9739\n",
            "Epoch 00302: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0897 - acc: 0.9742 - val_loss: 0.0704 - val_acc: 0.9825\n",
            "Epoch 303/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0824 - acc: 0.9721\n",
            "Epoch 00303: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0833 - acc: 0.9719 - val_loss: 0.0825 - val_acc: 0.9775\n",
            "Epoch 304/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0675 - acc: 0.9809\n",
            "Epoch 00304: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0680 - acc: 0.9806 - val_loss: 0.0864 - val_acc: 0.9758\n",
            "Epoch 305/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0623 - acc: 0.9838\n",
            "Epoch 00305: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0623 - acc: 0.9836 - val_loss: 0.0792 - val_acc: 0.9775\n",
            "Epoch 306/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0623 - acc: 0.9818\n",
            "Epoch 00306: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0621 - acc: 0.9817 - val_loss: 0.0913 - val_acc: 0.9750\n",
            "Epoch 307/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0787 - acc: 0.9768\n",
            "Epoch 00307: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0785 - acc: 0.9767 - val_loss: 0.0845 - val_acc: 0.9792\n",
            "Epoch 308/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0777 - acc: 0.9759\n",
            "Epoch 00308: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0775 - acc: 0.9758 - val_loss: 0.0795 - val_acc: 0.9758\n",
            "Epoch 309/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0676 - acc: 0.9815\n",
            "Epoch 00309: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0664 - acc: 0.9822 - val_loss: 0.0673 - val_acc: 0.9833\n",
            "Epoch 310/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9726\n",
            "Epoch 00310: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0853 - acc: 0.9731 - val_loss: 0.0791 - val_acc: 0.9783\n",
            "Epoch 311/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0854 - acc: 0.9726\n",
            "Epoch 00311: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0862 - acc: 0.9725 - val_loss: 0.0833 - val_acc: 0.9792\n",
            "Epoch 312/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0834 - acc: 0.9750\n",
            "Epoch 00312: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0827 - acc: 0.9758 - val_loss: 0.1012 - val_acc: 0.9775\n",
            "Epoch 313/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9754\n",
            "Epoch 00313: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0855 - acc: 0.9756 - val_loss: 0.0720 - val_acc: 0.9833\n",
            "Epoch 314/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0712 - acc: 0.9797\n",
            "Epoch 00314: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0709 - acc: 0.9797 - val_loss: 0.1159 - val_acc: 0.9625\n",
            "Epoch 315/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0784 - acc: 0.9762\n",
            "Epoch 00315: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0794 - acc: 0.9750 - val_loss: 0.0847 - val_acc: 0.9817\n",
            "Epoch 316/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0903 - acc: 0.9726\n",
            "Epoch 00316: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0901 - acc: 0.9728 - val_loss: 0.0980 - val_acc: 0.9725\n",
            "Epoch 317/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0778 - acc: 0.9741\n",
            "Epoch 00317: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0765 - acc: 0.9750 - val_loss: 0.0751 - val_acc: 0.9758\n",
            "Epoch 318/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0868 - acc: 0.9724\n",
            "Epoch 00318: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0857 - acc: 0.9728 - val_loss: 0.0882 - val_acc: 0.9750\n",
            "Epoch 319/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0794 - acc: 0.9770\n",
            "Epoch 00319: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0817 - acc: 0.9761 - val_loss: 0.0846 - val_acc: 0.9767\n",
            "Epoch 320/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0746 - acc: 0.9794\n",
            "Epoch 00320: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0732 - acc: 0.9797 - val_loss: 0.0935 - val_acc: 0.9733\n",
            "Epoch 321/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0778 - acc: 0.9759\n",
            "Epoch 00321: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0790 - acc: 0.9756 - val_loss: 0.0745 - val_acc: 0.9850\n",
            "Epoch 322/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9749\n",
            "Epoch 00322: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0773 - acc: 0.9753 - val_loss: 0.0626 - val_acc: 0.9875\n",
            "Epoch 323/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9789\n",
            "Epoch 00323: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0707 - acc: 0.9789 - val_loss: 0.1133 - val_acc: 0.9683\n",
            "Epoch 324/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9729\n",
            "Epoch 00324: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0852 - acc: 0.9728 - val_loss: 0.0755 - val_acc: 0.9833\n",
            "Epoch 325/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0845 - acc: 0.9741\n",
            "Epoch 00325: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0842 - acc: 0.9744 - val_loss: 0.0879 - val_acc: 0.9767\n",
            "Epoch 326/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0786 - acc: 0.9768\n",
            "Epoch 00326: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0800 - acc: 0.9767 - val_loss: 0.0695 - val_acc: 0.9800\n",
            "Epoch 327/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1038 - acc: 0.9661\n",
            "Epoch 00327: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1028 - acc: 0.9664 - val_loss: 0.0712 - val_acc: 0.9783\n",
            "Epoch 328/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0673 - acc: 0.9826\n",
            "Epoch 00328: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0687 - acc: 0.9819 - val_loss: 0.0771 - val_acc: 0.9800\n",
            "Epoch 329/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0721 - acc: 0.9803\n",
            "Epoch 00329: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0714 - acc: 0.9806 - val_loss: 0.0838 - val_acc: 0.9758\n",
            "Epoch 330/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0860 - acc: 0.9718\n",
            "Epoch 00330: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0861 - acc: 0.9714 - val_loss: 0.0751 - val_acc: 0.9750\n",
            "Epoch 331/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9729\n",
            "Epoch 00331: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0798 - acc: 0.9733 - val_loss: 0.0791 - val_acc: 0.9775\n",
            "Epoch 332/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0735 - acc: 0.9791\n",
            "Epoch 00332: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0760 - acc: 0.9781 - val_loss: 0.0883 - val_acc: 0.9742\n",
            "Epoch 333/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0737 - acc: 0.9787\n",
            "Epoch 00333: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0752 - acc: 0.9775 - val_loss: 0.0665 - val_acc: 0.9833\n",
            "Epoch 334/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0733 - acc: 0.9794\n",
            "Epoch 00334: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0709 - acc: 0.9803 - val_loss: 0.0787 - val_acc: 0.9767\n",
            "Epoch 335/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0664 - acc: 0.9791\n",
            "Epoch 00335: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0655 - acc: 0.9800 - val_loss: 0.0700 - val_acc: 0.9792\n",
            "Epoch 336/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9757\n",
            "Epoch 00336: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0722 - acc: 0.9756 - val_loss: 0.0791 - val_acc: 0.9775\n",
            "Epoch 337/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9766\n",
            "Epoch 00337: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0756 - acc: 0.9761 - val_loss: 0.0973 - val_acc: 0.9758\n",
            "Epoch 338/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0775 - acc: 0.9770\n",
            "Epoch 00338: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0752 - acc: 0.9775 - val_loss: 0.0772 - val_acc: 0.9775\n",
            "Epoch 339/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0723 - acc: 0.9781\n",
            "Epoch 00339: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0717 - acc: 0.9778 - val_loss: 0.0824 - val_acc: 0.9817\n",
            "Epoch 340/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0992 - acc: 0.9644\n",
            "Epoch 00340: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1005 - acc: 0.9642 - val_loss: 0.0944 - val_acc: 0.9725\n",
            "Epoch 341/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0925 - acc: 0.9679\n",
            "Epoch 00341: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0900 - acc: 0.9694 - val_loss: 0.0908 - val_acc: 0.9792\n",
            "Epoch 342/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9703\n",
            "Epoch 00342: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0942 - acc: 0.9697 - val_loss: 0.0784 - val_acc: 0.9825\n",
            "Epoch 343/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0837 - acc: 0.9756\n",
            "Epoch 00343: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0838 - acc: 0.9756 - val_loss: 0.0898 - val_acc: 0.9775\n",
            "Epoch 344/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9717\n",
            "Epoch 00344: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0852 - acc: 0.9722 - val_loss: 0.0693 - val_acc: 0.9825\n",
            "Epoch 345/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0785 - acc: 0.9750\n",
            "Epoch 00345: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0777 - acc: 0.9750 - val_loss: 0.0997 - val_acc: 0.9708\n",
            "Epoch 346/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0745 - acc: 0.9800\n",
            "Epoch 00346: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0732 - acc: 0.9800 - val_loss: 0.0703 - val_acc: 0.9775\n",
            "Epoch 347/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0737 - acc: 0.9791\n",
            "Epoch 00347: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0718 - acc: 0.9797 - val_loss: 0.0741 - val_acc: 0.9808\n",
            "Epoch 348/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0686 - acc: 0.9806\n",
            "Epoch 00348: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0695 - acc: 0.9800 - val_loss: 0.0663 - val_acc: 0.9842\n",
            "Epoch 349/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0593 - acc: 0.9829\n",
            "Epoch 00349: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0589 - acc: 0.9833 - val_loss: 0.0759 - val_acc: 0.9800\n",
            "Epoch 350/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9806\n",
            "Epoch 00350: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0679 - acc: 0.9806 - val_loss: 0.0666 - val_acc: 0.9808\n",
            "Epoch 351/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0837 - acc: 0.9771\n",
            "Epoch 00351: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0814 - acc: 0.9781 - val_loss: 0.0714 - val_acc: 0.9783\n",
            "Epoch 352/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0655 - acc: 0.9800\n",
            "Epoch 00352: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0671 - acc: 0.9797 - val_loss: 0.0931 - val_acc: 0.9750\n",
            "Epoch 353/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9794\n",
            "Epoch 00353: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0742 - acc: 0.9792 - val_loss: 0.0776 - val_acc: 0.9792\n",
            "Epoch 354/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0666 - acc: 0.9832\n",
            "Epoch 00354: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.0662 - acc: 0.9833 - val_loss: 0.0901 - val_acc: 0.9750\n",
            "Epoch 355/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0729 - acc: 0.9806\n",
            "Epoch 00355: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0722 - acc: 0.9808 - val_loss: 0.0750 - val_acc: 0.9842\n",
            "Epoch 356/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0716 - acc: 0.9818\n",
            "Epoch 00356: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0720 - acc: 0.9817 - val_loss: 0.0637 - val_acc: 0.9858\n",
            "Epoch 357/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0675 - acc: 0.9812\n",
            "Epoch 00357: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0694 - acc: 0.9797 - val_loss: 0.0664 - val_acc: 0.9842\n",
            "Epoch 358/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0694 - acc: 0.9784\n",
            "Epoch 00358: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0715 - acc: 0.9781 - val_loss: 0.0749 - val_acc: 0.9808\n",
            "Epoch 359/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0852 - acc: 0.9730\n",
            "Epoch 00359: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0841 - acc: 0.9731 - val_loss: 0.0842 - val_acc: 0.9767\n",
            "Epoch 360/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9843\n",
            "Epoch 00360: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0653 - acc: 0.9828 - val_loss: 0.0720 - val_acc: 0.9808\n",
            "Epoch 361/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0879 - acc: 0.9736\n",
            "Epoch 00361: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0859 - acc: 0.9747 - val_loss: 0.0896 - val_acc: 0.9725\n",
            "Epoch 362/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0787 - acc: 0.9765\n",
            "Epoch 00362: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0785 - acc: 0.9761 - val_loss: 0.0829 - val_acc: 0.9758\n",
            "Epoch 363/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0659 - acc: 0.9809\n",
            "Epoch 00363: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0650 - acc: 0.9808 - val_loss: 0.0690 - val_acc: 0.9817\n",
            "Epoch 364/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0528 - acc: 0.9867\n",
            "Epoch 00364: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0556 - acc: 0.9856 - val_loss: 0.0688 - val_acc: 0.9858\n",
            "Epoch 365/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9797\n",
            "Epoch 00365: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0658 - acc: 0.9803 - val_loss: 0.1007 - val_acc: 0.9692\n",
            "Epoch 366/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0610 - acc: 0.9806\n",
            "Epoch 00366: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0625 - acc: 0.9806 - val_loss: 0.0655 - val_acc: 0.9842\n",
            "Epoch 367/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0823 - acc: 0.9733\n",
            "Epoch 00367: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0805 - acc: 0.9739 - val_loss: 0.0884 - val_acc: 0.9725\n",
            "Epoch 368/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0610 - acc: 0.9853\n",
            "Epoch 00368: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0607 - acc: 0.9856 - val_loss: 0.0632 - val_acc: 0.9842\n",
            "Epoch 369/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9751\n",
            "Epoch 00369: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0784 - acc: 0.9753 - val_loss: 0.0899 - val_acc: 0.9758\n",
            "Epoch 370/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0774 - acc: 0.9752\n",
            "Epoch 00370: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0769 - acc: 0.9750 - val_loss: 0.0739 - val_acc: 0.9767\n",
            "Epoch 371/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0679 - acc: 0.9815\n",
            "Epoch 00371: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0682 - acc: 0.9814 - val_loss: 0.0711 - val_acc: 0.9817\n",
            "Epoch 372/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0845 - acc: 0.9724\n",
            "Epoch 00372: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0843 - acc: 0.9736 - val_loss: 0.0732 - val_acc: 0.9808\n",
            "Epoch 373/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0757 - acc: 0.9776\n",
            "Epoch 00373: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0771 - acc: 0.9767 - val_loss: 0.0819 - val_acc: 0.9758\n",
            "Epoch 374/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0716 - acc: 0.9794\n",
            "Epoch 00374: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0729 - acc: 0.9783 - val_loss: 0.0668 - val_acc: 0.9858\n",
            "Epoch 375/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9849\n",
            "Epoch 00375: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0637 - acc: 0.9844 - val_loss: 0.0839 - val_acc: 0.9792\n",
            "Epoch 376/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0585 - acc: 0.9836\n",
            "Epoch 00376: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0609 - acc: 0.9822 - val_loss: 0.0785 - val_acc: 0.9783\n",
            "Epoch 377/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9780\n",
            "Epoch 00377: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0744 - acc: 0.9772 - val_loss: 0.0850 - val_acc: 0.9767\n",
            "Epoch 378/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0743 - acc: 0.9785\n",
            "Epoch 00378: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0765 - acc: 0.9781 - val_loss: 0.0608 - val_acc: 0.9850\n",
            "Epoch 379/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0723 - acc: 0.9794\n",
            "Epoch 00379: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0723 - acc: 0.9797 - val_loss: 0.1293 - val_acc: 0.9600\n",
            "Epoch 380/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0725 - acc: 0.9782\n",
            "Epoch 00380: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0740 - acc: 0.9775 - val_loss: 0.0701 - val_acc: 0.9833\n",
            "Epoch 381/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0721 - acc: 0.9758\n",
            "Epoch 00381: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0718 - acc: 0.9758 - val_loss: 0.0889 - val_acc: 0.9775\n",
            "Epoch 382/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9803\n",
            "Epoch 00382: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0748 - acc: 0.9800 - val_loss: 0.0723 - val_acc: 0.9842\n",
            "Epoch 383/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0755 - acc: 0.9767\n",
            "Epoch 00383: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0764 - acc: 0.9764 - val_loss: 0.0630 - val_acc: 0.9825\n",
            "Epoch 384/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0800 - acc: 0.9770\n",
            "Epoch 00384: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0786 - acc: 0.9778 - val_loss: 0.0757 - val_acc: 0.9792\n",
            "Epoch 385/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0806 - acc: 0.9731\n",
            "Epoch 00385: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0816 - acc: 0.9728 - val_loss: 0.0751 - val_acc: 0.9808\n",
            "Epoch 386/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0732 - acc: 0.9788\n",
            "Epoch 00386: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0735 - acc: 0.9775 - val_loss: 0.0929 - val_acc: 0.9783\n",
            "Epoch 387/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9783\n",
            "Epoch 00387: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0765 - acc: 0.9781 - val_loss: 0.0834 - val_acc: 0.9792\n",
            "Epoch 388/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0686 - acc: 0.9806\n",
            "Epoch 00388: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0683 - acc: 0.9808 - val_loss: 0.0943 - val_acc: 0.9758\n",
            "Epoch 389/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0707 - acc: 0.9782\n",
            "Epoch 00389: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0709 - acc: 0.9786 - val_loss: 0.0808 - val_acc: 0.9808\n",
            "Epoch 390/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0824 - acc: 0.9735\n",
            "Epoch 00390: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0837 - acc: 0.9725 - val_loss: 0.0633 - val_acc: 0.9850\n",
            "Epoch 391/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9806\n",
            "Epoch 00391: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0693 - acc: 0.9806 - val_loss: 0.0702 - val_acc: 0.9858\n",
            "Epoch 392/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0791 - acc: 0.9755\n",
            "Epoch 00392: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0784 - acc: 0.9761 - val_loss: 0.0772 - val_acc: 0.9808\n",
            "Epoch 393/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0733 - acc: 0.9776\n",
            "Epoch 00393: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0728 - acc: 0.9778 - val_loss: 0.0701 - val_acc: 0.9833\n",
            "Epoch 394/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9777\n",
            "Epoch 00394: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0701 - acc: 0.9775 - val_loss: 0.0982 - val_acc: 0.9708\n",
            "Epoch 395/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0888 - acc: 0.9730\n",
            "Epoch 00395: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0871 - acc: 0.9744 - val_loss: 0.0759 - val_acc: 0.9825\n",
            "Epoch 396/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9786\n",
            "Epoch 00396: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0774 - acc: 0.9786 - val_loss: 0.0832 - val_acc: 0.9783\n",
            "Epoch 397/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0694 - acc: 0.9797\n",
            "Epoch 00397: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0711 - acc: 0.9794 - val_loss: 0.0894 - val_acc: 0.9742\n",
            "Epoch 398/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0739 - acc: 0.9781\n",
            "Epoch 00398: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0724 - acc: 0.9786 - val_loss: 0.0824 - val_acc: 0.9800\n",
            "Epoch 399/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0858 - acc: 0.9715\n",
            "Epoch 00399: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0886 - acc: 0.9708 - val_loss: 0.0827 - val_acc: 0.9792\n",
            "Epoch 400/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0801 - acc: 0.9773\n",
            "Epoch 00400: val_loss did not improve from 0.06035\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0783 - acc: 0.9778 - val_loss: 0.0713 - val_acc: 0.9817\n",
            "1200/1200 [==============================] - 0s 123us/sample - loss: 0.0713 - acc: 0.9817\n",
            "[0.07127948820590974, 0.9816667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1iiZnN3vexb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "cbace593-0bc5-4976-af0a-3c94a798beeb"
      },
      "source": [
        "for i in range(10, 11): # Итерација низ секој испитен примерок\n",
        "  print(f\"====================== Примерок ({i}) ======================\")\n",
        "  print(\"Вчитување тест податоци од испитниот примерок \" + str(i) + \"...\")\n",
        "  \n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_test_data = np.empty(0)\n",
        "  temp_test_events = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_test_set = 'S' + format(j, '02') + '/Test'\n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testData.mat\"\n",
        "    temp = loadmat(full_path)['testData']\n",
        "    if temp_test_data.size != 0:\n",
        "      temp_test_data = np.concatenate((temp_test_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_test_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_test_events.size != 0:\n",
        "        temp_test_events = np.append(temp_test_events, temp)\n",
        "      else:\n",
        "        temp_test_events = np.array(temp)\n",
        "\n",
        "    # Вчитување на бројот на runs \n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/runs_per_block.txt\"\n",
        "    with open(full_path, \"r\") as runs_per_block:\n",
        "      test_runs_per_block[i-1][j-1] = int(runs_per_block.read())\n",
        "\n",
        "    print(\"\\t - Тест податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "  # Зачувај ги тест податоците вчитани од испитниот примерок во низа\n",
        "  test_data.append(temp_test_data)\n",
        "  test_events.append(temp_test_events)\n",
        "  print(\"Тест податоците од испитниот примерок \" + str(i) + \" се вчитани.\\n\")\n",
        "\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_data: \" + str(test_data[i-1].shape)) # test_data to predict\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_events: \" + str(len(test_events[i-1]))) # test_events\n",
        "  for j in range (1,4):\n",
        "    print(\"SBJ\" + str(format(i-1, '02')) + \" / S\" + str(format(j-1, '02')) + \"| Runs per block: \" + str(test_runs_per_block[i-1][j-1])) # runs per block in SJB01, SJ00 \n",
        "\n",
        "  to_predict_data = reshape_data_to_mne_format(test_data[i-1])\n",
        "  predictions = model10.predict(to_predict_data)\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Predictions: \" + str(len(predictions)))\n",
        "  # np.savetxt(\"predictions.csv\", predictions, delimiter=\",\")\n",
        "\n",
        "\n",
        "  # ========= FALI USTE DA SE ISPARSIRA PREDICTIONOT... NE E SREDEN OVOJ KOD DOLE =======\n",
        "\n",
        "  int_pred = np.argmax(predictions, axis=1)\n",
        "  int_ytest = np.argmax(y_test, axis=1)\n",
        "\n",
        "  session_start = 0\n",
        "  start_prediction_index = 0\n",
        "  end_prediction_index = 0\n",
        "  for session in range(0, 3):\n",
        "    print(f\"============== Сесија ({session}) ==============\")\n",
        "    for block in range(0, 50):    \n",
        "      events_per_block = test_runs_per_block[i-1][session]\n",
        "\n",
        "      start_prediction_index = session_start + (block*events_per_block)*8\n",
        "      end_prediction_index = session_start + ((block+1)*events_per_block)*8\n",
        "\n",
        "      block_prediction = int_pred[start_prediction_index:end_prediction_index]\n",
        "      prediction = np.bincount(block_prediction).argmax()\n",
        "      df.iat[session+27,block+2] = prediction+1\n",
        "      # UNCOMMENT ZA PODOBAR PRIKAZ :)\n",
        "      # print(f\"Session {session} | Block: {block} | Prediction: {prediction} | Address: {end_prediction_index}\")\n",
        "\n",
        "      print(str(prediction+1) + \",\", end=\"\")\n",
        "    session_start = end_prediction_index\n",
        "    print(\"\")\n",
        "  print(\"Stigna li do kraj: \" + str(session_start == len(predictions)))\n",
        "  print(f\"====================== Примерок ({i}) ======================\\n\\n\")"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Примерок (10) ======================\n",
            "Вчитување тест податоци од испитниот примерок 10...\n",
            "\t - Тест податоците од сесија 1 се вчитани.\n",
            "\t - Тест податоците од сесија 2 се вчитани.\n",
            "\t - Тест податоците од сесија 3 се вчитани.\n",
            "Тест податоците од испитниот примерок 10 се вчитани.\n",
            "\n",
            "SBJ09| Test_data: (8, 350, 6400)\n",
            "SBJ09| Test_events: 6400\n",
            "SBJ09 / S00| Runs per block: 5\n",
            "SBJ09 / S01| Runs per block: 5\n",
            "SBJ09 / S02| Runs per block: 6\n",
            "SBJ09| Predictions: 6400\n",
            "============== Сесија (0) ==============\n",
            "1,1,8,7,3,1,1,8,1,5,1,1,7,7,6,1,1,6,4,6,1,1,1,5,1,1,5,1,1,5,1,1,1,1,1,5,1,6,2,6,2,5,5,1,1,1,1,5,1,5,\n",
            "============== Сесија (1) ==============\n",
            "1,5,5,6,1,1,1,1,5,6,1,5,6,1,2,5,8,4,6,6,1,1,1,1,1,1,2,6,5,8,6,1,1,8,1,1,5,8,6,2,1,8,5,5,6,6,2,1,2,1,\n",
            "============== Сесија (2) ==============\n",
            "5,8,5,5,5,6,5,2,5,5,5,2,6,6,2,5,5,5,1,5,5,5,1,1,6,2,6,6,2,5,5,5,5,5,5,5,1,6,5,5,5,5,5,5,2,5,5,5,5,5,\n",
            "Stigna li do kraj: True\n",
            "====================== Примерок (10) ======================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKBE9mrhvqpf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b693e440-89d0-4fe4-9e1a-02fe7c662946"
      },
      "source": [
        "df"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUBJECT</th>\n",
              "      <th>SESSION</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>Unnamed: 52</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    SUBJECT  SESSION  1  2  3  4  5  6  7  ... 43 44 45 46 47 48 49 50 Unnamed: 52\n",
              "0         1        1  6  6  3  6  6  3  6  ...  6  4  8  3  4  5  5  7         NaN\n",
              "1         1        2  6  3  2  1  2  1  3  ...  6  2  2  2  3  6  6  2         NaN\n",
              "2         1        3  3  3  3  3  3  3  3  ...  3  3  6  3  6  7  1  6         NaN\n",
              "3         2        1  8  8  8  8  8  8  8  ...  8  4  8  4  8  7  8  8         NaN\n",
              "4         2        2  2  7  6  6  6  6  7  ...  2  6  6  2  6  6  2  2         NaN\n",
              "5         2        3  2  7  2  6  7  4  2  ...  2  6  2  2  7  2  2  2         NaN\n",
              "6         3        1  3  3  4  4  4  3  6  ...  4  6  3  3  4  3  4  4         NaN\n",
              "7         3        2  6  1  7  7  7  7  7  ...  6  7  1  6  6  6  6  6         NaN\n",
              "8         3        3  6  4  8  8  5  7  8  ...  4  2  8  2  2  2  2  8         NaN\n",
              "9         4        1  1  1  2  1  5  5  6  ...  1  1  7  1  6  6  6  6         NaN\n",
              "10        4        2  7  7  7  7  6  6  7  ...  1  5  5  5  5  5  2  6         NaN\n",
              "11        4        3  7  3  7  3  6  6  7  ...  6  1  4  4  4  6  6  6         NaN\n",
              "12        5        1  5  4  4  4  6  4  5  ...  3  1  4  4  1  3  3  5         NaN\n",
              "13        5        2  3  6  3  5  2  6  7  ...  6  6  6  6  6  6  6  4         NaN\n",
              "14        5        3  4  3  3  6  5  7  6  ...  7  2  2  7  5  6  4  2         NaN\n",
              "15        6        1  1  3  1  8  3  3  3  ...  4  3  7  2  7  2  5  8         NaN\n",
              "16        6        2  3  4  1  7  1  1  1  ...  5  5  5  5  5  5  5  5         NaN\n",
              "17        6        3  2  3  2  5  3  3  3  ...  2  3  3  3  1  1  3  3         NaN\n",
              "18        7        1  4  7  5  7  4  5  4  ...  5  1  1  4  4  4  1  4         NaN\n",
              "19        7        2  2  2  2  8  2  5  2  ...  5  5  5  2  2  2  2  2         NaN\n",
              "20        7        3  2  7  7  5  7  5  4  ...  1  3  5  5  3  5  5  1         NaN\n",
              "21        8        1  5  8  2  8  2  2  1  ...  1  8  2  2  5  5  2  8         NaN\n",
              "22        8        2  2  7  1  5  1  2  1  ...  7  1  7  8  2  1  7  7         NaN\n",
              "23        8        3  4  1  6  6  1  2  1  ...  1  8  1  7  1  1  7  1         NaN\n",
              "24        9        1  6  5  6  6  7  6  8  ...  6  6  6  5  5  5  3  5         NaN\n",
              "25        9        2  3  4  8  4  1  6  1  ...  1  1  1  1  1  3  6  6         NaN\n",
              "26        9        3  5  2  2  2  2  3  2  ...  2  2  2  3  2  2  4  2         NaN\n",
              "27       10        1  1  1  8  7  3  1  1  ...  5  1  1  1  1  5  1  5         NaN\n",
              "28       10        2  1  5  5  6  1  1  1  ...  5  5  6  6  2  1  2  1         NaN\n",
              "29       10        3  5  8  5  5  5  6  5  ...  5  5  2  5  5  5  5  5         NaN\n",
              "30       11        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "31       11        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "32       11        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "33       12        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "34       12        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "35       12        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "36       13        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "37       13        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "38       13        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "39       14        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "40       14        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "41       14        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "42       15        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "43       15        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "44       15        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "\n",
              "[45 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U6cN61-v1q2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5bcadac9-f3d6-402e-819b-38d85caec767"
      },
      "source": [
        "for i in range(11, 12): # Итерација низ секој испитен примерок\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_data = np.empty(0)\n",
        "  temp_labels = np.empty(0)\n",
        "  temp_events = np.empty(0)\n",
        "  temp_targets = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainData.mat\"\n",
        "    temp = loadmat(full_path)['trainData']\n",
        "    if temp_data.size != 0:\n",
        "      temp_data = np.concatenate((temp_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на label-ите\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainLabels.txt\"\n",
        "    with open(full_path, \"r\") as file_labels:\n",
        "      temp = file_labels.read().splitlines()\n",
        "      temp = np.repeat(temp, 8*10)\n",
        "      if temp_labels.size != 0:\n",
        "        temp_labels = np.concatenate((temp_labels, temp))\n",
        "      else:\n",
        "        temp_labels = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_events.size != 0:\n",
        "        temp_events = np.append(temp_events, temp)\n",
        "      else:\n",
        "        temp_events = np.array(temp)\n",
        "      \n",
        "\n",
        "    # Вчитување на редоследот на објекти кои се target\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainTargets.txt\"\n",
        "    with open(full_path, \"r\") as file_targets:\n",
        "      temp = file_targets.read().splitlines()\n",
        "      if temp_targets.size != 0:\n",
        "        temp_targets = np.concatenate((temp_targets, temp))\n",
        "      else:\n",
        "        temp_targets = np.array(temp)\n",
        "    print(\"\\t - Податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "\n",
        "  for j in range(4, 8): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "      \n",
        "  # Зачувај ги податоците вчитани од испитниот примерок во низа\n",
        "  data.append(temp_data)\n",
        "  labels.append(temp_labels)\n",
        "  events.append(temp_events)\n",
        "  targets.append(temp_targets)\n",
        "\n",
        "  \n",
        "  print(\"Податоците од испитниот примерок \" + str(i) + \" се вчитани.\")\n",
        "\n",
        "\n",
        "  #data = target_events_data_scaled\n",
        "  mne_array = np.swapaxes(data[i-1], 0, 2) # (епохa, канал, настан). \n",
        "  mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "  mne_array = mne_array.reshape(mne_array.shape[0],1, 8,350)\n",
        "  print(mne_array.shape)\n",
        "\n",
        "  events_arr = events[i-1].astype(np.int)\n",
        "  labels_arr = labels[i-1].astype(np.int)\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(mne_array, labels_arr-1, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "  model11 = DeepConvNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "  model11.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = Adam(0.0009))\n",
        "  checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                                verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "  #clf = RandomForestClassifier(max_depth=5)\n",
        "  #clf.fit(X_train, y_train)\n",
        "  #score = clf.score(X_test, y_test)\n",
        "  # print(score)\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  num_batch_size=100\n",
        "  num_epochs=400\n",
        "  model11.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "            validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "  score = model11.evaluate(X_test, y_test, verbose=1)\n",
        "  print(score)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t - Податоците од сесија 1 се вчитани.\n",
            "\t - Податоците од сесија 2 се вчитани.\n",
            "\t - Податоците од сесија 3 се вчитани.\n",
            "Податоците од испитниот примерок 11 се вчитани.\n",
            "(4800, 1, 8, 350)\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Train on 3600 samples, validate on 1200 samples\n",
            "Epoch 1/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.2707 - acc: 0.1618\n",
            "Epoch 00001: val_loss improved from inf to 2.15439, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 8s 2ms/sample - loss: 2.2699 - acc: 0.1600 - val_loss: 2.1544 - val_acc: 0.1583\n",
            "Epoch 2/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.1430 - acc: 0.1911\n",
            "Epoch 00002: val_loss improved from 2.15439 to 2.03312, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 2.1464 - acc: 0.1900 - val_loss: 2.0331 - val_acc: 0.2075\n",
            "Epoch 3/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 2.0866 - acc: 0.1994\n",
            "Epoch 00003: val_loss did not improve from 2.03312\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 2.0835 - acc: 0.1994 - val_loss: 2.2562 - val_acc: 0.2100\n",
            "Epoch 4/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.0004 - acc: 0.2286\n",
            "Epoch 00004: val_loss improved from 2.03312 to 2.02851, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 2.0002 - acc: 0.2272 - val_loss: 2.0285 - val_acc: 0.1658\n",
            "Epoch 5/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.9516 - acc: 0.2491\n",
            "Epoch 00005: val_loss improved from 2.02851 to 1.97095, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 1.9518 - acc: 0.2483 - val_loss: 1.9709 - val_acc: 0.2383\n",
            "Epoch 6/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.9318 - acc: 0.2512\n",
            "Epoch 00006: val_loss did not improve from 1.97095\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 1.9264 - acc: 0.2503 - val_loss: 2.1198 - val_acc: 0.1842\n",
            "Epoch 7/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.9173 - acc: 0.2621\n",
            "Epoch 00007: val_loss did not improve from 1.97095\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 1.9206 - acc: 0.2619 - val_loss: 2.1699 - val_acc: 0.2517\n",
            "Epoch 8/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.9147 - acc: 0.2674\n",
            "Epoch 00008: val_loss did not improve from 1.97095\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 1.9117 - acc: 0.2694 - val_loss: 2.0038 - val_acc: 0.2283\n",
            "Epoch 9/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.8626 - acc: 0.2865\n",
            "Epoch 00009: val_loss did not improve from 1.97095\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 1.8649 - acc: 0.2850 - val_loss: 2.0495 - val_acc: 0.2650\n",
            "Epoch 10/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.8532 - acc: 0.3003\n",
            "Epoch 00010: val_loss did not improve from 1.97095\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 1.8561 - acc: 0.2972 - val_loss: 2.0474 - val_acc: 0.1942\n",
            "Epoch 11/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.8353 - acc: 0.2960\n",
            "Epoch 00011: val_loss did not improve from 1.97095\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 1.8407 - acc: 0.2939 - val_loss: 2.0723 - val_acc: 0.2767\n",
            "Epoch 12/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.8100 - acc: 0.3139\n",
            "Epoch 00012: val_loss did not improve from 1.97095\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 1.8093 - acc: 0.3128 - val_loss: 1.9795 - val_acc: 0.3067\n",
            "Epoch 13/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.7684 - acc: 0.3276\n",
            "Epoch 00013: val_loss did not improve from 1.97095\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 1.7650 - acc: 0.3306 - val_loss: 2.1220 - val_acc: 0.2192\n",
            "Epoch 14/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.7664 - acc: 0.3312\n",
            "Epoch 00014: val_loss improved from 1.97095 to 1.88503, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 1.7700 - acc: 0.3294 - val_loss: 1.8850 - val_acc: 0.2758\n",
            "Epoch 15/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.7498 - acc: 0.3409\n",
            "Epoch 00015: val_loss did not improve from 1.88503\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 1.7441 - acc: 0.3428 - val_loss: 1.8981 - val_acc: 0.2975\n",
            "Epoch 16/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.7147 - acc: 0.3411\n",
            "Epoch 00016: val_loss did not improve from 1.88503\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 1.7133 - acc: 0.3411 - val_loss: 1.9620 - val_acc: 0.3133\n",
            "Epoch 17/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.6466 - acc: 0.3742\n",
            "Epoch 00017: val_loss did not improve from 1.88503\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 1.6454 - acc: 0.3756 - val_loss: 2.1508 - val_acc: 0.2942\n",
            "Epoch 18/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.6300 - acc: 0.3885\n",
            "Epoch 00018: val_loss did not improve from 1.88503\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 1.6271 - acc: 0.3889 - val_loss: 2.0869 - val_acc: 0.3058\n",
            "Epoch 19/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.6245 - acc: 0.3889\n",
            "Epoch 00019: val_loss did not improve from 1.88503\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 1.6286 - acc: 0.3900 - val_loss: 2.1126 - val_acc: 0.2925\n",
            "Epoch 20/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.5818 - acc: 0.4003\n",
            "Epoch 00020: val_loss did not improve from 1.88503\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 1.5855 - acc: 0.3997 - val_loss: 2.0144 - val_acc: 0.3108\n",
            "Epoch 21/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.5447 - acc: 0.4175\n",
            "Epoch 00021: val_loss did not improve from 1.88503\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 1.5391 - acc: 0.4206 - val_loss: 1.9907 - val_acc: 0.2825\n",
            "Epoch 22/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.5559 - acc: 0.4218\n",
            "Epoch 00022: val_loss did not improve from 1.88503\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 1.5568 - acc: 0.4250 - val_loss: 2.3730 - val_acc: 0.2733\n",
            "Epoch 23/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.4971 - acc: 0.4263\n",
            "Epoch 00023: val_loss improved from 1.88503 to 1.78478, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 1.4975 - acc: 0.4258 - val_loss: 1.7848 - val_acc: 0.3725\n",
            "Epoch 24/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4609 - acc: 0.4494\n",
            "Epoch 00024: val_loss did not improve from 1.78478\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 1.4670 - acc: 0.4475 - val_loss: 1.9828 - val_acc: 0.3517\n",
            "Epoch 25/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4224 - acc: 0.4709\n",
            "Epoch 00025: val_loss did not improve from 1.78478\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 1.4225 - acc: 0.4689 - val_loss: 1.9168 - val_acc: 0.3675\n",
            "Epoch 26/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.4089 - acc: 0.4724\n",
            "Epoch 00026: val_loss did not improve from 1.78478\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 1.4006 - acc: 0.4753 - val_loss: 1.8000 - val_acc: 0.3758\n",
            "Epoch 27/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.3514 - acc: 0.4985\n",
            "Epoch 00027: val_loss did not improve from 1.78478\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 1.3679 - acc: 0.4928 - val_loss: 2.0317 - val_acc: 0.3383\n",
            "Epoch 28/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.3647 - acc: 0.4918\n",
            "Epoch 00028: val_loss did not improve from 1.78478\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 1.3729 - acc: 0.4897 - val_loss: 2.0552 - val_acc: 0.3633\n",
            "Epoch 29/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.3734 - acc: 0.4703\n",
            "Epoch 00029: val_loss did not improve from 1.78478\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 1.3733 - acc: 0.4708 - val_loss: 1.9004 - val_acc: 0.3575\n",
            "Epoch 30/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2892 - acc: 0.5165\n",
            "Epoch 00030: val_loss improved from 1.78478 to 1.70398, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 1.2914 - acc: 0.5164 - val_loss: 1.7040 - val_acc: 0.4100\n",
            "Epoch 31/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.3044 - acc: 0.5127\n",
            "Epoch 00031: val_loss improved from 1.70398 to 1.59746, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 1.3114 - acc: 0.5094 - val_loss: 1.5975 - val_acc: 0.4167\n",
            "Epoch 32/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.2611 - acc: 0.5203\n",
            "Epoch 00032: val_loss did not improve from 1.59746\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 1.2628 - acc: 0.5206 - val_loss: 1.6660 - val_acc: 0.4142\n",
            "Epoch 33/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.2253 - acc: 0.5500\n",
            "Epoch 00033: val_loss did not improve from 1.59746\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 1.2277 - acc: 0.5475 - val_loss: 1.6288 - val_acc: 0.4067\n",
            "Epoch 34/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.2240 - acc: 0.5417\n",
            "Epoch 00034: val_loss improved from 1.59746 to 1.50900, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 1.2248 - acc: 0.5417 - val_loss: 1.5090 - val_acc: 0.4517\n",
            "Epoch 35/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.1546 - acc: 0.5737\n",
            "Epoch 00035: val_loss improved from 1.50900 to 1.49329, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 1.1527 - acc: 0.5736 - val_loss: 1.4933 - val_acc: 0.4800\n",
            "Epoch 36/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1762 - acc: 0.5639\n",
            "Epoch 00036: val_loss improved from 1.49329 to 1.46158, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 1.1825 - acc: 0.5625 - val_loss: 1.4616 - val_acc: 0.4592\n",
            "Epoch 37/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.1617 - acc: 0.5656\n",
            "Epoch 00037: val_loss did not improve from 1.46158\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 1.1667 - acc: 0.5644 - val_loss: 1.5696 - val_acc: 0.4342\n",
            "Epoch 38/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.1304 - acc: 0.5809\n",
            "Epoch 00038: val_loss did not improve from 1.46158\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 1.1277 - acc: 0.5833 - val_loss: 1.4682 - val_acc: 0.4808\n",
            "Epoch 39/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.0706 - acc: 0.6066\n",
            "Epoch 00039: val_loss improved from 1.46158 to 1.39564, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 1.0745 - acc: 0.6042 - val_loss: 1.3956 - val_acc: 0.4975\n",
            "Epoch 40/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.0804 - acc: 0.5926\n",
            "Epoch 00040: val_loss did not improve from 1.39564\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 1.0824 - acc: 0.5906 - val_loss: 1.4882 - val_acc: 0.4667\n",
            "Epoch 41/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0333 - acc: 0.6191\n",
            "Epoch 00041: val_loss did not improve from 1.39564\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 1.0388 - acc: 0.6172 - val_loss: 1.4047 - val_acc: 0.4925\n",
            "Epoch 42/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0452 - acc: 0.6132\n",
            "Epoch 00042: val_loss improved from 1.39564 to 1.25947, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 1.0535 - acc: 0.6106 - val_loss: 1.2595 - val_acc: 0.5558\n",
            "Epoch 43/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0348 - acc: 0.6218\n",
            "Epoch 00043: val_loss improved from 1.25947 to 1.17216, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 1.0341 - acc: 0.6206 - val_loss: 1.1722 - val_acc: 0.5717\n",
            "Epoch 44/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9488 - acc: 0.6535\n",
            "Epoch 00044: val_loss did not improve from 1.17216\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.9538 - acc: 0.6517 - val_loss: 1.2665 - val_acc: 0.5383\n",
            "Epoch 45/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9251 - acc: 0.6518\n",
            "Epoch 00045: val_loss did not improve from 1.17216\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.9321 - acc: 0.6486 - val_loss: 1.2395 - val_acc: 0.5467\n",
            "Epoch 46/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9224 - acc: 0.6669\n",
            "Epoch 00046: val_loss did not improve from 1.17216\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.9282 - acc: 0.6639 - val_loss: 1.1784 - val_acc: 0.5617\n",
            "Epoch 47/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9450 - acc: 0.6494\n",
            "Epoch 00047: val_loss improved from 1.17216 to 1.16779, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.9440 - acc: 0.6508 - val_loss: 1.1678 - val_acc: 0.5717\n",
            "Epoch 48/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9025 - acc: 0.6800\n",
            "Epoch 00048: val_loss did not improve from 1.16779\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.9017 - acc: 0.6808 - val_loss: 1.1842 - val_acc: 0.5592\n",
            "Epoch 49/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8746 - acc: 0.6782\n",
            "Epoch 00049: val_loss improved from 1.16779 to 1.05941, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.8731 - acc: 0.6819 - val_loss: 1.0594 - val_acc: 0.5958\n",
            "Epoch 50/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.8650 - acc: 0.6766\n",
            "Epoch 00050: val_loss improved from 1.05941 to 1.04290, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.8660 - acc: 0.6758 - val_loss: 1.0429 - val_acc: 0.6308\n",
            "Epoch 51/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8114 - acc: 0.7076\n",
            "Epoch 00051: val_loss improved from 1.04290 to 0.99842, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.8283 - acc: 0.7019 - val_loss: 0.9984 - val_acc: 0.6367\n",
            "Epoch 52/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.8325 - acc: 0.6903\n",
            "Epoch 00052: val_loss did not improve from 0.99842\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.8315 - acc: 0.6919 - val_loss: 1.0903 - val_acc: 0.5992\n",
            "Epoch 53/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7881 - acc: 0.7106\n",
            "Epoch 00053: val_loss did not improve from 0.99842\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.7930 - acc: 0.7075 - val_loss: 1.0025 - val_acc: 0.6300\n",
            "Epoch 54/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7907 - acc: 0.7117\n",
            "Epoch 00054: val_loss did not improve from 0.99842\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.7917 - acc: 0.7094 - val_loss: 1.0534 - val_acc: 0.6400\n",
            "Epoch 55/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7774 - acc: 0.7197\n",
            "Epoch 00055: val_loss improved from 0.99842 to 0.95680, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.7818 - acc: 0.7200 - val_loss: 0.9568 - val_acc: 0.6608\n",
            "Epoch 56/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7875 - acc: 0.7224\n",
            "Epoch 00056: val_loss improved from 0.95680 to 0.89736, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.7864 - acc: 0.7228 - val_loss: 0.8974 - val_acc: 0.6617\n",
            "Epoch 57/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7486 - acc: 0.7279\n",
            "Epoch 00057: val_loss improved from 0.89736 to 0.85672, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.7529 - acc: 0.7264 - val_loss: 0.8567 - val_acc: 0.6833\n",
            "Epoch 58/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7745 - acc: 0.7200\n",
            "Epoch 00058: val_loss improved from 0.85672 to 0.85432, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.7754 - acc: 0.7192 - val_loss: 0.8543 - val_acc: 0.6900\n",
            "Epoch 59/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7341 - acc: 0.7283\n",
            "Epoch 00059: val_loss did not improve from 0.85432\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.7329 - acc: 0.7283 - val_loss: 0.8678 - val_acc: 0.6675\n",
            "Epoch 60/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7075 - acc: 0.7456\n",
            "Epoch 00060: val_loss did not improve from 0.85432\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.7123 - acc: 0.7417 - val_loss: 0.8794 - val_acc: 0.6750\n",
            "Epoch 61/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6720 - acc: 0.7483\n",
            "Epoch 00061: val_loss improved from 0.85432 to 0.82100, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.6755 - acc: 0.7475 - val_loss: 0.8210 - val_acc: 0.6858\n",
            "Epoch 62/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6736 - acc: 0.7614\n",
            "Epoch 00062: val_loss did not improve from 0.82100\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.6759 - acc: 0.7603 - val_loss: 0.8561 - val_acc: 0.6867\n",
            "Epoch 63/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.6349 - acc: 0.7697\n",
            "Epoch 00063: val_loss did not improve from 0.82100\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.6380 - acc: 0.7692 - val_loss: 0.9167 - val_acc: 0.6625\n",
            "Epoch 64/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.6441 - acc: 0.7700\n",
            "Epoch 00064: val_loss did not improve from 0.82100\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.6401 - acc: 0.7700 - val_loss: 0.9258 - val_acc: 0.6650\n",
            "Epoch 65/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6764 - acc: 0.7559\n",
            "Epoch 00065: val_loss improved from 0.82100 to 0.75300, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.6789 - acc: 0.7581 - val_loss: 0.7530 - val_acc: 0.7183\n",
            "Epoch 66/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.6259 - acc: 0.7748\n",
            "Epoch 00066: val_loss improved from 0.75300 to 0.73547, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.6190 - acc: 0.7778 - val_loss: 0.7355 - val_acc: 0.7317\n",
            "Epoch 67/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6415 - acc: 0.7726\n",
            "Epoch 00067: val_loss did not improve from 0.73547\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.6451 - acc: 0.7689 - val_loss: 0.7839 - val_acc: 0.7025\n",
            "Epoch 68/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6048 - acc: 0.7847\n",
            "Epoch 00068: val_loss did not improve from 0.73547\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.6093 - acc: 0.7836 - val_loss: 0.7534 - val_acc: 0.7233\n",
            "Epoch 69/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5850 - acc: 0.7983\n",
            "Epoch 00069: val_loss did not improve from 0.73547\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.5859 - acc: 0.7983 - val_loss: 0.8299 - val_acc: 0.6842\n",
            "Epoch 70/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5829 - acc: 0.8015\n",
            "Epoch 00070: val_loss improved from 0.73547 to 0.68050, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.5826 - acc: 0.8006 - val_loss: 0.6805 - val_acc: 0.7475\n",
            "Epoch 71/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5995 - acc: 0.7846\n",
            "Epoch 00071: val_loss improved from 0.68050 to 0.64255, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.6017 - acc: 0.7842 - val_loss: 0.6425 - val_acc: 0.7508\n",
            "Epoch 72/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5822 - acc: 0.7909\n",
            "Epoch 00072: val_loss did not improve from 0.64255\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.5858 - acc: 0.7889 - val_loss: 0.8075 - val_acc: 0.6992\n",
            "Epoch 73/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5735 - acc: 0.8012\n",
            "Epoch 00073: val_loss did not improve from 0.64255\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.5834 - acc: 0.7981 - val_loss: 0.7453 - val_acc: 0.7250\n",
            "Epoch 74/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5699 - acc: 0.7974\n",
            "Epoch 00074: val_loss did not improve from 0.64255\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.5744 - acc: 0.7961 - val_loss: 0.6538 - val_acc: 0.7575\n",
            "Epoch 75/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5313 - acc: 0.8103\n",
            "Epoch 00075: val_loss improved from 0.64255 to 0.60025, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.5364 - acc: 0.8092 - val_loss: 0.6003 - val_acc: 0.7867\n",
            "Epoch 76/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5532 - acc: 0.8003\n",
            "Epoch 00076: val_loss did not improve from 0.60025\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.5538 - acc: 0.7997 - val_loss: 0.6576 - val_acc: 0.7675\n",
            "Epoch 77/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5436 - acc: 0.8094\n",
            "Epoch 00077: val_loss did not improve from 0.60025\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.5457 - acc: 0.8075 - val_loss: 0.7098 - val_acc: 0.7408\n",
            "Epoch 78/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5145 - acc: 0.8254\n",
            "Epoch 00078: val_loss improved from 0.60025 to 0.57385, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.5188 - acc: 0.8242 - val_loss: 0.5738 - val_acc: 0.8017\n",
            "Epoch 79/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5002 - acc: 0.8265\n",
            "Epoch 00079: val_loss did not improve from 0.57385\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.4993 - acc: 0.8261 - val_loss: 0.6075 - val_acc: 0.7825\n",
            "Epoch 80/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4847 - acc: 0.8362\n",
            "Epoch 00080: val_loss did not improve from 0.57385\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.4901 - acc: 0.8322 - val_loss: 0.6339 - val_acc: 0.7775\n",
            "Epoch 81/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4662 - acc: 0.8426\n",
            "Epoch 00081: val_loss improved from 0.57385 to 0.51561, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 0.4669 - acc: 0.8431 - val_loss: 0.5156 - val_acc: 0.8233\n",
            "Epoch 82/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4702 - acc: 0.8346\n",
            "Epoch 00082: val_loss improved from 0.51561 to 0.51340, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.4719 - acc: 0.8336 - val_loss: 0.5134 - val_acc: 0.8083\n",
            "Epoch 83/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4544 - acc: 0.8415\n",
            "Epoch 00083: val_loss did not improve from 0.51340\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.4560 - acc: 0.8411 - val_loss: 0.5774 - val_acc: 0.7833\n",
            "Epoch 84/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4556 - acc: 0.8397\n",
            "Epoch 00084: val_loss did not improve from 0.51340\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.4564 - acc: 0.8400 - val_loss: 0.5536 - val_acc: 0.8017\n",
            "Epoch 85/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4378 - acc: 0.8509\n",
            "Epoch 00085: val_loss did not improve from 0.51340\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.4418 - acc: 0.8494 - val_loss: 0.6025 - val_acc: 0.7667\n",
            "Epoch 86/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4423 - acc: 0.8468\n",
            "Epoch 00086: val_loss did not improve from 0.51340\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.4450 - acc: 0.8444 - val_loss: 0.6000 - val_acc: 0.7650\n",
            "Epoch 87/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4308 - acc: 0.8544\n",
            "Epoch 00087: val_loss did not improve from 0.51340\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.4347 - acc: 0.8531 - val_loss: 0.5344 - val_acc: 0.8100\n",
            "Epoch 88/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4408 - acc: 0.8477\n",
            "Epoch 00088: val_loss improved from 0.51340 to 0.50295, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.4383 - acc: 0.8486 - val_loss: 0.5029 - val_acc: 0.8175\n",
            "Epoch 89/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8531\n",
            "Epoch 00089: val_loss did not improve from 0.50295\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.4094 - acc: 0.8528 - val_loss: 0.5304 - val_acc: 0.8242\n",
            "Epoch 90/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4279 - acc: 0.8571\n",
            "Epoch 00090: val_loss improved from 0.50295 to 0.47903, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.4298 - acc: 0.8569 - val_loss: 0.4790 - val_acc: 0.8383\n",
            "Epoch 91/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4088 - acc: 0.8591\n",
            "Epoch 00091: val_loss did not improve from 0.47903\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.4091 - acc: 0.8581 - val_loss: 0.5617 - val_acc: 0.7967\n",
            "Epoch 92/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3995 - acc: 0.8677\n",
            "Epoch 00092: val_loss did not improve from 0.47903\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.3986 - acc: 0.8678 - val_loss: 0.5667 - val_acc: 0.8017\n",
            "Epoch 93/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3981 - acc: 0.8624\n",
            "Epoch 00093: val_loss improved from 0.47903 to 0.43982, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.3949 - acc: 0.8650 - val_loss: 0.4398 - val_acc: 0.8433\n",
            "Epoch 94/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3991 - acc: 0.8646\n",
            "Epoch 00094: val_loss did not improve from 0.43982\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.3983 - acc: 0.8656 - val_loss: 0.4843 - val_acc: 0.8217\n",
            "Epoch 95/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3907 - acc: 0.8656\n",
            "Epoch 00095: val_loss did not improve from 0.43982\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.3918 - acc: 0.8658 - val_loss: 0.4981 - val_acc: 0.8125\n",
            "Epoch 96/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3981 - acc: 0.8677\n",
            "Epoch 00096: val_loss did not improve from 0.43982\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.3981 - acc: 0.8686 - val_loss: 0.5732 - val_acc: 0.7842\n",
            "Epoch 97/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3849 - acc: 0.8674\n",
            "Epoch 00097: val_loss did not improve from 0.43982\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.3878 - acc: 0.8653 - val_loss: 0.4643 - val_acc: 0.8375\n",
            "Epoch 98/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4103 - acc: 0.8579\n",
            "Epoch 00098: val_loss did not improve from 0.43982\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.4084 - acc: 0.8575 - val_loss: 0.5240 - val_acc: 0.8233\n",
            "Epoch 99/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3806 - acc: 0.8700\n",
            "Epoch 00099: val_loss did not improve from 0.43982\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.3830 - acc: 0.8686 - val_loss: 0.4580 - val_acc: 0.8383\n",
            "Epoch 100/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3834 - acc: 0.8697\n",
            "Epoch 00100: val_loss improved from 0.43982 to 0.40897, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.3829 - acc: 0.8697 - val_loss: 0.4090 - val_acc: 0.8725\n",
            "Epoch 101/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3620 - acc: 0.8806\n",
            "Epoch 00101: val_loss did not improve from 0.40897\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.3632 - acc: 0.8792 - val_loss: 0.4306 - val_acc: 0.8392\n",
            "Epoch 102/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3499 - acc: 0.8809\n",
            "Epoch 00102: val_loss improved from 0.40897 to 0.36460, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.3479 - acc: 0.8817 - val_loss: 0.3646 - val_acc: 0.8833\n",
            "Epoch 103/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3424 - acc: 0.8850\n",
            "Epoch 00103: val_loss did not improve from 0.36460\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.3427 - acc: 0.8856 - val_loss: 0.4082 - val_acc: 0.8617\n",
            "Epoch 104/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3396 - acc: 0.8826\n",
            "Epoch 00104: val_loss did not improve from 0.36460\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.3414 - acc: 0.8822 - val_loss: 0.4591 - val_acc: 0.8425\n",
            "Epoch 105/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3238 - acc: 0.8879\n",
            "Epoch 00105: val_loss did not improve from 0.36460\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.3238 - acc: 0.8894 - val_loss: 0.3995 - val_acc: 0.8625\n",
            "Epoch 106/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3433 - acc: 0.8837\n",
            "Epoch 00106: val_loss did not improve from 0.36460\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.3445 - acc: 0.8833 - val_loss: 0.4099 - val_acc: 0.8542\n",
            "Epoch 107/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3359 - acc: 0.8867\n",
            "Epoch 00107: val_loss did not improve from 0.36460\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.3353 - acc: 0.8867 - val_loss: 0.4363 - val_acc: 0.8458\n",
            "Epoch 108/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3519 - acc: 0.8762\n",
            "Epoch 00108: val_loss did not improve from 0.36460\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.3488 - acc: 0.8769 - val_loss: 0.3744 - val_acc: 0.8708\n",
            "Epoch 109/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3171 - acc: 0.8968\n",
            "Epoch 00109: val_loss improved from 0.36460 to 0.32630, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.3149 - acc: 0.8972 - val_loss: 0.3263 - val_acc: 0.8983\n",
            "Epoch 110/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3004 - acc: 0.9021\n",
            "Epoch 00110: val_loss did not improve from 0.32630\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.3042 - acc: 0.8997 - val_loss: 0.3451 - val_acc: 0.8917\n",
            "Epoch 111/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3388 - acc: 0.8906\n",
            "Epoch 00111: val_loss did not improve from 0.32630\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.3379 - acc: 0.8911 - val_loss: 0.3386 - val_acc: 0.8892\n",
            "Epoch 112/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3154 - acc: 0.8946\n",
            "Epoch 00112: val_loss did not improve from 0.32630\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.3144 - acc: 0.8947 - val_loss: 0.3864 - val_acc: 0.8608\n",
            "Epoch 113/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3242 - acc: 0.8935\n",
            "Epoch 00113: val_loss did not improve from 0.32630\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.3197 - acc: 0.8953 - val_loss: 0.3762 - val_acc: 0.8742\n",
            "Epoch 114/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.9020\n",
            "Epoch 00114: val_loss did not improve from 0.32630\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2978 - acc: 0.8989 - val_loss: 0.4175 - val_acc: 0.8517\n",
            "Epoch 115/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2916 - acc: 0.9000\n",
            "Epoch 00115: val_loss improved from 0.32630 to 0.29854, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.2955 - acc: 0.8964 - val_loss: 0.2985 - val_acc: 0.9167\n",
            "Epoch 116/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2914 - acc: 0.8894\n",
            "Epoch 00116: val_loss did not improve from 0.29854\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.2927 - acc: 0.8903 - val_loss: 0.3106 - val_acc: 0.9000\n",
            "Epoch 117/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2894 - acc: 0.9057\n",
            "Epoch 00117: val_loss did not improve from 0.29854\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.2898 - acc: 0.9061 - val_loss: 0.3159 - val_acc: 0.8942\n",
            "Epoch 118/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2919 - acc: 0.8989\n",
            "Epoch 00118: val_loss did not improve from 0.29854\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.2899 - acc: 0.9003 - val_loss: 0.3628 - val_acc: 0.8683\n",
            "Epoch 119/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2918 - acc: 0.9085\n",
            "Epoch 00119: val_loss did not improve from 0.29854\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2968 - acc: 0.9056 - val_loss: 0.3261 - val_acc: 0.8908\n",
            "Epoch 120/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2769 - acc: 0.9100\n",
            "Epoch 00120: val_loss did not improve from 0.29854\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2819 - acc: 0.9078 - val_loss: 0.3080 - val_acc: 0.8925\n",
            "Epoch 121/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2939 - acc: 0.8985\n",
            "Epoch 00121: val_loss did not improve from 0.29854\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2965 - acc: 0.8972 - val_loss: 0.3259 - val_acc: 0.8950\n",
            "Epoch 122/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2821 - acc: 0.9088\n",
            "Epoch 00122: val_loss improved from 0.29854 to 0.28076, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.2829 - acc: 0.9075 - val_loss: 0.2808 - val_acc: 0.9192\n",
            "Epoch 123/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2873 - acc: 0.9026\n",
            "Epoch 00123: val_loss did not improve from 0.28076\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.2879 - acc: 0.9028 - val_loss: 0.3611 - val_acc: 0.8758\n",
            "Epoch 124/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2940 - acc: 0.9038\n",
            "Epoch 00124: val_loss did not improve from 0.28076\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2941 - acc: 0.9050 - val_loss: 0.3126 - val_acc: 0.9025\n",
            "Epoch 125/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2809 - acc: 0.9094\n",
            "Epoch 00125: val_loss did not improve from 0.28076\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2755 - acc: 0.9114 - val_loss: 0.3441 - val_acc: 0.8875\n",
            "Epoch 126/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2898 - acc: 0.9049\n",
            "Epoch 00126: val_loss did not improve from 0.28076\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2896 - acc: 0.9050 - val_loss: 0.3673 - val_acc: 0.8733\n",
            "Epoch 127/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2735 - acc: 0.9103\n",
            "Epoch 00127: val_loss did not improve from 0.28076\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2737 - acc: 0.9103 - val_loss: 0.3039 - val_acc: 0.9050\n",
            "Epoch 128/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2467 - acc: 0.9203\n",
            "Epoch 00128: val_loss did not improve from 0.28076\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2488 - acc: 0.9197 - val_loss: 0.3479 - val_acc: 0.8667\n",
            "Epoch 129/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2717 - acc: 0.9085\n",
            "Epoch 00129: val_loss did not improve from 0.28076\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2694 - acc: 0.9111 - val_loss: 0.2902 - val_acc: 0.9017\n",
            "Epoch 130/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2599 - acc: 0.9156\n",
            "Epoch 00130: val_loss improved from 0.28076 to 0.27340, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.2623 - acc: 0.9147 - val_loss: 0.2734 - val_acc: 0.9067\n",
            "Epoch 131/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2526 - acc: 0.9209\n",
            "Epoch 00131: val_loss improved from 0.27340 to 0.26531, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.2523 - acc: 0.9206 - val_loss: 0.2653 - val_acc: 0.9183\n",
            "Epoch 132/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2642 - acc: 0.9163\n",
            "Epoch 00132: val_loss did not improve from 0.26531\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2658 - acc: 0.9158 - val_loss: 0.3546 - val_acc: 0.8808\n",
            "Epoch 133/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2666 - acc: 0.9112\n",
            "Epoch 00133: val_loss improved from 0.26531 to 0.24532, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.2679 - acc: 0.9111 - val_loss: 0.2453 - val_acc: 0.9300\n",
            "Epoch 134/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.9094\n",
            "Epoch 00134: val_loss did not improve from 0.24532\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.2767 - acc: 0.9089 - val_loss: 0.3080 - val_acc: 0.8967\n",
            "Epoch 135/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2636 - acc: 0.9151\n",
            "Epoch 00135: val_loss did not improve from 0.24532\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2620 - acc: 0.9158 - val_loss: 0.2842 - val_acc: 0.9075\n",
            "Epoch 136/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2598 - acc: 0.9175\n",
            "Epoch 00136: val_loss did not improve from 0.24532\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2643 - acc: 0.9147 - val_loss: 0.3184 - val_acc: 0.8850\n",
            "Epoch 137/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2390 - acc: 0.9306\n",
            "Epoch 00137: val_loss did not improve from 0.24532\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2393 - acc: 0.9297 - val_loss: 0.3962 - val_acc: 0.8533\n",
            "Epoch 138/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2597 - acc: 0.9151\n",
            "Epoch 00138: val_loss improved from 0.24532 to 0.24239, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.2580 - acc: 0.9158 - val_loss: 0.2424 - val_acc: 0.9275\n",
            "Epoch 139/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2357 - acc: 0.9247\n",
            "Epoch 00139: val_loss did not improve from 0.24239\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2382 - acc: 0.9225 - val_loss: 0.2777 - val_acc: 0.9150\n",
            "Epoch 140/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9314\n",
            "Epoch 00140: val_loss did not improve from 0.24239\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.2143 - acc: 0.9325 - val_loss: 0.2506 - val_acc: 0.9217\n",
            "Epoch 141/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2349 - acc: 0.9262\n",
            "Epoch 00141: val_loss did not improve from 0.24239\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.2365 - acc: 0.9261 - val_loss: 0.3626 - val_acc: 0.8808\n",
            "Epoch 142/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2407 - acc: 0.9155\n",
            "Epoch 00142: val_loss did not improve from 0.24239\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2384 - acc: 0.9178 - val_loss: 0.2543 - val_acc: 0.9158\n",
            "Epoch 143/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2375 - acc: 0.9197\n",
            "Epoch 00143: val_loss did not improve from 0.24239\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2390 - acc: 0.9192 - val_loss: 0.2509 - val_acc: 0.9225\n",
            "Epoch 144/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9274\n",
            "Epoch 00144: val_loss did not improve from 0.24239\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2307 - acc: 0.9278 - val_loss: 0.2694 - val_acc: 0.9117\n",
            "Epoch 145/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2382 - acc: 0.9218\n",
            "Epoch 00145: val_loss did not improve from 0.24239\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.2330 - acc: 0.9242 - val_loss: 0.2706 - val_acc: 0.9108\n",
            "Epoch 146/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2175 - acc: 0.9271\n",
            "Epoch 00146: val_loss improved from 0.24239 to 0.22991, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.2207 - acc: 0.9258 - val_loss: 0.2299 - val_acc: 0.9292\n",
            "Epoch 147/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9254\n",
            "Epoch 00147: val_loss did not improve from 0.22991\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2325 - acc: 0.9253 - val_loss: 0.2830 - val_acc: 0.9075\n",
            "Epoch 148/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9231\n",
            "Epoch 00148: val_loss improved from 0.22991 to 0.20043, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.2315 - acc: 0.9228 - val_loss: 0.2004 - val_acc: 0.9433\n",
            "Epoch 149/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 0.9260\n",
            "Epoch 00149: val_loss improved from 0.20043 to 0.18685, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.2177 - acc: 0.9267 - val_loss: 0.1869 - val_acc: 0.9467\n",
            "Epoch 150/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2162 - acc: 0.9282\n",
            "Epoch 00150: val_loss did not improve from 0.18685\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.2205 - acc: 0.9272 - val_loss: 0.2415 - val_acc: 0.9250\n",
            "Epoch 151/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9231\n",
            "Epoch 00151: val_loss did not improve from 0.18685\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.2253 - acc: 0.9239 - val_loss: 0.2805 - val_acc: 0.9075\n",
            "Epoch 152/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2201 - acc: 0.9312\n",
            "Epoch 00152: val_loss did not improve from 0.18685\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2190 - acc: 0.9314 - val_loss: 0.2213 - val_acc: 0.9333\n",
            "Epoch 153/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2138 - acc: 0.9332\n",
            "Epoch 00153: val_loss did not improve from 0.18685\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2155 - acc: 0.9311 - val_loss: 0.1934 - val_acc: 0.9442\n",
            "Epoch 154/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2177 - acc: 0.9276\n",
            "Epoch 00154: val_loss did not improve from 0.18685\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.2188 - acc: 0.9269 - val_loss: 0.2292 - val_acc: 0.9392\n",
            "Epoch 155/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2348 - acc: 0.9185\n",
            "Epoch 00155: val_loss did not improve from 0.18685\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.2365 - acc: 0.9189 - val_loss: 0.2081 - val_acc: 0.9392\n",
            "Epoch 156/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9320\n",
            "Epoch 00156: val_loss did not improve from 0.18685\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.2113 - acc: 0.9328 - val_loss: 0.2431 - val_acc: 0.9258\n",
            "Epoch 157/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9269\n",
            "Epoch 00157: val_loss did not improve from 0.18685\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2228 - acc: 0.9278 - val_loss: 0.2655 - val_acc: 0.9150\n",
            "Epoch 158/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2283 - acc: 0.9262\n",
            "Epoch 00158: val_loss did not improve from 0.18685\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.2268 - acc: 0.9261 - val_loss: 0.2315 - val_acc: 0.9283\n",
            "Epoch 159/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 0.9251\n",
            "Epoch 00159: val_loss did not improve from 0.18685\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2198 - acc: 0.9244 - val_loss: 0.2149 - val_acc: 0.9342\n",
            "Epoch 160/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2109 - acc: 0.9332\n",
            "Epoch 00160: val_loss improved from 0.18685 to 0.18261, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.2096 - acc: 0.9344 - val_loss: 0.1826 - val_acc: 0.9467\n",
            "Epoch 161/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1993 - acc: 0.9363\n",
            "Epoch 00161: val_loss did not improve from 0.18261\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1989 - acc: 0.9367 - val_loss: 0.2503 - val_acc: 0.9183\n",
            "Epoch 162/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2040 - acc: 0.9323\n",
            "Epoch 00162: val_loss did not improve from 0.18261\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.2049 - acc: 0.9322 - val_loss: 0.2809 - val_acc: 0.9050\n",
            "Epoch 163/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2139 - acc: 0.9285\n",
            "Epoch 00163: val_loss did not improve from 0.18261\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2146 - acc: 0.9289 - val_loss: 0.2119 - val_acc: 0.9317\n",
            "Epoch 164/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2025 - acc: 0.9353\n",
            "Epoch 00164: val_loss improved from 0.18261 to 0.16995, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.2054 - acc: 0.9339 - val_loss: 0.1699 - val_acc: 0.9517\n",
            "Epoch 165/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1955 - acc: 0.9362\n",
            "Epoch 00165: val_loss improved from 0.16995 to 0.16880, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.1987 - acc: 0.9339 - val_loss: 0.1688 - val_acc: 0.9508\n",
            "Epoch 166/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1865 - acc: 0.9443\n",
            "Epoch 00166: val_loss did not improve from 0.16880\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1866 - acc: 0.9442 - val_loss: 0.2028 - val_acc: 0.9325\n",
            "Epoch 167/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2012 - acc: 0.9344\n",
            "Epoch 00167: val_loss did not improve from 0.16880\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.2026 - acc: 0.9347 - val_loss: 0.1994 - val_acc: 0.9442\n",
            "Epoch 168/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1865 - acc: 0.9414\n",
            "Epoch 00168: val_loss did not improve from 0.16880\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1855 - acc: 0.9417 - val_loss: 0.2406 - val_acc: 0.9192\n",
            "Epoch 169/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9251\n",
            "Epoch 00169: val_loss did not improve from 0.16880\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2246 - acc: 0.9233 - val_loss: 0.2333 - val_acc: 0.9275\n",
            "Epoch 170/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2078 - acc: 0.9300\n",
            "Epoch 00170: val_loss did not improve from 0.16880\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.2077 - acc: 0.9294 - val_loss: 0.2850 - val_acc: 0.9008\n",
            "Epoch 171/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2228 - acc: 0.9242\n",
            "Epoch 00171: val_loss did not improve from 0.16880\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2248 - acc: 0.9236 - val_loss: 0.2236 - val_acc: 0.9217\n",
            "Epoch 172/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1965 - acc: 0.9370\n",
            "Epoch 00172: val_loss did not improve from 0.16880\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1972 - acc: 0.9369 - val_loss: 0.2007 - val_acc: 0.9342\n",
            "Epoch 173/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1857 - acc: 0.9409\n",
            "Epoch 00173: val_loss did not improve from 0.16880\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1897 - acc: 0.9397 - val_loss: 0.2124 - val_acc: 0.9375\n",
            "Epoch 174/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2004 - acc: 0.9326\n",
            "Epoch 00174: val_loss did not improve from 0.16880\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2009 - acc: 0.9325 - val_loss: 0.2416 - val_acc: 0.9192\n",
            "Epoch 175/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1912 - acc: 0.9394\n",
            "Epoch 00175: val_loss did not improve from 0.16880\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1926 - acc: 0.9386 - val_loss: 0.1999 - val_acc: 0.9317\n",
            "Epoch 176/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1983 - acc: 0.9379\n",
            "Epoch 00176: val_loss did not improve from 0.16880\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2006 - acc: 0.9361 - val_loss: 0.2197 - val_acc: 0.9333\n",
            "Epoch 177/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9294\n",
            "Epoch 00177: val_loss did not improve from 0.16880\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2036 - acc: 0.9289 - val_loss: 0.2002 - val_acc: 0.9433\n",
            "Epoch 178/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2137 - acc: 0.9276\n",
            "Epoch 00178: val_loss did not improve from 0.16880\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2116 - acc: 0.9289 - val_loss: 0.1825 - val_acc: 0.9458\n",
            "Epoch 179/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1730 - acc: 0.9462\n",
            "Epoch 00179: val_loss did not improve from 0.16880\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1719 - acc: 0.9469 - val_loss: 0.2229 - val_acc: 0.9225\n",
            "Epoch 180/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1774 - acc: 0.9432\n",
            "Epoch 00180: val_loss improved from 0.16880 to 0.15022, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.1802 - acc: 0.9422 - val_loss: 0.1502 - val_acc: 0.9567\n",
            "Epoch 181/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1894 - acc: 0.9355\n",
            "Epoch 00181: val_loss improved from 0.15022 to 0.13930, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.1872 - acc: 0.9367 - val_loss: 0.1393 - val_acc: 0.9533\n",
            "Epoch 182/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2152 - acc: 0.9267\n",
            "Epoch 00182: val_loss did not improve from 0.13930\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2150 - acc: 0.9275 - val_loss: 0.1766 - val_acc: 0.9483\n",
            "Epoch 183/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1948 - acc: 0.9379\n",
            "Epoch 00183: val_loss improved from 0.13930 to 0.13708, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.1957 - acc: 0.9386 - val_loss: 0.1371 - val_acc: 0.9625\n",
            "Epoch 184/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2036 - acc: 0.9324\n",
            "Epoch 00184: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.2034 - acc: 0.9322 - val_loss: 0.1922 - val_acc: 0.9425\n",
            "Epoch 185/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1944 - acc: 0.9339\n",
            "Epoch 00185: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1947 - acc: 0.9350 - val_loss: 0.1873 - val_acc: 0.9417\n",
            "Epoch 186/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1858 - acc: 0.9389\n",
            "Epoch 00186: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1836 - acc: 0.9400 - val_loss: 0.1913 - val_acc: 0.9400\n",
            "Epoch 187/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2025 - acc: 0.9327\n",
            "Epoch 00187: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2032 - acc: 0.9328 - val_loss: 0.1710 - val_acc: 0.9575\n",
            "Epoch 188/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1724 - acc: 0.9476\n",
            "Epoch 00188: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1740 - acc: 0.9461 - val_loss: 0.1766 - val_acc: 0.9442\n",
            "Epoch 189/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1525 - acc: 0.9532\n",
            "Epoch 00189: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1551 - acc: 0.9519 - val_loss: 0.1518 - val_acc: 0.9533\n",
            "Epoch 190/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1668 - acc: 0.9464\n",
            "Epoch 00190: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1657 - acc: 0.9467 - val_loss: 0.1815 - val_acc: 0.9283\n",
            "Epoch 191/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1683 - acc: 0.9458\n",
            "Epoch 00191: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1677 - acc: 0.9461 - val_loss: 0.1603 - val_acc: 0.9417\n",
            "Epoch 192/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1881 - acc: 0.9414\n",
            "Epoch 00192: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1867 - acc: 0.9422 - val_loss: 0.1623 - val_acc: 0.9483\n",
            "Epoch 193/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1735 - acc: 0.9468\n",
            "Epoch 00193: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1770 - acc: 0.9447 - val_loss: 0.1613 - val_acc: 0.9525\n",
            "Epoch 194/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1875 - acc: 0.9379\n",
            "Epoch 00194: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1876 - acc: 0.9386 - val_loss: 0.1763 - val_acc: 0.9442\n",
            "Epoch 195/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1881 - acc: 0.9422\n",
            "Epoch 00195: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1909 - acc: 0.9406 - val_loss: 0.2996 - val_acc: 0.8975\n",
            "Epoch 196/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1860 - acc: 0.9391\n",
            "Epoch 00196: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1887 - acc: 0.9386 - val_loss: 0.1472 - val_acc: 0.9550\n",
            "Epoch 197/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9471\n",
            "Epoch 00197: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1641 - acc: 0.9469 - val_loss: 0.1545 - val_acc: 0.9533\n",
            "Epoch 198/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1560 - acc: 0.9494\n",
            "Epoch 00198: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1559 - acc: 0.9489 - val_loss: 0.1420 - val_acc: 0.9567\n",
            "Epoch 199/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1638 - acc: 0.9489\n",
            "Epoch 00199: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1660 - acc: 0.9481 - val_loss: 0.1563 - val_acc: 0.9508\n",
            "Epoch 200/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1657 - acc: 0.9506\n",
            "Epoch 00200: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1663 - acc: 0.9511 - val_loss: 0.1837 - val_acc: 0.9500\n",
            "Epoch 201/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1436 - acc: 0.9575\n",
            "Epoch 00201: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1431 - acc: 0.9578 - val_loss: 0.1966 - val_acc: 0.9492\n",
            "Epoch 202/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1818 - acc: 0.9431\n",
            "Epoch 00202: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1830 - acc: 0.9431 - val_loss: 0.1789 - val_acc: 0.9383\n",
            "Epoch 203/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1845 - acc: 0.9412\n",
            "Epoch 00203: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1850 - acc: 0.9408 - val_loss: 0.1713 - val_acc: 0.9458\n",
            "Epoch 204/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1812 - acc: 0.9400\n",
            "Epoch 00204: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1794 - acc: 0.9397 - val_loss: 0.1499 - val_acc: 0.9517\n",
            "Epoch 205/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1856 - acc: 0.9409\n",
            "Epoch 00205: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1865 - acc: 0.9400 - val_loss: 0.1395 - val_acc: 0.9608\n",
            "Epoch 206/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1696 - acc: 0.9450\n",
            "Epoch 00206: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1684 - acc: 0.9450 - val_loss: 0.1793 - val_acc: 0.9475\n",
            "Epoch 207/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1855 - acc: 0.9415\n",
            "Epoch 00207: val_loss did not improve from 0.13708\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1860 - acc: 0.9417 - val_loss: 0.1766 - val_acc: 0.9517\n",
            "Epoch 208/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1649 - acc: 0.9485\n",
            "Epoch 00208: val_loss improved from 0.13708 to 0.12155, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.1663 - acc: 0.9486 - val_loss: 0.1215 - val_acc: 0.9658\n",
            "Epoch 209/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1744 - acc: 0.9444\n",
            "Epoch 00209: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1756 - acc: 0.9447 - val_loss: 0.1438 - val_acc: 0.9550\n",
            "Epoch 210/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1994 - acc: 0.9353\n",
            "Epoch 00210: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1974 - acc: 0.9361 - val_loss: 0.1457 - val_acc: 0.9592\n",
            "Epoch 211/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1796 - acc: 0.9409\n",
            "Epoch 00211: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1804 - acc: 0.9406 - val_loss: 0.1820 - val_acc: 0.9492\n",
            "Epoch 212/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1616 - acc: 0.9434\n",
            "Epoch 00212: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1625 - acc: 0.9431 - val_loss: 0.1653 - val_acc: 0.9417\n",
            "Epoch 213/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1827 - acc: 0.9374\n",
            "Epoch 00213: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1820 - acc: 0.9369 - val_loss: 0.1571 - val_acc: 0.9542\n",
            "Epoch 214/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1756 - acc: 0.9400\n",
            "Epoch 00214: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1761 - acc: 0.9406 - val_loss: 0.1753 - val_acc: 0.9425\n",
            "Epoch 215/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1684 - acc: 0.9458\n",
            "Epoch 00215: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1712 - acc: 0.9447 - val_loss: 0.1845 - val_acc: 0.9442\n",
            "Epoch 216/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9466\n",
            "Epoch 00216: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1627 - acc: 0.9472 - val_loss: 0.1247 - val_acc: 0.9617\n",
            "Epoch 217/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9540\n",
            "Epoch 00217: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1515 - acc: 0.9544 - val_loss: 0.1439 - val_acc: 0.9575\n",
            "Epoch 218/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1478 - acc: 0.9509\n",
            "Epoch 00218: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1489 - acc: 0.9508 - val_loss: 0.1560 - val_acc: 0.9567\n",
            "Epoch 219/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1697 - acc: 0.9415\n",
            "Epoch 00219: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1698 - acc: 0.9419 - val_loss: 0.1615 - val_acc: 0.9475\n",
            "Epoch 220/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1849 - acc: 0.9384\n",
            "Epoch 00220: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1857 - acc: 0.9372 - val_loss: 0.1626 - val_acc: 0.9533\n",
            "Epoch 221/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9360\n",
            "Epoch 00221: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1828 - acc: 0.9356 - val_loss: 0.1651 - val_acc: 0.9475\n",
            "Epoch 222/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1763 - acc: 0.9438\n",
            "Epoch 00222: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1757 - acc: 0.9442 - val_loss: 0.1302 - val_acc: 0.9633\n",
            "Epoch 223/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1652 - acc: 0.9488\n",
            "Epoch 00223: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1749 - acc: 0.9436 - val_loss: 0.1487 - val_acc: 0.9583\n",
            "Epoch 224/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9474\n",
            "Epoch 00224: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1620 - acc: 0.9472 - val_loss: 0.1330 - val_acc: 0.9625\n",
            "Epoch 225/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1554 - acc: 0.9491\n",
            "Epoch 00225: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1566 - acc: 0.9492 - val_loss: 0.1386 - val_acc: 0.9617\n",
            "Epoch 226/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1711 - acc: 0.9438\n",
            "Epoch 00226: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1687 - acc: 0.9442 - val_loss: 0.1339 - val_acc: 0.9575\n",
            "Epoch 227/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1506 - acc: 0.9503\n",
            "Epoch 00227: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1537 - acc: 0.9486 - val_loss: 0.1528 - val_acc: 0.9517\n",
            "Epoch 228/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9489\n",
            "Epoch 00228: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1611 - acc: 0.9481 - val_loss: 0.1451 - val_acc: 0.9550\n",
            "Epoch 229/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9423\n",
            "Epoch 00229: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1748 - acc: 0.9428 - val_loss: 0.1276 - val_acc: 0.9650\n",
            "Epoch 230/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1493 - acc: 0.9534\n",
            "Epoch 00230: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1512 - acc: 0.9522 - val_loss: 0.1532 - val_acc: 0.9575\n",
            "Epoch 231/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1426 - acc: 0.9585\n",
            "Epoch 00231: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1418 - acc: 0.9589 - val_loss: 0.1988 - val_acc: 0.9342\n",
            "Epoch 232/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1581 - acc: 0.9479\n",
            "Epoch 00232: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1596 - acc: 0.9469 - val_loss: 0.1643 - val_acc: 0.9467\n",
            "Epoch 233/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9523\n",
            "Epoch 00233: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1550 - acc: 0.9517 - val_loss: 0.1867 - val_acc: 0.9408\n",
            "Epoch 234/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9557\n",
            "Epoch 00234: val_loss did not improve from 0.12155\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1407 - acc: 0.9564 - val_loss: 0.1556 - val_acc: 0.9458\n",
            "Epoch 235/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9429\n",
            "Epoch 00235: val_loss improved from 0.12155 to 0.12087, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.1643 - acc: 0.9439 - val_loss: 0.1209 - val_acc: 0.9642\n",
            "Epoch 236/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1608 - acc: 0.9453\n",
            "Epoch 00236: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1597 - acc: 0.9461 - val_loss: 0.1684 - val_acc: 0.9550\n",
            "Epoch 237/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1549 - acc: 0.9500\n",
            "Epoch 00237: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1535 - acc: 0.9508 - val_loss: 0.1843 - val_acc: 0.9400\n",
            "Epoch 238/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9477\n",
            "Epoch 00238: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1513 - acc: 0.9483 - val_loss: 0.1568 - val_acc: 0.9550\n",
            "Epoch 239/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1703 - acc: 0.9432\n",
            "Epoch 00239: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1767 - acc: 0.9400 - val_loss: 0.1579 - val_acc: 0.9442\n",
            "Epoch 240/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1631 - acc: 0.9463\n",
            "Epoch 00240: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1636 - acc: 0.9461 - val_loss: 0.1665 - val_acc: 0.9483\n",
            "Epoch 241/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1549 - acc: 0.9488\n",
            "Epoch 00241: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1559 - acc: 0.9483 - val_loss: 0.1338 - val_acc: 0.9617\n",
            "Epoch 242/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1355 - acc: 0.9573\n",
            "Epoch 00242: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1382 - acc: 0.9558 - val_loss: 0.1359 - val_acc: 0.9617\n",
            "Epoch 243/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1649 - acc: 0.9452\n",
            "Epoch 00243: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1670 - acc: 0.9456 - val_loss: 0.1305 - val_acc: 0.9642\n",
            "Epoch 244/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1739 - acc: 0.9436\n",
            "Epoch 00244: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1732 - acc: 0.9439 - val_loss: 0.1730 - val_acc: 0.9467\n",
            "Epoch 245/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9506\n",
            "Epoch 00245: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1490 - acc: 0.9517 - val_loss: 0.1255 - val_acc: 0.9575\n",
            "Epoch 246/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1581 - acc: 0.9482\n",
            "Epoch 00246: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1589 - acc: 0.9475 - val_loss: 0.1568 - val_acc: 0.9617\n",
            "Epoch 247/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9526\n",
            "Epoch 00247: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1381 - acc: 0.9522 - val_loss: 0.1300 - val_acc: 0.9617\n",
            "Epoch 248/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1396 - acc: 0.9564\n",
            "Epoch 00248: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1412 - acc: 0.9567 - val_loss: 0.1261 - val_acc: 0.9600\n",
            "Epoch 249/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1589 - acc: 0.9453\n",
            "Epoch 00249: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1589 - acc: 0.9447 - val_loss: 0.1707 - val_acc: 0.9458\n",
            "Epoch 250/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9531\n",
            "Epoch 00250: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1413 - acc: 0.9539 - val_loss: 0.1558 - val_acc: 0.9525\n",
            "Epoch 251/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1438 - acc: 0.9545\n",
            "Epoch 00251: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1453 - acc: 0.9547 - val_loss: 0.2365 - val_acc: 0.9225\n",
            "Epoch 252/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9417\n",
            "Epoch 00252: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1759 - acc: 0.9417 - val_loss: 0.2076 - val_acc: 0.9342\n",
            "Epoch 253/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1638 - acc: 0.9491\n",
            "Epoch 00253: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1618 - acc: 0.9506 - val_loss: 0.1321 - val_acc: 0.9633\n",
            "Epoch 254/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9583\n",
            "Epoch 00254: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1407 - acc: 0.9578 - val_loss: 0.1563 - val_acc: 0.9475\n",
            "Epoch 255/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1563 - acc: 0.9491\n",
            "Epoch 00255: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1546 - acc: 0.9497 - val_loss: 0.1519 - val_acc: 0.9550\n",
            "Epoch 256/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9503\n",
            "Epoch 00256: val_loss did not improve from 0.12087\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1455 - acc: 0.9506 - val_loss: 0.1498 - val_acc: 0.9533\n",
            "Epoch 257/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9580\n",
            "Epoch 00257: val_loss improved from 0.12087 to 0.08340, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.1344 - acc: 0.9575 - val_loss: 0.0834 - val_acc: 0.9775\n",
            "Epoch 258/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9546\n",
            "Epoch 00258: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1439 - acc: 0.9550 - val_loss: 0.1625 - val_acc: 0.9492\n",
            "Epoch 259/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1541 - acc: 0.9500\n",
            "Epoch 00259: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1559 - acc: 0.9497 - val_loss: 0.1029 - val_acc: 0.9717\n",
            "Epoch 260/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1360 - acc: 0.9594\n",
            "Epoch 00260: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1363 - acc: 0.9594 - val_loss: 0.1349 - val_acc: 0.9567\n",
            "Epoch 261/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1426 - acc: 0.9538\n",
            "Epoch 00261: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1477 - acc: 0.9531 - val_loss: 0.1101 - val_acc: 0.9700\n",
            "Epoch 262/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9520\n",
            "Epoch 00262: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1394 - acc: 0.9522 - val_loss: 0.2083 - val_acc: 0.9292\n",
            "Epoch 263/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9569\n",
            "Epoch 00263: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1351 - acc: 0.9569 - val_loss: 0.1172 - val_acc: 0.9608\n",
            "Epoch 264/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.9534\n",
            "Epoch 00264: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1387 - acc: 0.9531 - val_loss: 0.1294 - val_acc: 0.9542\n",
            "Epoch 265/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.9543\n",
            "Epoch 00265: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1411 - acc: 0.9550 - val_loss: 0.0887 - val_acc: 0.9750\n",
            "Epoch 266/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1258 - acc: 0.9630\n",
            "Epoch 00266: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1286 - acc: 0.9614 - val_loss: 0.1331 - val_acc: 0.9658\n",
            "Epoch 267/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1407 - acc: 0.9509\n",
            "Epoch 00267: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1394 - acc: 0.9514 - val_loss: 0.1024 - val_acc: 0.9725\n",
            "Epoch 268/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1334 - acc: 0.9568\n",
            "Epoch 00268: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1339 - acc: 0.9561 - val_loss: 0.1118 - val_acc: 0.9658\n",
            "Epoch 269/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1528 - acc: 0.9518\n",
            "Epoch 00269: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1523 - acc: 0.9514 - val_loss: 0.1089 - val_acc: 0.9633\n",
            "Epoch 270/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1375 - acc: 0.9544\n",
            "Epoch 00270: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1368 - acc: 0.9553 - val_loss: 0.1100 - val_acc: 0.9642\n",
            "Epoch 271/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9574\n",
            "Epoch 00271: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1300 - acc: 0.9569 - val_loss: 0.0975 - val_acc: 0.9725\n",
            "Epoch 272/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1325 - acc: 0.9566\n",
            "Epoch 00272: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1321 - acc: 0.9564 - val_loss: 0.1268 - val_acc: 0.9650\n",
            "Epoch 273/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1295 - acc: 0.9582\n",
            "Epoch 00273: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1313 - acc: 0.9564 - val_loss: 0.1384 - val_acc: 0.9525\n",
            "Epoch 274/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1379 - acc: 0.9582\n",
            "Epoch 00274: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1370 - acc: 0.9592 - val_loss: 0.1175 - val_acc: 0.9650\n",
            "Epoch 275/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1489 - acc: 0.9544\n",
            "Epoch 00275: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1496 - acc: 0.9544 - val_loss: 0.1181 - val_acc: 0.9592\n",
            "Epoch 276/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1426 - acc: 0.9542\n",
            "Epoch 00276: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1434 - acc: 0.9525 - val_loss: 0.1162 - val_acc: 0.9608\n",
            "Epoch 277/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1443 - acc: 0.9512\n",
            "Epoch 00277: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1469 - acc: 0.9506 - val_loss: 0.1168 - val_acc: 0.9692\n",
            "Epoch 278/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9597\n",
            "Epoch 00278: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1312 - acc: 0.9589 - val_loss: 0.1207 - val_acc: 0.9658\n",
            "Epoch 279/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1356 - acc: 0.9591\n",
            "Epoch 00279: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1376 - acc: 0.9592 - val_loss: 0.1199 - val_acc: 0.9683\n",
            "Epoch 280/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1344 - acc: 0.9564\n",
            "Epoch 00280: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1363 - acc: 0.9553 - val_loss: 0.1186 - val_acc: 0.9650\n",
            "Epoch 281/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1346 - acc: 0.9563\n",
            "Epoch 00281: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1340 - acc: 0.9564 - val_loss: 0.0865 - val_acc: 0.9725\n",
            "Epoch 282/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1463 - acc: 0.9518\n",
            "Epoch 00282: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1487 - acc: 0.9517 - val_loss: 0.1122 - val_acc: 0.9642\n",
            "Epoch 283/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1282 - acc: 0.9579\n",
            "Epoch 00283: val_loss did not improve from 0.08340\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1281 - acc: 0.9578 - val_loss: 0.0942 - val_acc: 0.9683\n",
            "Epoch 284/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1401 - acc: 0.9538\n",
            "Epoch 00284: val_loss improved from 0.08340 to 0.07803, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.1419 - acc: 0.9533 - val_loss: 0.0780 - val_acc: 0.9767\n",
            "Epoch 285/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1585 - acc: 0.9497\n",
            "Epoch 00285: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1585 - acc: 0.9494 - val_loss: 0.1206 - val_acc: 0.9592\n",
            "Epoch 286/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1397 - acc: 0.9565\n",
            "Epoch 00286: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1418 - acc: 0.9561 - val_loss: 0.1331 - val_acc: 0.9625\n",
            "Epoch 287/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1481 - acc: 0.9526\n",
            "Epoch 00287: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1493 - acc: 0.9519 - val_loss: 0.0971 - val_acc: 0.9733\n",
            "Epoch 288/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1152 - acc: 0.9647\n",
            "Epoch 00288: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1156 - acc: 0.9647 - val_loss: 0.1168 - val_acc: 0.9650\n",
            "Epoch 289/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1170 - acc: 0.9641\n",
            "Epoch 00289: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1189 - acc: 0.9639 - val_loss: 0.1020 - val_acc: 0.9725\n",
            "Epoch 290/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1289 - acc: 0.9574\n",
            "Epoch 00290: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1304 - acc: 0.9572 - val_loss: 0.1144 - val_acc: 0.9667\n",
            "Epoch 291/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1427 - acc: 0.9545\n",
            "Epoch 00291: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1412 - acc: 0.9547 - val_loss: 0.1639 - val_acc: 0.9483\n",
            "Epoch 292/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1199 - acc: 0.9627\n",
            "Epoch 00292: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1185 - acc: 0.9633 - val_loss: 0.1307 - val_acc: 0.9583\n",
            "Epoch 293/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1394 - acc: 0.9562\n",
            "Epoch 00293: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1417 - acc: 0.9550 - val_loss: 0.1788 - val_acc: 0.9425\n",
            "Epoch 294/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1432 - acc: 0.9547\n",
            "Epoch 00294: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1439 - acc: 0.9544 - val_loss: 0.0960 - val_acc: 0.9683\n",
            "Epoch 295/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9566\n",
            "Epoch 00295: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1266 - acc: 0.9569 - val_loss: 0.1154 - val_acc: 0.9667\n",
            "Epoch 296/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1050 - acc: 0.9662\n",
            "Epoch 00296: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1058 - acc: 0.9656 - val_loss: 0.1143 - val_acc: 0.9675\n",
            "Epoch 297/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9663\n",
            "Epoch 00297: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1108 - acc: 0.9656 - val_loss: 0.0789 - val_acc: 0.9725\n",
            "Epoch 298/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1323 - acc: 0.9564\n",
            "Epoch 00298: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1330 - acc: 0.9572 - val_loss: 0.0985 - val_acc: 0.9675\n",
            "Epoch 299/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1320 - acc: 0.9561\n",
            "Epoch 00299: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1386 - acc: 0.9528 - val_loss: 0.1439 - val_acc: 0.9533\n",
            "Epoch 300/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1294 - acc: 0.9600\n",
            "Epoch 00300: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1288 - acc: 0.9600 - val_loss: 0.0843 - val_acc: 0.9792\n",
            "Epoch 301/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1327 - acc: 0.9541\n",
            "Epoch 00301: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1326 - acc: 0.9553 - val_loss: 0.1190 - val_acc: 0.9608\n",
            "Epoch 302/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1387 - acc: 0.9544\n",
            "Epoch 00302: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1376 - acc: 0.9547 - val_loss: 0.1165 - val_acc: 0.9642\n",
            "Epoch 303/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1197 - acc: 0.9609\n",
            "Epoch 00303: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1210 - acc: 0.9600 - val_loss: 0.1210 - val_acc: 0.9625\n",
            "Epoch 304/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1229 - acc: 0.9600\n",
            "Epoch 00304: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1237 - acc: 0.9606 - val_loss: 0.0968 - val_acc: 0.9675\n",
            "Epoch 305/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1158 - acc: 0.9647\n",
            "Epoch 00305: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1184 - acc: 0.9631 - val_loss: 0.1362 - val_acc: 0.9583\n",
            "Epoch 306/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1195 - acc: 0.9621\n",
            "Epoch 00306: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1205 - acc: 0.9619 - val_loss: 0.0948 - val_acc: 0.9658\n",
            "Epoch 307/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1066 - acc: 0.9638\n",
            "Epoch 00307: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1082 - acc: 0.9633 - val_loss: 0.0958 - val_acc: 0.9733\n",
            "Epoch 308/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1186 - acc: 0.9626\n",
            "Epoch 00308: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1202 - acc: 0.9622 - val_loss: 0.1053 - val_acc: 0.9642\n",
            "Epoch 309/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1412 - acc: 0.9518\n",
            "Epoch 00309: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1428 - acc: 0.9517 - val_loss: 0.0956 - val_acc: 0.9733\n",
            "Epoch 310/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1214 - acc: 0.9626\n",
            "Epoch 00310: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1202 - acc: 0.9636 - val_loss: 0.0850 - val_acc: 0.9700\n",
            "Epoch 311/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1189 - acc: 0.9609\n",
            "Epoch 00311: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1191 - acc: 0.9606 - val_loss: 0.1004 - val_acc: 0.9683\n",
            "Epoch 312/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1270 - acc: 0.9618\n",
            "Epoch 00312: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1268 - acc: 0.9622 - val_loss: 0.0975 - val_acc: 0.9700\n",
            "Epoch 313/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1266 - acc: 0.9621\n",
            "Epoch 00313: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1239 - acc: 0.9636 - val_loss: 0.1208 - val_acc: 0.9658\n",
            "Epoch 314/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1353 - acc: 0.9526\n",
            "Epoch 00314: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1353 - acc: 0.9525 - val_loss: 0.0893 - val_acc: 0.9700\n",
            "Epoch 315/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9666\n",
            "Epoch 00315: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1159 - acc: 0.9667 - val_loss: 0.0941 - val_acc: 0.9742\n",
            "Epoch 316/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1424 - acc: 0.9506\n",
            "Epoch 00316: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1432 - acc: 0.9497 - val_loss: 0.1003 - val_acc: 0.9708\n",
            "Epoch 317/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1164 - acc: 0.9641\n",
            "Epoch 00317: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1175 - acc: 0.9642 - val_loss: 0.1270 - val_acc: 0.9650\n",
            "Epoch 318/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1395 - acc: 0.9512\n",
            "Epoch 00318: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1417 - acc: 0.9500 - val_loss: 0.1004 - val_acc: 0.9683\n",
            "Epoch 319/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1338 - acc: 0.9600\n",
            "Epoch 00319: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1309 - acc: 0.9600 - val_loss: 0.0961 - val_acc: 0.9700\n",
            "Epoch 320/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1222 - acc: 0.9636\n",
            "Epoch 00320: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1237 - acc: 0.9625 - val_loss: 0.1379 - val_acc: 0.9533\n",
            "Epoch 321/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1154 - acc: 0.9624\n",
            "Epoch 00321: val_loss did not improve from 0.07803\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1161 - acc: 0.9614 - val_loss: 0.1059 - val_acc: 0.9683\n",
            "Epoch 322/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1400 - acc: 0.9491\n",
            "Epoch 00322: val_loss improved from 0.07803 to 0.07662, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.1386 - acc: 0.9497 - val_loss: 0.0766 - val_acc: 0.9783\n",
            "Epoch 323/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1464 - acc: 0.9521\n",
            "Epoch 00323: val_loss did not improve from 0.07662\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1471 - acc: 0.9522 - val_loss: 0.1054 - val_acc: 0.9675\n",
            "Epoch 324/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1233 - acc: 0.9626\n",
            "Epoch 00324: val_loss did not improve from 0.07662\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1237 - acc: 0.9617 - val_loss: 0.0997 - val_acc: 0.9725\n",
            "Epoch 325/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.9611\n",
            "Epoch 00325: val_loss did not improve from 0.07662\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1205 - acc: 0.9617 - val_loss: 0.1254 - val_acc: 0.9675\n",
            "Epoch 326/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9551\n",
            "Epoch 00326: val_loss did not improve from 0.07662\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1239 - acc: 0.9547 - val_loss: 0.1115 - val_acc: 0.9650\n",
            "Epoch 327/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1249 - acc: 0.9591\n",
            "Epoch 00327: val_loss did not improve from 0.07662\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1259 - acc: 0.9586 - val_loss: 0.1014 - val_acc: 0.9742\n",
            "Epoch 328/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1140 - acc: 0.9626\n",
            "Epoch 00328: val_loss did not improve from 0.07662\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1144 - acc: 0.9625 - val_loss: 0.1169 - val_acc: 0.9625\n",
            "Epoch 329/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1164 - acc: 0.9618\n",
            "Epoch 00329: val_loss did not improve from 0.07662\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1195 - acc: 0.9611 - val_loss: 0.1134 - val_acc: 0.9667\n",
            "Epoch 330/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9620\n",
            "Epoch 00330: val_loss did not improve from 0.07662\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1227 - acc: 0.9617 - val_loss: 0.0822 - val_acc: 0.9783\n",
            "Epoch 331/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9603\n",
            "Epoch 00331: val_loss did not improve from 0.07662\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1277 - acc: 0.9603 - val_loss: 0.0826 - val_acc: 0.9767\n",
            "Epoch 332/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1184 - acc: 0.9618\n",
            "Epoch 00332: val_loss improved from 0.07662 to 0.07363, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.1190 - acc: 0.9633 - val_loss: 0.0736 - val_acc: 0.9825\n",
            "Epoch 333/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1180 - acc: 0.9641\n",
            "Epoch 00333: val_loss did not improve from 0.07363\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1149 - acc: 0.9653 - val_loss: 0.1015 - val_acc: 0.9675\n",
            "Epoch 334/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1420 - acc: 0.9542\n",
            "Epoch 00334: val_loss did not improve from 0.07363\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1388 - acc: 0.9558 - val_loss: 0.0935 - val_acc: 0.9733\n",
            "Epoch 335/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1172 - acc: 0.9618\n",
            "Epoch 00335: val_loss did not improve from 0.07363\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1207 - acc: 0.9597 - val_loss: 0.0950 - val_acc: 0.9750\n",
            "Epoch 336/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9617\n",
            "Epoch 00336: val_loss did not improve from 0.07363\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1185 - acc: 0.9619 - val_loss: 0.0993 - val_acc: 0.9692\n",
            "Epoch 337/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1380 - acc: 0.9573\n",
            "Epoch 00337: val_loss did not improve from 0.07363\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1360 - acc: 0.9581 - val_loss: 0.1070 - val_acc: 0.9692\n",
            "Epoch 338/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1197 - acc: 0.9612\n",
            "Epoch 00338: val_loss improved from 0.07363 to 0.06963, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.1199 - acc: 0.9608 - val_loss: 0.0696 - val_acc: 0.9808\n",
            "Epoch 339/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1290 - acc: 0.9597\n",
            "Epoch 00339: val_loss did not improve from 0.06963\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1260 - acc: 0.9608 - val_loss: 0.0895 - val_acc: 0.9742\n",
            "Epoch 340/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1121 - acc: 0.9647\n",
            "Epoch 00340: val_loss did not improve from 0.06963\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1129 - acc: 0.9642 - val_loss: 0.1207 - val_acc: 0.9583\n",
            "Epoch 341/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1133 - acc: 0.9673\n",
            "Epoch 00341: val_loss did not improve from 0.06963\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1126 - acc: 0.9678 - val_loss: 0.1018 - val_acc: 0.9717\n",
            "Epoch 342/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1200 - acc: 0.9606\n",
            "Epoch 00342: val_loss did not improve from 0.06963\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1183 - acc: 0.9617 - val_loss: 0.0902 - val_acc: 0.9758\n",
            "Epoch 343/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1174 - acc: 0.9627\n",
            "Epoch 00343: val_loss did not improve from 0.06963\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1203 - acc: 0.9625 - val_loss: 0.0998 - val_acc: 0.9692\n",
            "Epoch 344/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9620\n",
            "Epoch 00344: val_loss did not improve from 0.06963\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1172 - acc: 0.9614 - val_loss: 0.1026 - val_acc: 0.9708\n",
            "Epoch 345/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1060 - acc: 0.9669\n",
            "Epoch 00345: val_loss did not improve from 0.06963\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1080 - acc: 0.9667 - val_loss: 0.0984 - val_acc: 0.9675\n",
            "Epoch 346/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1096 - acc: 0.9659\n",
            "Epoch 00346: val_loss did not improve from 0.06963\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1126 - acc: 0.9650 - val_loss: 0.0721 - val_acc: 0.9775\n",
            "Epoch 347/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1233 - acc: 0.9609\n",
            "Epoch 00347: val_loss did not improve from 0.06963\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1230 - acc: 0.9617 - val_loss: 0.0927 - val_acc: 0.9742\n",
            "Epoch 348/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1197 - acc: 0.9609\n",
            "Epoch 00348: val_loss did not improve from 0.06963\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1196 - acc: 0.9600 - val_loss: 0.1316 - val_acc: 0.9575\n",
            "Epoch 349/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1170 - acc: 0.9621\n",
            "Epoch 00349: val_loss did not improve from 0.06963\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1190 - acc: 0.9611 - val_loss: 0.1179 - val_acc: 0.9642\n",
            "Epoch 350/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1091 - acc: 0.9648\n",
            "Epoch 00350: val_loss did not improve from 0.06963\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1093 - acc: 0.9644 - val_loss: 0.0876 - val_acc: 0.9775\n",
            "Epoch 351/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0918 - acc: 0.9755\n",
            "Epoch 00351: val_loss did not improve from 0.06963\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0927 - acc: 0.9747 - val_loss: 0.1174 - val_acc: 0.9625\n",
            "Epoch 352/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1014 - acc: 0.9674\n",
            "Epoch 00352: val_loss did not improve from 0.06963\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0997 - acc: 0.9678 - val_loss: 0.0750 - val_acc: 0.9758\n",
            "Epoch 353/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1111 - acc: 0.9603\n",
            "Epoch 00353: val_loss did not improve from 0.06963\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1113 - acc: 0.9606 - val_loss: 0.0947 - val_acc: 0.9708\n",
            "Epoch 354/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9611\n",
            "Epoch 00354: val_loss did not improve from 0.06963\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1176 - acc: 0.9614 - val_loss: 0.1102 - val_acc: 0.9675\n",
            "Epoch 355/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1232 - acc: 0.9582\n",
            "Epoch 00355: val_loss did not improve from 0.06963\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1254 - acc: 0.9572 - val_loss: 0.1023 - val_acc: 0.9725\n",
            "Epoch 356/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1116 - acc: 0.9624\n",
            "Epoch 00356: val_loss did not improve from 0.06963\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1110 - acc: 0.9622 - val_loss: 0.0933 - val_acc: 0.9675\n",
            "Epoch 357/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9694\n",
            "Epoch 00357: val_loss improved from 0.06963 to 0.06631, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.1070 - acc: 0.9689 - val_loss: 0.0663 - val_acc: 0.9767\n",
            "Epoch 358/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1146 - acc: 0.9648\n",
            "Epoch 00358: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1155 - acc: 0.9642 - val_loss: 0.0984 - val_acc: 0.9725\n",
            "Epoch 359/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9634\n",
            "Epoch 00359: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1138 - acc: 0.9636 - val_loss: 0.0918 - val_acc: 0.9683\n",
            "Epoch 360/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9666\n",
            "Epoch 00360: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1191 - acc: 0.9664 - val_loss: 0.1026 - val_acc: 0.9708\n",
            "Epoch 361/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9649\n",
            "Epoch 00361: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1137 - acc: 0.9647 - val_loss: 0.0928 - val_acc: 0.9775\n",
            "Epoch 362/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1079 - acc: 0.9697\n",
            "Epoch 00362: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1059 - acc: 0.9689 - val_loss: 0.0930 - val_acc: 0.9750\n",
            "Epoch 363/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9737\n",
            "Epoch 00363: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0889 - acc: 0.9731 - val_loss: 0.1309 - val_acc: 0.9542\n",
            "Epoch 364/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1067 - acc: 0.9644\n",
            "Epoch 00364: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1061 - acc: 0.9656 - val_loss: 0.0759 - val_acc: 0.9775\n",
            "Epoch 365/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1190 - acc: 0.9603\n",
            "Epoch 00365: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1196 - acc: 0.9597 - val_loss: 0.1369 - val_acc: 0.9617\n",
            "Epoch 366/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1107 - acc: 0.9665\n",
            "Epoch 00366: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1109 - acc: 0.9661 - val_loss: 0.0736 - val_acc: 0.9808\n",
            "Epoch 367/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1019 - acc: 0.9676\n",
            "Epoch 00367: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1020 - acc: 0.9675 - val_loss: 0.0777 - val_acc: 0.9758\n",
            "Epoch 368/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9637\n",
            "Epoch 00368: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1125 - acc: 0.9619 - val_loss: 0.0916 - val_acc: 0.9733\n",
            "Epoch 369/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9609\n",
            "Epoch 00369: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1254 - acc: 0.9606 - val_loss: 0.1415 - val_acc: 0.9567\n",
            "Epoch 370/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9660\n",
            "Epoch 00370: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1048 - acc: 0.9661 - val_loss: 0.0726 - val_acc: 0.9758\n",
            "Epoch 371/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.9626\n",
            "Epoch 00371: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1190 - acc: 0.9622 - val_loss: 0.1290 - val_acc: 0.9608\n",
            "Epoch 372/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1162 - acc: 0.9647\n",
            "Epoch 00372: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1177 - acc: 0.9639 - val_loss: 0.1166 - val_acc: 0.9642\n",
            "Epoch 373/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1015 - acc: 0.9639\n",
            "Epoch 00373: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1047 - acc: 0.9636 - val_loss: 0.0792 - val_acc: 0.9775\n",
            "Epoch 374/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1114 - acc: 0.9644\n",
            "Epoch 00374: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1116 - acc: 0.9639 - val_loss: 0.1105 - val_acc: 0.9692\n",
            "Epoch 375/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1090 - acc: 0.9648\n",
            "Epoch 00375: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1099 - acc: 0.9644 - val_loss: 0.0976 - val_acc: 0.9675\n",
            "Epoch 376/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1045 - acc: 0.9691\n",
            "Epoch 00376: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1036 - acc: 0.9689 - val_loss: 0.1234 - val_acc: 0.9567\n",
            "Epoch 377/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9640\n",
            "Epoch 00377: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1120 - acc: 0.9636 - val_loss: 0.0909 - val_acc: 0.9725\n",
            "Epoch 378/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1163 - acc: 0.9609\n",
            "Epoch 00378: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1188 - acc: 0.9597 - val_loss: 0.1057 - val_acc: 0.9700\n",
            "Epoch 379/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1049 - acc: 0.9653\n",
            "Epoch 00379: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1050 - acc: 0.9653 - val_loss: 0.1040 - val_acc: 0.9700\n",
            "Epoch 380/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1104 - acc: 0.9679\n",
            "Epoch 00380: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1092 - acc: 0.9686 - val_loss: 0.0789 - val_acc: 0.9742\n",
            "Epoch 381/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1108 - acc: 0.9659\n",
            "Epoch 00381: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1136 - acc: 0.9656 - val_loss: 0.1050 - val_acc: 0.9692\n",
            "Epoch 382/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9626\n",
            "Epoch 00382: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1129 - acc: 0.9628 - val_loss: 0.0674 - val_acc: 0.9800\n",
            "Epoch 383/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1157 - acc: 0.9609\n",
            "Epoch 00383: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1161 - acc: 0.9611 - val_loss: 0.0892 - val_acc: 0.9717\n",
            "Epoch 384/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1163 - acc: 0.9653\n",
            "Epoch 00384: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1178 - acc: 0.9639 - val_loss: 0.1142 - val_acc: 0.9642\n",
            "Epoch 385/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1157 - acc: 0.9612\n",
            "Epoch 00385: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1214 - acc: 0.9600 - val_loss: 0.0834 - val_acc: 0.9733\n",
            "Epoch 386/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1350 - acc: 0.9541\n",
            "Epoch 00386: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1331 - acc: 0.9550 - val_loss: 0.0951 - val_acc: 0.9775\n",
            "Epoch 387/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1162 - acc: 0.9633\n",
            "Epoch 00387: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1182 - acc: 0.9628 - val_loss: 0.1361 - val_acc: 0.9608\n",
            "Epoch 388/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1091 - acc: 0.9691\n",
            "Epoch 00388: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1112 - acc: 0.9681 - val_loss: 0.1101 - val_acc: 0.9633\n",
            "Epoch 389/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1125 - acc: 0.9621\n",
            "Epoch 00389: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1120 - acc: 0.9625 - val_loss: 0.0773 - val_acc: 0.9767\n",
            "Epoch 390/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1123 - acc: 0.9638\n",
            "Epoch 00390: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1119 - acc: 0.9642 - val_loss: 0.0987 - val_acc: 0.9675\n",
            "Epoch 391/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9654\n",
            "Epoch 00391: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1107 - acc: 0.9644 - val_loss: 0.0895 - val_acc: 0.9683\n",
            "Epoch 392/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9671\n",
            "Epoch 00392: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1085 - acc: 0.9672 - val_loss: 0.1124 - val_acc: 0.9733\n",
            "Epoch 393/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9646\n",
            "Epoch 00393: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1038 - acc: 0.9636 - val_loss: 0.0969 - val_acc: 0.9742\n",
            "Epoch 394/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1176 - acc: 0.9581\n",
            "Epoch 00394: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1153 - acc: 0.9594 - val_loss: 0.0936 - val_acc: 0.9792\n",
            "Epoch 395/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0985 - acc: 0.9709\n",
            "Epoch 00395: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0984 - acc: 0.9711 - val_loss: 0.0777 - val_acc: 0.9742\n",
            "Epoch 396/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1060 - acc: 0.9679\n",
            "Epoch 00396: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1061 - acc: 0.9675 - val_loss: 0.0760 - val_acc: 0.9800\n",
            "Epoch 397/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1131 - acc: 0.9618\n",
            "Epoch 00397: val_loss improved from 0.06631 to 0.06483, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.1126 - acc: 0.9614 - val_loss: 0.0648 - val_acc: 0.9875\n",
            "Epoch 398/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9671\n",
            "Epoch 00398: val_loss did not improve from 0.06483\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1033 - acc: 0.9669 - val_loss: 0.0714 - val_acc: 0.9775\n",
            "Epoch 399/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1090 - acc: 0.9615\n",
            "Epoch 00399: val_loss did not improve from 0.06483\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1118 - acc: 0.9617 - val_loss: 0.0875 - val_acc: 0.9775\n",
            "Epoch 400/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9683\n",
            "Epoch 00400: val_loss did not improve from 0.06483\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0985 - acc: 0.9678 - val_loss: 0.0847 - val_acc: 0.9742\n",
            "1200/1200 [==============================] - 0s 125us/sample - loss: 0.0847 - acc: 0.9742\n",
            "[0.0847362923870484, 0.9741667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P06AGnX9wIs4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "278f5691-182e-4d6d-b132-6b859c3ece0b"
      },
      "source": [
        "for i in range(11, 12): # Итерација низ секој испитен примерок\n",
        "  print(f\"====================== Примерок ({i}) ======================\")\n",
        "  print(\"Вчитување тест податоци од испитниот примерок \" + str(i) + \"...\")\n",
        "  \n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_test_data = np.empty(0)\n",
        "  temp_test_events = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_test_set = 'S' + format(j, '02') + '/Test'\n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testData.mat\"\n",
        "    temp = loadmat(full_path)['testData']\n",
        "    if temp_test_data.size != 0:\n",
        "      temp_test_data = np.concatenate((temp_test_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_test_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_test_events.size != 0:\n",
        "        temp_test_events = np.append(temp_test_events, temp)\n",
        "      else:\n",
        "        temp_test_events = np.array(temp)\n",
        "\n",
        "    # Вчитување на бројот на runs \n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/runs_per_block.txt\"\n",
        "    with open(full_path, \"r\") as runs_per_block:\n",
        "      test_runs_per_block[i-1][j-1] = int(runs_per_block.read())\n",
        "\n",
        "    print(\"\\t - Тест податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "  # Зачувај ги тест податоците вчитани од испитниот примерок во низа\n",
        "  test_data.append(temp_test_data)\n",
        "  test_events.append(temp_test_events)\n",
        "  print(\"Тест податоците од испитниот примерок \" + str(i) + \" се вчитани.\\n\")\n",
        "\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_data: \" + str(test_data[i-1].shape)) # test_data to predict\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_events: \" + str(len(test_events[i-1]))) # test_events\n",
        "  for j in range (1,4):\n",
        "    print(\"SBJ\" + str(format(i-1, '02')) + \" / S\" + str(format(j-1, '02')) + \"| Runs per block: \" + str(test_runs_per_block[i-1][j-1])) # runs per block in SJB01, SJ00 \n",
        "\n",
        "  to_predict_data = reshape_data_to_mne_format(test_data[i-1])\n",
        "  predictions = model11.predict(to_predict_data)\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Predictions: \" + str(len(predictions)))\n",
        "  # np.savetxt(\"predictions.csv\", predictions, delimiter=\",\")\n",
        "\n",
        "\n",
        "  # ========= FALI USTE DA SE ISPARSIRA PREDICTIONOT... NE E SREDEN OVOJ KOD DOLE =======\n",
        "\n",
        "  int_pred = np.argmax(predictions, axis=1)\n",
        "  int_ytest = np.argmax(y_test, axis=1)\n",
        "\n",
        "  session_start = 0\n",
        "  start_prediction_index = 0\n",
        "  end_prediction_index = 0\n",
        "  for session in range(0, 3):\n",
        "    print(f\"============== Сесија ({session}) ==============\")\n",
        "    for block in range(0, 50):    \n",
        "      events_per_block = test_runs_per_block[i-1][session]\n",
        "\n",
        "      start_prediction_index = session_start + (block*events_per_block)*8\n",
        "      end_prediction_index = session_start + ((block+1)*events_per_block)*8\n",
        "\n",
        "      block_prediction = int_pred[start_prediction_index:end_prediction_index]\n",
        "      prediction = np.bincount(block_prediction).argmax()\n",
        "      df.iat[session+30,block+2] = prediction+1\n",
        "      # UNCOMMENT ZA PODOBAR PRIKAZ :)\n",
        "      # print(f\"Session {session} | Block: {block} | Prediction: {prediction} | Address: {end_prediction_index}\")\n",
        "\n",
        "      print(str(prediction+1) + \",\", end=\"\")\n",
        "    session_start = end_prediction_index\n",
        "    print(\"\")\n",
        "  print(\"Stigna li do kraj: \" + str(session_start == len(predictions)))\n",
        "  print(f\"====================== Примерок ({i}) ======================\\n\\n\")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Примерок (11) ======================\n",
            "Вчитување тест податоци од испитниот примерок 11...\n",
            "\t - Тест податоците од сесија 1 се вчитани.\n",
            "\t - Тест податоците од сесија 2 се вчитани.\n",
            "\t - Тест податоците од сесија 3 се вчитани.\n",
            "Тест податоците од испитниот примерок 11 се вчитани.\n",
            "\n",
            "SBJ10| Test_data: (8, 350, 8000)\n",
            "SBJ10| Test_events: 8000\n",
            "SBJ10 / S00| Runs per block: 8\n",
            "SBJ10 / S01| Runs per block: 4\n",
            "SBJ10 / S02| Runs per block: 8\n",
            "SBJ10| Predictions: 8000\n",
            "============== Сесија (0) ==============\n",
            "3,4,4,4,3,4,3,8,4,3,3,2,4,4,1,3,3,3,3,3,1,4,3,1,1,3,4,4,4,4,3,4,4,2,1,3,3,4,7,4,4,3,5,4,3,1,3,4,4,4,\n",
            "============== Сесија (1) ==============\n",
            "3,1,1,4,8,4,4,2,8,8,3,3,1,4,4,3,1,1,7,2,1,1,7,1,3,3,8,8,1,3,3,3,7,3,3,7,3,3,3,7,2,3,1,7,3,8,2,3,8,3,\n",
            "============== Сесија (2) ==============\n",
            "4,2,4,4,4,4,4,4,8,8,8,8,8,8,4,8,8,4,4,4,8,4,8,8,3,1,4,4,6,2,4,4,8,4,4,8,4,1,7,7,4,4,8,4,4,7,8,4,8,4,\n",
            "Stigna li do kraj: True\n",
            "====================== Примерок (11) ======================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC2ByIP8wTDn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d2d3488f-335b-49f1-b67e-6f3fe7a56453"
      },
      "source": [
        "df"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUBJECT</th>\n",
              "      <th>SESSION</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>Unnamed: 52</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    SUBJECT  SESSION  1  2  3  4  5  6  7  ... 43 44 45 46 47 48 49 50 Unnamed: 52\n",
              "0         1        1  6  6  3  6  6  3  6  ...  6  4  8  3  4  5  5  7         NaN\n",
              "1         1        2  6  3  2  1  2  1  3  ...  6  2  2  2  3  6  6  2         NaN\n",
              "2         1        3  3  3  3  3  3  3  3  ...  3  3  6  3  6  7  1  6         NaN\n",
              "3         2        1  8  8  8  8  8  8  8  ...  8  4  8  4  8  7  8  8         NaN\n",
              "4         2        2  2  7  6  6  6  6  7  ...  2  6  6  2  6  6  2  2         NaN\n",
              "5         2        3  2  7  2  6  7  4  2  ...  2  6  2  2  7  2  2  2         NaN\n",
              "6         3        1  3  3  4  4  4  3  6  ...  4  6  3  3  4  3  4  4         NaN\n",
              "7         3        2  6  1  7  7  7  7  7  ...  6  7  1  6  6  6  6  6         NaN\n",
              "8         3        3  6  4  8  8  5  7  8  ...  4  2  8  2  2  2  2  8         NaN\n",
              "9         4        1  1  1  2  1  5  5  6  ...  1  1  7  1  6  6  6  6         NaN\n",
              "10        4        2  7  7  7  7  6  6  7  ...  1  5  5  5  5  5  2  6         NaN\n",
              "11        4        3  7  3  7  3  6  6  7  ...  6  1  4  4  4  6  6  6         NaN\n",
              "12        5        1  5  4  4  4  6  4  5  ...  3  1  4  4  1  3  3  5         NaN\n",
              "13        5        2  3  6  3  5  2  6  7  ...  6  6  6  6  6  6  6  4         NaN\n",
              "14        5        3  4  3  3  6  5  7  6  ...  7  2  2  7  5  6  4  2         NaN\n",
              "15        6        1  1  3  1  8  3  3  3  ...  4  3  7  2  7  2  5  8         NaN\n",
              "16        6        2  3  4  1  7  1  1  1  ...  5  5  5  5  5  5  5  5         NaN\n",
              "17        6        3  2  3  2  5  3  3  3  ...  2  3  3  3  1  1  3  3         NaN\n",
              "18        7        1  4  7  5  7  4  5  4  ...  5  1  1  4  4  4  1  4         NaN\n",
              "19        7        2  2  2  2  8  2  5  2  ...  5  5  5  2  2  2  2  2         NaN\n",
              "20        7        3  2  7  7  5  7  5  4  ...  1  3  5  5  3  5  5  1         NaN\n",
              "21        8        1  5  8  2  8  2  2  1  ...  1  8  2  2  5  5  2  8         NaN\n",
              "22        8        2  2  7  1  5  1  2  1  ...  7  1  7  8  2  1  7  7         NaN\n",
              "23        8        3  4  1  6  6  1  2  1  ...  1  8  1  7  1  1  7  1         NaN\n",
              "24        9        1  6  5  6  6  7  6  8  ...  6  6  6  5  5  5  3  5         NaN\n",
              "25        9        2  3  4  8  4  1  6  1  ...  1  1  1  1  1  3  6  6         NaN\n",
              "26        9        3  5  2  2  2  2  3  2  ...  2  2  2  3  2  2  4  2         NaN\n",
              "27       10        1  1  1  8  7  3  1  1  ...  5  1  1  1  1  5  1  5         NaN\n",
              "28       10        2  1  5  5  6  1  1  1  ...  5  5  6  6  2  1  2  1         NaN\n",
              "29       10        3  5  8  5  5  5  6  5  ...  5  5  2  5  5  5  5  5         NaN\n",
              "30       11        1  3  4  4  4  3  4  3  ...  5  4  3  1  3  4  4  4         NaN\n",
              "31       11        2  3  1  1  4  8  4  4  ...  1  7  3  8  2  3  8  3         NaN\n",
              "32       11        3  4  2  4  4  4  4  4  ...  8  4  4  7  8  4  8  4         NaN\n",
              "33       12        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "34       12        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "35       12        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "36       13        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "37       13        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "38       13        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "39       14        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "40       14        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "41       14        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "42       15        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "43       15        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "44       15        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "\n",
              "[45 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrlcdvI7wwCs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6fe8d1e8-ecd7-41e4-a48e-29289443bdaa"
      },
      "source": [
        "for i in range(12, 13): # Итерација низ секој испитен примерок\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_data = np.empty(0)\n",
        "  temp_labels = np.empty(0)\n",
        "  temp_events = np.empty(0)\n",
        "  temp_targets = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainData.mat\"\n",
        "    temp = loadmat(full_path)['trainData']\n",
        "    if temp_data.size != 0:\n",
        "      temp_data = np.concatenate((temp_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на label-ите\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainLabels.txt\"\n",
        "    with open(full_path, \"r\") as file_labels:\n",
        "      temp = file_labels.read().splitlines()\n",
        "      temp = np.repeat(temp, 8*10)\n",
        "      if temp_labels.size != 0:\n",
        "        temp_labels = np.concatenate((temp_labels, temp))\n",
        "      else:\n",
        "        temp_labels = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_events.size != 0:\n",
        "        temp_events = np.append(temp_events, temp)\n",
        "      else:\n",
        "        temp_events = np.array(temp)\n",
        "      \n",
        "\n",
        "    # Вчитување на редоследот на објекти кои се target\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainTargets.txt\"\n",
        "    with open(full_path, \"r\") as file_targets:\n",
        "      temp = file_targets.read().splitlines()\n",
        "      if temp_targets.size != 0:\n",
        "        temp_targets = np.concatenate((temp_targets, temp))\n",
        "      else:\n",
        "        temp_targets = np.array(temp)\n",
        "    print(\"\\t - Податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "\n",
        "  for j in range(4, 8): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "      \n",
        "  # Зачувај ги податоците вчитани од испитниот примерок во низа\n",
        "  data.append(temp_data)\n",
        "  labels.append(temp_labels)\n",
        "  events.append(temp_events)\n",
        "  targets.append(temp_targets)\n",
        "\n",
        "  \n",
        "  print(\"Податоците од испитниот примерок \" + str(i) + \" се вчитани.\")\n",
        "\n",
        "\n",
        "  #data = target_events_data_scaled\n",
        "  mne_array = np.swapaxes(data[i-1], 0, 2) # (епохa, канал, настан). \n",
        "  mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "  mne_array = mne_array.reshape(mne_array.shape[0],1, 8,350)\n",
        "  print(mne_array.shape)\n",
        "\n",
        "  events_arr = events[i-1].astype(np.int)\n",
        "  labels_arr = labels[i-1].astype(np.int)\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(mne_array, labels_arr-1, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "  model12 = DeepConvNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "  model12.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = Adam(0.0009))\n",
        "  checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                                verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "  #clf = RandomForestClassifier(max_depth=5)\n",
        "  #clf.fit(X_train, y_train)\n",
        "  #score = clf.score(X_test, y_test)\n",
        "  # print(score)\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  num_batch_size=100\n",
        "  num_epochs=400\n",
        "  model12.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "            validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "  score = model12.evaluate(X_test, y_test, verbose=1)\n",
        "  print(score)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t - Податоците од сесија 1 се вчитани.\n",
            "\t - Податоците од сесија 2 се вчитани.\n",
            "\t - Податоците од сесија 3 се вчитани.\n",
            "Податоците од испитниот примерок 12 се вчитани.\n",
            "(4800, 1, 8, 350)\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Train on 3600 samples, validate on 1200 samples\n",
            "Epoch 1/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.2845 - acc: 0.1826\n",
            "Epoch 00001: val_loss improved from inf to 2.53376, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 8s 2ms/sample - loss: 2.2707 - acc: 0.1847 - val_loss: 2.5338 - val_acc: 0.1150\n",
            "Epoch 2/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 2.0837 - acc: 0.2300\n",
            "Epoch 00002: val_loss did not improve from 2.53376\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 2.0852 - acc: 0.2317 - val_loss: 2.7278 - val_acc: 0.1258\n",
            "Epoch 3/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.9880 - acc: 0.2885\n",
            "Epoch 00003: val_loss improved from 2.53376 to 2.18525, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 1.9843 - acc: 0.2858 - val_loss: 2.1852 - val_acc: 0.1875\n",
            "Epoch 4/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.8684 - acc: 0.3185\n",
            "Epoch 00004: val_loss improved from 2.18525 to 2.05964, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 1.8628 - acc: 0.3225 - val_loss: 2.0596 - val_acc: 0.2558\n",
            "Epoch 5/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.7699 - acc: 0.3532\n",
            "Epoch 00005: val_loss improved from 2.05964 to 1.94765, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 1.7674 - acc: 0.3533 - val_loss: 1.9476 - val_acc: 0.2883\n",
            "Epoch 6/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.6959 - acc: 0.3862\n",
            "Epoch 00006: val_loss did not improve from 1.94765\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 1.6920 - acc: 0.3861 - val_loss: 2.1360 - val_acc: 0.2883\n",
            "Epoch 7/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.5519 - acc: 0.4280\n",
            "Epoch 00007: val_loss did not improve from 1.94765\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 1.5581 - acc: 0.4253 - val_loss: 1.9491 - val_acc: 0.3133\n",
            "Epoch 8/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.5374 - acc: 0.4358\n",
            "Epoch 00008: val_loss did not improve from 1.94765\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 1.5327 - acc: 0.4408 - val_loss: 2.1340 - val_acc: 0.2942\n",
            "Epoch 9/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4766 - acc: 0.4521\n",
            "Epoch 00009: val_loss improved from 1.94765 to 1.80355, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 1.4756 - acc: 0.4528 - val_loss: 1.8036 - val_acc: 0.3650\n",
            "Epoch 10/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.4115 - acc: 0.4749\n",
            "Epoch 00010: val_loss did not improve from 1.80355\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 1.4132 - acc: 0.4744 - val_loss: 1.8068 - val_acc: 0.3792\n",
            "Epoch 11/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.3810 - acc: 0.4891\n",
            "Epoch 00011: val_loss improved from 1.80355 to 1.62093, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 1.3863 - acc: 0.4875 - val_loss: 1.6209 - val_acc: 0.4383\n",
            "Epoch 12/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.3230 - acc: 0.5063\n",
            "Epoch 00012: val_loss did not improve from 1.62093\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 1.3184 - acc: 0.5097 - val_loss: 1.6844 - val_acc: 0.4100\n",
            "Epoch 13/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.2863 - acc: 0.5200\n",
            "Epoch 00013: val_loss improved from 1.62093 to 1.50462, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 1.2881 - acc: 0.5186 - val_loss: 1.5046 - val_acc: 0.4583\n",
            "Epoch 14/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.2174 - acc: 0.5469\n",
            "Epoch 00014: val_loss did not improve from 1.50462\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 1.2316 - acc: 0.5444 - val_loss: 1.5417 - val_acc: 0.4333\n",
            "Epoch 15/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.2067 - acc: 0.5491\n",
            "Epoch 00015: val_loss improved from 1.50462 to 1.46252, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 1.2036 - acc: 0.5500 - val_loss: 1.4625 - val_acc: 0.4750\n",
            "Epoch 16/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1688 - acc: 0.5791\n",
            "Epoch 00016: val_loss improved from 1.46252 to 1.33344, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 1.1769 - acc: 0.5742 - val_loss: 1.3334 - val_acc: 0.5217\n",
            "Epoch 17/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.1396 - acc: 0.5797\n",
            "Epoch 00017: val_loss improved from 1.33344 to 1.29007, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 1.1408 - acc: 0.5775 - val_loss: 1.2901 - val_acc: 0.5217\n",
            "Epoch 18/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.0919 - acc: 0.6016\n",
            "Epoch 00018: val_loss did not improve from 1.29007\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 1.0974 - acc: 0.5967 - val_loss: 1.3539 - val_acc: 0.5050\n",
            "Epoch 19/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0606 - acc: 0.6076\n",
            "Epoch 00019: val_loss did not improve from 1.29007\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 1.0694 - acc: 0.6033 - val_loss: 1.3662 - val_acc: 0.4942\n",
            "Epoch 20/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0136 - acc: 0.6239\n",
            "Epoch 00020: val_loss improved from 1.29007 to 1.21292, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 1.0163 - acc: 0.6211 - val_loss: 1.2129 - val_acc: 0.5642\n",
            "Epoch 21/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.0251 - acc: 0.6277\n",
            "Epoch 00021: val_loss improved from 1.21292 to 1.20968, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 1.0234 - acc: 0.6292 - val_loss: 1.2097 - val_acc: 0.5483\n",
            "Epoch 22/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9942 - acc: 0.6386\n",
            "Epoch 00022: val_loss improved from 1.20968 to 1.18158, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.9964 - acc: 0.6378 - val_loss: 1.1816 - val_acc: 0.5675\n",
            "Epoch 23/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9369 - acc: 0.6615\n",
            "Epoch 00023: val_loss did not improve from 1.18158\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.9457 - acc: 0.6561 - val_loss: 1.2953 - val_acc: 0.5250\n",
            "Epoch 24/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9349 - acc: 0.6537\n",
            "Epoch 00024: val_loss improved from 1.18158 to 1.09759, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.9349 - acc: 0.6550 - val_loss: 1.0976 - val_acc: 0.5967\n",
            "Epoch 25/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9045 - acc: 0.6664\n",
            "Epoch 00025: val_loss did not improve from 1.09759\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.9020 - acc: 0.6692 - val_loss: 1.1000 - val_acc: 0.5917\n",
            "Epoch 26/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8470 - acc: 0.7021\n",
            "Epoch 00026: val_loss did not improve from 1.09759\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.8518 - acc: 0.6992 - val_loss: 1.2306 - val_acc: 0.5492\n",
            "Epoch 27/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.8410 - acc: 0.6891\n",
            "Epoch 00027: val_loss improved from 1.09759 to 1.00086, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.8391 - acc: 0.6892 - val_loss: 1.0009 - val_acc: 0.6350\n",
            "Epoch 28/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8306 - acc: 0.6971\n",
            "Epoch 00028: val_loss improved from 1.00086 to 0.96472, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.8359 - acc: 0.6942 - val_loss: 0.9647 - val_acc: 0.6325\n",
            "Epoch 29/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8185 - acc: 0.7026\n",
            "Epoch 00029: val_loss did not improve from 0.96472\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.8117 - acc: 0.7061 - val_loss: 1.0652 - val_acc: 0.6008\n",
            "Epoch 30/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7804 - acc: 0.7248\n",
            "Epoch 00030: val_loss did not improve from 0.96472\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.7871 - acc: 0.7219 - val_loss: 1.0227 - val_acc: 0.6292\n",
            "Epoch 31/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7508 - acc: 0.7294\n",
            "Epoch 00031: val_loss did not improve from 0.96472\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.7637 - acc: 0.7253 - val_loss: 1.0702 - val_acc: 0.6050\n",
            "Epoch 32/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7351 - acc: 0.7364\n",
            "Epoch 00032: val_loss improved from 0.96472 to 0.88992, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.7380 - acc: 0.7353 - val_loss: 0.8899 - val_acc: 0.6692\n",
            "Epoch 33/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7264 - acc: 0.7333\n",
            "Epoch 00033: val_loss did not improve from 0.88992\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.7209 - acc: 0.7367 - val_loss: 1.0216 - val_acc: 0.6175\n",
            "Epoch 34/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.6922 - acc: 0.7515\n",
            "Epoch 00034: val_loss did not improve from 0.88992\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.7022 - acc: 0.7492 - val_loss: 0.8961 - val_acc: 0.6758\n",
            "Epoch 35/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.6748 - acc: 0.7630\n",
            "Epoch 00035: val_loss improved from 0.88992 to 0.81231, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.6722 - acc: 0.7631 - val_loss: 0.8123 - val_acc: 0.6958\n",
            "Epoch 36/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6338 - acc: 0.7868\n",
            "Epoch 00036: val_loss improved from 0.81231 to 0.78292, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.6382 - acc: 0.7847 - val_loss: 0.7829 - val_acc: 0.7125\n",
            "Epoch 37/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6285 - acc: 0.7782\n",
            "Epoch 00037: val_loss did not improve from 0.78292\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.6313 - acc: 0.7764 - val_loss: 0.8089 - val_acc: 0.7075\n",
            "Epoch 38/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6127 - acc: 0.7834\n",
            "Epoch 00038: val_loss improved from 0.78292 to 0.75298, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.6150 - acc: 0.7819 - val_loss: 0.7530 - val_acc: 0.7225\n",
            "Epoch 39/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5910 - acc: 0.7949\n",
            "Epoch 00039: val_loss improved from 0.75298 to 0.72677, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.5951 - acc: 0.7936 - val_loss: 0.7268 - val_acc: 0.7317\n",
            "Epoch 40/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6020 - acc: 0.7900\n",
            "Epoch 00040: val_loss improved from 0.72677 to 0.69260, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.6037 - acc: 0.7906 - val_loss: 0.6926 - val_acc: 0.7675\n",
            "Epoch 41/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.5494 - acc: 0.8191\n",
            "Epoch 00041: val_loss did not improve from 0.69260\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.5528 - acc: 0.8192 - val_loss: 0.7045 - val_acc: 0.7550\n",
            "Epoch 42/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5340 - acc: 0.8211\n",
            "Epoch 00042: val_loss did not improve from 0.69260\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.5367 - acc: 0.8200 - val_loss: 0.7371 - val_acc: 0.7408\n",
            "Epoch 43/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5315 - acc: 0.8265\n",
            "Epoch 00043: val_loss improved from 0.69260 to 0.64709, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.5360 - acc: 0.8247 - val_loss: 0.6471 - val_acc: 0.7717\n",
            "Epoch 44/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4958 - acc: 0.8332\n",
            "Epoch 00044: val_loss did not improve from 0.64709\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.5018 - acc: 0.8317 - val_loss: 0.6622 - val_acc: 0.7633\n",
            "Epoch 45/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4963 - acc: 0.8294\n",
            "Epoch 00045: val_loss did not improve from 0.64709\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.4948 - acc: 0.8308 - val_loss: 0.6747 - val_acc: 0.7575\n",
            "Epoch 46/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4811 - acc: 0.8370\n",
            "Epoch 00046: val_loss did not improve from 0.64709\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.4907 - acc: 0.8342 - val_loss: 0.6805 - val_acc: 0.7625\n",
            "Epoch 47/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4675 - acc: 0.8491\n",
            "Epoch 00047: val_loss improved from 0.64709 to 0.60935, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.4725 - acc: 0.8475 - val_loss: 0.6094 - val_acc: 0.7900\n",
            "Epoch 48/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.4671 - acc: 0.8388\n",
            "Epoch 00048: val_loss did not improve from 0.60935\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.4728 - acc: 0.8383 - val_loss: 0.6479 - val_acc: 0.7792\n",
            "Epoch 49/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4573 - acc: 0.8503\n",
            "Epoch 00049: val_loss improved from 0.60935 to 0.59183, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.4616 - acc: 0.8483 - val_loss: 0.5918 - val_acc: 0.8083\n",
            "Epoch 50/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4266 - acc: 0.8536\n",
            "Epoch 00050: val_loss improved from 0.59183 to 0.58091, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.4210 - acc: 0.8550 - val_loss: 0.5809 - val_acc: 0.7950\n",
            "Epoch 51/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4216 - acc: 0.8652\n",
            "Epoch 00051: val_loss did not improve from 0.58091\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.4194 - acc: 0.8658 - val_loss: 0.5976 - val_acc: 0.7783\n",
            "Epoch 52/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4192 - acc: 0.8636\n",
            "Epoch 00052: val_loss did not improve from 0.58091\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.4292 - acc: 0.8578 - val_loss: 0.5915 - val_acc: 0.7800\n",
            "Epoch 53/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4070 - acc: 0.8612\n",
            "Epoch 00053: val_loss improved from 0.58091 to 0.49545, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.4052 - acc: 0.8622 - val_loss: 0.4954 - val_acc: 0.8258\n",
            "Epoch 54/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3839 - acc: 0.8762\n",
            "Epoch 00054: val_loss did not improve from 0.49545\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.3876 - acc: 0.8750 - val_loss: 0.5181 - val_acc: 0.8075\n",
            "Epoch 55/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4251 - acc: 0.8576\n",
            "Epoch 00055: val_loss did not improve from 0.49545\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.4281 - acc: 0.8561 - val_loss: 0.5107 - val_acc: 0.8308\n",
            "Epoch 56/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3742 - acc: 0.8822\n",
            "Epoch 00056: val_loss did not improve from 0.49545\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.3764 - acc: 0.8822 - val_loss: 0.4980 - val_acc: 0.8317\n",
            "Epoch 57/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3406 - acc: 0.8982\n",
            "Epoch 00057: val_loss did not improve from 0.49545\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.3403 - acc: 0.8967 - val_loss: 0.5057 - val_acc: 0.8217\n",
            "Epoch 58/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3623 - acc: 0.8826\n",
            "Epoch 00058: val_loss did not improve from 0.49545\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.3627 - acc: 0.8825 - val_loss: 0.5601 - val_acc: 0.8008\n",
            "Epoch 59/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3840 - acc: 0.8694\n",
            "Epoch 00059: val_loss improved from 0.49545 to 0.43892, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.3835 - acc: 0.8706 - val_loss: 0.4389 - val_acc: 0.8567\n",
            "Epoch 60/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.8797\n",
            "Epoch 00060: val_loss did not improve from 0.43892\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.3654 - acc: 0.8797 - val_loss: 0.4497 - val_acc: 0.8525\n",
            "Epoch 61/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3656 - acc: 0.8821\n",
            "Epoch 00061: val_loss did not improve from 0.43892\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.3695 - acc: 0.8800 - val_loss: 0.4792 - val_acc: 0.8342\n",
            "Epoch 62/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3295 - acc: 0.8936\n",
            "Epoch 00062: val_loss did not improve from 0.43892\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.3305 - acc: 0.8942 - val_loss: 0.4390 - val_acc: 0.8550\n",
            "Epoch 63/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3414 - acc: 0.8876\n",
            "Epoch 00063: val_loss did not improve from 0.43892\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.3425 - acc: 0.8886 - val_loss: 0.5686 - val_acc: 0.7967\n",
            "Epoch 64/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3028 - acc: 0.9062\n",
            "Epoch 00064: val_loss did not improve from 0.43892\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.3059 - acc: 0.9067 - val_loss: 0.4548 - val_acc: 0.8417\n",
            "Epoch 65/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2963 - acc: 0.9097\n",
            "Epoch 00065: val_loss improved from 0.43892 to 0.38631, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.2966 - acc: 0.9094 - val_loss: 0.3863 - val_acc: 0.8817\n",
            "Epoch 66/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3034 - acc: 0.9074\n",
            "Epoch 00066: val_loss did not improve from 0.38631\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.3065 - acc: 0.9061 - val_loss: 0.4171 - val_acc: 0.8625\n",
            "Epoch 67/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2886 - acc: 0.9106\n",
            "Epoch 00067: val_loss did not improve from 0.38631\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2900 - acc: 0.9097 - val_loss: 0.4035 - val_acc: 0.8658\n",
            "Epoch 68/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3077 - acc: 0.9012\n",
            "Epoch 00068: val_loss did not improve from 0.38631\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.3068 - acc: 0.9006 - val_loss: 0.4085 - val_acc: 0.8650\n",
            "Epoch 69/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2878 - acc: 0.9123\n",
            "Epoch 00069: val_loss did not improve from 0.38631\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2856 - acc: 0.9133 - val_loss: 0.4255 - val_acc: 0.8550\n",
            "Epoch 70/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2625 - acc: 0.9197\n",
            "Epoch 00070: val_loss improved from 0.38631 to 0.34247, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.2658 - acc: 0.9172 - val_loss: 0.3425 - val_acc: 0.8825\n",
            "Epoch 71/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.9269\n",
            "Epoch 00071: val_loss did not improve from 0.34247\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2471 - acc: 0.9267 - val_loss: 0.4177 - val_acc: 0.8592\n",
            "Epoch 72/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2684 - acc: 0.9149\n",
            "Epoch 00072: val_loss did not improve from 0.34247\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2682 - acc: 0.9158 - val_loss: 0.3943 - val_acc: 0.8600\n",
            "Epoch 73/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2639 - acc: 0.9218\n",
            "Epoch 00073: val_loss did not improve from 0.34247\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.2651 - acc: 0.9225 - val_loss: 0.4482 - val_acc: 0.8442\n",
            "Epoch 74/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2645 - acc: 0.9206\n",
            "Epoch 00074: val_loss did not improve from 0.34247\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2626 - acc: 0.9206 - val_loss: 0.3622 - val_acc: 0.8867\n",
            "Epoch 75/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2337 - acc: 0.9331\n",
            "Epoch 00075: val_loss improved from 0.34247 to 0.29218, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.2355 - acc: 0.9333 - val_loss: 0.2922 - val_acc: 0.9158\n",
            "Epoch 76/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2402 - acc: 0.9258\n",
            "Epoch 00076: val_loss did not improve from 0.29218\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2443 - acc: 0.9236 - val_loss: 0.3601 - val_acc: 0.8742\n",
            "Epoch 77/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2357 - acc: 0.9288\n",
            "Epoch 00077: val_loss did not improve from 0.29218\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2384 - acc: 0.9267 - val_loss: 0.4010 - val_acc: 0.8633\n",
            "Epoch 78/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2433 - acc: 0.9256\n",
            "Epoch 00078: val_loss did not improve from 0.29218\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2454 - acc: 0.9250 - val_loss: 0.3230 - val_acc: 0.8942\n",
            "Epoch 79/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2492 - acc: 0.9191\n",
            "Epoch 00079: val_loss did not improve from 0.29218\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.2493 - acc: 0.9175 - val_loss: 0.3260 - val_acc: 0.8958\n",
            "Epoch 80/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2156 - acc: 0.9369\n",
            "Epoch 00080: val_loss did not improve from 0.29218\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2177 - acc: 0.9361 - val_loss: 0.3034 - val_acc: 0.8992\n",
            "Epoch 81/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.9229\n",
            "Epoch 00081: val_loss improved from 0.29218 to 0.23914, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.2492 - acc: 0.9233 - val_loss: 0.2391 - val_acc: 0.9325\n",
            "Epoch 82/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2261 - acc: 0.9288\n",
            "Epoch 00082: val_loss did not improve from 0.23914\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2238 - acc: 0.9294 - val_loss: 0.3056 - val_acc: 0.9025\n",
            "Epoch 83/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2378 - acc: 0.9206\n",
            "Epoch 00083: val_loss did not improve from 0.23914\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2385 - acc: 0.9200 - val_loss: 0.3000 - val_acc: 0.9058\n",
            "Epoch 84/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9326\n",
            "Epoch 00084: val_loss did not improve from 0.23914\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.2213 - acc: 0.9336 - val_loss: 0.2610 - val_acc: 0.9258\n",
            "Epoch 85/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2200 - acc: 0.9355\n",
            "Epoch 00085: val_loss did not improve from 0.23914\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.2206 - acc: 0.9356 - val_loss: 0.2759 - val_acc: 0.9242\n",
            "Epoch 86/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2144 - acc: 0.9391\n",
            "Epoch 00086: val_loss did not improve from 0.23914\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.2150 - acc: 0.9386 - val_loss: 0.3242 - val_acc: 0.8942\n",
            "Epoch 87/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2181 - acc: 0.9360\n",
            "Epoch 00087: val_loss did not improve from 0.23914\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2210 - acc: 0.9350 - val_loss: 0.2902 - val_acc: 0.9075\n",
            "Epoch 88/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2035 - acc: 0.9406\n",
            "Epoch 00088: val_loss improved from 0.23914 to 0.23523, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.2045 - acc: 0.9397 - val_loss: 0.2352 - val_acc: 0.9250\n",
            "Epoch 89/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1833 - acc: 0.9460\n",
            "Epoch 00089: val_loss improved from 0.23523 to 0.22209, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.1839 - acc: 0.9464 - val_loss: 0.2221 - val_acc: 0.9342\n",
            "Epoch 90/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1954 - acc: 0.9459\n",
            "Epoch 00090: val_loss improved from 0.22209 to 0.19907, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.1983 - acc: 0.9442 - val_loss: 0.1991 - val_acc: 0.9417\n",
            "Epoch 91/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1997 - acc: 0.9369\n",
            "Epoch 00091: val_loss did not improve from 0.19907\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1987 - acc: 0.9375 - val_loss: 0.2526 - val_acc: 0.9317\n",
            "Epoch 92/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1859 - acc: 0.9418\n",
            "Epoch 00092: val_loss did not improve from 0.19907\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1908 - acc: 0.9392 - val_loss: 0.2798 - val_acc: 0.9192\n",
            "Epoch 93/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2017 - acc: 0.9367\n",
            "Epoch 00093: val_loss did not improve from 0.19907\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2038 - acc: 0.9364 - val_loss: 0.2648 - val_acc: 0.9142\n",
            "Epoch 94/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1822 - acc: 0.9462\n",
            "Epoch 00094: val_loss did not improve from 0.19907\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1807 - acc: 0.9472 - val_loss: 0.2338 - val_acc: 0.9308\n",
            "Epoch 95/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9534\n",
            "Epoch 00095: val_loss improved from 0.19907 to 0.19419, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.1635 - acc: 0.9536 - val_loss: 0.1942 - val_acc: 0.9475\n",
            "Epoch 96/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1756 - acc: 0.9471\n",
            "Epoch 00096: val_loss improved from 0.19419 to 0.18249, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.1760 - acc: 0.9472 - val_loss: 0.1825 - val_acc: 0.9517\n",
            "Epoch 97/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1790 - acc: 0.9456\n",
            "Epoch 00097: val_loss did not improve from 0.18249\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1776 - acc: 0.9464 - val_loss: 0.2022 - val_acc: 0.9442\n",
            "Epoch 98/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1709 - acc: 0.9479\n",
            "Epoch 00098: val_loss did not improve from 0.18249\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1705 - acc: 0.9483 - val_loss: 0.2222 - val_acc: 0.9308\n",
            "Epoch 99/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1557 - acc: 0.9574\n",
            "Epoch 00099: val_loss did not improve from 0.18249\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1560 - acc: 0.9572 - val_loss: 0.1843 - val_acc: 0.9450\n",
            "Epoch 100/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1830 - acc: 0.9476\n",
            "Epoch 00100: val_loss did not improve from 0.18249\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1834 - acc: 0.9475 - val_loss: 0.2547 - val_acc: 0.9158\n",
            "Epoch 101/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1888 - acc: 0.9453\n",
            "Epoch 00101: val_loss did not improve from 0.18249\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1873 - acc: 0.9456 - val_loss: 0.2107 - val_acc: 0.9333\n",
            "Epoch 102/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1870 - acc: 0.9430\n",
            "Epoch 00102: val_loss improved from 0.18249 to 0.18051, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.1872 - acc: 0.9433 - val_loss: 0.1805 - val_acc: 0.9542\n",
            "Epoch 103/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1696 - acc: 0.9477\n",
            "Epoch 00103: val_loss did not improve from 0.18051\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1707 - acc: 0.9478 - val_loss: 0.1906 - val_acc: 0.9350\n",
            "Epoch 104/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1669 - acc: 0.9503\n",
            "Epoch 00104: val_loss did not improve from 0.18051\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1677 - acc: 0.9497 - val_loss: 0.2079 - val_acc: 0.9417\n",
            "Epoch 105/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9526\n",
            "Epoch 00105: val_loss did not improve from 0.18051\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1604 - acc: 0.9525 - val_loss: 0.2328 - val_acc: 0.9275\n",
            "Epoch 106/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1612 - acc: 0.9526\n",
            "Epoch 00106: val_loss did not improve from 0.18051\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1623 - acc: 0.9528 - val_loss: 0.2301 - val_acc: 0.9267\n",
            "Epoch 107/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1637 - acc: 0.9564\n",
            "Epoch 00107: val_loss did not improve from 0.18051\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1616 - acc: 0.9569 - val_loss: 0.1913 - val_acc: 0.9450\n",
            "Epoch 108/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1451 - acc: 0.9615\n",
            "Epoch 00108: val_loss improved from 0.18051 to 0.17197, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 209us/sample - loss: 0.1485 - acc: 0.9606 - val_loss: 0.1720 - val_acc: 0.9558\n",
            "Epoch 109/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1563 - acc: 0.9559\n",
            "Epoch 00109: val_loss improved from 0.17197 to 0.13398, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.1565 - acc: 0.9558 - val_loss: 0.1340 - val_acc: 0.9658\n",
            "Epoch 110/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1464 - acc: 0.9582\n",
            "Epoch 00110: val_loss did not improve from 0.13398\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1452 - acc: 0.9592 - val_loss: 0.2057 - val_acc: 0.9417\n",
            "Epoch 111/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.9531\n",
            "Epoch 00111: val_loss did not improve from 0.13398\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1587 - acc: 0.9528 - val_loss: 0.2109 - val_acc: 0.9350\n",
            "Epoch 112/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1467 - acc: 0.9609\n",
            "Epoch 00112: val_loss did not improve from 0.13398\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1521 - acc: 0.9575 - val_loss: 0.1744 - val_acc: 0.9525\n",
            "Epoch 113/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1490 - acc: 0.9547\n",
            "Epoch 00113: val_loss did not improve from 0.13398\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1498 - acc: 0.9547 - val_loss: 0.1836 - val_acc: 0.9517\n",
            "Epoch 114/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9600\n",
            "Epoch 00114: val_loss did not improve from 0.13398\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1400 - acc: 0.9592 - val_loss: 0.1662 - val_acc: 0.9525\n",
            "Epoch 115/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1507 - acc: 0.9584\n",
            "Epoch 00115: val_loss did not improve from 0.13398\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1513 - acc: 0.9569 - val_loss: 0.2106 - val_acc: 0.9300\n",
            "Epoch 116/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.9577\n",
            "Epoch 00116: val_loss did not improve from 0.13398\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1483 - acc: 0.9569 - val_loss: 0.1937 - val_acc: 0.9358\n",
            "Epoch 117/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1632 - acc: 0.9509\n",
            "Epoch 00117: val_loss did not improve from 0.13398\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1668 - acc: 0.9486 - val_loss: 0.1838 - val_acc: 0.9458\n",
            "Epoch 118/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1653 - acc: 0.9470\n",
            "Epoch 00118: val_loss did not improve from 0.13398\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1639 - acc: 0.9475 - val_loss: 0.1708 - val_acc: 0.9558\n",
            "Epoch 119/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9494\n",
            "Epoch 00119: val_loss improved from 0.13398 to 0.12521, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.1578 - acc: 0.9500 - val_loss: 0.1252 - val_acc: 0.9700\n",
            "Epoch 120/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1612 - acc: 0.9539\n",
            "Epoch 00120: val_loss did not improve from 0.12521\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1601 - acc: 0.9553 - val_loss: 0.1434 - val_acc: 0.9667\n",
            "Epoch 121/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1463 - acc: 0.9561\n",
            "Epoch 00121: val_loss did not improve from 0.12521\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1440 - acc: 0.9578 - val_loss: 0.1660 - val_acc: 0.9525\n",
            "Epoch 122/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1398 - acc: 0.9588\n",
            "Epoch 00122: val_loss did not improve from 0.12521\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1373 - acc: 0.9597 - val_loss: 0.1703 - val_acc: 0.9483\n",
            "Epoch 123/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1423 - acc: 0.9600\n",
            "Epoch 00123: val_loss did not improve from 0.12521\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1422 - acc: 0.9592 - val_loss: 0.1410 - val_acc: 0.9658\n",
            "Epoch 124/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1426 - acc: 0.9597\n",
            "Epoch 00124: val_loss did not improve from 0.12521\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1415 - acc: 0.9594 - val_loss: 0.1678 - val_acc: 0.9492\n",
            "Epoch 125/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9526\n",
            "Epoch 00125: val_loss did not improve from 0.12521\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1491 - acc: 0.9528 - val_loss: 0.1577 - val_acc: 0.9533\n",
            "Epoch 126/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1539 - acc: 0.9529\n",
            "Epoch 00126: val_loss did not improve from 0.12521\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1532 - acc: 0.9528 - val_loss: 0.1666 - val_acc: 0.9567\n",
            "Epoch 127/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9569\n",
            "Epoch 00127: val_loss did not improve from 0.12521\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1455 - acc: 0.9572 - val_loss: 0.1286 - val_acc: 0.9692\n",
            "Epoch 128/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9589\n",
            "Epoch 00128: val_loss did not improve from 0.12521\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1455 - acc: 0.9594 - val_loss: 0.1322 - val_acc: 0.9683\n",
            "Epoch 129/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1231 - acc: 0.9656\n",
            "Epoch 00129: val_loss did not improve from 0.12521\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1233 - acc: 0.9650 - val_loss: 0.1259 - val_acc: 0.9650\n",
            "Epoch 130/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1408 - acc: 0.9615\n",
            "Epoch 00130: val_loss did not improve from 0.12521\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1417 - acc: 0.9622 - val_loss: 0.1450 - val_acc: 0.9658\n",
            "Epoch 131/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9526\n",
            "Epoch 00131: val_loss did not improve from 0.12521\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1555 - acc: 0.9517 - val_loss: 0.1496 - val_acc: 0.9533\n",
            "Epoch 132/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1527 - acc: 0.9512\n",
            "Epoch 00132: val_loss did not improve from 0.12521\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1526 - acc: 0.9503 - val_loss: 0.1444 - val_acc: 0.9575\n",
            "Epoch 133/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9574\n",
            "Epoch 00133: val_loss did not improve from 0.12521\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1450 - acc: 0.9572 - val_loss: 0.2322 - val_acc: 0.9133\n",
            "Epoch 134/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1407 - acc: 0.9584\n",
            "Epoch 00134: val_loss did not improve from 0.12521\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1407 - acc: 0.9578 - val_loss: 0.1560 - val_acc: 0.9542\n",
            "Epoch 135/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1322 - acc: 0.9618\n",
            "Epoch 00135: val_loss did not improve from 0.12521\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1340 - acc: 0.9608 - val_loss: 0.1803 - val_acc: 0.9458\n",
            "Epoch 136/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1305 - acc: 0.9615\n",
            "Epoch 00136: val_loss did not improve from 0.12521\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1308 - acc: 0.9614 - val_loss: 0.1631 - val_acc: 0.9508\n",
            "Epoch 137/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1327 - acc: 0.9585\n",
            "Epoch 00137: val_loss did not improve from 0.12521\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1330 - acc: 0.9583 - val_loss: 0.1608 - val_acc: 0.9550\n",
            "Epoch 138/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9671\n",
            "Epoch 00138: val_loss improved from 0.12521 to 0.11995, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.1210 - acc: 0.9678 - val_loss: 0.1200 - val_acc: 0.9642\n",
            "Epoch 139/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9709\n",
            "Epoch 00139: val_loss did not improve from 0.11995\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1151 - acc: 0.9694 - val_loss: 0.1447 - val_acc: 0.9550\n",
            "Epoch 140/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1229 - acc: 0.9600\n",
            "Epoch 00140: val_loss did not improve from 0.11995\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1243 - acc: 0.9600 - val_loss: 0.1233 - val_acc: 0.9642\n",
            "Epoch 141/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1349 - acc: 0.9612\n",
            "Epoch 00141: val_loss improved from 0.11995 to 0.11946, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.1337 - acc: 0.9614 - val_loss: 0.1195 - val_acc: 0.9700\n",
            "Epoch 142/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1263 - acc: 0.9612\n",
            "Epoch 00142: val_loss did not improve from 0.11946\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1256 - acc: 0.9611 - val_loss: 0.1801 - val_acc: 0.9375\n",
            "Epoch 143/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1158 - acc: 0.9674\n",
            "Epoch 00143: val_loss did not improve from 0.11946\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1169 - acc: 0.9667 - val_loss: 0.1335 - val_acc: 0.9717\n",
            "Epoch 144/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9594\n",
            "Epoch 00144: val_loss did not improve from 0.11946\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1380 - acc: 0.9594 - val_loss: 0.1261 - val_acc: 0.9625\n",
            "Epoch 145/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1234 - acc: 0.9635\n",
            "Epoch 00145: val_loss did not improve from 0.11946\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1227 - acc: 0.9633 - val_loss: 0.1208 - val_acc: 0.9667\n",
            "Epoch 146/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.9591\n",
            "Epoch 00146: val_loss did not improve from 0.11946\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1406 - acc: 0.9592 - val_loss: 0.1509 - val_acc: 0.9567\n",
            "Epoch 147/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1342 - acc: 0.9582\n",
            "Epoch 00147: val_loss did not improve from 0.11946\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1355 - acc: 0.9581 - val_loss: 0.1307 - val_acc: 0.9667\n",
            "Epoch 148/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1296 - acc: 0.9630\n",
            "Epoch 00148: val_loss did not improve from 0.11946\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1301 - acc: 0.9636 - val_loss: 0.1864 - val_acc: 0.9367\n",
            "Epoch 149/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1351 - acc: 0.9547\n",
            "Epoch 00149: val_loss did not improve from 0.11946\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1350 - acc: 0.9556 - val_loss: 0.1426 - val_acc: 0.9600\n",
            "Epoch 150/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9643\n",
            "Epoch 00150: val_loss did not improve from 0.11946\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1263 - acc: 0.9639 - val_loss: 0.1545 - val_acc: 0.9508\n",
            "Epoch 151/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1238 - acc: 0.9626\n",
            "Epoch 00151: val_loss improved from 0.11946 to 0.11308, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.1240 - acc: 0.9622 - val_loss: 0.1131 - val_acc: 0.9758\n",
            "Epoch 152/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9634\n",
            "Epoch 00152: val_loss improved from 0.11308 to 0.10461, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.1218 - acc: 0.9636 - val_loss: 0.1046 - val_acc: 0.9767\n",
            "Epoch 153/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1257 - acc: 0.9619\n",
            "Epoch 00153: val_loss did not improve from 0.10461\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1273 - acc: 0.9614 - val_loss: 0.1661 - val_acc: 0.9508\n",
            "Epoch 154/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1135 - acc: 0.9661\n",
            "Epoch 00154: val_loss did not improve from 0.10461\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1132 - acc: 0.9664 - val_loss: 0.1127 - val_acc: 0.9758\n",
            "Epoch 155/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1269 - acc: 0.9624\n",
            "Epoch 00155: val_loss did not improve from 0.10461\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1288 - acc: 0.9619 - val_loss: 0.1153 - val_acc: 0.9725\n",
            "Epoch 156/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1275 - acc: 0.9606\n",
            "Epoch 00156: val_loss did not improve from 0.10461\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1285 - acc: 0.9600 - val_loss: 0.1160 - val_acc: 0.9683\n",
            "Epoch 157/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9603\n",
            "Epoch 00157: val_loss did not improve from 0.10461\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1321 - acc: 0.9606 - val_loss: 0.1181 - val_acc: 0.9692\n",
            "Epoch 158/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1163 - acc: 0.9666\n",
            "Epoch 00158: val_loss did not improve from 0.10461\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1192 - acc: 0.9664 - val_loss: 0.1372 - val_acc: 0.9642\n",
            "Epoch 159/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9563\n",
            "Epoch 00159: val_loss did not improve from 0.10461\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1351 - acc: 0.9572 - val_loss: 0.1175 - val_acc: 0.9675\n",
            "Epoch 160/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9700\n",
            "Epoch 00160: val_loss did not improve from 0.10461\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1107 - acc: 0.9694 - val_loss: 0.1056 - val_acc: 0.9758\n",
            "Epoch 161/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9654\n",
            "Epoch 00161: val_loss improved from 0.10461 to 0.08931, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.1154 - acc: 0.9656 - val_loss: 0.0893 - val_acc: 0.9783\n",
            "Epoch 162/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1133 - acc: 0.9645\n",
            "Epoch 00162: val_loss did not improve from 0.08931\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1147 - acc: 0.9639 - val_loss: 0.1353 - val_acc: 0.9625\n",
            "Epoch 163/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9700\n",
            "Epoch 00163: val_loss did not improve from 0.08931\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1080 - acc: 0.9706 - val_loss: 0.1540 - val_acc: 0.9533\n",
            "Epoch 164/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.9666\n",
            "Epoch 00164: val_loss did not improve from 0.08931\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1119 - acc: 0.9667 - val_loss: 0.1066 - val_acc: 0.9733\n",
            "Epoch 165/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1027 - acc: 0.9742\n",
            "Epoch 00165: val_loss did not improve from 0.08931\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1013 - acc: 0.9747 - val_loss: 0.0986 - val_acc: 0.9758\n",
            "Epoch 166/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9623\n",
            "Epoch 00166: val_loss did not improve from 0.08931\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1201 - acc: 0.9628 - val_loss: 0.1171 - val_acc: 0.9683\n",
            "Epoch 167/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1309 - acc: 0.9629\n",
            "Epoch 00167: val_loss did not improve from 0.08931\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1319 - acc: 0.9619 - val_loss: 0.1064 - val_acc: 0.9783\n",
            "Epoch 168/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1067 - acc: 0.9694\n",
            "Epoch 00168: val_loss did not improve from 0.08931\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1083 - acc: 0.9706 - val_loss: 0.0960 - val_acc: 0.9775\n",
            "Epoch 169/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9709\n",
            "Epoch 00169: val_loss did not improve from 0.08931\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1070 - acc: 0.9711 - val_loss: 0.1242 - val_acc: 0.9642\n",
            "Epoch 170/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1109 - acc: 0.9712\n",
            "Epoch 00170: val_loss did not improve from 0.08931\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1101 - acc: 0.9711 - val_loss: 0.0957 - val_acc: 0.9750\n",
            "Epoch 171/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0988 - acc: 0.9724\n",
            "Epoch 00171: val_loss did not improve from 0.08931\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1002 - acc: 0.9719 - val_loss: 0.1012 - val_acc: 0.9750\n",
            "Epoch 172/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1088 - acc: 0.9659\n",
            "Epoch 00172: val_loss did not improve from 0.08931\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1082 - acc: 0.9664 - val_loss: 0.0896 - val_acc: 0.9783\n",
            "Epoch 173/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1028 - acc: 0.9709\n",
            "Epoch 00173: val_loss did not improve from 0.08931\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1034 - acc: 0.9717 - val_loss: 0.0896 - val_acc: 0.9733\n",
            "Epoch 174/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0971 - acc: 0.9739\n",
            "Epoch 00174: val_loss did not improve from 0.08931\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0964 - acc: 0.9742 - val_loss: 0.0927 - val_acc: 0.9800\n",
            "Epoch 175/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0939 - acc: 0.9747\n",
            "Epoch 00175: val_loss did not improve from 0.08931\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0944 - acc: 0.9742 - val_loss: 0.0991 - val_acc: 0.9733\n",
            "Epoch 176/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1331 - acc: 0.9568\n",
            "Epoch 00176: val_loss did not improve from 0.08931\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1344 - acc: 0.9561 - val_loss: 0.1406 - val_acc: 0.9600\n",
            "Epoch 177/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1184 - acc: 0.9652\n",
            "Epoch 00177: val_loss did not improve from 0.08931\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1238 - acc: 0.9644 - val_loss: 0.0953 - val_acc: 0.9792\n",
            "Epoch 178/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1116 - acc: 0.9691\n",
            "Epoch 00178: val_loss did not improve from 0.08931\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1160 - acc: 0.9683 - val_loss: 0.0950 - val_acc: 0.9758\n",
            "Epoch 179/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1118 - acc: 0.9676\n",
            "Epoch 00179: val_loss did not improve from 0.08931\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1149 - acc: 0.9661 - val_loss: 0.1201 - val_acc: 0.9667\n",
            "Epoch 180/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1154 - acc: 0.9655\n",
            "Epoch 00180: val_loss did not improve from 0.08931\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1173 - acc: 0.9650 - val_loss: 0.0972 - val_acc: 0.9758\n",
            "Epoch 181/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1145 - acc: 0.9653\n",
            "Epoch 00181: val_loss improved from 0.08931 to 0.08725, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.1136 - acc: 0.9661 - val_loss: 0.0873 - val_acc: 0.9833\n",
            "Epoch 182/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1001 - acc: 0.9721\n",
            "Epoch 00182: val_loss did not improve from 0.08725\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1007 - acc: 0.9722 - val_loss: 0.1120 - val_acc: 0.9767\n",
            "Epoch 183/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9720\n",
            "Epoch 00183: val_loss did not improve from 0.08725\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.1022 - acc: 0.9725 - val_loss: 0.0895 - val_acc: 0.9800\n",
            "Epoch 184/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0875 - acc: 0.9776\n",
            "Epoch 00184: val_loss did not improve from 0.08725\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0882 - acc: 0.9778 - val_loss: 0.1006 - val_acc: 0.9775\n",
            "Epoch 185/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1060 - acc: 0.9682\n",
            "Epoch 00185: val_loss did not improve from 0.08725\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1055 - acc: 0.9683 - val_loss: 0.1121 - val_acc: 0.9700\n",
            "Epoch 186/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9663\n",
            "Epoch 00186: val_loss improved from 0.08725 to 0.07543, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.1135 - acc: 0.9667 - val_loss: 0.0754 - val_acc: 0.9833\n",
            "Epoch 187/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1018 - acc: 0.9747\n",
            "Epoch 00187: val_loss did not improve from 0.07543\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1011 - acc: 0.9750 - val_loss: 0.1326 - val_acc: 0.9608\n",
            "Epoch 188/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9746\n",
            "Epoch 00188: val_loss did not improve from 0.07543\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0876 - acc: 0.9744 - val_loss: 0.0942 - val_acc: 0.9742\n",
            "Epoch 189/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0924 - acc: 0.9732\n",
            "Epoch 00189: val_loss did not improve from 0.07543\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0923 - acc: 0.9736 - val_loss: 0.0923 - val_acc: 0.9792\n",
            "Epoch 190/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1022 - acc: 0.9732\n",
            "Epoch 00190: val_loss did not improve from 0.07543\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1022 - acc: 0.9731 - val_loss: 0.0865 - val_acc: 0.9792\n",
            "Epoch 191/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1129 - acc: 0.9676\n",
            "Epoch 00191: val_loss improved from 0.07543 to 0.07245, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.1165 - acc: 0.9669 - val_loss: 0.0725 - val_acc: 0.9817\n",
            "Epoch 192/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1194 - acc: 0.9612\n",
            "Epoch 00192: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1239 - acc: 0.9583 - val_loss: 0.1025 - val_acc: 0.9717\n",
            "Epoch 193/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1057 - acc: 0.9712\n",
            "Epoch 00193: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1079 - acc: 0.9703 - val_loss: 0.0811 - val_acc: 0.9808\n",
            "Epoch 194/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0959 - acc: 0.9733\n",
            "Epoch 00194: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0961 - acc: 0.9731 - val_loss: 0.1190 - val_acc: 0.9667\n",
            "Epoch 195/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1203 - acc: 0.9615\n",
            "Epoch 00195: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1209 - acc: 0.9614 - val_loss: 0.1348 - val_acc: 0.9600\n",
            "Epoch 196/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9663\n",
            "Epoch 00196: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1146 - acc: 0.9661 - val_loss: 0.1140 - val_acc: 0.9675\n",
            "Epoch 197/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1067 - acc: 0.9688\n",
            "Epoch 00197: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1083 - acc: 0.9683 - val_loss: 0.1023 - val_acc: 0.9717\n",
            "Epoch 198/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0940 - acc: 0.9726\n",
            "Epoch 00198: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0940 - acc: 0.9719 - val_loss: 0.1157 - val_acc: 0.9700\n",
            "Epoch 199/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0962 - acc: 0.9726\n",
            "Epoch 00199: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0965 - acc: 0.9725 - val_loss: 0.0992 - val_acc: 0.9733\n",
            "Epoch 200/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1069 - acc: 0.9656\n",
            "Epoch 00200: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1059 - acc: 0.9669 - val_loss: 0.1145 - val_acc: 0.9658\n",
            "Epoch 201/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.9726\n",
            "Epoch 00201: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0974 - acc: 0.9722 - val_loss: 0.0843 - val_acc: 0.9750\n",
            "Epoch 202/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1087 - acc: 0.9647\n",
            "Epoch 00202: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1090 - acc: 0.9633 - val_loss: 0.0999 - val_acc: 0.9742\n",
            "Epoch 203/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9780\n",
            "Epoch 00203: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0873 - acc: 0.9778 - val_loss: 0.0912 - val_acc: 0.9758\n",
            "Epoch 204/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1036 - acc: 0.9685\n",
            "Epoch 00204: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1028 - acc: 0.9689 - val_loss: 0.0746 - val_acc: 0.9817\n",
            "Epoch 205/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9766\n",
            "Epoch 00205: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0924 - acc: 0.9767 - val_loss: 0.1024 - val_acc: 0.9708\n",
            "Epoch 206/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0920 - acc: 0.9729\n",
            "Epoch 00206: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0924 - acc: 0.9722 - val_loss: 0.0797 - val_acc: 0.9800\n",
            "Epoch 207/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1040 - acc: 0.9664\n",
            "Epoch 00207: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1026 - acc: 0.9672 - val_loss: 0.1039 - val_acc: 0.9642\n",
            "Epoch 208/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1164 - acc: 0.9653\n",
            "Epoch 00208: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1137 - acc: 0.9667 - val_loss: 0.0920 - val_acc: 0.9767\n",
            "Epoch 209/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.9674\n",
            "Epoch 00209: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1055 - acc: 0.9678 - val_loss: 0.0949 - val_acc: 0.9742\n",
            "Epoch 210/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0883 - acc: 0.9753\n",
            "Epoch 00210: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0899 - acc: 0.9739 - val_loss: 0.0762 - val_acc: 0.9825\n",
            "Epoch 211/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9709\n",
            "Epoch 00211: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0982 - acc: 0.9706 - val_loss: 0.0830 - val_acc: 0.9775\n",
            "Epoch 212/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0983 - acc: 0.9706\n",
            "Epoch 00212: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0972 - acc: 0.9711 - val_loss: 0.0835 - val_acc: 0.9767\n",
            "Epoch 213/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9706\n",
            "Epoch 00213: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0976 - acc: 0.9708 - val_loss: 0.1077 - val_acc: 0.9692\n",
            "Epoch 214/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1008 - acc: 0.9676\n",
            "Epoch 00214: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1000 - acc: 0.9683 - val_loss: 0.0769 - val_acc: 0.9808\n",
            "Epoch 215/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0987 - acc: 0.9706\n",
            "Epoch 00215: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0967 - acc: 0.9717 - val_loss: 0.0764 - val_acc: 0.9808\n",
            "Epoch 216/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1013 - acc: 0.9715\n",
            "Epoch 00216: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1007 - acc: 0.9711 - val_loss: 0.0868 - val_acc: 0.9758\n",
            "Epoch 217/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1124 - acc: 0.9633\n",
            "Epoch 00217: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1119 - acc: 0.9636 - val_loss: 0.0890 - val_acc: 0.9758\n",
            "Epoch 218/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1064 - acc: 0.9665\n",
            "Epoch 00218: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1082 - acc: 0.9656 - val_loss: 0.1188 - val_acc: 0.9675\n",
            "Epoch 219/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1023 - acc: 0.9676\n",
            "Epoch 00219: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1013 - acc: 0.9683 - val_loss: 0.1055 - val_acc: 0.9708\n",
            "Epoch 220/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1032 - acc: 0.9656\n",
            "Epoch 00220: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1054 - acc: 0.9653 - val_loss: 0.1145 - val_acc: 0.9617\n",
            "Epoch 221/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9680\n",
            "Epoch 00221: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1090 - acc: 0.9678 - val_loss: 0.0850 - val_acc: 0.9775\n",
            "Epoch 222/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9740\n",
            "Epoch 00222: val_loss did not improve from 0.07245\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0998 - acc: 0.9744 - val_loss: 0.0765 - val_acc: 0.9850\n",
            "Epoch 223/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9726\n",
            "Epoch 00223: val_loss improved from 0.07245 to 0.06019, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.0935 - acc: 0.9725 - val_loss: 0.0602 - val_acc: 0.9817\n",
            "Epoch 224/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1064 - acc: 0.9633\n",
            "Epoch 00224: val_loss did not improve from 0.06019\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1058 - acc: 0.9647 - val_loss: 0.1061 - val_acc: 0.9742\n",
            "Epoch 225/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9714\n",
            "Epoch 00225: val_loss did not improve from 0.06019\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0933 - acc: 0.9711 - val_loss: 0.0628 - val_acc: 0.9817\n",
            "Epoch 226/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9771\n",
            "Epoch 00226: val_loss did not improve from 0.06019\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0866 - acc: 0.9772 - val_loss: 0.0628 - val_acc: 0.9883\n",
            "Epoch 227/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0804 - acc: 0.9766\n",
            "Epoch 00227: val_loss did not improve from 0.06019\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0883 - acc: 0.9750 - val_loss: 0.0966 - val_acc: 0.9767\n",
            "Epoch 228/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1022 - acc: 0.9697\n",
            "Epoch 00228: val_loss did not improve from 0.06019\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1019 - acc: 0.9703 - val_loss: 0.0692 - val_acc: 0.9817\n",
            "Epoch 229/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1045 - acc: 0.9655\n",
            "Epoch 00229: val_loss did not improve from 0.06019\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1060 - acc: 0.9653 - val_loss: 0.0928 - val_acc: 0.9742\n",
            "Epoch 230/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1055 - acc: 0.9676\n",
            "Epoch 00230: val_loss did not improve from 0.06019\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1061 - acc: 0.9675 - val_loss: 0.0877 - val_acc: 0.9783\n",
            "Epoch 231/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1116 - acc: 0.9645\n",
            "Epoch 00231: val_loss did not improve from 0.06019\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1104 - acc: 0.9653 - val_loss: 0.0904 - val_acc: 0.9792\n",
            "Epoch 232/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1055 - acc: 0.9691\n",
            "Epoch 00232: val_loss did not improve from 0.06019\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1044 - acc: 0.9700 - val_loss: 0.0816 - val_acc: 0.9817\n",
            "Epoch 233/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1021 - acc: 0.9703\n",
            "Epoch 00233: val_loss did not improve from 0.06019\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1054 - acc: 0.9689 - val_loss: 0.0758 - val_acc: 0.9833\n",
            "Epoch 234/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0886 - acc: 0.9745\n",
            "Epoch 00234: val_loss did not improve from 0.06019\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0893 - acc: 0.9736 - val_loss: 0.0679 - val_acc: 0.9867\n",
            "Epoch 235/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0783 - acc: 0.9787\n",
            "Epoch 00235: val_loss did not improve from 0.06019\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0795 - acc: 0.9778 - val_loss: 0.0681 - val_acc: 0.9842\n",
            "Epoch 236/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0884 - acc: 0.9753\n",
            "Epoch 00236: val_loss improved from 0.06019 to 0.05366, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.0876 - acc: 0.9753 - val_loss: 0.0537 - val_acc: 0.9900\n",
            "Epoch 237/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0878 - acc: 0.9721\n",
            "Epoch 00237: val_loss did not improve from 0.05366\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0881 - acc: 0.9722 - val_loss: 0.0685 - val_acc: 0.9825\n",
            "Epoch 238/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9714\n",
            "Epoch 00238: val_loss did not improve from 0.05366\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0996 - acc: 0.9717 - val_loss: 0.0675 - val_acc: 0.9825\n",
            "Epoch 239/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0854 - acc: 0.9758\n",
            "Epoch 00239: val_loss did not improve from 0.05366\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0847 - acc: 0.9764 - val_loss: 0.0759 - val_acc: 0.9792\n",
            "Epoch 240/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0945 - acc: 0.9706\n",
            "Epoch 00240: val_loss did not improve from 0.05366\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0939 - acc: 0.9714 - val_loss: 0.0693 - val_acc: 0.9833\n",
            "Epoch 241/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0914 - acc: 0.9694\n",
            "Epoch 00241: val_loss did not improve from 0.05366\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0913 - acc: 0.9694 - val_loss: 0.0630 - val_acc: 0.9842\n",
            "Epoch 242/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9683\n",
            "Epoch 00242: val_loss did not improve from 0.05366\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1066 - acc: 0.9681 - val_loss: 0.0908 - val_acc: 0.9725\n",
            "Epoch 243/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1081 - acc: 0.9683\n",
            "Epoch 00243: val_loss did not improve from 0.05366\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1083 - acc: 0.9681 - val_loss: 0.0899 - val_acc: 0.9767\n",
            "Epoch 244/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0947 - acc: 0.9729\n",
            "Epoch 00244: val_loss did not improve from 0.05366\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0959 - acc: 0.9719 - val_loss: 0.0639 - val_acc: 0.9850\n",
            "Epoch 245/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0876 - acc: 0.9744\n",
            "Epoch 00245: val_loss did not improve from 0.05366\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0858 - acc: 0.9753 - val_loss: 0.0719 - val_acc: 0.9833\n",
            "Epoch 246/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9677\n",
            "Epoch 00246: val_loss did not improve from 0.05366\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1034 - acc: 0.9664 - val_loss: 0.0745 - val_acc: 0.9833\n",
            "Epoch 247/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0886 - acc: 0.9736\n",
            "Epoch 00247: val_loss did not improve from 0.05366\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0887 - acc: 0.9736 - val_loss: 0.0887 - val_acc: 0.9767\n",
            "Epoch 248/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0795 - acc: 0.9762\n",
            "Epoch 00248: val_loss did not improve from 0.05366\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0785 - acc: 0.9767 - val_loss: 0.0804 - val_acc: 0.9808\n",
            "Epoch 249/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9734\n",
            "Epoch 00249: val_loss improved from 0.05366 to 0.05000, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.0938 - acc: 0.9733 - val_loss: 0.0500 - val_acc: 0.9917\n",
            "Epoch 250/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0774 - acc: 0.9788\n",
            "Epoch 00250: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0804 - acc: 0.9778 - val_loss: 0.0579 - val_acc: 0.9858\n",
            "Epoch 251/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0842 - acc: 0.9761\n",
            "Epoch 00251: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0823 - acc: 0.9769 - val_loss: 0.0677 - val_acc: 0.9825\n",
            "Epoch 252/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0914 - acc: 0.9750\n",
            "Epoch 00252: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0935 - acc: 0.9736 - val_loss: 0.0740 - val_acc: 0.9800\n",
            "Epoch 253/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1082 - acc: 0.9666\n",
            "Epoch 00253: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1070 - acc: 0.9672 - val_loss: 0.0686 - val_acc: 0.9850\n",
            "Epoch 254/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9751\n",
            "Epoch 00254: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0869 - acc: 0.9750 - val_loss: 0.0532 - val_acc: 0.9908\n",
            "Epoch 255/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0849 - acc: 0.9745\n",
            "Epoch 00255: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0830 - acc: 0.9758 - val_loss: 0.0553 - val_acc: 0.9900\n",
            "Epoch 256/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0871 - acc: 0.9721\n",
            "Epoch 00256: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0889 - acc: 0.9708 - val_loss: 0.0716 - val_acc: 0.9867\n",
            "Epoch 257/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1022 - acc: 0.9656\n",
            "Epoch 00257: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1009 - acc: 0.9667 - val_loss: 0.0581 - val_acc: 0.9858\n",
            "Epoch 258/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0892 - acc: 0.9762\n",
            "Epoch 00258: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0891 - acc: 0.9750 - val_loss: 0.0636 - val_acc: 0.9867\n",
            "Epoch 259/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0833 - acc: 0.9771\n",
            "Epoch 00259: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0831 - acc: 0.9772 - val_loss: 0.0662 - val_acc: 0.9858\n",
            "Epoch 260/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0805 - acc: 0.9755\n",
            "Epoch 00260: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0833 - acc: 0.9744 - val_loss: 0.0816 - val_acc: 0.9767\n",
            "Epoch 261/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9703\n",
            "Epoch 00261: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0975 - acc: 0.9708 - val_loss: 0.1006 - val_acc: 0.9717\n",
            "Epoch 262/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0923 - acc: 0.9724\n",
            "Epoch 00262: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0915 - acc: 0.9725 - val_loss: 0.1292 - val_acc: 0.9617\n",
            "Epoch 263/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0762 - acc: 0.9779\n",
            "Epoch 00263: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0771 - acc: 0.9775 - val_loss: 0.0773 - val_acc: 0.9775\n",
            "Epoch 264/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0747 - acc: 0.9785\n",
            "Epoch 00264: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0755 - acc: 0.9781 - val_loss: 0.0517 - val_acc: 0.9892\n",
            "Epoch 265/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0870 - acc: 0.9735\n",
            "Epoch 00265: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0880 - acc: 0.9736 - val_loss: 0.0831 - val_acc: 0.9742\n",
            "Epoch 266/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0903 - acc: 0.9718\n",
            "Epoch 00266: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0922 - acc: 0.9714 - val_loss: 0.0991 - val_acc: 0.9767\n",
            "Epoch 267/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0896 - acc: 0.9729\n",
            "Epoch 00267: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0902 - acc: 0.9728 - val_loss: 0.0681 - val_acc: 0.9833\n",
            "Epoch 268/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0808 - acc: 0.9758\n",
            "Epoch 00268: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0807 - acc: 0.9753 - val_loss: 0.0589 - val_acc: 0.9867\n",
            "Epoch 269/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0867 - acc: 0.9747\n",
            "Epoch 00269: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0877 - acc: 0.9739 - val_loss: 0.0519 - val_acc: 0.9900\n",
            "Epoch 270/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9791\n",
            "Epoch 00270: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0834 - acc: 0.9792 - val_loss: 0.0660 - val_acc: 0.9833\n",
            "Epoch 271/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9723\n",
            "Epoch 00271: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0897 - acc: 0.9722 - val_loss: 0.0792 - val_acc: 0.9775\n",
            "Epoch 272/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0897 - acc: 0.9709\n",
            "Epoch 00272: val_loss did not improve from 0.05000\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0915 - acc: 0.9708 - val_loss: 0.0670 - val_acc: 0.9875\n",
            "Epoch 273/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9763\n",
            "Epoch 00273: val_loss improved from 0.05000 to 0.04879, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.0823 - acc: 0.9764 - val_loss: 0.0488 - val_acc: 0.9900\n",
            "Epoch 274/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0843 - acc: 0.9765\n",
            "Epoch 00274: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0837 - acc: 0.9767 - val_loss: 0.0636 - val_acc: 0.9867\n",
            "Epoch 275/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0792 - acc: 0.9767\n",
            "Epoch 00275: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0810 - acc: 0.9761 - val_loss: 0.0605 - val_acc: 0.9858\n",
            "Epoch 276/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0975 - acc: 0.9694\n",
            "Epoch 00276: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0951 - acc: 0.9711 - val_loss: 0.0508 - val_acc: 0.9875\n",
            "Epoch 277/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0748 - acc: 0.9794\n",
            "Epoch 00277: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0780 - acc: 0.9781 - val_loss: 0.0687 - val_acc: 0.9792\n",
            "Epoch 278/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0866 - acc: 0.9741\n",
            "Epoch 00278: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0876 - acc: 0.9731 - val_loss: 0.0713 - val_acc: 0.9783\n",
            "Epoch 279/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0804 - acc: 0.9742\n",
            "Epoch 00279: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0822 - acc: 0.9733 - val_loss: 0.0508 - val_acc: 0.9883\n",
            "Epoch 280/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0886 - acc: 0.9715\n",
            "Epoch 00280: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0894 - acc: 0.9719 - val_loss: 0.0685 - val_acc: 0.9825\n",
            "Epoch 281/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1050 - acc: 0.9668\n",
            "Epoch 00281: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1097 - acc: 0.9647 - val_loss: 0.0567 - val_acc: 0.9867\n",
            "Epoch 282/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9614\n",
            "Epoch 00282: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1139 - acc: 0.9622 - val_loss: 0.0658 - val_acc: 0.9850\n",
            "Epoch 283/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0828 - acc: 0.9748\n",
            "Epoch 00283: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0828 - acc: 0.9744 - val_loss: 0.0643 - val_acc: 0.9867\n",
            "Epoch 284/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0783 - acc: 0.9767\n",
            "Epoch 00284: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0789 - acc: 0.9764 - val_loss: 0.0867 - val_acc: 0.9800\n",
            "Epoch 285/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0887 - acc: 0.9718\n",
            "Epoch 00285: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0908 - acc: 0.9717 - val_loss: 0.0787 - val_acc: 0.9758\n",
            "Epoch 286/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0703 - acc: 0.9815\n",
            "Epoch 00286: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0710 - acc: 0.9817 - val_loss: 0.0712 - val_acc: 0.9783\n",
            "Epoch 287/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0815 - acc: 0.9773\n",
            "Epoch 00287: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0815 - acc: 0.9775 - val_loss: 0.0595 - val_acc: 0.9850\n",
            "Epoch 288/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0916 - acc: 0.9721\n",
            "Epoch 00288: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0929 - acc: 0.9714 - val_loss: 0.0917 - val_acc: 0.9725\n",
            "Epoch 289/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0935 - acc: 0.9700\n",
            "Epoch 00289: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0933 - acc: 0.9703 - val_loss: 0.0844 - val_acc: 0.9783\n",
            "Epoch 290/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0901 - acc: 0.9709\n",
            "Epoch 00290: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0920 - acc: 0.9703 - val_loss: 0.0755 - val_acc: 0.9750\n",
            "Epoch 291/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0879 - acc: 0.9721\n",
            "Epoch 00291: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0867 - acc: 0.9728 - val_loss: 0.0887 - val_acc: 0.9733\n",
            "Epoch 292/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9666\n",
            "Epoch 00292: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0953 - acc: 0.9667 - val_loss: 0.0946 - val_acc: 0.9733\n",
            "Epoch 293/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0935 - acc: 0.9730\n",
            "Epoch 00293: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0963 - acc: 0.9722 - val_loss: 0.0701 - val_acc: 0.9792\n",
            "Epoch 294/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9751\n",
            "Epoch 00294: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0827 - acc: 0.9756 - val_loss: 0.0614 - val_acc: 0.9850\n",
            "Epoch 295/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0821 - acc: 0.9747\n",
            "Epoch 00295: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.0824 - acc: 0.9744 - val_loss: 0.0534 - val_acc: 0.9875\n",
            "Epoch 296/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9723\n",
            "Epoch 00296: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0871 - acc: 0.9722 - val_loss: 0.0573 - val_acc: 0.9842\n",
            "Epoch 297/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9734\n",
            "Epoch 00297: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0822 - acc: 0.9733 - val_loss: 0.0650 - val_acc: 0.9825\n",
            "Epoch 298/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9717\n",
            "Epoch 00298: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0888 - acc: 0.9722 - val_loss: 0.0699 - val_acc: 0.9817\n",
            "Epoch 299/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0805 - acc: 0.9750\n",
            "Epoch 00299: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0810 - acc: 0.9744 - val_loss: 0.0810 - val_acc: 0.9775\n",
            "Epoch 300/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0761 - acc: 0.9759\n",
            "Epoch 00300: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0759 - acc: 0.9764 - val_loss: 0.0562 - val_acc: 0.9792\n",
            "Epoch 301/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0784 - acc: 0.9776\n",
            "Epoch 00301: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0778 - acc: 0.9781 - val_loss: 0.0743 - val_acc: 0.9800\n",
            "Epoch 302/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0869 - acc: 0.9735\n",
            "Epoch 00302: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0896 - acc: 0.9733 - val_loss: 0.0792 - val_acc: 0.9767\n",
            "Epoch 303/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0900 - acc: 0.9697\n",
            "Epoch 00303: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0890 - acc: 0.9703 - val_loss: 0.0801 - val_acc: 0.9775\n",
            "Epoch 304/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0928 - acc: 0.9700\n",
            "Epoch 00304: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0922 - acc: 0.9703 - val_loss: 0.0719 - val_acc: 0.9792\n",
            "Epoch 305/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0938 - acc: 0.9706\n",
            "Epoch 00305: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0945 - acc: 0.9711 - val_loss: 0.0792 - val_acc: 0.9800\n",
            "Epoch 306/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0757 - acc: 0.9765\n",
            "Epoch 00306: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0736 - acc: 0.9778 - val_loss: 0.0538 - val_acc: 0.9842\n",
            "Epoch 307/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0774 - acc: 0.9741\n",
            "Epoch 00307: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0773 - acc: 0.9736 - val_loss: 0.0550 - val_acc: 0.9867\n",
            "Epoch 308/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9737\n",
            "Epoch 00308: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0844 - acc: 0.9739 - val_loss: 0.0790 - val_acc: 0.9792\n",
            "Epoch 309/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9763\n",
            "Epoch 00309: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0748 - acc: 0.9767 - val_loss: 0.0535 - val_acc: 0.9875\n",
            "Epoch 310/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9760\n",
            "Epoch 00310: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0751 - acc: 0.9758 - val_loss: 0.0709 - val_acc: 0.9817\n",
            "Epoch 311/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9754\n",
            "Epoch 00311: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0782 - acc: 0.9758 - val_loss: 0.0608 - val_acc: 0.9817\n",
            "Epoch 312/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0861 - acc: 0.9748\n",
            "Epoch 00312: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0840 - acc: 0.9756 - val_loss: 0.0604 - val_acc: 0.9842\n",
            "Epoch 313/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0771 - acc: 0.9791\n",
            "Epoch 00313: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0759 - acc: 0.9792 - val_loss: 0.0534 - val_acc: 0.9858\n",
            "Epoch 314/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9757\n",
            "Epoch 00314: val_loss did not improve from 0.04879\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0793 - acc: 0.9761 - val_loss: 0.0590 - val_acc: 0.9825\n",
            "Epoch 315/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9714\n",
            "Epoch 00315: val_loss improved from 0.04879 to 0.04215, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.0892 - acc: 0.9714 - val_loss: 0.0421 - val_acc: 0.9900\n",
            "Epoch 316/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9797\n",
            "Epoch 00316: val_loss did not improve from 0.04215\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0720 - acc: 0.9800 - val_loss: 0.0541 - val_acc: 0.9858\n",
            "Epoch 317/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0806 - acc: 0.9769\n",
            "Epoch 00317: val_loss did not improve from 0.04215\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0847 - acc: 0.9756 - val_loss: 0.0662 - val_acc: 0.9800\n",
            "Epoch 318/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0673 - acc: 0.9829\n",
            "Epoch 00318: val_loss did not improve from 0.04215\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0680 - acc: 0.9822 - val_loss: 0.0567 - val_acc: 0.9858\n",
            "Epoch 319/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0732 - acc: 0.9806\n",
            "Epoch 00319: val_loss did not improve from 0.04215\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0731 - acc: 0.9808 - val_loss: 0.0473 - val_acc: 0.9908\n",
            "Epoch 320/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0776 - acc: 0.9779\n",
            "Epoch 00320: val_loss did not improve from 0.04215\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0752 - acc: 0.9786 - val_loss: 0.0534 - val_acc: 0.9875\n",
            "Epoch 321/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0609 - acc: 0.9812\n",
            "Epoch 00321: val_loss improved from 0.04215 to 0.04113, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 210us/sample - loss: 0.0615 - acc: 0.9814 - val_loss: 0.0411 - val_acc: 0.9883\n",
            "Epoch 322/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0755 - acc: 0.9791\n",
            "Epoch 00322: val_loss did not improve from 0.04113\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0750 - acc: 0.9794 - val_loss: 0.0595 - val_acc: 0.9842\n",
            "Epoch 323/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0756 - acc: 0.9776\n",
            "Epoch 00323: val_loss did not improve from 0.04113\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0737 - acc: 0.9783 - val_loss: 0.0559 - val_acc: 0.9883\n",
            "Epoch 324/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9811\n",
            "Epoch 00324: val_loss did not improve from 0.04113\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0658 - acc: 0.9808 - val_loss: 0.0489 - val_acc: 0.9858\n",
            "Epoch 325/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0686 - acc: 0.9797\n",
            "Epoch 00325: val_loss did not improve from 0.04113\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0719 - acc: 0.9786 - val_loss: 0.0505 - val_acc: 0.9875\n",
            "Epoch 326/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0705 - acc: 0.9779\n",
            "Epoch 00326: val_loss did not improve from 0.04113\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0697 - acc: 0.9786 - val_loss: 0.0586 - val_acc: 0.9842\n",
            "Epoch 327/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0777 - acc: 0.9765\n",
            "Epoch 00327: val_loss did not improve from 0.04113\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0757 - acc: 0.9769 - val_loss: 0.0683 - val_acc: 0.9833\n",
            "Epoch 328/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0830 - acc: 0.9712\n",
            "Epoch 00328: val_loss did not improve from 0.04113\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0833 - acc: 0.9722 - val_loss: 0.0787 - val_acc: 0.9750\n",
            "Epoch 329/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9737\n",
            "Epoch 00329: val_loss did not improve from 0.04113\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0851 - acc: 0.9744 - val_loss: 0.0642 - val_acc: 0.9858\n",
            "Epoch 330/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0764 - acc: 0.9806\n",
            "Epoch 00330: val_loss did not improve from 0.04113\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0782 - acc: 0.9794 - val_loss: 0.0738 - val_acc: 0.9767\n",
            "Epoch 331/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0796 - acc: 0.9764\n",
            "Epoch 00331: val_loss did not improve from 0.04113\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0812 - acc: 0.9758 - val_loss: 0.0606 - val_acc: 0.9842\n",
            "Epoch 332/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0863 - acc: 0.9756\n",
            "Epoch 00332: val_loss did not improve from 0.04113\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0881 - acc: 0.9756 - val_loss: 0.0651 - val_acc: 0.9817\n",
            "Epoch 333/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0813 - acc: 0.9747\n",
            "Epoch 00333: val_loss did not improve from 0.04113\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0817 - acc: 0.9744 - val_loss: 0.0633 - val_acc: 0.9808\n",
            "Epoch 334/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0773 - acc: 0.9800\n",
            "Epoch 00334: val_loss did not improve from 0.04113\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0779 - acc: 0.9800 - val_loss: 0.0745 - val_acc: 0.9775\n",
            "Epoch 335/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0857 - acc: 0.9712\n",
            "Epoch 00335: val_loss did not improve from 0.04113\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0840 - acc: 0.9714 - val_loss: 0.0573 - val_acc: 0.9858\n",
            "Epoch 336/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9694\n",
            "Epoch 00336: val_loss did not improve from 0.04113\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0891 - acc: 0.9700 - val_loss: 0.0436 - val_acc: 0.9900\n",
            "Epoch 337/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0803 - acc: 0.9744\n",
            "Epoch 00337: val_loss did not improve from 0.04113\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0823 - acc: 0.9739 - val_loss: 0.0693 - val_acc: 0.9825\n",
            "Epoch 338/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0734 - acc: 0.9776\n",
            "Epoch 00338: val_loss improved from 0.04113 to 0.03892, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.0739 - acc: 0.9778 - val_loss: 0.0389 - val_acc: 0.9892\n",
            "Epoch 339/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0583 - acc: 0.9848\n",
            "Epoch 00339: val_loss did not improve from 0.03892\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0611 - acc: 0.9844 - val_loss: 0.0453 - val_acc: 0.9883\n",
            "Epoch 340/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0703 - acc: 0.9809\n",
            "Epoch 00340: val_loss did not improve from 0.03892\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0704 - acc: 0.9814 - val_loss: 0.0557 - val_acc: 0.9850\n",
            "Epoch 341/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0721 - acc: 0.9758\n",
            "Epoch 00341: val_loss did not improve from 0.03892\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0719 - acc: 0.9761 - val_loss: 0.0442 - val_acc: 0.9917\n",
            "Epoch 342/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0681 - acc: 0.9828\n",
            "Epoch 00342: val_loss did not improve from 0.03892\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0670 - acc: 0.9833 - val_loss: 0.0482 - val_acc: 0.9850\n",
            "Epoch 343/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0602 - acc: 0.9824\n",
            "Epoch 00343: val_loss did not improve from 0.03892\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0615 - acc: 0.9822 - val_loss: 0.0439 - val_acc: 0.9908\n",
            "Epoch 344/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0753 - acc: 0.9767\n",
            "Epoch 00344: val_loss did not improve from 0.03892\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0738 - acc: 0.9781 - val_loss: 0.0633 - val_acc: 0.9808\n",
            "Epoch 345/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0756 - acc: 0.9773\n",
            "Epoch 00345: val_loss did not improve from 0.03892\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0768 - acc: 0.9772 - val_loss: 0.0564 - val_acc: 0.9842\n",
            "Epoch 346/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9774\n",
            "Epoch 00346: val_loss did not improve from 0.03892\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0765 - acc: 0.9772 - val_loss: 0.0403 - val_acc: 0.9917\n",
            "Epoch 347/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0741 - acc: 0.9773\n",
            "Epoch 00347: val_loss did not improve from 0.03892\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.0747 - acc: 0.9775 - val_loss: 0.0430 - val_acc: 0.9908\n",
            "Epoch 348/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0875 - acc: 0.9758\n",
            "Epoch 00348: val_loss did not improve from 0.03892\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0894 - acc: 0.9739 - val_loss: 0.0774 - val_acc: 0.9817\n",
            "Epoch 349/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9674\n",
            "Epoch 00349: val_loss did not improve from 0.03892\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1075 - acc: 0.9672 - val_loss: 0.0688 - val_acc: 0.9792\n",
            "Epoch 350/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0860 - acc: 0.9748\n",
            "Epoch 00350: val_loss improved from 0.03892 to 0.03643, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 211us/sample - loss: 0.0861 - acc: 0.9742 - val_loss: 0.0364 - val_acc: 0.9917\n",
            "Epoch 351/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0717 - acc: 0.9782\n",
            "Epoch 00351: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0698 - acc: 0.9794 - val_loss: 0.0558 - val_acc: 0.9858\n",
            "Epoch 352/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0605 - acc: 0.9828\n",
            "Epoch 00352: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0607 - acc: 0.9831 - val_loss: 0.0438 - val_acc: 0.9892\n",
            "Epoch 353/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0673 - acc: 0.9815\n",
            "Epoch 00353: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0682 - acc: 0.9811 - val_loss: 0.0485 - val_acc: 0.9867\n",
            "Epoch 354/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9703\n",
            "Epoch 00354: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0851 - acc: 0.9706 - val_loss: 0.0625 - val_acc: 0.9842\n",
            "Epoch 355/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0809 - acc: 0.9771\n",
            "Epoch 00355: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0792 - acc: 0.9778 - val_loss: 0.0561 - val_acc: 0.9808\n",
            "Epoch 356/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0723 - acc: 0.9776\n",
            "Epoch 00356: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0729 - acc: 0.9775 - val_loss: 0.0575 - val_acc: 0.9842\n",
            "Epoch 357/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9814\n",
            "Epoch 00357: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0643 - acc: 0.9814 - val_loss: 0.0544 - val_acc: 0.9883\n",
            "Epoch 358/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9754\n",
            "Epoch 00358: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0824 - acc: 0.9753 - val_loss: 0.0519 - val_acc: 0.9867\n",
            "Epoch 359/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0731 - acc: 0.9785\n",
            "Epoch 00359: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0729 - acc: 0.9792 - val_loss: 0.0406 - val_acc: 0.9917\n",
            "Epoch 360/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0692 - acc: 0.9797\n",
            "Epoch 00360: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0682 - acc: 0.9803 - val_loss: 0.0436 - val_acc: 0.9908\n",
            "Epoch 361/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0724 - acc: 0.9824\n",
            "Epoch 00361: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0720 - acc: 0.9817 - val_loss: 0.0592 - val_acc: 0.9817\n",
            "Epoch 362/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0871 - acc: 0.9712\n",
            "Epoch 00362: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0885 - acc: 0.9708 - val_loss: 0.0542 - val_acc: 0.9875\n",
            "Epoch 363/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9763\n",
            "Epoch 00363: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0797 - acc: 0.9764 - val_loss: 0.0509 - val_acc: 0.9892\n",
            "Epoch 364/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0711 - acc: 0.9776\n",
            "Epoch 00364: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0711 - acc: 0.9781 - val_loss: 0.0551 - val_acc: 0.9867\n",
            "Epoch 365/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9757\n",
            "Epoch 00365: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0815 - acc: 0.9753 - val_loss: 0.0562 - val_acc: 0.9833\n",
            "Epoch 366/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9754\n",
            "Epoch 00366: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0827 - acc: 0.9756 - val_loss: 0.0476 - val_acc: 0.9883\n",
            "Epoch 367/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0660 - acc: 0.9812\n",
            "Epoch 00367: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0666 - acc: 0.9806 - val_loss: 0.0582 - val_acc: 0.9825\n",
            "Epoch 368/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0744 - acc: 0.9776\n",
            "Epoch 00368: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0745 - acc: 0.9775 - val_loss: 0.0536 - val_acc: 0.9858\n",
            "Epoch 369/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9766\n",
            "Epoch 00369: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0744 - acc: 0.9772 - val_loss: 0.0420 - val_acc: 0.9908\n",
            "Epoch 370/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9714\n",
            "Epoch 00370: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0951 - acc: 0.9708 - val_loss: 0.0522 - val_acc: 0.9883\n",
            "Epoch 371/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0804 - acc: 0.9782\n",
            "Epoch 00371: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0819 - acc: 0.9778 - val_loss: 0.0492 - val_acc: 0.9900\n",
            "Epoch 372/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9769\n",
            "Epoch 00372: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0833 - acc: 0.9769 - val_loss: 0.0389 - val_acc: 0.9925\n",
            "Epoch 373/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9760\n",
            "Epoch 00373: val_loss did not improve from 0.03643\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0727 - acc: 0.9761 - val_loss: 0.0501 - val_acc: 0.9908\n",
            "Epoch 374/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0719 - acc: 0.9791\n",
            "Epoch 00374: val_loss improved from 0.03643 to 0.03605, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.0710 - acc: 0.9794 - val_loss: 0.0360 - val_acc: 0.9917\n",
            "Epoch 375/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9791\n",
            "Epoch 00375: val_loss did not improve from 0.03605\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0731 - acc: 0.9792 - val_loss: 0.0364 - val_acc: 0.9883\n",
            "Epoch 376/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0803 - acc: 0.9755\n",
            "Epoch 00376: val_loss did not improve from 0.03605\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0824 - acc: 0.9742 - val_loss: 0.0500 - val_acc: 0.9850\n",
            "Epoch 377/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0691 - acc: 0.9821\n",
            "Epoch 00377: val_loss did not improve from 0.03605\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0682 - acc: 0.9822 - val_loss: 0.0462 - val_acc: 0.9900\n",
            "Epoch 378/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0653 - acc: 0.9791\n",
            "Epoch 00378: val_loss did not improve from 0.03605\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0645 - acc: 0.9797 - val_loss: 0.0548 - val_acc: 0.9867\n",
            "Epoch 379/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9763\n",
            "Epoch 00379: val_loss did not improve from 0.03605\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0777 - acc: 0.9767 - val_loss: 0.0845 - val_acc: 0.9733\n",
            "Epoch 380/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0865 - acc: 0.9735\n",
            "Epoch 00380: val_loss did not improve from 0.03605\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0870 - acc: 0.9731 - val_loss: 0.0531 - val_acc: 0.9900\n",
            "Epoch 381/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0912 - acc: 0.9727\n",
            "Epoch 00381: val_loss did not improve from 0.03605\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0924 - acc: 0.9725 - val_loss: 0.0610 - val_acc: 0.9808\n",
            "Epoch 382/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0867 - acc: 0.9715\n",
            "Epoch 00382: val_loss did not improve from 0.03605\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0870 - acc: 0.9714 - val_loss: 0.0584 - val_acc: 0.9800\n",
            "Epoch 383/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0751 - acc: 0.9803\n",
            "Epoch 00383: val_loss did not improve from 0.03605\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0739 - acc: 0.9808 - val_loss: 0.0469 - val_acc: 0.9883\n",
            "Epoch 384/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0645 - acc: 0.9818\n",
            "Epoch 00384: val_loss did not improve from 0.03605\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0667 - acc: 0.9814 - val_loss: 0.0502 - val_acc: 0.9867\n",
            "Epoch 385/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9820\n",
            "Epoch 00385: val_loss did not improve from 0.03605\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0687 - acc: 0.9817 - val_loss: 0.0483 - val_acc: 0.9867\n",
            "Epoch 386/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9769\n",
            "Epoch 00386: val_loss did not improve from 0.03605\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0782 - acc: 0.9764 - val_loss: 0.0447 - val_acc: 0.9867\n",
            "Epoch 387/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0789 - acc: 0.9755\n",
            "Epoch 00387: val_loss did not improve from 0.03605\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0781 - acc: 0.9758 - val_loss: 0.0674 - val_acc: 0.9808\n",
            "Epoch 388/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9800\n",
            "Epoch 00388: val_loss improved from 0.03605 to 0.03282, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.0653 - acc: 0.9803 - val_loss: 0.0328 - val_acc: 0.9900\n",
            "Epoch 389/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0791 - acc: 0.9773\n",
            "Epoch 00389: val_loss did not improve from 0.03282\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0799 - acc: 0.9769 - val_loss: 0.0401 - val_acc: 0.9883\n",
            "Epoch 390/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0630 - acc: 0.9839\n",
            "Epoch 00390: val_loss did not improve from 0.03282\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0636 - acc: 0.9831 - val_loss: 0.0560 - val_acc: 0.9858\n",
            "Epoch 391/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0751 - acc: 0.9745\n",
            "Epoch 00391: val_loss did not improve from 0.03282\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0748 - acc: 0.9747 - val_loss: 0.0664 - val_acc: 0.9808\n",
            "Epoch 392/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9769\n",
            "Epoch 00392: val_loss did not improve from 0.03282\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0738 - acc: 0.9767 - val_loss: 0.0531 - val_acc: 0.9867\n",
            "Epoch 393/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0792 - acc: 0.9750\n",
            "Epoch 00393: val_loss did not improve from 0.03282\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0775 - acc: 0.9756 - val_loss: 0.0565 - val_acc: 0.9850\n",
            "Epoch 394/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0655 - acc: 0.9818\n",
            "Epoch 00394: val_loss did not improve from 0.03282\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0648 - acc: 0.9822 - val_loss: 0.0503 - val_acc: 0.9883\n",
            "Epoch 395/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9834\n",
            "Epoch 00395: val_loss did not improve from 0.03282\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0617 - acc: 0.9836 - val_loss: 0.0499 - val_acc: 0.9908\n",
            "Epoch 396/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0791 - acc: 0.9759\n",
            "Epoch 00396: val_loss did not improve from 0.03282\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0774 - acc: 0.9761 - val_loss: 0.0655 - val_acc: 0.9842\n",
            "Epoch 397/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9826\n",
            "Epoch 00397: val_loss did not improve from 0.03282\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0619 - acc: 0.9825 - val_loss: 0.0522 - val_acc: 0.9850\n",
            "Epoch 398/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0764 - acc: 0.9753\n",
            "Epoch 00398: val_loss did not improve from 0.03282\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0776 - acc: 0.9744 - val_loss: 0.0491 - val_acc: 0.9858\n",
            "Epoch 399/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9754\n",
            "Epoch 00399: val_loss did not improve from 0.03282\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.0717 - acc: 0.9756 - val_loss: 0.0585 - val_acc: 0.9833\n",
            "Epoch 400/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0718 - acc: 0.9797\n",
            "Epoch 00400: val_loss did not improve from 0.03282\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0745 - acc: 0.9792 - val_loss: 0.0396 - val_acc: 0.9908\n",
            "1200/1200 [==============================] - 0s 127us/sample - loss: 0.0396 - acc: 0.9908\n",
            "[0.03961971625685692, 0.99083334]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AddOC036w_99",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "fee303d1-cf78-496a-c274-76b7852eff6f"
      },
      "source": [
        "for i in range(12, 13): # Итерација низ секој испитен примерок\n",
        "  print(f\"====================== Примерок ({i}) ======================\")\n",
        "  print(\"Вчитување тест податоци од испитниот примерок \" + str(i) + \"...\")\n",
        "  \n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_test_data = np.empty(0)\n",
        "  temp_test_events = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_test_set = 'S' + format(j, '02') + '/Test'\n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testData.mat\"\n",
        "    temp = loadmat(full_path)['testData']\n",
        "    if temp_test_data.size != 0:\n",
        "      temp_test_data = np.concatenate((temp_test_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_test_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_test_events.size != 0:\n",
        "        temp_test_events = np.append(temp_test_events, temp)\n",
        "      else:\n",
        "        temp_test_events = np.array(temp)\n",
        "\n",
        "    # Вчитување на бројот на runs \n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/runs_per_block.txt\"\n",
        "    with open(full_path, \"r\") as runs_per_block:\n",
        "      test_runs_per_block[i-1][j-1] = int(runs_per_block.read())\n",
        "\n",
        "    print(\"\\t - Тест податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "  # Зачувај ги тест податоците вчитани од испитниот примерок во низа\n",
        "  test_data.append(temp_test_data)\n",
        "  test_events.append(temp_test_events)\n",
        "  print(\"Тест податоците од испитниот примерок \" + str(i) + \" се вчитани.\\n\")\n",
        "\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_data: \" + str(test_data[i-1].shape)) # test_data to predict\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_events: \" + str(len(test_events[i-1]))) # test_events\n",
        "  for j in range (1,4):\n",
        "    print(\"SBJ\" + str(format(i-1, '02')) + \" / S\" + str(format(j-1, '02')) + \"| Runs per block: \" + str(test_runs_per_block[i-1][j-1])) # runs per block in SJB01, SJ00 \n",
        "\n",
        "  to_predict_data = reshape_data_to_mne_format(test_data[i-1])\n",
        "  predictions = model12.predict(to_predict_data)\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Predictions: \" + str(len(predictions)))\n",
        "  # np.savetxt(\"predictions.csv\", predictions, delimiter=\",\")\n",
        "\n",
        "\n",
        "  # ========= FALI USTE DA SE ISPARSIRA PREDICTIONOT... NE E SREDEN OVOJ KOD DOLE =======\n",
        "\n",
        "  int_pred = np.argmax(predictions, axis=1)\n",
        "  int_ytest = np.argmax(y_test, axis=1)\n",
        "\n",
        "  session_start = 0\n",
        "  start_prediction_index = 0\n",
        "  end_prediction_index = 0\n",
        "  for session in range(0, 3):\n",
        "    print(f\"============== Сесија ({session}) ==============\")\n",
        "    for block in range(0, 50):    \n",
        "      events_per_block = test_runs_per_block[i-1][session]\n",
        "\n",
        "      start_prediction_index = session_start + (block*events_per_block)*8\n",
        "      end_prediction_index = session_start + ((block+1)*events_per_block)*8\n",
        "\n",
        "      block_prediction = int_pred[start_prediction_index:end_prediction_index]\n",
        "      prediction = np.bincount(block_prediction).argmax()\n",
        "      df.iat[session+33,block+2] = prediction+1\n",
        "      # UNCOMMENT ZA PODOBAR PRIKAZ :)\n",
        "      # print(f\"Session {session} | Block: {block} | Prediction: {prediction} | Address: {end_prediction_index}\")\n",
        "\n",
        "      print(str(prediction+1) + \",\", end=\"\")\n",
        "    session_start = end_prediction_index\n",
        "    print(\"\")\n",
        "  print(\"Stigna li do kraj: \" + str(session_start == len(predictions)))\n",
        "  print(f\"====================== Примерок ({i}) ======================\\n\\n\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Примерок (12) ======================\n",
            "Вчитување тест податоци од испитниот примерок 12...\n",
            "\t - Тест податоците од сесија 1 се вчитани.\n",
            "\t - Тест податоците од сесија 2 се вчитани.\n",
            "\t - Тест податоците од сесија 3 се вчитани.\n",
            "Тест податоците од испитниот примерок 12 се вчитани.\n",
            "\n",
            "SBJ11| Test_data: (8, 350, 8800)\n",
            "SBJ11| Test_events: 8800\n",
            "SBJ11 / S00| Runs per block: 8\n",
            "SBJ11 / S01| Runs per block: 9\n",
            "SBJ11 / S02| Runs per block: 5\n",
            "SBJ11| Predictions: 8800\n",
            "============== Сесија (0) ==============\n",
            "1,1,1,1,1,1,6,3,6,1,1,6,1,6,3,1,3,7,2,2,3,5,5,2,6,1,3,7,7,7,6,6,1,3,6,5,7,3,2,5,1,3,1,6,7,3,1,7,7,3,\n",
            "============== Сесија (1) ==============\n",
            "3,2,6,2,2,3,2,5,7,7,7,3,1,1,3,7,3,1,7,5,7,7,3,7,7,1,2,1,6,5,3,1,1,2,5,8,3,5,5,7,1,3,5,1,2,1,5,5,2,5,\n",
            "============== Сесија (2) ==============\n",
            "1,6,2,2,1,2,2,2,2,2,2,2,2,2,6,2,3,2,5,2,2,6,2,1,2,5,2,6,6,1,1,2,2,4,1,2,2,2,6,3,6,5,5,6,2,1,2,2,2,2,\n",
            "Stigna li do kraj: True\n",
            "====================== Примерок (12) ======================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIS_2rkpxMay",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb869976-c025-441a-f1d0-7b7acdd2d7e0"
      },
      "source": [
        "df"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUBJECT</th>\n",
              "      <th>SESSION</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>Unnamed: 52</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    SUBJECT  SESSION  1  2  3  4  5  6  7  ... 43 44 45 46 47 48 49 50 Unnamed: 52\n",
              "0         1        1  6  6  3  6  6  3  6  ...  6  4  8  3  4  5  5  7         NaN\n",
              "1         1        2  6  3  2  1  2  1  3  ...  6  2  2  2  3  6  6  2         NaN\n",
              "2         1        3  3  3  3  3  3  3  3  ...  3  3  6  3  6  7  1  6         NaN\n",
              "3         2        1  8  8  8  8  8  8  8  ...  8  4  8  4  8  7  8  8         NaN\n",
              "4         2        2  2  7  6  6  6  6  7  ...  2  6  6  2  6  6  2  2         NaN\n",
              "5         2        3  2  7  2  6  7  4  2  ...  2  6  2  2  7  2  2  2         NaN\n",
              "6         3        1  3  3  4  4  4  3  6  ...  4  6  3  3  4  3  4  4         NaN\n",
              "7         3        2  6  1  7  7  7  7  7  ...  6  7  1  6  6  6  6  6         NaN\n",
              "8         3        3  6  4  8  8  5  7  8  ...  4  2  8  2  2  2  2  8         NaN\n",
              "9         4        1  1  1  2  1  5  5  6  ...  1  1  7  1  6  6  6  6         NaN\n",
              "10        4        2  7  7  7  7  6  6  7  ...  1  5  5  5  5  5  2  6         NaN\n",
              "11        4        3  7  3  7  3  6  6  7  ...  6  1  4  4  4  6  6  6         NaN\n",
              "12        5        1  5  4  4  4  6  4  5  ...  3  1  4  4  1  3  3  5         NaN\n",
              "13        5        2  3  6  3  5  2  6  7  ...  6  6  6  6  6  6  6  4         NaN\n",
              "14        5        3  4  3  3  6  5  7  6  ...  7  2  2  7  5  6  4  2         NaN\n",
              "15        6        1  1  3  1  8  3  3  3  ...  4  3  7  2  7  2  5  8         NaN\n",
              "16        6        2  3  4  1  7  1  1  1  ...  5  5  5  5  5  5  5  5         NaN\n",
              "17        6        3  2  3  2  5  3  3  3  ...  2  3  3  3  1  1  3  3         NaN\n",
              "18        7        1  4  7  5  7  4  5  4  ...  5  1  1  4  4  4  1  4         NaN\n",
              "19        7        2  2  2  2  8  2  5  2  ...  5  5  5  2  2  2  2  2         NaN\n",
              "20        7        3  2  7  7  5  7  5  4  ...  1  3  5  5  3  5  5  1         NaN\n",
              "21        8        1  5  8  2  8  2  2  1  ...  1  8  2  2  5  5  2  8         NaN\n",
              "22        8        2  2  7  1  5  1  2  1  ...  7  1  7  8  2  1  7  7         NaN\n",
              "23        8        3  4  1  6  6  1  2  1  ...  1  8  1  7  1  1  7  1         NaN\n",
              "24        9        1  6  5  6  6  7  6  8  ...  6  6  6  5  5  5  3  5         NaN\n",
              "25        9        2  3  4  8  4  1  6  1  ...  1  1  1  1  1  3  6  6         NaN\n",
              "26        9        3  5  2  2  2  2  3  2  ...  2  2  2  3  2  2  4  2         NaN\n",
              "27       10        1  1  1  8  7  3  1  1  ...  5  1  1  1  1  5  1  5         NaN\n",
              "28       10        2  1  5  5  6  1  1  1  ...  5  5  6  6  2  1  2  1         NaN\n",
              "29       10        3  5  8  5  5  5  6  5  ...  5  5  2  5  5  5  5  5         NaN\n",
              "30       11        1  3  4  4  4  3  4  3  ...  5  4  3  1  3  4  4  4         NaN\n",
              "31       11        2  3  1  1  4  8  4  4  ...  1  7  3  8  2  3  8  3         NaN\n",
              "32       11        3  4  2  4  4  4  4  4  ...  8  4  4  7  8  4  8  4         NaN\n",
              "33       12        1  1  1  1  1  1  1  6  ...  1  6  7  3  1  7  7  3         NaN\n",
              "34       12        2  3  2  6  2  2  3  2  ...  5  1  2  1  5  5  2  5         NaN\n",
              "35       12        3  1  6  2  2  1  2  2  ...  5  6  2  1  2  2  2  2         NaN\n",
              "36       13        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "37       13        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "38       13        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "39       14        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "40       14        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "41       14        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "42       15        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "43       15        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "44       15        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "\n",
              "[45 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFblX3sOxr5h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2db364ec-8d0b-4062-9c66-4e4944567055"
      },
      "source": [
        "for i in range(13, 14): # Итерација низ секој испитен примерок\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_data = np.empty(0)\n",
        "  temp_labels = np.empty(0)\n",
        "  temp_events = np.empty(0)\n",
        "  temp_targets = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainData.mat\"\n",
        "    temp = loadmat(full_path)['trainData']\n",
        "    if temp_data.size != 0:\n",
        "      temp_data = np.concatenate((temp_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на label-ите\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainLabels.txt\"\n",
        "    with open(full_path, \"r\") as file_labels:\n",
        "      temp = file_labels.read().splitlines()\n",
        "      temp = np.repeat(temp, 8*10)\n",
        "      if temp_labels.size != 0:\n",
        "        temp_labels = np.concatenate((temp_labels, temp))\n",
        "      else:\n",
        "        temp_labels = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_events.size != 0:\n",
        "        temp_events = np.append(temp_events, temp)\n",
        "      else:\n",
        "        temp_events = np.array(temp)\n",
        "      \n",
        "\n",
        "    # Вчитување на редоследот на објекти кои се target\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainTargets.txt\"\n",
        "    with open(full_path, \"r\") as file_targets:\n",
        "      temp = file_targets.read().splitlines()\n",
        "      if temp_targets.size != 0:\n",
        "        temp_targets = np.concatenate((temp_targets, temp))\n",
        "      else:\n",
        "        temp_targets = np.array(temp)\n",
        "    print(\"\\t - Податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "\n",
        "  for j in range(4, 8): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "      \n",
        "  # Зачувај ги податоците вчитани од испитниот примерок во низа\n",
        "  data.append(temp_data)\n",
        "  labels.append(temp_labels)\n",
        "  events.append(temp_events)\n",
        "  targets.append(temp_targets)\n",
        "\n",
        "  \n",
        "  print(\"Податоците од испитниот примерок \" + str(i) + \" се вчитани.\")\n",
        "\n",
        "\n",
        "  #data = target_events_data_scaled\n",
        "  mne_array = np.swapaxes(data[i-1], 0, 2) # (епохa, канал, настан). \n",
        "  mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "  mne_array = mne_array.reshape(mne_array.shape[0],1, 8,350)\n",
        "  print(mne_array.shape)\n",
        "\n",
        "  events_arr = events[i-1].astype(np.int)\n",
        "  labels_arr = labels[i-1].astype(np.int)\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(mne_array, labels_arr-1, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "  model13 = DeepConvNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "  model13.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = Adam(0.0009))\n",
        "  checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                                verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "  #clf = RandomForestClassifier(max_depth=5)\n",
        "  #clf.fit(X_train, y_train)\n",
        "  #score = clf.score(X_test, y_test)\n",
        "  # print(score)\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  num_batch_size=100\n",
        "  num_epochs=400\n",
        "  model13.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "            validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "  score = model13.evaluate(X_test, y_test, verbose=1)\n",
        "  print(score)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t - Податоците од сесија 1 се вчитани.\n",
            "\t - Податоците од сесија 2 се вчитани.\n",
            "\t - Податоците од сесија 3 се вчитани.\n",
            "Податоците од испитниот примерок 13 се вчитани.\n",
            "(4800, 1, 8, 350)\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Train on 3600 samples, validate on 1200 samples\n",
            "Epoch 1/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.2161 - acc: 0.2059\n",
            "Epoch 00001: val_loss improved from inf to 1.91527, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 9s 3ms/sample - loss: 2.1991 - acc: 0.2100 - val_loss: 1.9153 - val_acc: 0.2700\n",
            "Epoch 2/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.8254 - acc: 0.2800\n",
            "Epoch 00002: val_loss improved from 1.91527 to 1.70765, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 1.8227 - acc: 0.2789 - val_loss: 1.7077 - val_acc: 0.2942\n",
            "Epoch 3/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.7451 - acc: 0.2891\n",
            "Epoch 00003: val_loss did not improve from 1.70765\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 1.7462 - acc: 0.2906 - val_loss: 1.7805 - val_acc: 0.3033\n",
            "Epoch 4/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.7031 - acc: 0.3017\n",
            "Epoch 00004: val_loss did not improve from 1.70765\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 1.7061 - acc: 0.3003 - val_loss: 1.7670 - val_acc: 0.3133\n",
            "Epoch 5/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.6951 - acc: 0.3250\n",
            "Epoch 00005: val_loss did not improve from 1.70765\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 1.6992 - acc: 0.3244 - val_loss: 2.1403 - val_acc: 0.2842\n",
            "Epoch 6/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.6110 - acc: 0.3430\n",
            "Epoch 00006: val_loss did not improve from 1.70765\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 1.6180 - acc: 0.3428 - val_loss: 1.7923 - val_acc: 0.3042\n",
            "Epoch 7/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.5938 - acc: 0.3656\n",
            "Epoch 00007: val_loss improved from 1.70765 to 1.66328, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 1.5860 - acc: 0.3694 - val_loss: 1.6633 - val_acc: 0.3483\n",
            "Epoch 8/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4826 - acc: 0.3953\n",
            "Epoch 00008: val_loss did not improve from 1.66328\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 1.4856 - acc: 0.3950 - val_loss: 1.7346 - val_acc: 0.3342\n",
            "Epoch 9/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.4866 - acc: 0.4140\n",
            "Epoch 00009: val_loss improved from 1.66328 to 1.43419, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 1.4891 - acc: 0.4111 - val_loss: 1.4342 - val_acc: 0.4108\n",
            "Epoch 10/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4146 - acc: 0.4324\n",
            "Epoch 00010: val_loss did not improve from 1.43419\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 1.4235 - acc: 0.4297 - val_loss: 1.5389 - val_acc: 0.3725\n",
            "Epoch 11/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4081 - acc: 0.4544\n",
            "Epoch 00011: val_loss did not improve from 1.43419\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 1.4078 - acc: 0.4531 - val_loss: 1.6028 - val_acc: 0.3842\n",
            "Epoch 12/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.3081 - acc: 0.4774\n",
            "Epoch 00012: val_loss did not improve from 1.43419\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 1.3078 - acc: 0.4783 - val_loss: 1.5210 - val_acc: 0.3967\n",
            "Epoch 13/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2756 - acc: 0.4988\n",
            "Epoch 00013: val_loss did not improve from 1.43419\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 1.2785 - acc: 0.4953 - val_loss: 1.4845 - val_acc: 0.4092\n",
            "Epoch 14/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2305 - acc: 0.5109\n",
            "Epoch 00014: val_loss improved from 1.43419 to 1.36686, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 221us/sample - loss: 1.2327 - acc: 0.5103 - val_loss: 1.3669 - val_acc: 0.4583\n",
            "Epoch 15/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.1832 - acc: 0.5280\n",
            "Epoch 00015: val_loss improved from 1.36686 to 1.25193, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 1.1869 - acc: 0.5261 - val_loss: 1.2519 - val_acc: 0.4983\n",
            "Epoch 16/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.1148 - acc: 0.5646\n",
            "Epoch 00016: val_loss did not improve from 1.25193\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 1.1198 - acc: 0.5633 - val_loss: 1.5475 - val_acc: 0.4133\n",
            "Epoch 17/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.0794 - acc: 0.5723\n",
            "Epoch 00017: val_loss did not improve from 1.25193\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 1.0905 - acc: 0.5686 - val_loss: 1.5812 - val_acc: 0.4325\n",
            "Epoch 18/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0837 - acc: 0.5800\n",
            "Epoch 00018: val_loss improved from 1.25193 to 1.19975, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 1.0838 - acc: 0.5811 - val_loss: 1.1998 - val_acc: 0.5242\n",
            "Epoch 19/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.0360 - acc: 0.5981\n",
            "Epoch 00019: val_loss did not improve from 1.19975\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 1.0466 - acc: 0.5944 - val_loss: 1.4247 - val_acc: 0.4608\n",
            "Epoch 20/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0462 - acc: 0.5909\n",
            "Epoch 00020: val_loss did not improve from 1.19975\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 1.0507 - acc: 0.5875 - val_loss: 1.2318 - val_acc: 0.5192\n",
            "Epoch 21/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9726 - acc: 0.6151\n",
            "Epoch 00021: val_loss did not improve from 1.19975\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.9726 - acc: 0.6161 - val_loss: 1.3190 - val_acc: 0.4958\n",
            "Epoch 22/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9546 - acc: 0.6249\n",
            "Epoch 00022: val_loss did not improve from 1.19975\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.9579 - acc: 0.6247 - val_loss: 1.2906 - val_acc: 0.5017\n",
            "Epoch 23/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9261 - acc: 0.6334\n",
            "Epoch 00023: val_loss improved from 1.19975 to 1.05404, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.9255 - acc: 0.6344 - val_loss: 1.0540 - val_acc: 0.5692\n",
            "Epoch 24/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8667 - acc: 0.6700\n",
            "Epoch 00024: val_loss did not improve from 1.05404\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.8677 - acc: 0.6694 - val_loss: 1.0913 - val_acc: 0.5575\n",
            "Epoch 25/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.8605 - acc: 0.6746\n",
            "Epoch 00025: val_loss did not improve from 1.05404\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.8586 - acc: 0.6739 - val_loss: 1.1294 - val_acc: 0.5550\n",
            "Epoch 26/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8409 - acc: 0.6788\n",
            "Epoch 00026: val_loss improved from 1.05404 to 1.03305, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 0.8426 - acc: 0.6778 - val_loss: 1.0330 - val_acc: 0.5908\n",
            "Epoch 27/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8057 - acc: 0.6932\n",
            "Epoch 00027: val_loss did not improve from 1.03305\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.8080 - acc: 0.6933 - val_loss: 1.0755 - val_acc: 0.5750\n",
            "Epoch 28/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7971 - acc: 0.6909\n",
            "Epoch 00028: val_loss did not improve from 1.03305\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.8013 - acc: 0.6894 - val_loss: 1.0487 - val_acc: 0.5950\n",
            "Epoch 29/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7885 - acc: 0.6897\n",
            "Epoch 00029: val_loss did not improve from 1.03305\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.7865 - acc: 0.6917 - val_loss: 1.1841 - val_acc: 0.5650\n",
            "Epoch 30/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7712 - acc: 0.7112\n",
            "Epoch 00030: val_loss improved from 1.03305 to 0.96993, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 223us/sample - loss: 0.7708 - acc: 0.7117 - val_loss: 0.9699 - val_acc: 0.6125\n",
            "Epoch 31/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7382 - acc: 0.7149\n",
            "Epoch 00031: val_loss did not improve from 0.96993\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.7392 - acc: 0.7147 - val_loss: 1.0018 - val_acc: 0.5942\n",
            "Epoch 32/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7197 - acc: 0.7234\n",
            "Epoch 00032: val_loss improved from 0.96993 to 0.96821, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.7163 - acc: 0.7250 - val_loss: 0.9682 - val_acc: 0.6308\n",
            "Epoch 33/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6955 - acc: 0.7409\n",
            "Epoch 00033: val_loss improved from 0.96821 to 0.88381, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.6945 - acc: 0.7408 - val_loss: 0.8838 - val_acc: 0.6417\n",
            "Epoch 34/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.6614 - acc: 0.7612\n",
            "Epoch 00034: val_loss did not improve from 0.88381\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.6676 - acc: 0.7583 - val_loss: 0.9100 - val_acc: 0.6525\n",
            "Epoch 35/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6462 - acc: 0.7626\n",
            "Epoch 00035: val_loss improved from 0.88381 to 0.86143, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.6459 - acc: 0.7633 - val_loss: 0.8614 - val_acc: 0.6617\n",
            "Epoch 36/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6589 - acc: 0.7520\n",
            "Epoch 00036: val_loss did not improve from 0.86143\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.6607 - acc: 0.7533 - val_loss: 1.0850 - val_acc: 0.6025\n",
            "Epoch 37/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6471 - acc: 0.7709\n",
            "Epoch 00037: val_loss improved from 0.86143 to 0.83888, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.6480 - acc: 0.7697 - val_loss: 0.8389 - val_acc: 0.6675\n",
            "Epoch 38/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6061 - acc: 0.7821\n",
            "Epoch 00038: val_loss did not improve from 0.83888\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.6102 - acc: 0.7808 - val_loss: 0.8918 - val_acc: 0.6542\n",
            "Epoch 39/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6028 - acc: 0.7740\n",
            "Epoch 00039: val_loss improved from 0.83888 to 0.82025, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 0.6023 - acc: 0.7742 - val_loss: 0.8202 - val_acc: 0.6775\n",
            "Epoch 40/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5848 - acc: 0.7932\n",
            "Epoch 00040: val_loss did not improve from 0.82025\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.5840 - acc: 0.7942 - val_loss: 0.8936 - val_acc: 0.6600\n",
            "Epoch 41/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5534 - acc: 0.8003\n",
            "Epoch 00041: val_loss did not improve from 0.82025\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.5553 - acc: 0.7989 - val_loss: 0.8544 - val_acc: 0.6675\n",
            "Epoch 42/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5552 - acc: 0.8021\n",
            "Epoch 00042: val_loss improved from 0.82025 to 0.76560, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.5543 - acc: 0.8028 - val_loss: 0.7656 - val_acc: 0.7058\n",
            "Epoch 43/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5306 - acc: 0.8129\n",
            "Epoch 00043: val_loss did not improve from 0.76560\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.5302 - acc: 0.8122 - val_loss: 0.7753 - val_acc: 0.7150\n",
            "Epoch 44/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.5321 - acc: 0.8112\n",
            "Epoch 00044: val_loss did not improve from 0.76560\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.5357 - acc: 0.8089 - val_loss: 0.9163 - val_acc: 0.6517\n",
            "Epoch 45/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5267 - acc: 0.8097\n",
            "Epoch 00045: val_loss did not improve from 0.76560\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.5309 - acc: 0.8078 - val_loss: 0.8269 - val_acc: 0.6867\n",
            "Epoch 46/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4744 - acc: 0.8359\n",
            "Epoch 00046: val_loss improved from 0.76560 to 0.72705, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.4810 - acc: 0.8336 - val_loss: 0.7271 - val_acc: 0.7183\n",
            "Epoch 47/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4758 - acc: 0.8338\n",
            "Epoch 00047: val_loss improved from 0.72705 to 0.70409, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.4776 - acc: 0.8317 - val_loss: 0.7041 - val_acc: 0.7258\n",
            "Epoch 48/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4494 - acc: 0.8438\n",
            "Epoch 00048: val_loss improved from 0.70409 to 0.67952, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.4491 - acc: 0.8447 - val_loss: 0.6795 - val_acc: 0.7275\n",
            "Epoch 49/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4327 - acc: 0.8560\n",
            "Epoch 00049: val_loss improved from 0.67952 to 0.67189, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 0.4321 - acc: 0.8561 - val_loss: 0.6719 - val_acc: 0.7367\n",
            "Epoch 50/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.4327 - acc: 0.8528\n",
            "Epoch 00050: val_loss improved from 0.67189 to 0.64917, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.4355 - acc: 0.8519 - val_loss: 0.6492 - val_acc: 0.7675\n",
            "Epoch 51/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.4667 - acc: 0.8334\n",
            "Epoch 00051: val_loss did not improve from 0.64917\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.4729 - acc: 0.8308 - val_loss: 0.6571 - val_acc: 0.7392\n",
            "Epoch 52/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4376 - acc: 0.8491\n",
            "Epoch 00052: val_loss did not improve from 0.64917\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.4343 - acc: 0.8508 - val_loss: 0.7473 - val_acc: 0.7292\n",
            "Epoch 53/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4290 - acc: 0.8488\n",
            "Epoch 00053: val_loss improved from 0.64917 to 0.63285, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 223us/sample - loss: 0.4249 - acc: 0.8511 - val_loss: 0.6328 - val_acc: 0.7625\n",
            "Epoch 54/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4265 - acc: 0.8531\n",
            "Epoch 00054: val_loss did not improve from 0.63285\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.4256 - acc: 0.8536 - val_loss: 0.7167 - val_acc: 0.7267\n",
            "Epoch 55/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.4022 - acc: 0.8658\n",
            "Epoch 00055: val_loss improved from 0.63285 to 0.57359, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.3977 - acc: 0.8689 - val_loss: 0.5736 - val_acc: 0.7775\n",
            "Epoch 56/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3781 - acc: 0.8694\n",
            "Epoch 00056: val_loss improved from 0.57359 to 0.57166, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.3785 - acc: 0.8697 - val_loss: 0.5717 - val_acc: 0.7717\n",
            "Epoch 57/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3845 - acc: 0.8785\n",
            "Epoch 00057: val_loss improved from 0.57166 to 0.54751, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 223us/sample - loss: 0.3899 - acc: 0.8744 - val_loss: 0.5475 - val_acc: 0.7975\n",
            "Epoch 58/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3699 - acc: 0.8773\n",
            "Epoch 00058: val_loss improved from 0.54751 to 0.50416, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.3751 - acc: 0.8731 - val_loss: 0.5042 - val_acc: 0.8075\n",
            "Epoch 59/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3370 - acc: 0.8926\n",
            "Epoch 00059: val_loss did not improve from 0.50416\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.3388 - acc: 0.8922 - val_loss: 0.5398 - val_acc: 0.7942\n",
            "Epoch 60/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3697 - acc: 0.8744\n",
            "Epoch 00060: val_loss did not improve from 0.50416\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.3668 - acc: 0.8756 - val_loss: 0.5390 - val_acc: 0.7917\n",
            "Epoch 61/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3637 - acc: 0.8774\n",
            "Epoch 00061: val_loss did not improve from 0.50416\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.3635 - acc: 0.8775 - val_loss: 0.5044 - val_acc: 0.8108\n",
            "Epoch 62/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3459 - acc: 0.8826\n",
            "Epoch 00062: val_loss did not improve from 0.50416\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.3449 - acc: 0.8828 - val_loss: 0.5431 - val_acc: 0.8017\n",
            "Epoch 63/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3468 - acc: 0.8800\n",
            "Epoch 00063: val_loss did not improve from 0.50416\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.3499 - acc: 0.8789 - val_loss: 0.5665 - val_acc: 0.7842\n",
            "Epoch 64/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3243 - acc: 0.8927\n",
            "Epoch 00064: val_loss did not improve from 0.50416\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.3300 - acc: 0.8903 - val_loss: 0.5068 - val_acc: 0.8200\n",
            "Epoch 65/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3325 - acc: 0.8926\n",
            "Epoch 00065: val_loss improved from 0.50416 to 0.50057, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.3315 - acc: 0.8919 - val_loss: 0.5006 - val_acc: 0.8058\n",
            "Epoch 66/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3063 - acc: 0.9048\n",
            "Epoch 00066: val_loss improved from 0.50057 to 0.49876, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.3110 - acc: 0.9017 - val_loss: 0.4988 - val_acc: 0.8058\n",
            "Epoch 67/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3048 - acc: 0.8976\n",
            "Epoch 00067: val_loss improved from 0.49876 to 0.49105, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 0.3035 - acc: 0.8986 - val_loss: 0.4911 - val_acc: 0.8125\n",
            "Epoch 68/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2970 - acc: 0.9035\n",
            "Epoch 00068: val_loss did not improve from 0.49105\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.2967 - acc: 0.9044 - val_loss: 0.5069 - val_acc: 0.8100\n",
            "Epoch 69/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2943 - acc: 0.9011\n",
            "Epoch 00069: val_loss improved from 0.49105 to 0.40873, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.2954 - acc: 0.9008 - val_loss: 0.4087 - val_acc: 0.8550\n",
            "Epoch 70/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2865 - acc: 0.9103\n",
            "Epoch 00070: val_loss did not improve from 0.40873\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.2878 - acc: 0.9089 - val_loss: 0.4364 - val_acc: 0.8317\n",
            "Epoch 71/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2823 - acc: 0.9091\n",
            "Epoch 00071: val_loss did not improve from 0.40873\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2814 - acc: 0.9097 - val_loss: 0.4122 - val_acc: 0.8500\n",
            "Epoch 72/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2882 - acc: 0.9060\n",
            "Epoch 00072: val_loss did not improve from 0.40873\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.2878 - acc: 0.9056 - val_loss: 0.4910 - val_acc: 0.8158\n",
            "Epoch 73/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2985 - acc: 0.9069\n",
            "Epoch 00073: val_loss did not improve from 0.40873\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2958 - acc: 0.9064 - val_loss: 0.4437 - val_acc: 0.8375\n",
            "Epoch 74/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2766 - acc: 0.9128\n",
            "Epoch 00074: val_loss did not improve from 0.40873\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.2795 - acc: 0.9128 - val_loss: 0.4409 - val_acc: 0.8333\n",
            "Epoch 75/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2832 - acc: 0.9083\n",
            "Epoch 00075: val_loss did not improve from 0.40873\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.2855 - acc: 0.9078 - val_loss: 0.4262 - val_acc: 0.8450\n",
            "Epoch 76/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2462 - acc: 0.9242\n",
            "Epoch 00076: val_loss did not improve from 0.40873\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.2471 - acc: 0.9250 - val_loss: 0.4210 - val_acc: 0.8325\n",
            "Epoch 77/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2563 - acc: 0.9174\n",
            "Epoch 00077: val_loss improved from 0.40873 to 0.37753, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.2579 - acc: 0.9172 - val_loss: 0.3775 - val_acc: 0.8733\n",
            "Epoch 78/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2485 - acc: 0.9188\n",
            "Epoch 00078: val_loss improved from 0.37753 to 0.35523, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.2484 - acc: 0.9194 - val_loss: 0.3552 - val_acc: 0.8833\n",
            "Epoch 79/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9309\n",
            "Epoch 00079: val_loss did not improve from 0.35523\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2298 - acc: 0.9311 - val_loss: 0.4252 - val_acc: 0.8467\n",
            "Epoch 80/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9309\n",
            "Epoch 00080: val_loss did not improve from 0.35523\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.2255 - acc: 0.9308 - val_loss: 0.3869 - val_acc: 0.8625\n",
            "Epoch 81/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2221 - acc: 0.9309\n",
            "Epoch 00081: val_loss did not improve from 0.35523\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.2258 - acc: 0.9300 - val_loss: 0.4098 - val_acc: 0.8608\n",
            "Epoch 82/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 0.9349\n",
            "Epoch 00082: val_loss did not improve from 0.35523\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2188 - acc: 0.9336 - val_loss: 0.3713 - val_acc: 0.8717\n",
            "Epoch 83/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9309\n",
            "Epoch 00083: val_loss did not improve from 0.35523\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.2218 - acc: 0.9308 - val_loss: 0.3606 - val_acc: 0.8717\n",
            "Epoch 84/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 0.9374\n",
            "Epoch 00084: val_loss did not improve from 0.35523\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2159 - acc: 0.9369 - val_loss: 0.3628 - val_acc: 0.8658\n",
            "Epoch 85/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2116 - acc: 0.9341\n",
            "Epoch 00085: val_loss improved from 0.35523 to 0.29805, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.2108 - acc: 0.9347 - val_loss: 0.2981 - val_acc: 0.8908\n",
            "Epoch 86/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2304 - acc: 0.9245\n",
            "Epoch 00086: val_loss did not improve from 0.29805\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.2293 - acc: 0.9258 - val_loss: 0.3986 - val_acc: 0.8550\n",
            "Epoch 87/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9209\n",
            "Epoch 00087: val_loss did not improve from 0.29805\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.2391 - acc: 0.9203 - val_loss: 0.3259 - val_acc: 0.8858\n",
            "Epoch 88/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9286\n",
            "Epoch 00088: val_loss did not improve from 0.29805\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.2274 - acc: 0.9278 - val_loss: 0.3444 - val_acc: 0.8717\n",
            "Epoch 89/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9340\n",
            "Epoch 00089: val_loss did not improve from 0.29805\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.2199 - acc: 0.9342 - val_loss: 0.3543 - val_acc: 0.8742\n",
            "Epoch 90/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1971 - acc: 0.9403\n",
            "Epoch 00090: val_loss did not improve from 0.29805\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1963 - acc: 0.9414 - val_loss: 0.3819 - val_acc: 0.8575\n",
            "Epoch 91/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2053 - acc: 0.9371\n",
            "Epoch 00091: val_loss did not improve from 0.29805\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.2031 - acc: 0.9386 - val_loss: 0.3526 - val_acc: 0.8842\n",
            "Epoch 92/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1955 - acc: 0.9442\n",
            "Epoch 00092: val_loss did not improve from 0.29805\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.1954 - acc: 0.9447 - val_loss: 0.3355 - val_acc: 0.8833\n",
            "Epoch 93/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2074 - acc: 0.9347\n",
            "Epoch 00093: val_loss did not improve from 0.29805\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.2088 - acc: 0.9344 - val_loss: 0.3438 - val_acc: 0.8817\n",
            "Epoch 94/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.9363\n",
            "Epoch 00094: val_loss did not improve from 0.29805\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.2048 - acc: 0.9358 - val_loss: 0.4141 - val_acc: 0.8567\n",
            "Epoch 95/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1995 - acc: 0.9376\n",
            "Epoch 00095: val_loss did not improve from 0.29805\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1989 - acc: 0.9381 - val_loss: 0.3712 - val_acc: 0.8583\n",
            "Epoch 96/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1912 - acc: 0.9418\n",
            "Epoch 00096: val_loss did not improve from 0.29805\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1899 - acc: 0.9428 - val_loss: 0.3534 - val_acc: 0.8708\n",
            "Epoch 97/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9397\n",
            "Epoch 00097: val_loss improved from 0.29805 to 0.28897, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 0.1896 - acc: 0.9392 - val_loss: 0.2890 - val_acc: 0.9067\n",
            "Epoch 98/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1825 - acc: 0.9459\n",
            "Epoch 00098: val_loss improved from 0.28897 to 0.27751, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.1813 - acc: 0.9450 - val_loss: 0.2775 - val_acc: 0.9075\n",
            "Epoch 99/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2017 - acc: 0.9347\n",
            "Epoch 00099: val_loss improved from 0.27751 to 0.25392, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.1999 - acc: 0.9353 - val_loss: 0.2539 - val_acc: 0.9175\n",
            "Epoch 100/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2138 - acc: 0.9323\n",
            "Epoch 00100: val_loss did not improve from 0.25392\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.2141 - acc: 0.9322 - val_loss: 0.3051 - val_acc: 0.8875\n",
            "Epoch 101/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1812 - acc: 0.9453\n",
            "Epoch 00101: val_loss did not improve from 0.25392\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1831 - acc: 0.9444 - val_loss: 0.2986 - val_acc: 0.8983\n",
            "Epoch 102/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1763 - acc: 0.9485\n",
            "Epoch 00102: val_loss did not improve from 0.25392\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1769 - acc: 0.9489 - val_loss: 0.3010 - val_acc: 0.8983\n",
            "Epoch 103/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1729 - acc: 0.9462\n",
            "Epoch 00103: val_loss did not improve from 0.25392\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1715 - acc: 0.9475 - val_loss: 0.3078 - val_acc: 0.9025\n",
            "Epoch 104/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1663 - acc: 0.9497\n",
            "Epoch 00104: val_loss did not improve from 0.25392\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1685 - acc: 0.9483 - val_loss: 0.3036 - val_acc: 0.9025\n",
            "Epoch 105/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1695 - acc: 0.9503\n",
            "Epoch 00105: val_loss did not improve from 0.25392\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1696 - acc: 0.9500 - val_loss: 0.2728 - val_acc: 0.9058\n",
            "Epoch 106/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1668 - acc: 0.9464\n",
            "Epoch 00106: val_loss did not improve from 0.25392\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1626 - acc: 0.9483 - val_loss: 0.2585 - val_acc: 0.9142\n",
            "Epoch 107/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1952 - acc: 0.9355\n",
            "Epoch 00107: val_loss did not improve from 0.25392\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1955 - acc: 0.9361 - val_loss: 0.3234 - val_acc: 0.8842\n",
            "Epoch 108/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1857 - acc: 0.9415\n",
            "Epoch 00108: val_loss did not improve from 0.25392\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1842 - acc: 0.9431 - val_loss: 0.3172 - val_acc: 0.8825\n",
            "Epoch 109/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1769 - acc: 0.9458\n",
            "Epoch 00109: val_loss improved from 0.25392 to 0.23976, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.1764 - acc: 0.9458 - val_loss: 0.2398 - val_acc: 0.9158\n",
            "Epoch 110/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9477\n",
            "Epoch 00110: val_loss did not improve from 0.23976\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1759 - acc: 0.9483 - val_loss: 0.2579 - val_acc: 0.9075\n",
            "Epoch 111/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9457\n",
            "Epoch 00111: val_loss did not improve from 0.23976\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1771 - acc: 0.9453 - val_loss: 0.2812 - val_acc: 0.9075\n",
            "Epoch 112/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1653 - acc: 0.9482\n",
            "Epoch 00112: val_loss improved from 0.23976 to 0.21961, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 221us/sample - loss: 0.1701 - acc: 0.9461 - val_loss: 0.2196 - val_acc: 0.9208\n",
            "Epoch 113/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1686 - acc: 0.9479\n",
            "Epoch 00113: val_loss did not improve from 0.21961\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1673 - acc: 0.9486 - val_loss: 0.2872 - val_acc: 0.9000\n",
            "Epoch 114/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9594\n",
            "Epoch 00114: val_loss did not improve from 0.21961\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1440 - acc: 0.9592 - val_loss: 0.2553 - val_acc: 0.9175\n",
            "Epoch 115/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9549\n",
            "Epoch 00115: val_loss did not improve from 0.21961\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1562 - acc: 0.9547 - val_loss: 0.2247 - val_acc: 0.9225\n",
            "Epoch 116/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9511\n",
            "Epoch 00116: val_loss did not improve from 0.21961\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1590 - acc: 0.9517 - val_loss: 0.2441 - val_acc: 0.9183\n",
            "Epoch 117/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9500\n",
            "Epoch 00117: val_loss did not improve from 0.21961\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1532 - acc: 0.9511 - val_loss: 0.2471 - val_acc: 0.9117\n",
            "Epoch 118/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1425 - acc: 0.9600\n",
            "Epoch 00118: val_loss improved from 0.21961 to 0.19492, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.1424 - acc: 0.9597 - val_loss: 0.1949 - val_acc: 0.9383\n",
            "Epoch 119/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1657 - acc: 0.9503\n",
            "Epoch 00119: val_loss did not improve from 0.19492\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1641 - acc: 0.9511 - val_loss: 0.2300 - val_acc: 0.9292\n",
            "Epoch 120/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1631 - acc: 0.9474\n",
            "Epoch 00120: val_loss did not improve from 0.19492\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1649 - acc: 0.9467 - val_loss: 0.2405 - val_acc: 0.9142\n",
            "Epoch 121/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1595 - acc: 0.9525\n",
            "Epoch 00121: val_loss did not improve from 0.19492\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1586 - acc: 0.9531 - val_loss: 0.2222 - val_acc: 0.9258\n",
            "Epoch 122/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1558 - acc: 0.9506\n",
            "Epoch 00122: val_loss did not improve from 0.19492\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1553 - acc: 0.9503 - val_loss: 0.2179 - val_acc: 0.9292\n",
            "Epoch 123/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9514\n",
            "Epoch 00123: val_loss did not improve from 0.19492\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 0.1570 - acc: 0.9514 - val_loss: 0.2262 - val_acc: 0.9250\n",
            "Epoch 124/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1420 - acc: 0.9591\n",
            "Epoch 00124: val_loss did not improve from 0.19492\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1412 - acc: 0.9594 - val_loss: 0.2275 - val_acc: 0.9275\n",
            "Epoch 125/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.9563\n",
            "Epoch 00125: val_loss did not improve from 0.19492\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1481 - acc: 0.9569 - val_loss: 0.2404 - val_acc: 0.9275\n",
            "Epoch 126/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1564 - acc: 0.9524\n",
            "Epoch 00126: val_loss did not improve from 0.19492\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1544 - acc: 0.9536 - val_loss: 0.2220 - val_acc: 0.9300\n",
            "Epoch 127/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1676 - acc: 0.9426\n",
            "Epoch 00127: val_loss did not improve from 0.19492\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1659 - acc: 0.9436 - val_loss: 0.2546 - val_acc: 0.9125\n",
            "Epoch 128/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1671 - acc: 0.9456\n",
            "Epoch 00128: val_loss did not improve from 0.19492\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1666 - acc: 0.9467 - val_loss: 0.2036 - val_acc: 0.9342\n",
            "Epoch 129/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.9460\n",
            "Epoch 00129: val_loss did not improve from 0.19492\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1650 - acc: 0.9458 - val_loss: 0.3053 - val_acc: 0.8892\n",
            "Epoch 130/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9531\n",
            "Epoch 00130: val_loss did not improve from 0.19492\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1507 - acc: 0.9542 - val_loss: 0.2040 - val_acc: 0.9308\n",
            "Epoch 131/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9560\n",
            "Epoch 00131: val_loss did not improve from 0.19492\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1394 - acc: 0.9556 - val_loss: 0.2144 - val_acc: 0.9242\n",
            "Epoch 132/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1406 - acc: 0.9591\n",
            "Epoch 00132: val_loss improved from 0.19492 to 0.18221, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.1390 - acc: 0.9603 - val_loss: 0.1822 - val_acc: 0.9458\n",
            "Epoch 133/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1218 - acc: 0.9653\n",
            "Epoch 00133: val_loss did not improve from 0.18221\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1247 - acc: 0.9631 - val_loss: 0.2055 - val_acc: 0.9342\n",
            "Epoch 134/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1336 - acc: 0.9638\n",
            "Epoch 00134: val_loss did not improve from 0.18221\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1340 - acc: 0.9631 - val_loss: 0.2013 - val_acc: 0.9400\n",
            "Epoch 135/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1334 - acc: 0.9619\n",
            "Epoch 00135: val_loss did not improve from 0.18221\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1348 - acc: 0.9606 - val_loss: 0.2810 - val_acc: 0.8983\n",
            "Epoch 136/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1602 - acc: 0.9497\n",
            "Epoch 00136: val_loss did not improve from 0.18221\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1629 - acc: 0.9492 - val_loss: 0.2523 - val_acc: 0.9142\n",
            "Epoch 137/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9474\n",
            "Epoch 00137: val_loss did not improve from 0.18221\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1532 - acc: 0.9478 - val_loss: 0.2004 - val_acc: 0.9417\n",
            "Epoch 138/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9671\n",
            "Epoch 00138: val_loss did not improve from 0.18221\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1179 - acc: 0.9672 - val_loss: 0.2333 - val_acc: 0.9200\n",
            "Epoch 139/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1294 - acc: 0.9603\n",
            "Epoch 00139: val_loss did not improve from 0.18221\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1293 - acc: 0.9606 - val_loss: 0.2137 - val_acc: 0.9217\n",
            "Epoch 140/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1325 - acc: 0.9644\n",
            "Epoch 00140: val_loss did not improve from 0.18221\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1334 - acc: 0.9644 - val_loss: 0.2294 - val_acc: 0.9208\n",
            "Epoch 141/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9606\n",
            "Epoch 00141: val_loss improved from 0.18221 to 0.17341, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.1271 - acc: 0.9614 - val_loss: 0.1734 - val_acc: 0.9442\n",
            "Epoch 142/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1528 - acc: 0.9450\n",
            "Epoch 00142: val_loss did not improve from 0.17341\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1560 - acc: 0.9442 - val_loss: 0.2323 - val_acc: 0.9242\n",
            "Epoch 143/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1489 - acc: 0.9535\n",
            "Epoch 00143: val_loss did not improve from 0.17341\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1478 - acc: 0.9539 - val_loss: 0.1864 - val_acc: 0.9475\n",
            "Epoch 144/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1221 - acc: 0.9670\n",
            "Epoch 00144: val_loss did not improve from 0.17341\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1224 - acc: 0.9669 - val_loss: 0.2316 - val_acc: 0.9317\n",
            "Epoch 145/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1230 - acc: 0.9627\n",
            "Epoch 00145: val_loss did not improve from 0.17341\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1249 - acc: 0.9622 - val_loss: 0.2124 - val_acc: 0.9350\n",
            "Epoch 146/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1329 - acc: 0.9623\n",
            "Epoch 00146: val_loss did not improve from 0.17341\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1315 - acc: 0.9628 - val_loss: 0.1956 - val_acc: 0.9292\n",
            "Epoch 147/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9651\n",
            "Epoch 00147: val_loss did not improve from 0.17341\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1148 - acc: 0.9658 - val_loss: 0.1980 - val_acc: 0.9342\n",
            "Epoch 148/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1134 - acc: 0.9682\n",
            "Epoch 00148: val_loss did not improve from 0.17341\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1146 - acc: 0.9689 - val_loss: 0.2131 - val_acc: 0.9275\n",
            "Epoch 149/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1257 - acc: 0.9615\n",
            "Epoch 00149: val_loss improved from 0.17341 to 0.15883, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.1248 - acc: 0.9614 - val_loss: 0.1588 - val_acc: 0.9475\n",
            "Epoch 150/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1354 - acc: 0.9573\n",
            "Epoch 00150: val_loss did not improve from 0.15883\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1342 - acc: 0.9578 - val_loss: 0.2046 - val_acc: 0.9383\n",
            "Epoch 151/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1277 - acc: 0.9571\n",
            "Epoch 00151: val_loss did not improve from 0.15883\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1294 - acc: 0.9558 - val_loss: 0.2389 - val_acc: 0.9158\n",
            "Epoch 152/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.9603\n",
            "Epoch 00152: val_loss did not improve from 0.15883\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1359 - acc: 0.9600 - val_loss: 0.1938 - val_acc: 0.9333\n",
            "Epoch 153/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9611\n",
            "Epoch 00153: val_loss did not improve from 0.15883\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1216 - acc: 0.9611 - val_loss: 0.1626 - val_acc: 0.9492\n",
            "Epoch 154/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.9591\n",
            "Epoch 00154: val_loss did not improve from 0.15883\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1323 - acc: 0.9600 - val_loss: 0.1700 - val_acc: 0.9425\n",
            "Epoch 155/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1459 - acc: 0.9553\n",
            "Epoch 00155: val_loss did not improve from 0.15883\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1447 - acc: 0.9550 - val_loss: 0.1909 - val_acc: 0.9375\n",
            "Epoch 156/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9617\n",
            "Epoch 00156: val_loss did not improve from 0.15883\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1269 - acc: 0.9625 - val_loss: 0.1790 - val_acc: 0.9442\n",
            "Epoch 157/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9643\n",
            "Epoch 00157: val_loss did not improve from 0.15883\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1210 - acc: 0.9642 - val_loss: 0.2030 - val_acc: 0.9308\n",
            "Epoch 158/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1194 - acc: 0.9650\n",
            "Epoch 00158: val_loss did not improve from 0.15883\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1171 - acc: 0.9664 - val_loss: 0.1829 - val_acc: 0.9383\n",
            "Epoch 159/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1307 - acc: 0.9597\n",
            "Epoch 00159: val_loss did not improve from 0.15883\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1327 - acc: 0.9589 - val_loss: 0.1690 - val_acc: 0.9383\n",
            "Epoch 160/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9646\n",
            "Epoch 00160: val_loss improved from 0.15883 to 0.14382, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.1172 - acc: 0.9642 - val_loss: 0.1438 - val_acc: 0.9575\n",
            "Epoch 161/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1169 - acc: 0.9653\n",
            "Epoch 00161: val_loss did not improve from 0.14382\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1177 - acc: 0.9656 - val_loss: 0.1655 - val_acc: 0.9450\n",
            "Epoch 162/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1207 - acc: 0.9636\n",
            "Epoch 00162: val_loss did not improve from 0.14382\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1266 - acc: 0.9611 - val_loss: 0.2233 - val_acc: 0.9308\n",
            "Epoch 163/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1485 - acc: 0.9503\n",
            "Epoch 00163: val_loss did not improve from 0.14382\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1467 - acc: 0.9508 - val_loss: 0.2050 - val_acc: 0.9275\n",
            "Epoch 164/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1139 - acc: 0.9679\n",
            "Epoch 00164: val_loss did not improve from 0.14382\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1153 - acc: 0.9678 - val_loss: 0.1790 - val_acc: 0.9433\n",
            "Epoch 165/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1081 - acc: 0.9675\n",
            "Epoch 00165: val_loss did not improve from 0.14382\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1075 - acc: 0.9686 - val_loss: 0.1833 - val_acc: 0.9367\n",
            "Epoch 166/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9589\n",
            "Epoch 00166: val_loss did not improve from 0.14382\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1284 - acc: 0.9592 - val_loss: 0.1546 - val_acc: 0.9483\n",
            "Epoch 167/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9629\n",
            "Epoch 00167: val_loss did not improve from 0.14382\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1178 - acc: 0.9631 - val_loss: 0.1632 - val_acc: 0.9500\n",
            "Epoch 168/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1290 - acc: 0.9579\n",
            "Epoch 00168: val_loss did not improve from 0.14382\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1271 - acc: 0.9592 - val_loss: 0.1550 - val_acc: 0.9500\n",
            "Epoch 169/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1390 - acc: 0.9562\n",
            "Epoch 00169: val_loss improved from 0.14382 to 0.13861, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.1358 - acc: 0.9578 - val_loss: 0.1386 - val_acc: 0.9583\n",
            "Epoch 170/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1279 - acc: 0.9578\n",
            "Epoch 00170: val_loss did not improve from 0.13861\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1274 - acc: 0.9594 - val_loss: 0.1459 - val_acc: 0.9517\n",
            "Epoch 171/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9674\n",
            "Epoch 00171: val_loss did not improve from 0.13861\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1120 - acc: 0.9672 - val_loss: 0.1644 - val_acc: 0.9450\n",
            "Epoch 172/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9714\n",
            "Epoch 00172: val_loss did not improve from 0.13861\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1034 - acc: 0.9711 - val_loss: 0.1430 - val_acc: 0.9567\n",
            "Epoch 173/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1162 - acc: 0.9668\n",
            "Epoch 00173: val_loss did not improve from 0.13861\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1151 - acc: 0.9678 - val_loss: 0.1716 - val_acc: 0.9433\n",
            "Epoch 174/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9674\n",
            "Epoch 00174: val_loss did not improve from 0.13861\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1070 - acc: 0.9672 - val_loss: 0.2102 - val_acc: 0.9233\n",
            "Epoch 175/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1056 - acc: 0.9732\n",
            "Epoch 00175: val_loss did not improve from 0.13861\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1053 - acc: 0.9733 - val_loss: 0.1680 - val_acc: 0.9433\n",
            "Epoch 176/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1245 - acc: 0.9594\n",
            "Epoch 00176: val_loss did not improve from 0.13861\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1232 - acc: 0.9606 - val_loss: 0.1785 - val_acc: 0.9458\n",
            "Epoch 177/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1472 - acc: 0.9473\n",
            "Epoch 00177: val_loss did not improve from 0.13861\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1476 - acc: 0.9475 - val_loss: 0.1790 - val_acc: 0.9400\n",
            "Epoch 178/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1158 - acc: 0.9671\n",
            "Epoch 00178: val_loss improved from 0.13861 to 0.12827, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.1156 - acc: 0.9672 - val_loss: 0.1283 - val_acc: 0.9567\n",
            "Epoch 179/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1117 - acc: 0.9647\n",
            "Epoch 00179: val_loss did not improve from 0.12827\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1121 - acc: 0.9644 - val_loss: 0.1341 - val_acc: 0.9575\n",
            "Epoch 180/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1116 - acc: 0.9639\n",
            "Epoch 00180: val_loss did not improve from 0.12827\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1148 - acc: 0.9636 - val_loss: 0.1713 - val_acc: 0.9475\n",
            "Epoch 181/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1107 - acc: 0.9676\n",
            "Epoch 00181: val_loss did not improve from 0.12827\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1109 - acc: 0.9675 - val_loss: 0.1388 - val_acc: 0.9583\n",
            "Epoch 182/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1037 - acc: 0.9706\n",
            "Epoch 00182: val_loss did not improve from 0.12827\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1038 - acc: 0.9708 - val_loss: 0.1347 - val_acc: 0.9600\n",
            "Epoch 183/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1225 - acc: 0.9591\n",
            "Epoch 00183: val_loss did not improve from 0.12827\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1238 - acc: 0.9581 - val_loss: 0.1900 - val_acc: 0.9425\n",
            "Epoch 184/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1279 - acc: 0.9566\n",
            "Epoch 00184: val_loss improved from 0.12827 to 0.12515, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.1271 - acc: 0.9572 - val_loss: 0.1252 - val_acc: 0.9633\n",
            "Epoch 185/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9603\n",
            "Epoch 00185: val_loss did not improve from 0.12515\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1257 - acc: 0.9603 - val_loss: 0.1637 - val_acc: 0.9492\n",
            "Epoch 186/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9694\n",
            "Epoch 00186: val_loss did not improve from 0.12515\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1067 - acc: 0.9689 - val_loss: 0.1329 - val_acc: 0.9625\n",
            "Epoch 187/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9631\n",
            "Epoch 00187: val_loss did not improve from 0.12515\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1176 - acc: 0.9633 - val_loss: 0.1588 - val_acc: 0.9550\n",
            "Epoch 188/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1245 - acc: 0.9576\n",
            "Epoch 00188: val_loss improved from 0.12515 to 0.11594, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 0.1270 - acc: 0.9572 - val_loss: 0.1159 - val_acc: 0.9675\n",
            "Epoch 189/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1129 - acc: 0.9650\n",
            "Epoch 00189: val_loss did not improve from 0.11594\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1125 - acc: 0.9650 - val_loss: 0.1553 - val_acc: 0.9533\n",
            "Epoch 190/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9686\n",
            "Epoch 00190: val_loss did not improve from 0.11594\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1005 - acc: 0.9686 - val_loss: 0.1660 - val_acc: 0.9442\n",
            "Epoch 191/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9663\n",
            "Epoch 00191: val_loss improved from 0.11594 to 0.11449, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.1120 - acc: 0.9672 - val_loss: 0.1145 - val_acc: 0.9642\n",
            "Epoch 192/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9643\n",
            "Epoch 00192: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1060 - acc: 0.9650 - val_loss: 0.1380 - val_acc: 0.9542\n",
            "Epoch 193/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1045 - acc: 0.9635\n",
            "Epoch 00193: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1044 - acc: 0.9639 - val_loss: 0.1705 - val_acc: 0.9475\n",
            "Epoch 194/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1161 - acc: 0.9653\n",
            "Epoch 00194: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1143 - acc: 0.9658 - val_loss: 0.1248 - val_acc: 0.9675\n",
            "Epoch 195/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1031 - acc: 0.9700\n",
            "Epoch 00195: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1031 - acc: 0.9700 - val_loss: 0.2545 - val_acc: 0.9208\n",
            "Epoch 196/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1075 - acc: 0.9700\n",
            "Epoch 00196: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1062 - acc: 0.9711 - val_loss: 0.1400 - val_acc: 0.9575\n",
            "Epoch 197/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9637\n",
            "Epoch 00197: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1113 - acc: 0.9636 - val_loss: 0.1155 - val_acc: 0.9642\n",
            "Epoch 198/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1068 - acc: 0.9666\n",
            "Epoch 00198: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1081 - acc: 0.9661 - val_loss: 0.1670 - val_acc: 0.9483\n",
            "Epoch 199/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9717\n",
            "Epoch 00199: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0871 - acc: 0.9717 - val_loss: 0.1531 - val_acc: 0.9508\n",
            "Epoch 200/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1038 - acc: 0.9667\n",
            "Epoch 00200: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1055 - acc: 0.9653 - val_loss: 0.1682 - val_acc: 0.9408\n",
            "Epoch 201/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9700\n",
            "Epoch 00201: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1012 - acc: 0.9689 - val_loss: 0.1476 - val_acc: 0.9475\n",
            "Epoch 202/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1136 - acc: 0.9644\n",
            "Epoch 00202: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1132 - acc: 0.9653 - val_loss: 0.1295 - val_acc: 0.9575\n",
            "Epoch 203/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9683\n",
            "Epoch 00203: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1078 - acc: 0.9686 - val_loss: 0.1597 - val_acc: 0.9467\n",
            "Epoch 204/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9654\n",
            "Epoch 00204: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1151 - acc: 0.9653 - val_loss: 0.1758 - val_acc: 0.9383\n",
            "Epoch 205/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9611\n",
            "Epoch 00205: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1223 - acc: 0.9617 - val_loss: 0.1588 - val_acc: 0.9425\n",
            "Epoch 206/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9720\n",
            "Epoch 00206: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0922 - acc: 0.9722 - val_loss: 0.1354 - val_acc: 0.9625\n",
            "Epoch 207/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9663\n",
            "Epoch 00207: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1036 - acc: 0.9667 - val_loss: 0.1659 - val_acc: 0.9425\n",
            "Epoch 208/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0996 - acc: 0.9688\n",
            "Epoch 00208: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0992 - acc: 0.9694 - val_loss: 0.1370 - val_acc: 0.9550\n",
            "Epoch 209/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9620\n",
            "Epoch 00209: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1150 - acc: 0.9619 - val_loss: 0.1604 - val_acc: 0.9433\n",
            "Epoch 210/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1074 - acc: 0.9685\n",
            "Epoch 00210: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1120 - acc: 0.9658 - val_loss: 0.1610 - val_acc: 0.9517\n",
            "Epoch 211/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1338 - acc: 0.9565\n",
            "Epoch 00211: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1337 - acc: 0.9564 - val_loss: 0.1346 - val_acc: 0.9592\n",
            "Epoch 212/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9657\n",
            "Epoch 00212: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1021 - acc: 0.9650 - val_loss: 0.1469 - val_acc: 0.9500\n",
            "Epoch 213/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9717\n",
            "Epoch 00213: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0908 - acc: 0.9717 - val_loss: 0.1312 - val_acc: 0.9567\n",
            "Epoch 214/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9666\n",
            "Epoch 00214: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1135 - acc: 0.9661 - val_loss: 0.1948 - val_acc: 0.9325\n",
            "Epoch 215/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1119 - acc: 0.9668\n",
            "Epoch 00215: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1105 - acc: 0.9669 - val_loss: 0.1358 - val_acc: 0.9592\n",
            "Epoch 216/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9586\n",
            "Epoch 00216: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1223 - acc: 0.9583 - val_loss: 0.1577 - val_acc: 0.9567\n",
            "Epoch 217/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1005 - acc: 0.9700\n",
            "Epoch 00217: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1035 - acc: 0.9681 - val_loss: 0.1231 - val_acc: 0.9550\n",
            "Epoch 218/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9611\n",
            "Epoch 00218: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1176 - acc: 0.9611 - val_loss: 0.1825 - val_acc: 0.9350\n",
            "Epoch 219/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1050 - acc: 0.9685\n",
            "Epoch 00219: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1051 - acc: 0.9686 - val_loss: 0.1298 - val_acc: 0.9600\n",
            "Epoch 220/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1073 - acc: 0.9658\n",
            "Epoch 00220: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1074 - acc: 0.9658 - val_loss: 0.1537 - val_acc: 0.9425\n",
            "Epoch 221/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9520\n",
            "Epoch 00221: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1337 - acc: 0.9519 - val_loss: 0.1227 - val_acc: 0.9667\n",
            "Epoch 222/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1225 - acc: 0.9600\n",
            "Epoch 00222: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1256 - acc: 0.9597 - val_loss: 0.2209 - val_acc: 0.9275\n",
            "Epoch 223/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1044 - acc: 0.9682\n",
            "Epoch 00223: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1025 - acc: 0.9692 - val_loss: 0.1696 - val_acc: 0.9542\n",
            "Epoch 224/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9723\n",
            "Epoch 00224: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0946 - acc: 0.9719 - val_loss: 0.1542 - val_acc: 0.9517\n",
            "Epoch 225/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1192 - acc: 0.9603\n",
            "Epoch 00225: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1245 - acc: 0.9581 - val_loss: 0.1598 - val_acc: 0.9433\n",
            "Epoch 226/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1300 - acc: 0.9584\n",
            "Epoch 00226: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1315 - acc: 0.9583 - val_loss: 0.1523 - val_acc: 0.9550\n",
            "Epoch 227/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9703\n",
            "Epoch 00227: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1016 - acc: 0.9706 - val_loss: 0.1182 - val_acc: 0.9650\n",
            "Epoch 228/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1092 - acc: 0.9676\n",
            "Epoch 00228: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1068 - acc: 0.9683 - val_loss: 0.1377 - val_acc: 0.9500\n",
            "Epoch 229/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0951 - acc: 0.9709\n",
            "Epoch 00229: val_loss did not improve from 0.11449\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0944 - acc: 0.9706 - val_loss: 0.1486 - val_acc: 0.9542\n",
            "Epoch 230/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1034 - acc: 0.9688\n",
            "Epoch 00230: val_loss improved from 0.11449 to 0.11153, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.1020 - acc: 0.9697 - val_loss: 0.1115 - val_acc: 0.9633\n",
            "Epoch 231/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1028 - acc: 0.9676\n",
            "Epoch 00231: val_loss did not improve from 0.11153\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1053 - acc: 0.9669 - val_loss: 0.1343 - val_acc: 0.9600\n",
            "Epoch 232/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1004 - acc: 0.9671\n",
            "Epoch 00232: val_loss did not improve from 0.11153\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1007 - acc: 0.9669 - val_loss: 0.1366 - val_acc: 0.9508\n",
            "Epoch 233/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9674\n",
            "Epoch 00233: val_loss improved from 0.11153 to 0.10915, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.1023 - acc: 0.9669 - val_loss: 0.1092 - val_acc: 0.9683\n",
            "Epoch 234/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9686\n",
            "Epoch 00234: val_loss did not improve from 0.10915\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1017 - acc: 0.9686 - val_loss: 0.1317 - val_acc: 0.9592\n",
            "Epoch 235/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1031 - acc: 0.9641\n",
            "Epoch 00235: val_loss did not improve from 0.10915\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1076 - acc: 0.9625 - val_loss: 0.1675 - val_acc: 0.9483\n",
            "Epoch 236/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1129 - acc: 0.9635\n",
            "Epoch 00236: val_loss did not improve from 0.10915\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1124 - acc: 0.9644 - val_loss: 0.1554 - val_acc: 0.9492\n",
            "Epoch 237/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9731\n",
            "Epoch 00237: val_loss did not improve from 0.10915\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0921 - acc: 0.9719 - val_loss: 0.1233 - val_acc: 0.9633\n",
            "Epoch 238/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0962 - acc: 0.9712\n",
            "Epoch 00238: val_loss did not improve from 0.10915\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0971 - acc: 0.9711 - val_loss: 0.1354 - val_acc: 0.9525\n",
            "Epoch 239/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0849 - acc: 0.9759\n",
            "Epoch 00239: val_loss did not improve from 0.10915\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0859 - acc: 0.9753 - val_loss: 0.1521 - val_acc: 0.9450\n",
            "Epoch 240/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0869 - acc: 0.9703\n",
            "Epoch 00240: val_loss did not improve from 0.10915\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0849 - acc: 0.9714 - val_loss: 0.1368 - val_acc: 0.9567\n",
            "Epoch 241/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0949 - acc: 0.9709\n",
            "Epoch 00241: val_loss did not improve from 0.10915\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0959 - acc: 0.9697 - val_loss: 0.1170 - val_acc: 0.9658\n",
            "Epoch 242/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0969 - acc: 0.9712\n",
            "Epoch 00242: val_loss did not improve from 0.10915\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0982 - acc: 0.9708 - val_loss: 0.1473 - val_acc: 0.9508\n",
            "Epoch 243/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9760\n",
            "Epoch 00243: val_loss improved from 0.10915 to 0.09266, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 0.0825 - acc: 0.9767 - val_loss: 0.0927 - val_acc: 0.9708\n",
            "Epoch 244/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0822 - acc: 0.9761\n",
            "Epoch 00244: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0819 - acc: 0.9758 - val_loss: 0.1065 - val_acc: 0.9700\n",
            "Epoch 245/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0877 - acc: 0.9747\n",
            "Epoch 00245: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0891 - acc: 0.9736 - val_loss: 0.1203 - val_acc: 0.9575\n",
            "Epoch 246/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9663\n",
            "Epoch 00246: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1007 - acc: 0.9661 - val_loss: 0.1275 - val_acc: 0.9667\n",
            "Epoch 247/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9797\n",
            "Epoch 00247: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0681 - acc: 0.9800 - val_loss: 0.1323 - val_acc: 0.9575\n",
            "Epoch 248/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0781 - acc: 0.9764\n",
            "Epoch 00248: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0772 - acc: 0.9769 - val_loss: 0.1301 - val_acc: 0.9567\n",
            "Epoch 249/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0802 - acc: 0.9782\n",
            "Epoch 00249: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0811 - acc: 0.9778 - val_loss: 0.2457 - val_acc: 0.9092\n",
            "Epoch 250/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0853 - acc: 0.9732\n",
            "Epoch 00250: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0875 - acc: 0.9722 - val_loss: 0.1815 - val_acc: 0.9417\n",
            "Epoch 251/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0995 - acc: 0.9676\n",
            "Epoch 00251: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1021 - acc: 0.9656 - val_loss: 0.1156 - val_acc: 0.9633\n",
            "Epoch 252/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0915 - acc: 0.9738\n",
            "Epoch 00252: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0910 - acc: 0.9736 - val_loss: 0.1264 - val_acc: 0.9633\n",
            "Epoch 253/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0910 - acc: 0.9706\n",
            "Epoch 00253: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0957 - acc: 0.9689 - val_loss: 0.1418 - val_acc: 0.9508\n",
            "Epoch 254/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9709\n",
            "Epoch 00254: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0967 - acc: 0.9700 - val_loss: 0.0981 - val_acc: 0.9658\n",
            "Epoch 255/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0958 - acc: 0.9718\n",
            "Epoch 00255: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0979 - acc: 0.9708 - val_loss: 0.1536 - val_acc: 0.9467\n",
            "Epoch 256/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0986 - acc: 0.9735\n",
            "Epoch 00256: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.0971 - acc: 0.9739 - val_loss: 0.1320 - val_acc: 0.9583\n",
            "Epoch 257/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0966 - acc: 0.9674\n",
            "Epoch 00257: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0976 - acc: 0.9672 - val_loss: 0.1246 - val_acc: 0.9625\n",
            "Epoch 258/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0871 - acc: 0.9744\n",
            "Epoch 00258: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0885 - acc: 0.9731 - val_loss: 0.1605 - val_acc: 0.9508\n",
            "Epoch 259/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.9706\n",
            "Epoch 00259: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0954 - acc: 0.9706 - val_loss: 0.1046 - val_acc: 0.9642\n",
            "Epoch 260/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1021 - acc: 0.9639\n",
            "Epoch 00260: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1003 - acc: 0.9650 - val_loss: 0.1373 - val_acc: 0.9533\n",
            "Epoch 261/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0860 - acc: 0.9729\n",
            "Epoch 00261: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0848 - acc: 0.9736 - val_loss: 0.1290 - val_acc: 0.9633\n",
            "Epoch 262/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0821 - acc: 0.9747\n",
            "Epoch 00262: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0826 - acc: 0.9744 - val_loss: 0.1301 - val_acc: 0.9608\n",
            "Epoch 263/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0882 - acc: 0.9726\n",
            "Epoch 00263: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0862 - acc: 0.9731 - val_loss: 0.1350 - val_acc: 0.9592\n",
            "Epoch 264/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0936 - acc: 0.9706\n",
            "Epoch 00264: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0926 - acc: 0.9706 - val_loss: 0.1325 - val_acc: 0.9550\n",
            "Epoch 265/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1043 - acc: 0.9652\n",
            "Epoch 00265: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1032 - acc: 0.9669 - val_loss: 0.1183 - val_acc: 0.9550\n",
            "Epoch 266/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0994 - acc: 0.9674\n",
            "Epoch 00266: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0996 - acc: 0.9672 - val_loss: 0.1293 - val_acc: 0.9542\n",
            "Epoch 267/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0920 - acc: 0.9700\n",
            "Epoch 00267: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0936 - acc: 0.9689 - val_loss: 0.1216 - val_acc: 0.9583\n",
            "Epoch 268/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0858 - acc: 0.9738\n",
            "Epoch 00268: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0865 - acc: 0.9733 - val_loss: 0.1157 - val_acc: 0.9592\n",
            "Epoch 269/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0865 - acc: 0.9753\n",
            "Epoch 00269: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0890 - acc: 0.9736 - val_loss: 0.1474 - val_acc: 0.9492\n",
            "Epoch 270/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9783\n",
            "Epoch 00270: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0760 - acc: 0.9783 - val_loss: 0.1284 - val_acc: 0.9592\n",
            "Epoch 271/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9846\n",
            "Epoch 00271: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0659 - acc: 0.9833 - val_loss: 0.1072 - val_acc: 0.9633\n",
            "Epoch 272/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9754\n",
            "Epoch 00272: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0846 - acc: 0.9742 - val_loss: 0.0980 - val_acc: 0.9725\n",
            "Epoch 273/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9754\n",
            "Epoch 00273: val_loss did not improve from 0.09266\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0801 - acc: 0.9753 - val_loss: 0.1370 - val_acc: 0.9558\n",
            "Epoch 274/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0855 - acc: 0.9735\n",
            "Epoch 00274: val_loss improved from 0.09266 to 0.08837, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.0879 - acc: 0.9728 - val_loss: 0.0884 - val_acc: 0.9767\n",
            "Epoch 275/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1006 - acc: 0.9659\n",
            "Epoch 00275: val_loss did not improve from 0.08837\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.0983 - acc: 0.9669 - val_loss: 0.1093 - val_acc: 0.9667\n",
            "Epoch 276/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0948 - acc: 0.9671\n",
            "Epoch 00276: val_loss did not improve from 0.08837\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0997 - acc: 0.9642 - val_loss: 0.1680 - val_acc: 0.9425\n",
            "Epoch 277/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.9677\n",
            "Epoch 00277: val_loss did not improve from 0.08837\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0983 - acc: 0.9675 - val_loss: 0.1217 - val_acc: 0.9683\n",
            "Epoch 278/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0937 - acc: 0.9682\n",
            "Epoch 00278: val_loss improved from 0.08837 to 0.08058, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.0935 - acc: 0.9686 - val_loss: 0.0806 - val_acc: 0.9700\n",
            "Epoch 279/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0801 - acc: 0.9776\n",
            "Epoch 00279: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0836 - acc: 0.9769 - val_loss: 0.0868 - val_acc: 0.9750\n",
            "Epoch 280/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9723\n",
            "Epoch 00280: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0796 - acc: 0.9728 - val_loss: 0.1157 - val_acc: 0.9675\n",
            "Epoch 281/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0784 - acc: 0.9771\n",
            "Epoch 00281: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0774 - acc: 0.9772 - val_loss: 0.0965 - val_acc: 0.9692\n",
            "Epoch 282/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0901 - acc: 0.9712\n",
            "Epoch 00282: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0893 - acc: 0.9717 - val_loss: 0.1324 - val_acc: 0.9542\n",
            "Epoch 283/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9669\n",
            "Epoch 00283: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1029 - acc: 0.9675 - val_loss: 0.1251 - val_acc: 0.9558\n",
            "Epoch 284/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9660\n",
            "Epoch 00284: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1031 - acc: 0.9661 - val_loss: 0.1151 - val_acc: 0.9633\n",
            "Epoch 285/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1033 - acc: 0.9679\n",
            "Epoch 00285: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1051 - acc: 0.9664 - val_loss: 0.1385 - val_acc: 0.9550\n",
            "Epoch 286/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0857 - acc: 0.9744\n",
            "Epoch 00286: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0845 - acc: 0.9744 - val_loss: 0.0908 - val_acc: 0.9675\n",
            "Epoch 287/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0892 - acc: 0.9726\n",
            "Epoch 00287: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.0881 - acc: 0.9733 - val_loss: 0.1261 - val_acc: 0.9625\n",
            "Epoch 288/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0823 - acc: 0.9744\n",
            "Epoch 00288: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0819 - acc: 0.9750 - val_loss: 0.0885 - val_acc: 0.9733\n",
            "Epoch 289/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0842 - acc: 0.9706\n",
            "Epoch 00289: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0836 - acc: 0.9714 - val_loss: 0.1392 - val_acc: 0.9525\n",
            "Epoch 290/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0935 - acc: 0.9732\n",
            "Epoch 00290: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0926 - acc: 0.9731 - val_loss: 0.0919 - val_acc: 0.9742\n",
            "Epoch 291/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0863 - acc: 0.9729\n",
            "Epoch 00291: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0851 - acc: 0.9733 - val_loss: 0.0954 - val_acc: 0.9683\n",
            "Epoch 292/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0880 - acc: 0.9706\n",
            "Epoch 00292: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0879 - acc: 0.9708 - val_loss: 0.0872 - val_acc: 0.9750\n",
            "Epoch 293/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0835 - acc: 0.9726\n",
            "Epoch 00293: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0843 - acc: 0.9725 - val_loss: 0.1022 - val_acc: 0.9667\n",
            "Epoch 294/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0765 - acc: 0.9768\n",
            "Epoch 00294: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.0760 - acc: 0.9767 - val_loss: 0.1207 - val_acc: 0.9617\n",
            "Epoch 295/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0755 - acc: 0.9785\n",
            "Epoch 00295: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0749 - acc: 0.9783 - val_loss: 0.1033 - val_acc: 0.9667\n",
            "Epoch 296/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9743\n",
            "Epoch 00296: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0858 - acc: 0.9747 - val_loss: 0.1349 - val_acc: 0.9550\n",
            "Epoch 297/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0838 - acc: 0.9741\n",
            "Epoch 00297: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.0825 - acc: 0.9747 - val_loss: 0.1021 - val_acc: 0.9658\n",
            "Epoch 298/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0831 - acc: 0.9747\n",
            "Epoch 00298: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0847 - acc: 0.9744 - val_loss: 0.1235 - val_acc: 0.9592\n",
            "Epoch 299/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0816 - acc: 0.9765\n",
            "Epoch 00299: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0844 - acc: 0.9750 - val_loss: 0.1167 - val_acc: 0.9617\n",
            "Epoch 300/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9811\n",
            "Epoch 00300: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0643 - acc: 0.9811 - val_loss: 0.1003 - val_acc: 0.9767\n",
            "Epoch 301/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0804 - acc: 0.9768\n",
            "Epoch 00301: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0784 - acc: 0.9775 - val_loss: 0.1249 - val_acc: 0.9583\n",
            "Epoch 302/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0838 - acc: 0.9738\n",
            "Epoch 00302: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0844 - acc: 0.9742 - val_loss: 0.0925 - val_acc: 0.9750\n",
            "Epoch 303/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0882 - acc: 0.9724\n",
            "Epoch 00303: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0891 - acc: 0.9719 - val_loss: 0.1094 - val_acc: 0.9592\n",
            "Epoch 304/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0785 - acc: 0.9797\n",
            "Epoch 00304: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0791 - acc: 0.9794 - val_loss: 0.0893 - val_acc: 0.9683\n",
            "Epoch 305/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0773 - acc: 0.9779\n",
            "Epoch 00305: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0772 - acc: 0.9778 - val_loss: 0.1097 - val_acc: 0.9583\n",
            "Epoch 306/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0915 - acc: 0.9726\n",
            "Epoch 00306: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0896 - acc: 0.9731 - val_loss: 0.0831 - val_acc: 0.9800\n",
            "Epoch 307/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9706\n",
            "Epoch 00307: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0871 - acc: 0.9706 - val_loss: 0.1006 - val_acc: 0.9692\n",
            "Epoch 308/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0732 - acc: 0.9774\n",
            "Epoch 00308: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0730 - acc: 0.9783 - val_loss: 0.1224 - val_acc: 0.9575\n",
            "Epoch 309/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9826\n",
            "Epoch 00309: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0727 - acc: 0.9828 - val_loss: 0.0979 - val_acc: 0.9642\n",
            "Epoch 310/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9777\n",
            "Epoch 00310: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0655 - acc: 0.9781 - val_loss: 0.0862 - val_acc: 0.9750\n",
            "Epoch 311/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0739 - acc: 0.9788\n",
            "Epoch 00311: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.0753 - acc: 0.9778 - val_loss: 0.0908 - val_acc: 0.9717\n",
            "Epoch 312/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9709\n",
            "Epoch 00312: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0926 - acc: 0.9711 - val_loss: 0.0811 - val_acc: 0.9767\n",
            "Epoch 313/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0843 - acc: 0.9768\n",
            "Epoch 00313: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0848 - acc: 0.9764 - val_loss: 0.1563 - val_acc: 0.9483\n",
            "Epoch 314/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0867 - acc: 0.9718\n",
            "Epoch 00314: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0869 - acc: 0.9714 - val_loss: 0.1190 - val_acc: 0.9600\n",
            "Epoch 315/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0790 - acc: 0.9764\n",
            "Epoch 00315: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0769 - acc: 0.9775 - val_loss: 0.0923 - val_acc: 0.9717\n",
            "Epoch 316/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0717 - acc: 0.9791\n",
            "Epoch 00316: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0725 - acc: 0.9786 - val_loss: 0.0942 - val_acc: 0.9700\n",
            "Epoch 317/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0768 - acc: 0.9770\n",
            "Epoch 00317: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0761 - acc: 0.9772 - val_loss: 0.0998 - val_acc: 0.9675\n",
            "Epoch 318/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0982 - acc: 0.9685\n",
            "Epoch 00318: val_loss did not improve from 0.08058\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0968 - acc: 0.9689 - val_loss: 0.0861 - val_acc: 0.9667\n",
            "Epoch 319/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9777\n",
            "Epoch 00319: val_loss improved from 0.08058 to 0.07325, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.0825 - acc: 0.9778 - val_loss: 0.0733 - val_acc: 0.9800\n",
            "Epoch 320/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0669 - acc: 0.9800\n",
            "Epoch 00320: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0691 - acc: 0.9794 - val_loss: 0.1218 - val_acc: 0.9625\n",
            "Epoch 321/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0871 - acc: 0.9694\n",
            "Epoch 00321: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0881 - acc: 0.9689 - val_loss: 0.0938 - val_acc: 0.9717\n",
            "Epoch 322/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0813 - acc: 0.9735\n",
            "Epoch 00322: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0809 - acc: 0.9742 - val_loss: 0.1270 - val_acc: 0.9533\n",
            "Epoch 323/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0769 - acc: 0.9759\n",
            "Epoch 00323: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0766 - acc: 0.9761 - val_loss: 0.0995 - val_acc: 0.9708\n",
            "Epoch 324/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9694\n",
            "Epoch 00324: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0910 - acc: 0.9694 - val_loss: 0.1485 - val_acc: 0.9517\n",
            "Epoch 325/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1197 - acc: 0.9634\n",
            "Epoch 00325: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1192 - acc: 0.9636 - val_loss: 0.1271 - val_acc: 0.9592\n",
            "Epoch 326/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0763 - acc: 0.9776\n",
            "Epoch 00326: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0800 - acc: 0.9764 - val_loss: 0.1646 - val_acc: 0.9500\n",
            "Epoch 327/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0811 - acc: 0.9765\n",
            "Epoch 00327: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0832 - acc: 0.9753 - val_loss: 0.0802 - val_acc: 0.9708\n",
            "Epoch 328/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0690 - acc: 0.9794\n",
            "Epoch 00328: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0690 - acc: 0.9797 - val_loss: 0.0877 - val_acc: 0.9708\n",
            "Epoch 329/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0831 - acc: 0.9721\n",
            "Epoch 00329: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.0834 - acc: 0.9717 - val_loss: 0.1232 - val_acc: 0.9617\n",
            "Epoch 330/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9763\n",
            "Epoch 00330: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0766 - acc: 0.9761 - val_loss: 0.0890 - val_acc: 0.9783\n",
            "Epoch 331/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9737\n",
            "Epoch 00331: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0847 - acc: 0.9742 - val_loss: 0.0797 - val_acc: 0.9717\n",
            "Epoch 332/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0688 - acc: 0.9788\n",
            "Epoch 00332: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0679 - acc: 0.9792 - val_loss: 0.0793 - val_acc: 0.9725\n",
            "Epoch 333/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0738 - acc: 0.9765\n",
            "Epoch 00333: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0741 - acc: 0.9764 - val_loss: 0.0769 - val_acc: 0.9742\n",
            "Epoch 334/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0723 - acc: 0.9797\n",
            "Epoch 00334: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.0723 - acc: 0.9794 - val_loss: 0.0819 - val_acc: 0.9783\n",
            "Epoch 335/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9777\n",
            "Epoch 00335: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0746 - acc: 0.9772 - val_loss: 0.0818 - val_acc: 0.9825\n",
            "Epoch 336/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0783 - acc: 0.9765\n",
            "Epoch 00336: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0776 - acc: 0.9764 - val_loss: 0.0799 - val_acc: 0.9767\n",
            "Epoch 337/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0780 - acc: 0.9776\n",
            "Epoch 00337: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0781 - acc: 0.9772 - val_loss: 0.1023 - val_acc: 0.9683\n",
            "Epoch 338/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0769 - acc: 0.9748\n",
            "Epoch 00338: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0778 - acc: 0.9744 - val_loss: 0.0801 - val_acc: 0.9775\n",
            "Epoch 339/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0763 - acc: 0.9785\n",
            "Epoch 00339: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.0788 - acc: 0.9778 - val_loss: 0.0928 - val_acc: 0.9717\n",
            "Epoch 340/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0826 - acc: 0.9736\n",
            "Epoch 00340: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0815 - acc: 0.9742 - val_loss: 0.0916 - val_acc: 0.9750\n",
            "Epoch 341/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9786\n",
            "Epoch 00341: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0759 - acc: 0.9786 - val_loss: 0.1074 - val_acc: 0.9667\n",
            "Epoch 342/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9760\n",
            "Epoch 00342: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0822 - acc: 0.9764 - val_loss: 0.0866 - val_acc: 0.9767\n",
            "Epoch 343/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0790 - acc: 0.9747\n",
            "Epoch 00343: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0797 - acc: 0.9747 - val_loss: 0.0876 - val_acc: 0.9733\n",
            "Epoch 344/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0745 - acc: 0.9776\n",
            "Epoch 00344: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0767 - acc: 0.9764 - val_loss: 0.1035 - val_acc: 0.9700\n",
            "Epoch 345/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9711\n",
            "Epoch 00345: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0932 - acc: 0.9714 - val_loss: 0.1085 - val_acc: 0.9725\n",
            "Epoch 346/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0838 - acc: 0.9748\n",
            "Epoch 00346: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0872 - acc: 0.9731 - val_loss: 0.1003 - val_acc: 0.9692\n",
            "Epoch 347/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0897 - acc: 0.9712\n",
            "Epoch 00347: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0908 - acc: 0.9708 - val_loss: 0.0897 - val_acc: 0.9717\n",
            "Epoch 348/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9677\n",
            "Epoch 00348: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0879 - acc: 0.9681 - val_loss: 0.0913 - val_acc: 0.9708\n",
            "Epoch 349/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0738 - acc: 0.9794\n",
            "Epoch 00349: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0759 - acc: 0.9783 - val_loss: 0.0872 - val_acc: 0.9700\n",
            "Epoch 350/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0773 - acc: 0.9759\n",
            "Epoch 00350: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.0782 - acc: 0.9756 - val_loss: 0.0852 - val_acc: 0.9725\n",
            "Epoch 351/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0871 - acc: 0.9724\n",
            "Epoch 00351: val_loss did not improve from 0.07325\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0879 - acc: 0.9722 - val_loss: 0.0766 - val_acc: 0.9767\n",
            "Epoch 352/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9706\n",
            "Epoch 00352: val_loss improved from 0.07325 to 0.06661, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.0992 - acc: 0.9697 - val_loss: 0.0666 - val_acc: 0.9792\n",
            "Epoch 353/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0864 - acc: 0.9729\n",
            "Epoch 00353: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0857 - acc: 0.9731 - val_loss: 0.1321 - val_acc: 0.9575\n",
            "Epoch 354/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9800\n",
            "Epoch 00354: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.0725 - acc: 0.9792 - val_loss: 0.0859 - val_acc: 0.9725\n",
            "Epoch 355/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9737\n",
            "Epoch 00355: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0814 - acc: 0.9739 - val_loss: 0.0926 - val_acc: 0.9700\n",
            "Epoch 356/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0819 - acc: 0.9736\n",
            "Epoch 00356: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0823 - acc: 0.9731 - val_loss: 0.1005 - val_acc: 0.9708\n",
            "Epoch 357/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0747 - acc: 0.9762\n",
            "Epoch 00357: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0756 - acc: 0.9761 - val_loss: 0.0981 - val_acc: 0.9708\n",
            "Epoch 358/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0846 - acc: 0.9738\n",
            "Epoch 00358: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0838 - acc: 0.9739 - val_loss: 0.1133 - val_acc: 0.9600\n",
            "Epoch 359/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0702 - acc: 0.9812\n",
            "Epoch 00359: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0728 - acc: 0.9797 - val_loss: 0.0930 - val_acc: 0.9708\n",
            "Epoch 360/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0817 - acc: 0.9726\n",
            "Epoch 00360: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0842 - acc: 0.9711 - val_loss: 0.0989 - val_acc: 0.9650\n",
            "Epoch 361/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9766\n",
            "Epoch 00361: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0712 - acc: 0.9764 - val_loss: 0.0954 - val_acc: 0.9725\n",
            "Epoch 362/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0819 - acc: 0.9756\n",
            "Epoch 00362: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0802 - acc: 0.9761 - val_loss: 0.0718 - val_acc: 0.9817\n",
            "Epoch 363/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0686 - acc: 0.9797\n",
            "Epoch 00363: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0700 - acc: 0.9789 - val_loss: 0.0707 - val_acc: 0.9708\n",
            "Epoch 364/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0688 - acc: 0.9803\n",
            "Epoch 00364: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0692 - acc: 0.9800 - val_loss: 0.1024 - val_acc: 0.9708\n",
            "Epoch 365/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0908 - acc: 0.9709\n",
            "Epoch 00365: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0920 - acc: 0.9700 - val_loss: 0.1547 - val_acc: 0.9475\n",
            "Epoch 366/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0920 - acc: 0.9712\n",
            "Epoch 00366: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0898 - acc: 0.9722 - val_loss: 0.0875 - val_acc: 0.9700\n",
            "Epoch 367/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0853 - acc: 0.9724\n",
            "Epoch 00367: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0858 - acc: 0.9725 - val_loss: 0.0953 - val_acc: 0.9658\n",
            "Epoch 368/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9774\n",
            "Epoch 00368: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0771 - acc: 0.9769 - val_loss: 0.0938 - val_acc: 0.9708\n",
            "Epoch 369/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0863 - acc: 0.9715\n",
            "Epoch 00369: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0864 - acc: 0.9717 - val_loss: 0.0948 - val_acc: 0.9750\n",
            "Epoch 370/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0928 - acc: 0.9688\n",
            "Epoch 00370: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.0912 - acc: 0.9697 - val_loss: 0.0864 - val_acc: 0.9758\n",
            "Epoch 371/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0809 - acc: 0.9768\n",
            "Epoch 00371: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0814 - acc: 0.9769 - val_loss: 0.0841 - val_acc: 0.9775\n",
            "Epoch 372/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0745 - acc: 0.9750\n",
            "Epoch 00372: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0732 - acc: 0.9756 - val_loss: 0.0875 - val_acc: 0.9733\n",
            "Epoch 373/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9780\n",
            "Epoch 00373: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0711 - acc: 0.9778 - val_loss: 0.0669 - val_acc: 0.9833\n",
            "Epoch 374/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9720\n",
            "Epoch 00374: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0906 - acc: 0.9719 - val_loss: 0.0803 - val_acc: 0.9750\n",
            "Epoch 375/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9709\n",
            "Epoch 00375: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0945 - acc: 0.9700 - val_loss: 0.1126 - val_acc: 0.9708\n",
            "Epoch 376/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0775 - acc: 0.9755\n",
            "Epoch 00376: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0783 - acc: 0.9747 - val_loss: 0.0803 - val_acc: 0.9758\n",
            "Epoch 377/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1139 - acc: 0.9609\n",
            "Epoch 00377: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1133 - acc: 0.9614 - val_loss: 0.0966 - val_acc: 0.9717\n",
            "Epoch 378/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0916 - acc: 0.9718\n",
            "Epoch 00378: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0909 - acc: 0.9714 - val_loss: 0.0700 - val_acc: 0.9808\n",
            "Epoch 379/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9777\n",
            "Epoch 00379: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0768 - acc: 0.9758 - val_loss: 0.0863 - val_acc: 0.9758\n",
            "Epoch 380/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9751\n",
            "Epoch 00380: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0818 - acc: 0.9750 - val_loss: 0.1213 - val_acc: 0.9575\n",
            "Epoch 381/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9760\n",
            "Epoch 00381: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0721 - acc: 0.9764 - val_loss: 0.0962 - val_acc: 0.9708\n",
            "Epoch 382/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0632 - acc: 0.9803\n",
            "Epoch 00382: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0633 - acc: 0.9806 - val_loss: 0.0686 - val_acc: 0.9792\n",
            "Epoch 383/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9809\n",
            "Epoch 00383: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0646 - acc: 0.9803 - val_loss: 0.0906 - val_acc: 0.9758\n",
            "Epoch 384/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0823 - acc: 0.9741\n",
            "Epoch 00384: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.0817 - acc: 0.9736 - val_loss: 0.0865 - val_acc: 0.9733\n",
            "Epoch 385/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9820\n",
            "Epoch 00385: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0638 - acc: 0.9822 - val_loss: 0.0796 - val_acc: 0.9783\n",
            "Epoch 386/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0647 - acc: 0.9812\n",
            "Epoch 00386: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0669 - acc: 0.9806 - val_loss: 0.0757 - val_acc: 0.9758\n",
            "Epoch 387/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0750 - acc: 0.9755\n",
            "Epoch 00387: val_loss did not improve from 0.06661\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0741 - acc: 0.9756 - val_loss: 0.0866 - val_acc: 0.9700\n",
            "Epoch 388/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0752 - acc: 0.9762\n",
            "Epoch 00388: val_loss improved from 0.06661 to 0.06631, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.0759 - acc: 0.9756 - val_loss: 0.0663 - val_acc: 0.9783\n",
            "Epoch 389/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0795 - acc: 0.9755\n",
            "Epoch 00389: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0775 - acc: 0.9764 - val_loss: 0.0733 - val_acc: 0.9800\n",
            "Epoch 390/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0650 - acc: 0.9818\n",
            "Epoch 00390: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0659 - acc: 0.9808 - val_loss: 0.0962 - val_acc: 0.9717\n",
            "Epoch 391/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0671 - acc: 0.9806\n",
            "Epoch 00391: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0673 - acc: 0.9806 - val_loss: 0.0752 - val_acc: 0.9817\n",
            "Epoch 392/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0706 - acc: 0.9779\n",
            "Epoch 00392: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0771 - acc: 0.9758 - val_loss: 0.1491 - val_acc: 0.9508\n",
            "Epoch 393/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9849\n",
            "Epoch 00393: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0555 - acc: 0.9850 - val_loss: 0.0740 - val_acc: 0.9792\n",
            "Epoch 394/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9797\n",
            "Epoch 00394: val_loss did not improve from 0.06631\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0676 - acc: 0.9797 - val_loss: 0.0689 - val_acc: 0.9808\n",
            "Epoch 395/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9831\n",
            "Epoch 00395: val_loss improved from 0.06631 to 0.06440, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.0585 - acc: 0.9828 - val_loss: 0.0644 - val_acc: 0.9808\n",
            "Epoch 396/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9837\n",
            "Epoch 00396: val_loss did not improve from 0.06440\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.0598 - acc: 0.9839 - val_loss: 0.0813 - val_acc: 0.9800\n",
            "Epoch 397/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0860 - acc: 0.9726\n",
            "Epoch 00397: val_loss did not improve from 0.06440\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0834 - acc: 0.9736 - val_loss: 0.0848 - val_acc: 0.9717\n",
            "Epoch 398/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0755 - acc: 0.9791\n",
            "Epoch 00398: val_loss did not improve from 0.06440\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0756 - acc: 0.9789 - val_loss: 0.0979 - val_acc: 0.9692\n",
            "Epoch 399/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0724 - acc: 0.9779\n",
            "Epoch 00399: val_loss did not improve from 0.06440\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0718 - acc: 0.9781 - val_loss: 0.1293 - val_acc: 0.9583\n",
            "Epoch 400/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0690 - acc: 0.9768\n",
            "Epoch 00400: val_loss did not improve from 0.06440\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0702 - acc: 0.9761 - val_loss: 0.1192 - val_acc: 0.9617\n",
            "1200/1200 [==============================] - 0s 128us/sample - loss: 0.1192 - acc: 0.9617\n",
            "[0.11916137605905533, 0.96166664]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFR1eYg3x8Wr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "7685101c-1e76-4df0-806e-c4396fec4939"
      },
      "source": [
        "for i in range(13, 14): # Итерација низ секој испитен примерок\n",
        "  print(f\"====================== Примерок ({i}) ======================\")\n",
        "  print(\"Вчитување тест податоци од испитниот примерок \" + str(i) + \"...\")\n",
        "  \n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_test_data = np.empty(0)\n",
        "  temp_test_events = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_test_set = 'S' + format(j, '02') + '/Test'\n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testData.mat\"\n",
        "    temp = loadmat(full_path)['testData']\n",
        "    if temp_test_data.size != 0:\n",
        "      temp_test_data = np.concatenate((temp_test_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_test_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_test_events.size != 0:\n",
        "        temp_test_events = np.append(temp_test_events, temp)\n",
        "      else:\n",
        "        temp_test_events = np.array(temp)\n",
        "\n",
        "    # Вчитување на бројот на runs \n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/runs_per_block.txt\"\n",
        "    with open(full_path, \"r\") as runs_per_block:\n",
        "      test_runs_per_block[i-1][j-1] = int(runs_per_block.read())\n",
        "\n",
        "    print(\"\\t - Тест податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "  # Зачувај ги тест податоците вчитани од испитниот примерок во низа\n",
        "  test_data.append(temp_test_data)\n",
        "  test_events.append(temp_test_events)\n",
        "  print(\"Тест податоците од испитниот примерок \" + str(i) + \" се вчитани.\\n\")\n",
        "\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_data: \" + str(test_data[i-1].shape)) # test_data to predict\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_events: \" + str(len(test_events[i-1]))) # test_events\n",
        "  for j in range (1,4):\n",
        "    print(\"SBJ\" + str(format(i-1, '02')) + \" / S\" + str(format(j-1, '02')) + \"| Runs per block: \" + str(test_runs_per_block[i-1][j-1])) # runs per block in SJB01, SJ00 \n",
        "\n",
        "  to_predict_data = reshape_data_to_mne_format(test_data[i-1])\n",
        "  predictions = model13.predict(to_predict_data)\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Predictions: \" + str(len(predictions)))\n",
        "  # np.savetxt(\"predictions.csv\", predictions, delimiter=\",\")\n",
        "\n",
        "\n",
        "  # ========= FALI USTE DA SE ISPARSIRA PREDICTIONOT... NE E SREDEN OVOJ KOD DOLE =======\n",
        "\n",
        "  int_pred = np.argmax(predictions, axis=1)\n",
        "  int_ytest = np.argmax(y_test, axis=1)\n",
        "\n",
        "  session_start = 0\n",
        "  start_prediction_index = 0\n",
        "  end_prediction_index = 0\n",
        "  for session in range(0, 3):\n",
        "    print(f\"============== Сесија ({session}) ==============\")\n",
        "    for block in range(0, 50):    \n",
        "      events_per_block = test_runs_per_block[i-1][session]\n",
        "\n",
        "      start_prediction_index = session_start + (block*events_per_block)*8\n",
        "      end_prediction_index = session_start + ((block+1)*events_per_block)*8\n",
        "\n",
        "      block_prediction = int_pred[start_prediction_index:end_prediction_index]\n",
        "      prediction = np.bincount(block_prediction).argmax()\n",
        "      df.iat[session+36,block+2] = prediction+1\n",
        "      # UNCOMMENT ZA PODOBAR PRIKAZ :)\n",
        "      # print(f\"Session {session} | Block: {block} | Prediction: {prediction} | Address: {end_prediction_index}\")\n",
        "\n",
        "      print(str(prediction+1) + \",\", end=\"\")\n",
        "    session_start = end_prediction_index\n",
        "    print(\"\")\n",
        "  print(\"Stigna li do kraj: \" + str(session_start == len(predictions)))\n",
        "  print(f\"====================== Примерок ({i}) ======================\\n\\n\")"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Примерок (13) ======================\n",
            "Вчитување тест податоци од испитниот примерок 13...\n",
            "\t - Тест податоците од сесија 1 се вчитани.\n",
            "\t - Тест податоците од сесија 2 се вчитани.\n",
            "\t - Тест податоците од сесија 3 се вчитани.\n",
            "Тест податоците од испитниот примерок 13 се вчитани.\n",
            "\n",
            "SBJ12| Test_data: (8, 350, 10000)\n",
            "SBJ12| Test_events: 10000\n",
            "SBJ12 / S00| Runs per block: 9\n",
            "SBJ12 / S01| Runs per block: 8\n",
            "SBJ12 / S02| Runs per block: 8\n",
            "SBJ12| Predictions: 10000\n",
            "============== Сесија (0) ==============\n",
            "6,6,1,5,3,1,1,3,5,6,6,6,6,6,6,1,1,1,1,1,4,1,1,5,1,1,1,4,4,1,6,3,4,4,1,6,1,5,1,1,1,4,1,1,1,5,6,1,1,1,\n",
            "============== Сесија (1) ==============\n",
            "6,1,5,4,1,1,6,6,1,6,8,6,1,1,6,1,6,1,5,6,1,1,1,1,1,1,1,6,1,1,5,6,7,1,4,1,4,5,6,4,1,6,7,6,8,6,1,6,6,1,\n",
            "============== Сесија (2) ==============\n",
            "1,6,5,1,5,2,6,7,6,6,2,1,1,1,4,4,1,6,1,5,5,2,6,4,1,5,1,6,1,6,7,7,1,6,6,1,6,4,2,6,6,6,7,1,1,7,2,2,6,1,\n",
            "Stigna li do kraj: True\n",
            "====================== Примерок (13) ======================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m78jBnfiyJWQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d9fe79db-5498-4158-f233-9de761679e8c"
      },
      "source": [
        "df"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUBJECT</th>\n",
              "      <th>SESSION</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>Unnamed: 52</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    SUBJECT  SESSION  1  2  3  4  5  6  7  ... 43 44 45 46 47 48 49 50 Unnamed: 52\n",
              "0         1        1  6  6  3  6  6  3  6  ...  6  4  8  3  4  5  5  7         NaN\n",
              "1         1        2  6  3  2  1  2  1  3  ...  6  2  2  2  3  6  6  2         NaN\n",
              "2         1        3  3  3  3  3  3  3  3  ...  3  3  6  3  6  7  1  6         NaN\n",
              "3         2        1  8  8  8  8  8  8  8  ...  8  4  8  4  8  7  8  8         NaN\n",
              "4         2        2  2  7  6  6  6  6  7  ...  2  6  6  2  6  6  2  2         NaN\n",
              "5         2        3  2  7  2  6  7  4  2  ...  2  6  2  2  7  2  2  2         NaN\n",
              "6         3        1  3  3  4  4  4  3  6  ...  4  6  3  3  4  3  4  4         NaN\n",
              "7         3        2  6  1  7  7  7  7  7  ...  6  7  1  6  6  6  6  6         NaN\n",
              "8         3        3  6  4  8  8  5  7  8  ...  4  2  8  2  2  2  2  8         NaN\n",
              "9         4        1  1  1  2  1  5  5  6  ...  1  1  7  1  6  6  6  6         NaN\n",
              "10        4        2  7  7  7  7  6  6  7  ...  1  5  5  5  5  5  2  6         NaN\n",
              "11        4        3  7  3  7  3  6  6  7  ...  6  1  4  4  4  6  6  6         NaN\n",
              "12        5        1  5  4  4  4  6  4  5  ...  3  1  4  4  1  3  3  5         NaN\n",
              "13        5        2  3  6  3  5  2  6  7  ...  6  6  6  6  6  6  6  4         NaN\n",
              "14        5        3  4  3  3  6  5  7  6  ...  7  2  2  7  5  6  4  2         NaN\n",
              "15        6        1  1  3  1  8  3  3  3  ...  4  3  7  2  7  2  5  8         NaN\n",
              "16        6        2  3  4  1  7  1  1  1  ...  5  5  5  5  5  5  5  5         NaN\n",
              "17        6        3  2  3  2  5  3  3  3  ...  2  3  3  3  1  1  3  3         NaN\n",
              "18        7        1  4  7  5  7  4  5  4  ...  5  1  1  4  4  4  1  4         NaN\n",
              "19        7        2  2  2  2  8  2  5  2  ...  5  5  5  2  2  2  2  2         NaN\n",
              "20        7        3  2  7  7  5  7  5  4  ...  1  3  5  5  3  5  5  1         NaN\n",
              "21        8        1  5  8  2  8  2  2  1  ...  1  8  2  2  5  5  2  8         NaN\n",
              "22        8        2  2  7  1  5  1  2  1  ...  7  1  7  8  2  1  7  7         NaN\n",
              "23        8        3  4  1  6  6  1  2  1  ...  1  8  1  7  1  1  7  1         NaN\n",
              "24        9        1  6  5  6  6  7  6  8  ...  6  6  6  5  5  5  3  5         NaN\n",
              "25        9        2  3  4  8  4  1  6  1  ...  1  1  1  1  1  3  6  6         NaN\n",
              "26        9        3  5  2  2  2  2  3  2  ...  2  2  2  3  2  2  4  2         NaN\n",
              "27       10        1  1  1  8  7  3  1  1  ...  5  1  1  1  1  5  1  5         NaN\n",
              "28       10        2  1  5  5  6  1  1  1  ...  5  5  6  6  2  1  2  1         NaN\n",
              "29       10        3  5  8  5  5  5  6  5  ...  5  5  2  5  5  5  5  5         NaN\n",
              "30       11        1  3  4  4  4  3  4  3  ...  5  4  3  1  3  4  4  4         NaN\n",
              "31       11        2  3  1  1  4  8  4  4  ...  1  7  3  8  2  3  8  3         NaN\n",
              "32       11        3  4  2  4  4  4  4  4  ...  8  4  4  7  8  4  8  4         NaN\n",
              "33       12        1  1  1  1  1  1  1  6  ...  1  6  7  3  1  7  7  3         NaN\n",
              "34       12        2  3  2  6  2  2  3  2  ...  5  1  2  1  5  5  2  5         NaN\n",
              "35       12        3  1  6  2  2  1  2  2  ...  5  6  2  1  2  2  2  2         NaN\n",
              "36       13        1  6  6  1  5  3  1  1  ...  1  1  1  5  6  1  1  1         NaN\n",
              "37       13        2  6  1  5  4  1  1  6  ...  7  6  8  6  1  6  6  1         NaN\n",
              "38       13        3  1  6  5  1  5  2  6  ...  7  1  1  7  2  2  6  1         NaN\n",
              "39       14        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "40       14        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "41       14        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "42       15        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "43       15        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "44       15        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "\n",
              "[45 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC8tV__9yS64",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "349c7d3c-a651-40e5-fe65-6918e63c42d5"
      },
      "source": [
        "for i in range(14, 15): # Итерација низ секој испитен примерок\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_data = np.empty(0)\n",
        "  temp_labels = np.empty(0)\n",
        "  temp_events = np.empty(0)\n",
        "  temp_targets = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainData.mat\"\n",
        "    temp = loadmat(full_path)['trainData']\n",
        "    if temp_data.size != 0:\n",
        "      temp_data = np.concatenate((temp_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на label-ите\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainLabels.txt\"\n",
        "    with open(full_path, \"r\") as file_labels:\n",
        "      temp = file_labels.read().splitlines()\n",
        "      temp = np.repeat(temp, 8*10)\n",
        "      if temp_labels.size != 0:\n",
        "        temp_labels = np.concatenate((temp_labels, temp))\n",
        "      else:\n",
        "        temp_labels = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_events.size != 0:\n",
        "        temp_events = np.append(temp_events, temp)\n",
        "      else:\n",
        "        temp_events = np.array(temp)\n",
        "      \n",
        "\n",
        "    # Вчитување на редоследот на објекти кои се target\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainTargets.txt\"\n",
        "    with open(full_path, \"r\") as file_targets:\n",
        "      temp = file_targets.read().splitlines()\n",
        "      if temp_targets.size != 0:\n",
        "        temp_targets = np.concatenate((temp_targets, temp))\n",
        "      else:\n",
        "        temp_targets = np.array(temp)\n",
        "    print(\"\\t - Податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "\n",
        "  for j in range(4, 8): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "      \n",
        "  # Зачувај ги податоците вчитани од испитниот примерок во низа\n",
        "  data.append(temp_data)\n",
        "  labels.append(temp_labels)\n",
        "  events.append(temp_events)\n",
        "  targets.append(temp_targets)\n",
        "\n",
        "  \n",
        "  print(\"Податоците од испитниот примерок \" + str(i) + \" се вчитани.\")\n",
        "\n",
        "\n",
        "  #data = target_events_data_scaled\n",
        "  mne_array = np.swapaxes(data[i-1], 0, 2) # (епохa, канал, настан). \n",
        "  mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "  mne_array = mne_array.reshape(mne_array.shape[0],1, 8,350)\n",
        "  print(mne_array.shape)\n",
        "\n",
        "  events_arr = events[i-1].astype(np.int)\n",
        "  labels_arr = labels[i-1].astype(np.int)\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(mne_array, labels_arr-1, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "  model14 = DeepConvNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "  model14.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = Adam(0.0009))\n",
        "  checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                                verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "  #clf = RandomForestClassifier(max_depth=5)\n",
        "  #clf.fit(X_train, y_train)\n",
        "  #score = clf.score(X_test, y_test)\n",
        "  # print(score)\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  num_batch_size=100\n",
        "  num_epochs=400\n",
        "  model14.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "            validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "  score = model14.evaluate(X_test, y_test, verbose=1)\n",
        "  print(score)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t - Податоците од сесија 1 се вчитани.\n",
            "\t - Податоците од сесија 2 се вчитани.\n",
            "\t - Податоците од сесија 3 се вчитани.\n",
            "Податоците од испитниот примерок 14 се вчитани.\n",
            "(4800, 1, 8, 350)\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Train on 3600 samples, validate on 1200 samples\n",
            "Epoch 1/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.2940 - acc: 0.1565\n",
            "Epoch 00001: val_loss improved from inf to 2.20905, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 10s 3ms/sample - loss: 2.2871 - acc: 0.1561 - val_loss: 2.2091 - val_acc: 0.1417\n",
            "Epoch 2/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.2158 - acc: 0.1797\n",
            "Epoch 00002: val_loss improved from 2.20905 to 2.19250, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 223us/sample - loss: 2.2197 - acc: 0.1783 - val_loss: 2.1925 - val_acc: 0.1342\n",
            "Epoch 3/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.1888 - acc: 0.1762\n",
            "Epoch 00003: val_loss did not improve from 2.19250\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 2.1836 - acc: 0.1772 - val_loss: 2.2535 - val_acc: 0.1642\n",
            "Epoch 4/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.1296 - acc: 0.1809\n",
            "Epoch 00004: val_loss did not improve from 2.19250\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 2.1242 - acc: 0.1844 - val_loss: 2.4205 - val_acc: 0.1550\n",
            "Epoch 5/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.1013 - acc: 0.2003\n",
            "Epoch 00005: val_loss did not improve from 2.19250\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 2.1033 - acc: 0.2019 - val_loss: 2.3233 - val_acc: 0.1900\n",
            "Epoch 6/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.0824 - acc: 0.2003\n",
            "Epoch 00006: val_loss did not improve from 2.19250\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 2.0789 - acc: 0.1986 - val_loss: 2.2091 - val_acc: 0.1783\n",
            "Epoch 7/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.0424 - acc: 0.2147\n",
            "Epoch 00007: val_loss did not improve from 2.19250\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 2.0487 - acc: 0.2119 - val_loss: 2.2446 - val_acc: 0.2008\n",
            "Epoch 8/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.0250 - acc: 0.2260\n",
            "Epoch 00008: val_loss did not improve from 2.19250\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 2.0247 - acc: 0.2272 - val_loss: 2.2407 - val_acc: 0.1800\n",
            "Epoch 9/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.9785 - acc: 0.2444\n",
            "Epoch 00009: val_loss improved from 2.19250 to 2.15083, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 1.9790 - acc: 0.2450 - val_loss: 2.1508 - val_acc: 0.1758\n",
            "Epoch 10/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.9570 - acc: 0.2485\n",
            "Epoch 00010: val_loss did not improve from 2.15083\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 1.9493 - acc: 0.2497 - val_loss: 2.2371 - val_acc: 0.1958\n",
            "Epoch 11/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.9122 - acc: 0.2653\n",
            "Epoch 00011: val_loss improved from 2.15083 to 2.14702, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 221us/sample - loss: 1.9120 - acc: 0.2661 - val_loss: 2.1470 - val_acc: 0.2008\n",
            "Epoch 12/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.9115 - acc: 0.2724\n",
            "Epoch 00012: val_loss did not improve from 2.14702\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 1.9138 - acc: 0.2706 - val_loss: 2.1488 - val_acc: 0.2225\n",
            "Epoch 13/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.8906 - acc: 0.2859\n",
            "Epoch 00013: val_loss did not improve from 2.14702\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 1.8950 - acc: 0.2831 - val_loss: 2.4148 - val_acc: 0.1967\n",
            "Epoch 14/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.8467 - acc: 0.3077\n",
            "Epoch 00014: val_loss did not improve from 2.14702\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 1.8453 - acc: 0.3072 - val_loss: 2.2471 - val_acc: 0.2367\n",
            "Epoch 15/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.8124 - acc: 0.3159\n",
            "Epoch 00015: val_loss did not improve from 2.14702\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 1.8073 - acc: 0.3172 - val_loss: 2.3041 - val_acc: 0.2075\n",
            "Epoch 16/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.7778 - acc: 0.3253\n",
            "Epoch 00016: val_loss did not improve from 2.14702\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 1.7867 - acc: 0.3225 - val_loss: 2.5195 - val_acc: 0.1933\n",
            "Epoch 17/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.8071 - acc: 0.3138\n",
            "Epoch 00017: val_loss improved from 2.14702 to 2.13685, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 1.8030 - acc: 0.3144 - val_loss: 2.1369 - val_acc: 0.2483\n",
            "Epoch 18/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.7329 - acc: 0.3437\n",
            "Epoch 00018: val_loss improved from 2.13685 to 2.13218, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 1.7368 - acc: 0.3431 - val_loss: 2.1322 - val_acc: 0.2742\n",
            "Epoch 19/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.6806 - acc: 0.3606\n",
            "Epoch 00019: val_loss improved from 2.13218 to 2.01696, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 1.6823 - acc: 0.3589 - val_loss: 2.0170 - val_acc: 0.2617\n",
            "Epoch 20/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.6982 - acc: 0.3447\n",
            "Epoch 00020: val_loss improved from 2.01696 to 1.97331, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 1.6943 - acc: 0.3464 - val_loss: 1.9733 - val_acc: 0.2975\n",
            "Epoch 21/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.6594 - acc: 0.3679\n",
            "Epoch 00021: val_loss did not improve from 1.97331\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 1.6636 - acc: 0.3656 - val_loss: 2.0041 - val_acc: 0.2933\n",
            "Epoch 22/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.6212 - acc: 0.3847\n",
            "Epoch 00022: val_loss did not improve from 1.97331\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 1.6293 - acc: 0.3839 - val_loss: 2.4094 - val_acc: 0.2667\n",
            "Epoch 23/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.5813 - acc: 0.4015\n",
            "Epoch 00023: val_loss improved from 1.97331 to 1.92254, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 224us/sample - loss: 1.5791 - acc: 0.4019 - val_loss: 1.9225 - val_acc: 0.2958\n",
            "Epoch 24/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.5962 - acc: 0.4056\n",
            "Epoch 00024: val_loss improved from 1.92254 to 1.79863, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 1.5984 - acc: 0.4053 - val_loss: 1.7986 - val_acc: 0.3358\n",
            "Epoch 25/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.5322 - acc: 0.4185\n",
            "Epoch 00025: val_loss did not improve from 1.79863\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 1.5359 - acc: 0.4194 - val_loss: 2.0085 - val_acc: 0.2917\n",
            "Epoch 26/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.5436 - acc: 0.4126\n",
            "Epoch 00026: val_loss did not improve from 1.79863\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 1.5462 - acc: 0.4106 - val_loss: 2.1510 - val_acc: 0.2733\n",
            "Epoch 27/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.4925 - acc: 0.4388\n",
            "Epoch 00027: val_loss improved from 1.79863 to 1.75459, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 1.4884 - acc: 0.4400 - val_loss: 1.7546 - val_acc: 0.3567\n",
            "Epoch 28/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4943 - acc: 0.4344\n",
            "Epoch 00028: val_loss did not improve from 1.75459\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 1.4985 - acc: 0.4347 - val_loss: 1.7903 - val_acc: 0.3350\n",
            "Epoch 29/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4483 - acc: 0.4576\n",
            "Epoch 00029: val_loss did not improve from 1.75459\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 1.4566 - acc: 0.4547 - val_loss: 1.8440 - val_acc: 0.3392\n",
            "Epoch 30/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4079 - acc: 0.4809\n",
            "Epoch 00030: val_loss did not improve from 1.75459\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 1.4085 - acc: 0.4811 - val_loss: 1.7777 - val_acc: 0.3525\n",
            "Epoch 31/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.3822 - acc: 0.4885\n",
            "Epoch 00031: val_loss improved from 1.75459 to 1.63119, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 1.3904 - acc: 0.4869 - val_loss: 1.6312 - val_acc: 0.3925\n",
            "Epoch 32/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.3300 - acc: 0.5029\n",
            "Epoch 00032: val_loss improved from 1.63119 to 1.60963, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 1.3368 - acc: 0.5006 - val_loss: 1.6096 - val_acc: 0.3950\n",
            "Epoch 33/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.2932 - acc: 0.5191\n",
            "Epoch 00033: val_loss improved from 1.60963 to 1.51929, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 222us/sample - loss: 1.2928 - acc: 0.5194 - val_loss: 1.5193 - val_acc: 0.4442\n",
            "Epoch 34/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2913 - acc: 0.5168\n",
            "Epoch 00034: val_loss did not improve from 1.51929\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 1.2924 - acc: 0.5181 - val_loss: 1.5436 - val_acc: 0.4117\n",
            "Epoch 35/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2656 - acc: 0.5279\n",
            "Epoch 00035: val_loss improved from 1.51929 to 1.46341, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 1.2598 - acc: 0.5306 - val_loss: 1.4634 - val_acc: 0.4558\n",
            "Epoch 36/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2173 - acc: 0.5435\n",
            "Epoch 00036: val_loss improved from 1.46341 to 1.42149, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 1.2155 - acc: 0.5436 - val_loss: 1.4215 - val_acc: 0.4717\n",
            "Epoch 37/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2091 - acc: 0.5541\n",
            "Epoch 00037: val_loss improved from 1.42149 to 1.38564, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 1.2184 - acc: 0.5508 - val_loss: 1.3856 - val_acc: 0.4842\n",
            "Epoch 38/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.1746 - acc: 0.5726\n",
            "Epoch 00038: val_loss improved from 1.38564 to 1.37813, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 227us/sample - loss: 1.1763 - acc: 0.5703 - val_loss: 1.3781 - val_acc: 0.4875\n",
            "Epoch 39/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.1375 - acc: 0.5834\n",
            "Epoch 00039: val_loss improved from 1.37813 to 1.34783, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 226us/sample - loss: 1.1437 - acc: 0.5811 - val_loss: 1.3478 - val_acc: 0.5000\n",
            "Epoch 40/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1154 - acc: 0.5945\n",
            "Epoch 00040: val_loss did not improve from 1.34783\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 1.1218 - acc: 0.5886 - val_loss: 1.3815 - val_acc: 0.4925\n",
            "Epoch 41/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.0792 - acc: 0.6034\n",
            "Epoch 00041: val_loss did not improve from 1.34783\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 1.0792 - acc: 0.6022 - val_loss: 1.3606 - val_acc: 0.4892\n",
            "Epoch 42/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0777 - acc: 0.5988\n",
            "Epoch 00042: val_loss improved from 1.34783 to 1.26986, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 224us/sample - loss: 1.0755 - acc: 0.5986 - val_loss: 1.2699 - val_acc: 0.5375\n",
            "Epoch 43/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0344 - acc: 0.6206\n",
            "Epoch 00043: val_loss did not improve from 1.26986\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 1.0376 - acc: 0.6175 - val_loss: 1.3240 - val_acc: 0.5075\n",
            "Epoch 44/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0185 - acc: 0.6191\n",
            "Epoch 00044: val_loss improved from 1.26986 to 1.17015, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 226us/sample - loss: 1.0204 - acc: 0.6175 - val_loss: 1.1701 - val_acc: 0.5667\n",
            "Epoch 45/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0063 - acc: 0.6332\n",
            "Epoch 00045: val_loss did not improve from 1.17015\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 1.0080 - acc: 0.6317 - val_loss: 1.2033 - val_acc: 0.5533\n",
            "Epoch 46/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9625 - acc: 0.6500\n",
            "Epoch 00046: val_loss improved from 1.17015 to 1.14539, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.9644 - acc: 0.6497 - val_loss: 1.1454 - val_acc: 0.5683\n",
            "Epoch 47/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9225 - acc: 0.6785\n",
            "Epoch 00047: val_loss improved from 1.14539 to 1.10633, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 223us/sample - loss: 0.9288 - acc: 0.6750 - val_loss: 1.1063 - val_acc: 0.5917\n",
            "Epoch 48/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9091 - acc: 0.6791\n",
            "Epoch 00048: val_loss improved from 1.10633 to 1.07329, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 221us/sample - loss: 0.9137 - acc: 0.6778 - val_loss: 1.0733 - val_acc: 0.6158\n",
            "Epoch 49/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8800 - acc: 0.6926\n",
            "Epoch 00049: val_loss improved from 1.07329 to 1.05147, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 222us/sample - loss: 0.8822 - acc: 0.6922 - val_loss: 1.0515 - val_acc: 0.6050\n",
            "Epoch 50/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8954 - acc: 0.6794\n",
            "Epoch 00050: val_loss did not improve from 1.05147\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.9014 - acc: 0.6797 - val_loss: 1.0721 - val_acc: 0.6092\n",
            "Epoch 51/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7997 - acc: 0.7218\n",
            "Epoch 00051: val_loss did not improve from 1.05147\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.8093 - acc: 0.7178 - val_loss: 1.1353 - val_acc: 0.5758\n",
            "Epoch 52/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8363 - acc: 0.7106\n",
            "Epoch 00052: val_loss did not improve from 1.05147\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.8376 - acc: 0.7111 - val_loss: 1.0592 - val_acc: 0.6083\n",
            "Epoch 53/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8075 - acc: 0.7138\n",
            "Epoch 00053: val_loss improved from 1.05147 to 1.00209, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 225us/sample - loss: 0.8093 - acc: 0.7128 - val_loss: 1.0021 - val_acc: 0.6283\n",
            "Epoch 54/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7975 - acc: 0.7141\n",
            "Epoch 00054: val_loss improved from 1.00209 to 0.99742, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 0.8005 - acc: 0.7122 - val_loss: 0.9974 - val_acc: 0.6567\n",
            "Epoch 55/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7608 - acc: 0.7415\n",
            "Epoch 00055: val_loss did not improve from 0.99742\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.7572 - acc: 0.7439 - val_loss: 0.9998 - val_acc: 0.6450\n",
            "Epoch 56/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7488 - acc: 0.7291\n",
            "Epoch 00056: val_loss did not improve from 0.99742\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.7507 - acc: 0.7297 - val_loss: 1.0231 - val_acc: 0.6117\n",
            "Epoch 57/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7059 - acc: 0.7533\n",
            "Epoch 00057: val_loss improved from 0.99742 to 0.86823, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 0.7147 - acc: 0.7489 - val_loss: 0.8682 - val_acc: 0.6858\n",
            "Epoch 58/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6974 - acc: 0.7612\n",
            "Epoch 00058: val_loss improved from 0.86823 to 0.82966, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 223us/sample - loss: 0.7004 - acc: 0.7600 - val_loss: 0.8297 - val_acc: 0.7267\n",
            "Epoch 59/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6802 - acc: 0.7706\n",
            "Epoch 00059: val_loss did not improve from 0.82966\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.6826 - acc: 0.7694 - val_loss: 0.8466 - val_acc: 0.7100\n",
            "Epoch 60/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6807 - acc: 0.7753\n",
            "Epoch 00060: val_loss did not improve from 0.82966\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.6816 - acc: 0.7769 - val_loss: 0.8723 - val_acc: 0.6917\n",
            "Epoch 61/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.6549 - acc: 0.7849\n",
            "Epoch 00061: val_loss improved from 0.82966 to 0.82726, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 222us/sample - loss: 0.6549 - acc: 0.7847 - val_loss: 0.8273 - val_acc: 0.7000\n",
            "Epoch 62/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6473 - acc: 0.7821\n",
            "Epoch 00062: val_loss improved from 0.82726 to 0.80931, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.6459 - acc: 0.7836 - val_loss: 0.8093 - val_acc: 0.7117\n",
            "Epoch 63/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6256 - acc: 0.7915\n",
            "Epoch 00063: val_loss did not improve from 0.80931\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.6270 - acc: 0.7914 - val_loss: 0.8116 - val_acc: 0.7117\n",
            "Epoch 64/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6223 - acc: 0.7938\n",
            "Epoch 00064: val_loss improved from 0.80931 to 0.80473, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 0.6199 - acc: 0.7953 - val_loss: 0.8047 - val_acc: 0.7192\n",
            "Epoch 65/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6121 - acc: 0.7868\n",
            "Epoch 00065: val_loss improved from 0.80473 to 0.80041, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 226us/sample - loss: 0.6154 - acc: 0.7850 - val_loss: 0.8004 - val_acc: 0.7233\n",
            "Epoch 66/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5941 - acc: 0.8026\n",
            "Epoch 00066: val_loss improved from 0.80041 to 0.79963, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 0.5944 - acc: 0.8022 - val_loss: 0.7996 - val_acc: 0.7117\n",
            "Epoch 67/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6056 - acc: 0.8021\n",
            "Epoch 00067: val_loss improved from 0.79963 to 0.70824, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 221us/sample - loss: 0.6110 - acc: 0.7983 - val_loss: 0.7082 - val_acc: 0.7600\n",
            "Epoch 68/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5807 - acc: 0.8056\n",
            "Epoch 00068: val_loss did not improve from 0.70824\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.5809 - acc: 0.8047 - val_loss: 0.7712 - val_acc: 0.7283\n",
            "Epoch 69/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5474 - acc: 0.8162\n",
            "Epoch 00069: val_loss did not improve from 0.70824\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.5475 - acc: 0.8175 - val_loss: 0.7109 - val_acc: 0.7533\n",
            "Epoch 70/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5374 - acc: 0.8156\n",
            "Epoch 00070: val_loss improved from 0.70824 to 0.69378, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.5329 - acc: 0.8192 - val_loss: 0.6938 - val_acc: 0.7633\n",
            "Epoch 71/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5205 - acc: 0.8235\n",
            "Epoch 00071: val_loss did not improve from 0.69378\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.5245 - acc: 0.8236 - val_loss: 0.7005 - val_acc: 0.7500\n",
            "Epoch 72/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5344 - acc: 0.8144\n",
            "Epoch 00072: val_loss improved from 0.69378 to 0.64007, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 0.5384 - acc: 0.8133 - val_loss: 0.6401 - val_acc: 0.7858\n",
            "Epoch 73/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5183 - acc: 0.8338\n",
            "Epoch 00073: val_loss did not improve from 0.64007\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.5177 - acc: 0.8336 - val_loss: 0.6472 - val_acc: 0.7825\n",
            "Epoch 74/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4974 - acc: 0.8360\n",
            "Epoch 00074: val_loss improved from 0.64007 to 0.57688, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 221us/sample - loss: 0.4992 - acc: 0.8364 - val_loss: 0.5769 - val_acc: 0.8100\n",
            "Epoch 75/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4802 - acc: 0.8479\n",
            "Epoch 00075: val_loss did not improve from 0.57688\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.4807 - acc: 0.8472 - val_loss: 0.6196 - val_acc: 0.7875\n",
            "Epoch 76/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4924 - acc: 0.8362\n",
            "Epoch 00076: val_loss did not improve from 0.57688\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.4950 - acc: 0.8344 - val_loss: 0.5825 - val_acc: 0.8042\n",
            "Epoch 77/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4629 - acc: 0.8474\n",
            "Epoch 00077: val_loss improved from 0.57688 to 0.56610, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 224us/sample - loss: 0.4642 - acc: 0.8467 - val_loss: 0.5661 - val_acc: 0.8125\n",
            "Epoch 78/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4493 - acc: 0.8541\n",
            "Epoch 00078: val_loss improved from 0.56610 to 0.55301, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 222us/sample - loss: 0.4541 - acc: 0.8531 - val_loss: 0.5530 - val_acc: 0.8117\n",
            "Epoch 79/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4581 - acc: 0.8471\n",
            "Epoch 00079: val_loss did not improve from 0.55301\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.4588 - acc: 0.8472 - val_loss: 0.5657 - val_acc: 0.8267\n",
            "Epoch 80/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4297 - acc: 0.8665\n",
            "Epoch 00080: val_loss did not improve from 0.55301\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.4302 - acc: 0.8667 - val_loss: 0.5549 - val_acc: 0.8208\n",
            "Epoch 81/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4219 - acc: 0.8624\n",
            "Epoch 00081: val_loss improved from 0.55301 to 0.52096, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 222us/sample - loss: 0.4204 - acc: 0.8633 - val_loss: 0.5210 - val_acc: 0.8267\n",
            "Epoch 82/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4082 - acc: 0.8718\n",
            "Epoch 00082: val_loss did not improve from 0.52096\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.4147 - acc: 0.8683 - val_loss: 0.5353 - val_acc: 0.8258\n",
            "Epoch 83/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8763\n",
            "Epoch 00083: val_loss did not improve from 0.52096\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.3992 - acc: 0.8744 - val_loss: 0.5793 - val_acc: 0.7958\n",
            "Epoch 84/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3978 - acc: 0.8765\n",
            "Epoch 00084: val_loss improved from 0.52096 to 0.49154, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 226us/sample - loss: 0.3991 - acc: 0.8767 - val_loss: 0.4915 - val_acc: 0.8433\n",
            "Epoch 85/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3935 - acc: 0.8712\n",
            "Epoch 00085: val_loss improved from 0.49154 to 0.46756, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 222us/sample - loss: 0.3968 - acc: 0.8706 - val_loss: 0.4676 - val_acc: 0.8492\n",
            "Epoch 86/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3863 - acc: 0.8806\n",
            "Epoch 00086: val_loss did not improve from 0.46756\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.3877 - acc: 0.8803 - val_loss: 0.4831 - val_acc: 0.8333\n",
            "Epoch 87/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3866 - acc: 0.8803\n",
            "Epoch 00087: val_loss did not improve from 0.46756\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.3851 - acc: 0.8808 - val_loss: 0.5007 - val_acc: 0.8350\n",
            "Epoch 88/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3691 - acc: 0.8797\n",
            "Epoch 00088: val_loss did not improve from 0.46756\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.3684 - acc: 0.8822 - val_loss: 0.4825 - val_acc: 0.8342\n",
            "Epoch 89/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3955 - acc: 0.8765\n",
            "Epoch 00089: val_loss did not improve from 0.46756\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.3897 - acc: 0.8786 - val_loss: 0.4680 - val_acc: 0.8475\n",
            "Epoch 90/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3663 - acc: 0.8859\n",
            "Epoch 00090: val_loss improved from 0.46756 to 0.42641, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 221us/sample - loss: 0.3705 - acc: 0.8836 - val_loss: 0.4264 - val_acc: 0.8625\n",
            "Epoch 91/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3529 - acc: 0.8940\n",
            "Epoch 00091: val_loss improved from 0.42641 to 0.41441, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 221us/sample - loss: 0.3522 - acc: 0.8947 - val_loss: 0.4144 - val_acc: 0.8708\n",
            "Epoch 92/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3590 - acc: 0.8862\n",
            "Epoch 00092: val_loss did not improve from 0.41441\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.3669 - acc: 0.8831 - val_loss: 0.4482 - val_acc: 0.8492\n",
            "Epoch 93/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3525 - acc: 0.8909\n",
            "Epoch 00093: val_loss did not improve from 0.41441\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.3491 - acc: 0.8917 - val_loss: 0.4235 - val_acc: 0.8583\n",
            "Epoch 94/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3265 - acc: 0.9003\n",
            "Epoch 00094: val_loss improved from 0.41441 to 0.40727, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 0.3253 - acc: 0.8997 - val_loss: 0.4073 - val_acc: 0.8775\n",
            "Epoch 95/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3332 - acc: 0.9000\n",
            "Epoch 00095: val_loss improved from 0.40727 to 0.40351, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 222us/sample - loss: 0.3347 - acc: 0.8972 - val_loss: 0.4035 - val_acc: 0.8717\n",
            "Epoch 96/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3192 - acc: 0.9085\n",
            "Epoch 00096: val_loss did not improve from 0.40351\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.3217 - acc: 0.9075 - val_loss: 0.4162 - val_acc: 0.8608\n",
            "Epoch 97/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3184 - acc: 0.9059\n",
            "Epoch 00097: val_loss improved from 0.40351 to 0.35247, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 226us/sample - loss: 0.3219 - acc: 0.9047 - val_loss: 0.3525 - val_acc: 0.8900\n",
            "Epoch 98/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3153 - acc: 0.9035\n",
            "Epoch 00098: val_loss did not improve from 0.35247\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.3187 - acc: 0.9031 - val_loss: 0.3758 - val_acc: 0.8883\n",
            "Epoch 99/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3290 - acc: 0.8926\n",
            "Epoch 00099: val_loss did not improve from 0.35247\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.3303 - acc: 0.8914 - val_loss: 0.3920 - val_acc: 0.8767\n",
            "Epoch 100/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3116 - acc: 0.9076\n",
            "Epoch 00100: val_loss did not improve from 0.35247\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.3107 - acc: 0.9072 - val_loss: 0.3531 - val_acc: 0.8858\n",
            "Epoch 101/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3079 - acc: 0.9097\n",
            "Epoch 00101: val_loss did not improve from 0.35247\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.3089 - acc: 0.9097 - val_loss: 0.3772 - val_acc: 0.8700\n",
            "Epoch 102/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3190 - acc: 0.8997\n",
            "Epoch 00102: val_loss did not improve from 0.35247\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.3206 - acc: 0.8997 - val_loss: 0.3837 - val_acc: 0.8775\n",
            "Epoch 103/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3019 - acc: 0.9020\n",
            "Epoch 00103: val_loss improved from 0.35247 to 0.35047, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 221us/sample - loss: 0.3024 - acc: 0.9014 - val_loss: 0.3505 - val_acc: 0.8892\n",
            "Epoch 104/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3000 - acc: 0.9024\n",
            "Epoch 00104: val_loss did not improve from 0.35047\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.3014 - acc: 0.9022 - val_loss: 0.3849 - val_acc: 0.8592\n",
            "Epoch 105/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3107 - acc: 0.8976\n",
            "Epoch 00105: val_loss did not improve from 0.35047\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.3093 - acc: 0.8986 - val_loss: 0.3837 - val_acc: 0.8825\n",
            "Epoch 106/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2843 - acc: 0.9153\n",
            "Epoch 00106: val_loss did not improve from 0.35047\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.2866 - acc: 0.9142 - val_loss: 0.3546 - val_acc: 0.8958\n",
            "Epoch 107/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2900 - acc: 0.9076\n",
            "Epoch 00107: val_loss improved from 0.35047 to 0.34400, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.2904 - acc: 0.9086 - val_loss: 0.3440 - val_acc: 0.8908\n",
            "Epoch 108/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3003 - acc: 0.9074\n",
            "Epoch 00108: val_loss did not improve from 0.34400\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.3020 - acc: 0.9069 - val_loss: 0.3687 - val_acc: 0.8892\n",
            "Epoch 109/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2934 - acc: 0.9068\n",
            "Epoch 00109: val_loss improved from 0.34400 to 0.32418, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 224us/sample - loss: 0.2926 - acc: 0.9086 - val_loss: 0.3242 - val_acc: 0.8950\n",
            "Epoch 110/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2648 - acc: 0.9246\n",
            "Epoch 00110: val_loss improved from 0.32418 to 0.29078, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 222us/sample - loss: 0.2645 - acc: 0.9244 - val_loss: 0.2908 - val_acc: 0.9158\n",
            "Epoch 111/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2781 - acc: 0.9112\n",
            "Epoch 00111: val_loss did not improve from 0.29078\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.2784 - acc: 0.9111 - val_loss: 0.3519 - val_acc: 0.8842\n",
            "Epoch 112/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2738 - acc: 0.9179\n",
            "Epoch 00112: val_loss did not improve from 0.29078\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.2750 - acc: 0.9181 - val_loss: 0.3432 - val_acc: 0.8917\n",
            "Epoch 113/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2672 - acc: 0.9218\n",
            "Epoch 00113: val_loss did not improve from 0.29078\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.2646 - acc: 0.9228 - val_loss: 0.3241 - val_acc: 0.9017\n",
            "Epoch 114/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2714 - acc: 0.9162\n",
            "Epoch 00114: val_loss did not improve from 0.29078\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.2755 - acc: 0.9142 - val_loss: 0.3064 - val_acc: 0.9100\n",
            "Epoch 115/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2686 - acc: 0.9171\n",
            "Epoch 00115: val_loss did not improve from 0.29078\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.2706 - acc: 0.9172 - val_loss: 0.3130 - val_acc: 0.9050\n",
            "Epoch 116/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2755 - acc: 0.9121\n",
            "Epoch 00116: val_loss did not improve from 0.29078\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.2719 - acc: 0.9136 - val_loss: 0.3319 - val_acc: 0.8908\n",
            "Epoch 117/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2508 - acc: 0.9268\n",
            "Epoch 00117: val_loss did not improve from 0.29078\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.2515 - acc: 0.9269 - val_loss: 0.3059 - val_acc: 0.9025\n",
            "Epoch 118/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2781 - acc: 0.9106\n",
            "Epoch 00118: val_loss did not improve from 0.29078\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.2767 - acc: 0.9111 - val_loss: 0.3142 - val_acc: 0.9067\n",
            "Epoch 119/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.2618 - acc: 0.9221\n",
            "Epoch 00119: val_loss did not improve from 0.29078\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.2611 - acc: 0.9231 - val_loss: 0.3189 - val_acc: 0.9033\n",
            "Epoch 120/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2397 - acc: 0.9279\n",
            "Epoch 00120: val_loss did not improve from 0.29078\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.2399 - acc: 0.9281 - val_loss: 0.3039 - val_acc: 0.9075\n",
            "Epoch 121/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2767 - acc: 0.9047\n",
            "Epoch 00121: val_loss improved from 0.29078 to 0.28998, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 221us/sample - loss: 0.2792 - acc: 0.9044 - val_loss: 0.2900 - val_acc: 0.9208\n",
            "Epoch 122/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2466 - acc: 0.9265\n",
            "Epoch 00122: val_loss improved from 0.28998 to 0.28825, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 221us/sample - loss: 0.2476 - acc: 0.9258 - val_loss: 0.2883 - val_acc: 0.9133\n",
            "Epoch 123/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.9194\n",
            "Epoch 00123: val_loss did not improve from 0.28825\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.2528 - acc: 0.9197 - val_loss: 0.3186 - val_acc: 0.9033\n",
            "Epoch 124/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2362 - acc: 0.9318\n",
            "Epoch 00124: val_loss improved from 0.28825 to 0.28003, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 222us/sample - loss: 0.2393 - acc: 0.9308 - val_loss: 0.2800 - val_acc: 0.9067\n",
            "Epoch 125/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2384 - acc: 0.9235\n",
            "Epoch 00125: val_loss did not improve from 0.28003\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.2396 - acc: 0.9239 - val_loss: 0.2820 - val_acc: 0.9108\n",
            "Epoch 126/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2372 - acc: 0.9279\n",
            "Epoch 00126: val_loss improved from 0.28003 to 0.27314, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 0.2374 - acc: 0.9275 - val_loss: 0.2731 - val_acc: 0.9267\n",
            "Epoch 127/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2241 - acc: 0.9294\n",
            "Epoch 00127: val_loss did not improve from 0.27314\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.2223 - acc: 0.9306 - val_loss: 0.3106 - val_acc: 0.9117\n",
            "Epoch 128/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9363\n",
            "Epoch 00128: val_loss improved from 0.27314 to 0.26439, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 223us/sample - loss: 0.2260 - acc: 0.9361 - val_loss: 0.2644 - val_acc: 0.9225\n",
            "Epoch 129/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2228 - acc: 0.9353\n",
            "Epoch 00129: val_loss did not improve from 0.26439\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.2271 - acc: 0.9342 - val_loss: 0.2897 - val_acc: 0.9075\n",
            "Epoch 130/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2300 - acc: 0.9315\n",
            "Epoch 00130: val_loss improved from 0.26439 to 0.23086, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 222us/sample - loss: 0.2313 - acc: 0.9308 - val_loss: 0.2309 - val_acc: 0.9350\n",
            "Epoch 131/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2388 - acc: 0.9253\n",
            "Epoch 00131: val_loss did not improve from 0.23086\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.2367 - acc: 0.9261 - val_loss: 0.2543 - val_acc: 0.9167\n",
            "Epoch 132/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2358 - acc: 0.9300\n",
            "Epoch 00132: val_loss did not improve from 0.23086\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.2367 - acc: 0.9297 - val_loss: 0.2586 - val_acc: 0.9242\n",
            "Epoch 133/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2154 - acc: 0.9371\n",
            "Epoch 00133: val_loss did not improve from 0.23086\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.2164 - acc: 0.9358 - val_loss: 0.2793 - val_acc: 0.9092\n",
            "Epoch 134/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2071 - acc: 0.9371\n",
            "Epoch 00134: val_loss improved from 0.23086 to 0.21219, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 224us/sample - loss: 0.2071 - acc: 0.9375 - val_loss: 0.2122 - val_acc: 0.9417\n",
            "Epoch 135/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2149 - acc: 0.9353\n",
            "Epoch 00135: val_loss did not improve from 0.21219\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.2133 - acc: 0.9356 - val_loss: 0.2352 - val_acc: 0.9242\n",
            "Epoch 136/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2084 - acc: 0.9403\n",
            "Epoch 00136: val_loss did not improve from 0.21219\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.2055 - acc: 0.9417 - val_loss: 0.2365 - val_acc: 0.9342\n",
            "Epoch 137/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2012 - acc: 0.9450\n",
            "Epoch 00137: val_loss improved from 0.21219 to 0.20880, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 0.2019 - acc: 0.9456 - val_loss: 0.2088 - val_acc: 0.9425\n",
            "Epoch 138/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1992 - acc: 0.9403\n",
            "Epoch 00138: val_loss did not improve from 0.20880\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.2011 - acc: 0.9394 - val_loss: 0.2532 - val_acc: 0.9183\n",
            "Epoch 139/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2061 - acc: 0.9382\n",
            "Epoch 00139: val_loss did not improve from 0.20880\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.2039 - acc: 0.9394 - val_loss: 0.2976 - val_acc: 0.9033\n",
            "Epoch 140/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2040 - acc: 0.9347\n",
            "Epoch 00140: val_loss did not improve from 0.20880\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.2031 - acc: 0.9356 - val_loss: 0.2391 - val_acc: 0.9342\n",
            "Epoch 141/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2097 - acc: 0.9354\n",
            "Epoch 00141: val_loss did not improve from 0.20880\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.2097 - acc: 0.9350 - val_loss: 0.2700 - val_acc: 0.9158\n",
            "Epoch 142/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9383\n",
            "Epoch 00142: val_loss did not improve from 0.20880\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.2108 - acc: 0.9372 - val_loss: 0.2396 - val_acc: 0.9350\n",
            "Epoch 143/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2082 - acc: 0.9411\n",
            "Epoch 00143: val_loss improved from 0.20880 to 0.20178, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 0.2095 - acc: 0.9406 - val_loss: 0.2018 - val_acc: 0.9400\n",
            "Epoch 144/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1866 - acc: 0.9482\n",
            "Epoch 00144: val_loss did not improve from 0.20178\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1910 - acc: 0.9472 - val_loss: 0.2356 - val_acc: 0.9258\n",
            "Epoch 145/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1808 - acc: 0.9491\n",
            "Epoch 00145: val_loss did not improve from 0.20178\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1813 - acc: 0.9497 - val_loss: 0.2162 - val_acc: 0.9400\n",
            "Epoch 146/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2131 - acc: 0.9306\n",
            "Epoch 00146: val_loss did not improve from 0.20178\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.2114 - acc: 0.9317 - val_loss: 0.2200 - val_acc: 0.9367\n",
            "Epoch 147/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2053 - acc: 0.9418\n",
            "Epoch 00147: val_loss did not improve from 0.20178\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.2058 - acc: 0.9422 - val_loss: 0.2227 - val_acc: 0.9333\n",
            "Epoch 148/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9397\n",
            "Epoch 00148: val_loss improved from 0.20178 to 0.19864, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.1948 - acc: 0.9403 - val_loss: 0.1986 - val_acc: 0.9450\n",
            "Epoch 149/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9394\n",
            "Epoch 00149: val_loss did not improve from 0.19864\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.2005 - acc: 0.9403 - val_loss: 0.2804 - val_acc: 0.9133\n",
            "Epoch 150/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2068 - acc: 0.9382\n",
            "Epoch 00150: val_loss did not improve from 0.19864\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.2063 - acc: 0.9381 - val_loss: 0.2304 - val_acc: 0.9408\n",
            "Epoch 151/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1953 - acc: 0.9365\n",
            "Epoch 00151: val_loss did not improve from 0.19864\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1954 - acc: 0.9361 - val_loss: 0.2333 - val_acc: 0.9275\n",
            "Epoch 152/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2050 - acc: 0.9374\n",
            "Epoch 00152: val_loss did not improve from 0.19864\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.2058 - acc: 0.9361 - val_loss: 0.1991 - val_acc: 0.9392\n",
            "Epoch 153/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1851 - acc: 0.9462\n",
            "Epoch 00153: val_loss did not improve from 0.19864\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1862 - acc: 0.9450 - val_loss: 0.2066 - val_acc: 0.9400\n",
            "Epoch 154/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1737 - acc: 0.9515\n",
            "Epoch 00154: val_loss improved from 0.19864 to 0.18069, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 222us/sample - loss: 0.1765 - acc: 0.9500 - val_loss: 0.1807 - val_acc: 0.9517\n",
            "Epoch 155/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1985 - acc: 0.9347\n",
            "Epoch 00155: val_loss did not improve from 0.18069\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1989 - acc: 0.9344 - val_loss: 0.2205 - val_acc: 0.9375\n",
            "Epoch 156/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9383\n",
            "Epoch 00156: val_loss did not improve from 0.18069\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1939 - acc: 0.9383 - val_loss: 0.2348 - val_acc: 0.9317\n",
            "Epoch 157/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1941 - acc: 0.9418\n",
            "Epoch 00157: val_loss did not improve from 0.18069\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1962 - acc: 0.9417 - val_loss: 0.2049 - val_acc: 0.9433\n",
            "Epoch 158/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1887 - acc: 0.9449\n",
            "Epoch 00158: val_loss did not improve from 0.18069\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1890 - acc: 0.9444 - val_loss: 0.2388 - val_acc: 0.9358\n",
            "Epoch 159/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1982 - acc: 0.9397\n",
            "Epoch 00159: val_loss did not improve from 0.18069\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1977 - acc: 0.9403 - val_loss: 0.2210 - val_acc: 0.9292\n",
            "Epoch 160/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1870 - acc: 0.9432\n",
            "Epoch 00160: val_loss did not improve from 0.18069\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1878 - acc: 0.9425 - val_loss: 0.2296 - val_acc: 0.9367\n",
            "Epoch 161/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.2071 - acc: 0.9350\n",
            "Epoch 00161: val_loss did not improve from 0.18069\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.2068 - acc: 0.9356 - val_loss: 0.1845 - val_acc: 0.9467\n",
            "Epoch 162/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9454\n",
            "Epoch 00162: val_loss did not improve from 0.18069\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1803 - acc: 0.9453 - val_loss: 0.2008 - val_acc: 0.9458\n",
            "Epoch 163/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1967 - acc: 0.9385\n",
            "Epoch 00163: val_loss did not improve from 0.18069\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1944 - acc: 0.9392 - val_loss: 0.1930 - val_acc: 0.9417\n",
            "Epoch 164/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1957 - acc: 0.9356\n",
            "Epoch 00164: val_loss did not improve from 0.18069\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1946 - acc: 0.9372 - val_loss: 0.2013 - val_acc: 0.9425\n",
            "Epoch 165/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1939 - acc: 0.9371\n",
            "Epoch 00165: val_loss improved from 0.18069 to 0.17114, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.1940 - acc: 0.9375 - val_loss: 0.1711 - val_acc: 0.9550\n",
            "Epoch 166/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1784 - acc: 0.9444\n",
            "Epoch 00166: val_loss did not improve from 0.17114\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1773 - acc: 0.9453 - val_loss: 0.2046 - val_acc: 0.9458\n",
            "Epoch 167/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1705 - acc: 0.9471\n",
            "Epoch 00167: val_loss did not improve from 0.17114\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1686 - acc: 0.9478 - val_loss: 0.1925 - val_acc: 0.9375\n",
            "Epoch 168/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1812 - acc: 0.9497\n",
            "Epoch 00168: val_loss did not improve from 0.17114\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1795 - acc: 0.9503 - val_loss: 0.1918 - val_acc: 0.9467\n",
            "Epoch 169/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1831 - acc: 0.9418\n",
            "Epoch 00169: val_loss did not improve from 0.17114\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1820 - acc: 0.9431 - val_loss: 0.2263 - val_acc: 0.9325\n",
            "Epoch 170/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1813 - acc: 0.9447\n",
            "Epoch 00170: val_loss did not improve from 0.17114\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1815 - acc: 0.9450 - val_loss: 0.1968 - val_acc: 0.9358\n",
            "Epoch 171/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1697 - acc: 0.9476\n",
            "Epoch 00171: val_loss did not improve from 0.17114\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1704 - acc: 0.9475 - val_loss: 0.1963 - val_acc: 0.9450\n",
            "Epoch 172/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1556 - acc: 0.9553\n",
            "Epoch 00172: val_loss did not improve from 0.17114\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1574 - acc: 0.9542 - val_loss: 0.1755 - val_acc: 0.9500\n",
            "Epoch 173/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1736 - acc: 0.9483\n",
            "Epoch 00173: val_loss did not improve from 0.17114\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1735 - acc: 0.9486 - val_loss: 0.1776 - val_acc: 0.9567\n",
            "Epoch 174/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9466\n",
            "Epoch 00174: val_loss did not improve from 0.17114\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1785 - acc: 0.9464 - val_loss: 0.2104 - val_acc: 0.9400\n",
            "Epoch 175/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1778 - acc: 0.9465\n",
            "Epoch 00175: val_loss did not improve from 0.17114\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1762 - acc: 0.9478 - val_loss: 0.1818 - val_acc: 0.9458\n",
            "Epoch 176/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1709 - acc: 0.9497\n",
            "Epoch 00176: val_loss did not improve from 0.17114\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1753 - acc: 0.9472 - val_loss: 0.1789 - val_acc: 0.9533\n",
            "Epoch 177/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1746 - acc: 0.9494\n",
            "Epoch 00177: val_loss did not improve from 0.17114\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1737 - acc: 0.9500 - val_loss: 0.1767 - val_acc: 0.9500\n",
            "Epoch 178/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1820 - acc: 0.9440\n",
            "Epoch 00178: val_loss did not improve from 0.17114\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1811 - acc: 0.9442 - val_loss: 0.1720 - val_acc: 0.9492\n",
            "Epoch 179/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1725 - acc: 0.9479\n",
            "Epoch 00179: val_loss did not improve from 0.17114\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1771 - acc: 0.9469 - val_loss: 0.1754 - val_acc: 0.9492\n",
            "Epoch 180/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1853 - acc: 0.9382\n",
            "Epoch 00180: val_loss did not improve from 0.17114\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1845 - acc: 0.9389 - val_loss: 0.1827 - val_acc: 0.9450\n",
            "Epoch 181/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1607 - acc: 0.9557\n",
            "Epoch 00181: val_loss did not improve from 0.17114\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1609 - acc: 0.9553 - val_loss: 0.1886 - val_acc: 0.9425\n",
            "Epoch 182/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1703 - acc: 0.9450\n",
            "Epoch 00182: val_loss did not improve from 0.17114\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1715 - acc: 0.9444 - val_loss: 0.1916 - val_acc: 0.9408\n",
            "Epoch 183/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.9514\n",
            "Epoch 00183: val_loss improved from 0.17114 to 0.15554, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.1681 - acc: 0.9511 - val_loss: 0.1555 - val_acc: 0.9550\n",
            "Epoch 184/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1570 - acc: 0.9559\n",
            "Epoch 00184: val_loss did not improve from 0.15554\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.1558 - acc: 0.9561 - val_loss: 0.1626 - val_acc: 0.9542\n",
            "Epoch 185/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1664 - acc: 0.9514\n",
            "Epoch 00185: val_loss did not improve from 0.15554\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1691 - acc: 0.9500 - val_loss: 0.1993 - val_acc: 0.9350\n",
            "Epoch 186/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1664 - acc: 0.9488\n",
            "Epoch 00186: val_loss did not improve from 0.15554\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1653 - acc: 0.9494 - val_loss: 0.1562 - val_acc: 0.9558\n",
            "Epoch 187/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1619 - acc: 0.9538\n",
            "Epoch 00187: val_loss did not improve from 0.15554\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1615 - acc: 0.9539 - val_loss: 0.1823 - val_acc: 0.9442\n",
            "Epoch 188/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1625 - acc: 0.9447\n",
            "Epoch 00188: val_loss did not improve from 0.15554\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1660 - acc: 0.9439 - val_loss: 0.2130 - val_acc: 0.9383\n",
            "Epoch 189/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1569 - acc: 0.9512\n",
            "Epoch 00189: val_loss did not improve from 0.15554\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1558 - acc: 0.9519 - val_loss: 0.1757 - val_acc: 0.9500\n",
            "Epoch 190/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1620 - acc: 0.9538\n",
            "Epoch 00190: val_loss did not improve from 0.15554\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1620 - acc: 0.9542 - val_loss: 0.1699 - val_acc: 0.9492\n",
            "Epoch 191/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1532 - acc: 0.9562\n",
            "Epoch 00191: val_loss did not improve from 0.15554\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1564 - acc: 0.9544 - val_loss: 0.1873 - val_acc: 0.9533\n",
            "Epoch 192/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1791 - acc: 0.9406\n",
            "Epoch 00192: val_loss did not improve from 0.15554\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1772 - acc: 0.9414 - val_loss: 0.1609 - val_acc: 0.9492\n",
            "Epoch 193/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1649 - acc: 0.9482\n",
            "Epoch 00193: val_loss did not improve from 0.15554\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1656 - acc: 0.9472 - val_loss: 0.2234 - val_acc: 0.9258\n",
            "Epoch 194/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1595 - acc: 0.9532\n",
            "Epoch 00194: val_loss did not improve from 0.15554\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1606 - acc: 0.9522 - val_loss: 0.1773 - val_acc: 0.9400\n",
            "Epoch 195/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9523\n",
            "Epoch 00195: val_loss did not improve from 0.15554\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1641 - acc: 0.9528 - val_loss: 0.1624 - val_acc: 0.9533\n",
            "Epoch 196/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1531 - acc: 0.9556\n",
            "Epoch 00196: val_loss improved from 0.15554 to 0.15305, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 0.1552 - acc: 0.9544 - val_loss: 0.1530 - val_acc: 0.9600\n",
            "Epoch 197/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9537\n",
            "Epoch 00197: val_loss did not improve from 0.15305\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1561 - acc: 0.9539 - val_loss: 0.1677 - val_acc: 0.9550\n",
            "Epoch 198/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1418 - acc: 0.9594\n",
            "Epoch 00198: val_loss did not improve from 0.15305\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1420 - acc: 0.9597 - val_loss: 0.2167 - val_acc: 0.9350\n",
            "Epoch 199/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1617 - acc: 0.9500\n",
            "Epoch 00199: val_loss improved from 0.15305 to 0.14170, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 0.1611 - acc: 0.9508 - val_loss: 0.1417 - val_acc: 0.9650\n",
            "Epoch 200/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1558 - acc: 0.9562\n",
            "Epoch 00200: val_loss improved from 0.14170 to 0.13566, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 0.1561 - acc: 0.9558 - val_loss: 0.1357 - val_acc: 0.9600\n",
            "Epoch 201/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1556 - acc: 0.9518\n",
            "Epoch 00201: val_loss did not improve from 0.13566\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1553 - acc: 0.9511 - val_loss: 0.1410 - val_acc: 0.9575\n",
            "Epoch 202/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1645 - acc: 0.9488\n",
            "Epoch 00202: val_loss did not improve from 0.13566\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1657 - acc: 0.9481 - val_loss: 0.1668 - val_acc: 0.9525\n",
            "Epoch 203/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1509 - acc: 0.9546\n",
            "Epoch 00203: val_loss did not improve from 0.13566\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1510 - acc: 0.9547 - val_loss: 0.1560 - val_acc: 0.9567\n",
            "Epoch 204/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9560\n",
            "Epoch 00204: val_loss did not improve from 0.13566\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1563 - acc: 0.9544 - val_loss: 0.1420 - val_acc: 0.9567\n",
            "Epoch 205/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9511\n",
            "Epoch 00205: val_loss did not improve from 0.13566\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1597 - acc: 0.9511 - val_loss: 0.1384 - val_acc: 0.9608\n",
            "Epoch 206/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1631 - acc: 0.9512\n",
            "Epoch 00206: val_loss did not improve from 0.13566\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1607 - acc: 0.9528 - val_loss: 0.1490 - val_acc: 0.9583\n",
            "Epoch 207/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1658 - acc: 0.9512\n",
            "Epoch 00207: val_loss did not improve from 0.13566\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1665 - acc: 0.9508 - val_loss: 0.1606 - val_acc: 0.9508\n",
            "Epoch 208/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1522 - acc: 0.9532\n",
            "Epoch 00208: val_loss improved from 0.13566 to 0.11769, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 223us/sample - loss: 0.1538 - acc: 0.9528 - val_loss: 0.1177 - val_acc: 0.9667\n",
            "Epoch 209/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1368 - acc: 0.9594\n",
            "Epoch 00209: val_loss did not improve from 0.11769\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1377 - acc: 0.9592 - val_loss: 0.1268 - val_acc: 0.9633\n",
            "Epoch 210/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1540 - acc: 0.9515\n",
            "Epoch 00210: val_loss did not improve from 0.11769\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1563 - acc: 0.9511 - val_loss: 0.1334 - val_acc: 0.9633\n",
            "Epoch 211/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1553 - acc: 0.9547\n",
            "Epoch 00211: val_loss did not improve from 0.11769\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1610 - acc: 0.9522 - val_loss: 0.1388 - val_acc: 0.9608\n",
            "Epoch 212/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1491 - acc: 0.9541\n",
            "Epoch 00212: val_loss did not improve from 0.11769\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1484 - acc: 0.9542 - val_loss: 0.1221 - val_acc: 0.9650\n",
            "Epoch 213/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1445 - acc: 0.9576\n",
            "Epoch 00213: val_loss did not improve from 0.11769\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1446 - acc: 0.9586 - val_loss: 0.1547 - val_acc: 0.9575\n",
            "Epoch 214/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1646 - acc: 0.9529\n",
            "Epoch 00214: val_loss did not improve from 0.11769\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1632 - acc: 0.9528 - val_loss: 0.1767 - val_acc: 0.9492\n",
            "Epoch 215/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1588 - acc: 0.9497\n",
            "Epoch 00215: val_loss did not improve from 0.11769\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1566 - acc: 0.9508 - val_loss: 0.1507 - val_acc: 0.9617\n",
            "Epoch 216/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9497\n",
            "Epoch 00216: val_loss did not improve from 0.11769\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1528 - acc: 0.9503 - val_loss: 0.1709 - val_acc: 0.9492\n",
            "Epoch 217/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1533 - acc: 0.9506\n",
            "Epoch 00217: val_loss did not improve from 0.11769\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1555 - acc: 0.9500 - val_loss: 0.1292 - val_acc: 0.9650\n",
            "Epoch 218/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1694 - acc: 0.9438\n",
            "Epoch 00218: val_loss did not improve from 0.11769\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1730 - acc: 0.9428 - val_loss: 0.1775 - val_acc: 0.9425\n",
            "Epoch 219/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1550 - acc: 0.9521\n",
            "Epoch 00219: val_loss improved from 0.11769 to 0.11659, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.1516 - acc: 0.9542 - val_loss: 0.1166 - val_acc: 0.9675\n",
            "Epoch 220/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9629\n",
            "Epoch 00220: val_loss did not improve from 0.11659\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1285 - acc: 0.9631 - val_loss: 0.1664 - val_acc: 0.9467\n",
            "Epoch 221/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1350 - acc: 0.9565\n",
            "Epoch 00221: val_loss did not improve from 0.11659\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1356 - acc: 0.9564 - val_loss: 0.1703 - val_acc: 0.9408\n",
            "Epoch 222/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1379 - acc: 0.9594\n",
            "Epoch 00222: val_loss did not improve from 0.11659\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1371 - acc: 0.9603 - val_loss: 0.1226 - val_acc: 0.9642\n",
            "Epoch 223/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1317 - acc: 0.9656\n",
            "Epoch 00223: val_loss did not improve from 0.11659\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1313 - acc: 0.9667 - val_loss: 0.1296 - val_acc: 0.9592\n",
            "Epoch 224/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1311 - acc: 0.9588\n",
            "Epoch 00224: val_loss did not improve from 0.11659\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1329 - acc: 0.9592 - val_loss: 0.1442 - val_acc: 0.9575\n",
            "Epoch 225/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1292 - acc: 0.9629\n",
            "Epoch 00225: val_loss did not improve from 0.11659\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1294 - acc: 0.9631 - val_loss: 0.1349 - val_acc: 0.9575\n",
            "Epoch 226/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1413 - acc: 0.9541\n",
            "Epoch 00226: val_loss did not improve from 0.11659\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1442 - acc: 0.9533 - val_loss: 0.1623 - val_acc: 0.9542\n",
            "Epoch 227/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1617 - acc: 0.9488\n",
            "Epoch 00227: val_loss did not improve from 0.11659\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1639 - acc: 0.9481 - val_loss: 0.1369 - val_acc: 0.9583\n",
            "Epoch 228/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1419 - acc: 0.9591\n",
            "Epoch 00228: val_loss did not improve from 0.11659\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1427 - acc: 0.9589 - val_loss: 0.1269 - val_acc: 0.9642\n",
            "Epoch 229/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1302 - acc: 0.9641\n",
            "Epoch 00229: val_loss did not improve from 0.11659\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1271 - acc: 0.9656 - val_loss: 0.1190 - val_acc: 0.9650\n",
            "Epoch 230/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1240 - acc: 0.9644\n",
            "Epoch 00230: val_loss did not improve from 0.11659\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1254 - acc: 0.9639 - val_loss: 0.1379 - val_acc: 0.9592\n",
            "Epoch 231/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1343 - acc: 0.9591\n",
            "Epoch 00231: val_loss improved from 0.11659 to 0.11641, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.1344 - acc: 0.9594 - val_loss: 0.1164 - val_acc: 0.9667\n",
            "Epoch 232/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1535 - acc: 0.9535\n",
            "Epoch 00232: val_loss did not improve from 0.11641\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1533 - acc: 0.9528 - val_loss: 0.1525 - val_acc: 0.9533\n",
            "Epoch 233/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1477 - acc: 0.9532\n",
            "Epoch 00233: val_loss did not improve from 0.11641\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1492 - acc: 0.9533 - val_loss: 0.1390 - val_acc: 0.9600\n",
            "Epoch 234/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9543\n",
            "Epoch 00234: val_loss did not improve from 0.11641\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1429 - acc: 0.9544 - val_loss: 0.1229 - val_acc: 0.9692\n",
            "Epoch 235/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1363 - acc: 0.9650\n",
            "Epoch 00235: val_loss did not improve from 0.11641\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1370 - acc: 0.9650 - val_loss: 0.1212 - val_acc: 0.9608\n",
            "Epoch 236/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1413 - acc: 0.9591\n",
            "Epoch 00236: val_loss did not improve from 0.11641\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1434 - acc: 0.9583 - val_loss: 0.1291 - val_acc: 0.9633\n",
            "Epoch 237/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1261 - acc: 0.9662\n",
            "Epoch 00237: val_loss did not improve from 0.11641\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1265 - acc: 0.9658 - val_loss: 0.1251 - val_acc: 0.9633\n",
            "Epoch 238/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1307 - acc: 0.9632\n",
            "Epoch 00238: val_loss did not improve from 0.11641\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1317 - acc: 0.9631 - val_loss: 0.1368 - val_acc: 0.9567\n",
            "Epoch 239/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1343 - acc: 0.9597\n",
            "Epoch 00239: val_loss improved from 0.11641 to 0.11559, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 222us/sample - loss: 0.1354 - acc: 0.9592 - val_loss: 0.1156 - val_acc: 0.9658\n",
            "Epoch 240/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1432 - acc: 0.9589\n",
            "Epoch 00240: val_loss did not improve from 0.11559\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1453 - acc: 0.9578 - val_loss: 0.1243 - val_acc: 0.9667\n",
            "Epoch 241/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1418 - acc: 0.9512\n",
            "Epoch 00241: val_loss did not improve from 0.11559\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1394 - acc: 0.9528 - val_loss: 0.1161 - val_acc: 0.9633\n",
            "Epoch 242/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1433 - acc: 0.9556\n",
            "Epoch 00242: val_loss did not improve from 0.11559\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1431 - acc: 0.9561 - val_loss: 0.1341 - val_acc: 0.9600\n",
            "Epoch 243/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9597\n",
            "Epoch 00243: val_loss did not improve from 0.11559\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1444 - acc: 0.9592 - val_loss: 0.1294 - val_acc: 0.9592\n",
            "Epoch 244/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1442 - acc: 0.9570\n",
            "Epoch 00244: val_loss did not improve from 0.11559\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1471 - acc: 0.9553 - val_loss: 0.1286 - val_acc: 0.9650\n",
            "Epoch 245/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1303 - acc: 0.9653\n",
            "Epoch 00245: val_loss did not improve from 0.11559\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1312 - acc: 0.9639 - val_loss: 0.1494 - val_acc: 0.9600\n",
            "Epoch 246/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1274 - acc: 0.9635\n",
            "Epoch 00246: val_loss did not improve from 0.11559\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1268 - acc: 0.9636 - val_loss: 0.1468 - val_acc: 0.9617\n",
            "Epoch 247/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1244 - acc: 0.9624\n",
            "Epoch 00247: val_loss did not improve from 0.11559\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1245 - acc: 0.9617 - val_loss: 0.1408 - val_acc: 0.9592\n",
            "Epoch 248/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1305 - acc: 0.9606\n",
            "Epoch 00248: val_loss did not improve from 0.11559\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1299 - acc: 0.9608 - val_loss: 0.1289 - val_acc: 0.9625\n",
            "Epoch 249/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9606\n",
            "Epoch 00249: val_loss did not improve from 0.11559\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1348 - acc: 0.9603 - val_loss: 0.1327 - val_acc: 0.9608\n",
            "Epoch 250/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1324 - acc: 0.9626\n",
            "Epoch 00250: val_loss did not improve from 0.11559\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1327 - acc: 0.9628 - val_loss: 0.1226 - val_acc: 0.9633\n",
            "Epoch 251/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1393 - acc: 0.9606\n",
            "Epoch 00251: val_loss did not improve from 0.11559\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1396 - acc: 0.9606 - val_loss: 0.1348 - val_acc: 0.9592\n",
            "Epoch 252/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1155 - acc: 0.9703\n",
            "Epoch 00252: val_loss did not improve from 0.11559\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1156 - acc: 0.9700 - val_loss: 0.1354 - val_acc: 0.9575\n",
            "Epoch 253/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1305 - acc: 0.9579\n",
            "Epoch 00253: val_loss improved from 0.11559 to 0.10721, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 0.1313 - acc: 0.9575 - val_loss: 0.1072 - val_acc: 0.9717\n",
            "Epoch 254/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1383 - acc: 0.9559\n",
            "Epoch 00254: val_loss did not improve from 0.10721\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1395 - acc: 0.9547 - val_loss: 0.1173 - val_acc: 0.9650\n",
            "Epoch 255/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9597\n",
            "Epoch 00255: val_loss did not improve from 0.10721\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1356 - acc: 0.9594 - val_loss: 0.1688 - val_acc: 0.9517\n",
            "Epoch 256/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1288 - acc: 0.9618\n",
            "Epoch 00256: val_loss did not improve from 0.10721\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1296 - acc: 0.9614 - val_loss: 0.1445 - val_acc: 0.9608\n",
            "Epoch 257/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9580\n",
            "Epoch 00257: val_loss did not improve from 0.10721\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1312 - acc: 0.9572 - val_loss: 0.1445 - val_acc: 0.9567\n",
            "Epoch 258/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1270 - acc: 0.9618\n",
            "Epoch 00258: val_loss improved from 0.10721 to 0.10486, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 0.1314 - acc: 0.9614 - val_loss: 0.1049 - val_acc: 0.9700\n",
            "Epoch 259/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1269 - acc: 0.9629\n",
            "Epoch 00259: val_loss did not improve from 0.10486\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1267 - acc: 0.9631 - val_loss: 0.1294 - val_acc: 0.9608\n",
            "Epoch 260/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9646\n",
            "Epoch 00260: val_loss did not improve from 0.10486\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1229 - acc: 0.9642 - val_loss: 0.1097 - val_acc: 0.9600\n",
            "Epoch 261/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1293 - acc: 0.9588\n",
            "Epoch 00261: val_loss did not improve from 0.10486\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1326 - acc: 0.9575 - val_loss: 0.1187 - val_acc: 0.9642\n",
            "Epoch 262/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9560\n",
            "Epoch 00262: val_loss did not improve from 0.10486\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1357 - acc: 0.9567 - val_loss: 0.1508 - val_acc: 0.9533\n",
            "Epoch 263/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1363 - acc: 0.9574\n",
            "Epoch 00263: val_loss did not improve from 0.10486\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1353 - acc: 0.9589 - val_loss: 0.1115 - val_acc: 0.9692\n",
            "Epoch 264/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1258 - acc: 0.9653\n",
            "Epoch 00264: val_loss did not improve from 0.10486\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1273 - acc: 0.9642 - val_loss: 0.1960 - val_acc: 0.9317\n",
            "Epoch 265/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1340 - acc: 0.9571\n",
            "Epoch 00265: val_loss improved from 0.10486 to 0.10363, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 226us/sample - loss: 0.1322 - acc: 0.9578 - val_loss: 0.1036 - val_acc: 0.9683\n",
            "Epoch 266/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1183 - acc: 0.9685\n",
            "Epoch 00266: val_loss improved from 0.10363 to 0.09686, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 224us/sample - loss: 0.1240 - acc: 0.9669 - val_loss: 0.0969 - val_acc: 0.9717\n",
            "Epoch 267/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1309 - acc: 0.9571\n",
            "Epoch 00267: val_loss improved from 0.09686 to 0.09680, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 222us/sample - loss: 0.1310 - acc: 0.9581 - val_loss: 0.0968 - val_acc: 0.9700\n",
            "Epoch 268/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1324 - acc: 0.9562\n",
            "Epoch 00268: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1319 - acc: 0.9558 - val_loss: 0.0991 - val_acc: 0.9758\n",
            "Epoch 269/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1228 - acc: 0.9635\n",
            "Epoch 00269: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1256 - acc: 0.9617 - val_loss: 0.1242 - val_acc: 0.9633\n",
            "Epoch 270/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1131 - acc: 0.9688\n",
            "Epoch 00270: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1113 - acc: 0.9694 - val_loss: 0.1013 - val_acc: 0.9725\n",
            "Epoch 271/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1277 - acc: 0.9621\n",
            "Epoch 00271: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1280 - acc: 0.9617 - val_loss: 0.1130 - val_acc: 0.9650\n",
            "Epoch 272/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1251 - acc: 0.9626\n",
            "Epoch 00272: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1259 - acc: 0.9622 - val_loss: 0.1119 - val_acc: 0.9675\n",
            "Epoch 273/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1294 - acc: 0.9591\n",
            "Epoch 00273: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1323 - acc: 0.9569 - val_loss: 0.1317 - val_acc: 0.9625\n",
            "Epoch 274/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1408 - acc: 0.9562\n",
            "Epoch 00274: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1424 - acc: 0.9556 - val_loss: 0.1081 - val_acc: 0.9675\n",
            "Epoch 275/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1327 - acc: 0.9615\n",
            "Epoch 00275: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1335 - acc: 0.9608 - val_loss: 0.1037 - val_acc: 0.9725\n",
            "Epoch 276/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1213 - acc: 0.9656\n",
            "Epoch 00276: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.1205 - acc: 0.9664 - val_loss: 0.1163 - val_acc: 0.9658\n",
            "Epoch 277/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1223 - acc: 0.9674\n",
            "Epoch 00277: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1218 - acc: 0.9675 - val_loss: 0.1128 - val_acc: 0.9675\n",
            "Epoch 278/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1167 - acc: 0.9650\n",
            "Epoch 00278: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1167 - acc: 0.9656 - val_loss: 0.1472 - val_acc: 0.9508\n",
            "Epoch 279/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1277 - acc: 0.9629\n",
            "Epoch 00279: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1276 - acc: 0.9628 - val_loss: 0.1171 - val_acc: 0.9633\n",
            "Epoch 280/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9514\n",
            "Epoch 00280: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1451 - acc: 0.9508 - val_loss: 0.1319 - val_acc: 0.9617\n",
            "Epoch 281/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1206 - acc: 0.9658\n",
            "Epoch 00281: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1229 - acc: 0.9644 - val_loss: 0.1389 - val_acc: 0.9567\n",
            "Epoch 282/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1194 - acc: 0.9647\n",
            "Epoch 00282: val_loss did not improve from 0.09680\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1199 - acc: 0.9639 - val_loss: 0.1109 - val_acc: 0.9692\n",
            "Epoch 283/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1346 - acc: 0.9576\n",
            "Epoch 00283: val_loss improved from 0.09680 to 0.08959, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 0.1353 - acc: 0.9575 - val_loss: 0.0896 - val_acc: 0.9708\n",
            "Epoch 284/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9546\n",
            "Epoch 00284: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1383 - acc: 0.9556 - val_loss: 0.0991 - val_acc: 0.9725\n",
            "Epoch 285/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1149 - acc: 0.9676\n",
            "Epoch 00285: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1154 - acc: 0.9675 - val_loss: 0.0975 - val_acc: 0.9725\n",
            "Epoch 286/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1021 - acc: 0.9688\n",
            "Epoch 00286: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1038 - acc: 0.9686 - val_loss: 0.1121 - val_acc: 0.9650\n",
            "Epoch 287/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9691\n",
            "Epoch 00287: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1105 - acc: 0.9692 - val_loss: 0.1028 - val_acc: 0.9692\n",
            "Epoch 288/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1188 - acc: 0.9641\n",
            "Epoch 00288: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1165 - acc: 0.9647 - val_loss: 0.0998 - val_acc: 0.9675\n",
            "Epoch 289/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1154 - acc: 0.9638\n",
            "Epoch 00289: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1166 - acc: 0.9639 - val_loss: 0.1065 - val_acc: 0.9642\n",
            "Epoch 290/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1326 - acc: 0.9594\n",
            "Epoch 00290: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1320 - acc: 0.9592 - val_loss: 0.0987 - val_acc: 0.9700\n",
            "Epoch 291/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1247 - acc: 0.9629\n",
            "Epoch 00291: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1245 - acc: 0.9636 - val_loss: 0.1066 - val_acc: 0.9683\n",
            "Epoch 292/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1431 - acc: 0.9538\n",
            "Epoch 00292: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1432 - acc: 0.9544 - val_loss: 0.1056 - val_acc: 0.9675\n",
            "Epoch 293/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1236 - acc: 0.9626\n",
            "Epoch 00293: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1229 - acc: 0.9628 - val_loss: 0.0992 - val_acc: 0.9733\n",
            "Epoch 294/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1203 - acc: 0.9635\n",
            "Epoch 00294: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1192 - acc: 0.9642 - val_loss: 0.0950 - val_acc: 0.9733\n",
            "Epoch 295/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1059 - acc: 0.9756\n",
            "Epoch 00295: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1062 - acc: 0.9758 - val_loss: 0.0988 - val_acc: 0.9683\n",
            "Epoch 296/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9666\n",
            "Epoch 00296: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1102 - acc: 0.9661 - val_loss: 0.1279 - val_acc: 0.9583\n",
            "Epoch 297/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1222 - acc: 0.9629\n",
            "Epoch 00297: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1198 - acc: 0.9639 - val_loss: 0.0991 - val_acc: 0.9717\n",
            "Epoch 298/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1115 - acc: 0.9676\n",
            "Epoch 00298: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1112 - acc: 0.9681 - val_loss: 0.1175 - val_acc: 0.9683\n",
            "Epoch 299/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1121 - acc: 0.9676\n",
            "Epoch 00299: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1098 - acc: 0.9686 - val_loss: 0.0901 - val_acc: 0.9775\n",
            "Epoch 300/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1224 - acc: 0.9632\n",
            "Epoch 00300: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1209 - acc: 0.9639 - val_loss: 0.0953 - val_acc: 0.9725\n",
            "Epoch 301/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9651\n",
            "Epoch 00301: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1238 - acc: 0.9658 - val_loss: 0.1117 - val_acc: 0.9725\n",
            "Epoch 302/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1451 - acc: 0.9509\n",
            "Epoch 00302: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1439 - acc: 0.9519 - val_loss: 0.1331 - val_acc: 0.9608\n",
            "Epoch 303/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1351 - acc: 0.9600\n",
            "Epoch 00303: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1339 - acc: 0.9603 - val_loss: 0.1186 - val_acc: 0.9658\n",
            "Epoch 304/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1194 - acc: 0.9632\n",
            "Epoch 00304: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1212 - acc: 0.9633 - val_loss: 0.1025 - val_acc: 0.9725\n",
            "Epoch 305/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1139 - acc: 0.9679\n",
            "Epoch 00305: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1139 - acc: 0.9672 - val_loss: 0.1577 - val_acc: 0.9525\n",
            "Epoch 306/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1390 - acc: 0.9559\n",
            "Epoch 00306: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1381 - acc: 0.9558 - val_loss: 0.1099 - val_acc: 0.9725\n",
            "Epoch 307/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1102 - acc: 0.9703\n",
            "Epoch 00307: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1089 - acc: 0.9708 - val_loss: 0.1022 - val_acc: 0.9683\n",
            "Epoch 308/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1095 - acc: 0.9665\n",
            "Epoch 00308: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1125 - acc: 0.9650 - val_loss: 0.1300 - val_acc: 0.9608\n",
            "Epoch 309/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1265 - acc: 0.9653\n",
            "Epoch 00309: val_loss did not improve from 0.08959\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1268 - acc: 0.9650 - val_loss: 0.0925 - val_acc: 0.9783\n",
            "Epoch 310/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9623\n",
            "Epoch 00310: val_loss improved from 0.08959 to 0.08567, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.1242 - acc: 0.9625 - val_loss: 0.0857 - val_acc: 0.9792\n",
            "Epoch 311/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1284 - acc: 0.9613\n",
            "Epoch 00311: val_loss did not improve from 0.08567\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1282 - acc: 0.9611 - val_loss: 0.1040 - val_acc: 0.9750\n",
            "Epoch 312/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1215 - acc: 0.9639\n",
            "Epoch 00312: val_loss did not improve from 0.08567\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1227 - acc: 0.9642 - val_loss: 0.1010 - val_acc: 0.9717\n",
            "Epoch 313/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1161 - acc: 0.9668\n",
            "Epoch 00313: val_loss did not improve from 0.08567\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1161 - acc: 0.9664 - val_loss: 0.0859 - val_acc: 0.9758\n",
            "Epoch 314/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9674\n",
            "Epoch 00314: val_loss did not improve from 0.08567\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1150 - acc: 0.9669 - val_loss: 0.1043 - val_acc: 0.9742\n",
            "Epoch 315/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1177 - acc: 0.9641\n",
            "Epoch 00315: val_loss did not improve from 0.08567\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1158 - acc: 0.9650 - val_loss: 0.1040 - val_acc: 0.9708\n",
            "Epoch 316/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1106 - acc: 0.9653\n",
            "Epoch 00316: val_loss did not improve from 0.08567\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1103 - acc: 0.9661 - val_loss: 0.1591 - val_acc: 0.9467\n",
            "Epoch 317/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1131 - acc: 0.9624\n",
            "Epoch 00317: val_loss did not improve from 0.08567\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1119 - acc: 0.9633 - val_loss: 0.1236 - val_acc: 0.9658\n",
            "Epoch 318/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1187 - acc: 0.9647\n",
            "Epoch 00318: val_loss improved from 0.08567 to 0.08451, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 0.1174 - acc: 0.9658 - val_loss: 0.0845 - val_acc: 0.9808\n",
            "Epoch 319/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1237 - acc: 0.9609\n",
            "Epoch 00319: val_loss did not improve from 0.08451\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1229 - acc: 0.9619 - val_loss: 0.0993 - val_acc: 0.9700\n",
            "Epoch 320/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1063 - acc: 0.9674\n",
            "Epoch 00320: val_loss did not improve from 0.08451\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1079 - acc: 0.9669 - val_loss: 0.0929 - val_acc: 0.9725\n",
            "Epoch 321/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1203 - acc: 0.9676\n",
            "Epoch 00321: val_loss did not improve from 0.08451\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1255 - acc: 0.9658 - val_loss: 0.1045 - val_acc: 0.9708\n",
            "Epoch 322/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1218 - acc: 0.9609\n",
            "Epoch 00322: val_loss did not improve from 0.08451\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1217 - acc: 0.9603 - val_loss: 0.0946 - val_acc: 0.9742\n",
            "Epoch 323/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1206 - acc: 0.9618\n",
            "Epoch 00323: val_loss did not improve from 0.08451\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1192 - acc: 0.9625 - val_loss: 0.1141 - val_acc: 0.9650\n",
            "Epoch 324/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1261 - acc: 0.9618\n",
            "Epoch 00324: val_loss did not improve from 0.08451\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1277 - acc: 0.9611 - val_loss: 0.1048 - val_acc: 0.9675\n",
            "Epoch 325/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9660\n",
            "Epoch 00325: val_loss did not improve from 0.08451\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1185 - acc: 0.9667 - val_loss: 0.1084 - val_acc: 0.9683\n",
            "Epoch 326/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0998 - acc: 0.9738\n",
            "Epoch 00326: val_loss did not improve from 0.08451\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1006 - acc: 0.9733 - val_loss: 0.0947 - val_acc: 0.9742\n",
            "Epoch 327/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1037 - acc: 0.9724\n",
            "Epoch 00327: val_loss improved from 0.08451 to 0.08438, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 0.1047 - acc: 0.9717 - val_loss: 0.0844 - val_acc: 0.9733\n",
            "Epoch 328/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9674\n",
            "Epoch 00328: val_loss improved from 0.08438 to 0.07596, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.1046 - acc: 0.9678 - val_loss: 0.0760 - val_acc: 0.9808\n",
            "Epoch 329/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1031 - acc: 0.9712\n",
            "Epoch 00329: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1031 - acc: 0.9714 - val_loss: 0.1229 - val_acc: 0.9650\n",
            "Epoch 330/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1244 - acc: 0.9612\n",
            "Epoch 00330: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1258 - acc: 0.9608 - val_loss: 0.0975 - val_acc: 0.9708\n",
            "Epoch 331/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1188 - acc: 0.9624\n",
            "Epoch 00331: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1202 - acc: 0.9619 - val_loss: 0.0889 - val_acc: 0.9725\n",
            "Epoch 332/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1284 - acc: 0.9582\n",
            "Epoch 00332: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1311 - acc: 0.9572 - val_loss: 0.1266 - val_acc: 0.9658\n",
            "Epoch 333/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9600\n",
            "Epoch 00333: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1299 - acc: 0.9592 - val_loss: 0.0928 - val_acc: 0.9800\n",
            "Epoch 334/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1103 - acc: 0.9679\n",
            "Epoch 00334: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1096 - acc: 0.9686 - val_loss: 0.0906 - val_acc: 0.9792\n",
            "Epoch 335/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1130 - acc: 0.9656\n",
            "Epoch 00335: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1121 - acc: 0.9661 - val_loss: 0.0967 - val_acc: 0.9717\n",
            "Epoch 336/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1161 - acc: 0.9641\n",
            "Epoch 00336: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1175 - acc: 0.9631 - val_loss: 0.0937 - val_acc: 0.9758\n",
            "Epoch 337/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1119 - acc: 0.9700\n",
            "Epoch 00337: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1100 - acc: 0.9703 - val_loss: 0.1040 - val_acc: 0.9725\n",
            "Epoch 338/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1175 - acc: 0.9574\n",
            "Epoch 00338: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1170 - acc: 0.9581 - val_loss: 0.0986 - val_acc: 0.9683\n",
            "Epoch 339/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1255 - acc: 0.9585\n",
            "Epoch 00339: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1273 - acc: 0.9592 - val_loss: 0.1093 - val_acc: 0.9675\n",
            "Epoch 340/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.9597\n",
            "Epoch 00340: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1384 - acc: 0.9594 - val_loss: 0.1314 - val_acc: 0.9583\n",
            "Epoch 341/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1239 - acc: 0.9609\n",
            "Epoch 00341: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1273 - acc: 0.9594 - val_loss: 0.1144 - val_acc: 0.9667\n",
            "Epoch 342/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1170 - acc: 0.9656\n",
            "Epoch 00342: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1170 - acc: 0.9658 - val_loss: 0.1029 - val_acc: 0.9708\n",
            "Epoch 343/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1177 - acc: 0.9638\n",
            "Epoch 00343: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1187 - acc: 0.9633 - val_loss: 0.0765 - val_acc: 0.9775\n",
            "Epoch 344/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1223 - acc: 0.9591\n",
            "Epoch 00344: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1218 - acc: 0.9597 - val_loss: 0.0988 - val_acc: 0.9725\n",
            "Epoch 345/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1055 - acc: 0.9709\n",
            "Epoch 00345: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1060 - acc: 0.9706 - val_loss: 0.0916 - val_acc: 0.9700\n",
            "Epoch 346/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9717\n",
            "Epoch 00346: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1021 - acc: 0.9717 - val_loss: 0.0880 - val_acc: 0.9750\n",
            "Epoch 347/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1050 - acc: 0.9685\n",
            "Epoch 00347: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1059 - acc: 0.9686 - val_loss: 0.0767 - val_acc: 0.9800\n",
            "Epoch 348/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1041 - acc: 0.9712\n",
            "Epoch 00348: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1056 - acc: 0.9706 - val_loss: 0.0856 - val_acc: 0.9783\n",
            "Epoch 349/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0925 - acc: 0.9744\n",
            "Epoch 00349: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0932 - acc: 0.9750 - val_loss: 0.0906 - val_acc: 0.9750\n",
            "Epoch 350/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0944 - acc: 0.9726\n",
            "Epoch 00350: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0943 - acc: 0.9728 - val_loss: 0.0859 - val_acc: 0.9717\n",
            "Epoch 351/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0976 - acc: 0.9724\n",
            "Epoch 00351: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0981 - acc: 0.9719 - val_loss: 0.1194 - val_acc: 0.9617\n",
            "Epoch 352/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0892 - acc: 0.9756\n",
            "Epoch 00352: val_loss did not improve from 0.07596\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.0900 - acc: 0.9756 - val_loss: 0.0912 - val_acc: 0.9767\n",
            "Epoch 353/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9749\n",
            "Epoch 00353: val_loss improved from 0.07596 to 0.07061, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.0882 - acc: 0.9747 - val_loss: 0.0706 - val_acc: 0.9783\n",
            "Epoch 354/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9669\n",
            "Epoch 00354: val_loss did not improve from 0.07061\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1017 - acc: 0.9669 - val_loss: 0.0835 - val_acc: 0.9758\n",
            "Epoch 355/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0952 - acc: 0.9721\n",
            "Epoch 00355: val_loss did not improve from 0.07061\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.0971 - acc: 0.9714 - val_loss: 0.0946 - val_acc: 0.9708\n",
            "Epoch 356/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9631\n",
            "Epoch 00356: val_loss did not improve from 0.07061\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1187 - acc: 0.9639 - val_loss: 0.0951 - val_acc: 0.9758\n",
            "Epoch 357/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1067 - acc: 0.9682\n",
            "Epoch 00357: val_loss did not improve from 0.07061\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1080 - acc: 0.9675 - val_loss: 0.0941 - val_acc: 0.9750\n",
            "Epoch 358/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1242 - acc: 0.9612\n",
            "Epoch 00358: val_loss did not improve from 0.07061\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1246 - acc: 0.9606 - val_loss: 0.0858 - val_acc: 0.9758\n",
            "Epoch 359/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1109 - acc: 0.9650\n",
            "Epoch 00359: val_loss did not improve from 0.07061\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1100 - acc: 0.9656 - val_loss: 0.1054 - val_acc: 0.9658\n",
            "Epoch 360/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1243 - acc: 0.9635\n",
            "Epoch 00360: val_loss did not improve from 0.07061\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.1235 - acc: 0.9639 - val_loss: 0.0920 - val_acc: 0.9700\n",
            "Epoch 361/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1118 - acc: 0.9674\n",
            "Epoch 00361: val_loss did not improve from 0.07061\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1128 - acc: 0.9667 - val_loss: 0.1019 - val_acc: 0.9708\n",
            "Epoch 362/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1156 - acc: 0.9643\n",
            "Epoch 00362: val_loss improved from 0.07061 to 0.06874, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 0.1156 - acc: 0.9642 - val_loss: 0.0687 - val_acc: 0.9833\n",
            "Epoch 363/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1073 - acc: 0.9656\n",
            "Epoch 00363: val_loss improved from 0.06874 to 0.06820, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 0.1077 - acc: 0.9653 - val_loss: 0.0682 - val_acc: 0.9833\n",
            "Epoch 364/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1046 - acc: 0.9691\n",
            "Epoch 00364: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.1057 - acc: 0.9686 - val_loss: 0.0981 - val_acc: 0.9725\n",
            "Epoch 365/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1158 - acc: 0.9659\n",
            "Epoch 00365: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1136 - acc: 0.9669 - val_loss: 0.0875 - val_acc: 0.9733\n",
            "Epoch 366/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1084 - acc: 0.9665\n",
            "Epoch 00366: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1080 - acc: 0.9664 - val_loss: 0.0796 - val_acc: 0.9742\n",
            "Epoch 367/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9677\n",
            "Epoch 00367: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1068 - acc: 0.9678 - val_loss: 0.0775 - val_acc: 0.9733\n",
            "Epoch 368/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1111 - acc: 0.9688\n",
            "Epoch 00368: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1122 - acc: 0.9675 - val_loss: 0.1006 - val_acc: 0.9700\n",
            "Epoch 369/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9571\n",
            "Epoch 00369: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1287 - acc: 0.9564 - val_loss: 0.1200 - val_acc: 0.9608\n",
            "Epoch 370/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1229 - acc: 0.9621\n",
            "Epoch 00370: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1219 - acc: 0.9622 - val_loss: 0.0775 - val_acc: 0.9750\n",
            "Epoch 371/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1122 - acc: 0.9665\n",
            "Epoch 00371: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1117 - acc: 0.9669 - val_loss: 0.1062 - val_acc: 0.9700\n",
            "Epoch 372/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1072 - acc: 0.9650\n",
            "Epoch 00372: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1069 - acc: 0.9642 - val_loss: 0.1104 - val_acc: 0.9617\n",
            "Epoch 373/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0973 - acc: 0.9741\n",
            "Epoch 00373: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0979 - acc: 0.9739 - val_loss: 0.0850 - val_acc: 0.9792\n",
            "Epoch 374/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0977 - acc: 0.9726\n",
            "Epoch 00374: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1012 - acc: 0.9714 - val_loss: 0.0761 - val_acc: 0.9825\n",
            "Epoch 375/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0872 - acc: 0.9738\n",
            "Epoch 00375: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0877 - acc: 0.9733 - val_loss: 0.1091 - val_acc: 0.9633\n",
            "Epoch 376/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1168 - acc: 0.9647\n",
            "Epoch 00376: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1176 - acc: 0.9642 - val_loss: 0.0953 - val_acc: 0.9733\n",
            "Epoch 377/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0994 - acc: 0.9668\n",
            "Epoch 00377: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1012 - acc: 0.9664 - val_loss: 0.0856 - val_acc: 0.9767\n",
            "Epoch 378/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9700\n",
            "Epoch 00378: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1052 - acc: 0.9692 - val_loss: 0.0803 - val_acc: 0.9783\n",
            "Epoch 379/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1017 - acc: 0.9729\n",
            "Epoch 00379: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1002 - acc: 0.9733 - val_loss: 0.0906 - val_acc: 0.9767\n",
            "Epoch 380/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0980 - acc: 0.9715\n",
            "Epoch 00380: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0974 - acc: 0.9719 - val_loss: 0.0880 - val_acc: 0.9742\n",
            "Epoch 381/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0955 - acc: 0.9721\n",
            "Epoch 00381: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0946 - acc: 0.9719 - val_loss: 0.0999 - val_acc: 0.9683\n",
            "Epoch 382/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1081 - acc: 0.9659\n",
            "Epoch 00382: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1071 - acc: 0.9661 - val_loss: 0.0733 - val_acc: 0.9825\n",
            "Epoch 383/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1129 - acc: 0.9656\n",
            "Epoch 00383: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1124 - acc: 0.9664 - val_loss: 0.0907 - val_acc: 0.9758\n",
            "Epoch 384/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9660\n",
            "Epoch 00384: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1025 - acc: 0.9664 - val_loss: 0.0869 - val_acc: 0.9750\n",
            "Epoch 385/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1014 - acc: 0.9694\n",
            "Epoch 00385: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1030 - acc: 0.9692 - val_loss: 0.0971 - val_acc: 0.9692\n",
            "Epoch 386/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1106 - acc: 0.9703\n",
            "Epoch 00386: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1089 - acc: 0.9706 - val_loss: 0.1101 - val_acc: 0.9650\n",
            "Epoch 387/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1105 - acc: 0.9647\n",
            "Epoch 00387: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1106 - acc: 0.9644 - val_loss: 0.0999 - val_acc: 0.9717\n",
            "Epoch 388/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1129 - acc: 0.9676\n",
            "Epoch 00388: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1115 - acc: 0.9681 - val_loss: 0.0848 - val_acc: 0.9750\n",
            "Epoch 389/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9669\n",
            "Epoch 00389: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1053 - acc: 0.9667 - val_loss: 0.0924 - val_acc: 0.9725\n",
            "Epoch 390/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1051 - acc: 0.9706\n",
            "Epoch 00390: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1052 - acc: 0.9708 - val_loss: 0.0731 - val_acc: 0.9817\n",
            "Epoch 391/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1049 - acc: 0.9712\n",
            "Epoch 00391: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1070 - acc: 0.9703 - val_loss: 0.0994 - val_acc: 0.9717\n",
            "Epoch 392/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9691\n",
            "Epoch 00392: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1011 - acc: 0.9689 - val_loss: 0.0704 - val_acc: 0.9825\n",
            "Epoch 393/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9706\n",
            "Epoch 00393: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0941 - acc: 0.9700 - val_loss: 0.0932 - val_acc: 0.9708\n",
            "Epoch 394/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0987 - acc: 0.9700\n",
            "Epoch 00394: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0986 - acc: 0.9706 - val_loss: 0.0973 - val_acc: 0.9700\n",
            "Epoch 395/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9669\n",
            "Epoch 00395: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1097 - acc: 0.9667 - val_loss: 0.0841 - val_acc: 0.9767\n",
            "Epoch 396/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1007 - acc: 0.9703\n",
            "Epoch 00396: val_loss did not improve from 0.06820\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1009 - acc: 0.9703 - val_loss: 0.0755 - val_acc: 0.9783\n",
            "Epoch 397/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0990 - acc: 0.9694\n",
            "Epoch 00397: val_loss improved from 0.06820 to 0.06676, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 0.1011 - acc: 0.9689 - val_loss: 0.0668 - val_acc: 0.9825\n",
            "Epoch 398/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1014 - acc: 0.9668\n",
            "Epoch 00398: val_loss did not improve from 0.06676\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1003 - acc: 0.9675 - val_loss: 0.0686 - val_acc: 0.9825\n",
            "Epoch 399/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1057 - acc: 0.9682\n",
            "Epoch 00399: val_loss improved from 0.06676 to 0.06344, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 0.1051 - acc: 0.9686 - val_loss: 0.0634 - val_acc: 0.9850\n",
            "Epoch 400/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9720\n",
            "Epoch 00400: val_loss did not improve from 0.06344\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.0973 - acc: 0.9717 - val_loss: 0.0949 - val_acc: 0.9692\n",
            "1200/1200 [==============================] - 0s 140us/sample - loss: 0.0949 - acc: 0.9692\n",
            "[0.09488598503172398, 0.9691667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_NX04ssyk2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "26bfe4d1-6d60-4527-d3a7-f6d4d91075d9"
      },
      "source": [
        "for i in range(14, 15): # Итерација низ секој испитен примерок\n",
        "  print(f\"====================== Примерок ({i}) ======================\")\n",
        "  print(\"Вчитување тест податоци од испитниот примерок \" + str(i) + \"...\")\n",
        "  \n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_test_data = np.empty(0)\n",
        "  temp_test_events = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_test_set = 'S' + format(j, '02') + '/Test'\n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testData.mat\"\n",
        "    temp = loadmat(full_path)['testData']\n",
        "    if temp_test_data.size != 0:\n",
        "      temp_test_data = np.concatenate((temp_test_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_test_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_test_events.size != 0:\n",
        "        temp_test_events = np.append(temp_test_events, temp)\n",
        "      else:\n",
        "        temp_test_events = np.array(temp)\n",
        "\n",
        "    # Вчитување на бројот на runs \n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/runs_per_block.txt\"\n",
        "    with open(full_path, \"r\") as runs_per_block:\n",
        "      test_runs_per_block[i-1][j-1] = int(runs_per_block.read())\n",
        "\n",
        "    print(\"\\t - Тест податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "  # Зачувај ги тест податоците вчитани од испитниот примерок во низа\n",
        "  test_data.append(temp_test_data)\n",
        "  test_events.append(temp_test_events)\n",
        "  print(\"Тест податоците од испитниот примерок \" + str(i) + \" се вчитани.\\n\")\n",
        "\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_data: \" + str(test_data[i-1].shape)) # test_data to predict\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_events: \" + str(len(test_events[i-1]))) # test_events\n",
        "  for j in range (1,4):\n",
        "    print(\"SBJ\" + str(format(i-1, '02')) + \" / S\" + str(format(j-1, '02')) + \"| Runs per block: \" + str(test_runs_per_block[i-1][j-1])) # runs per block in SJB01, SJ00 \n",
        "\n",
        "  to_predict_data = reshape_data_to_mne_format(test_data[i-1])\n",
        "  predictions = model14.predict(to_predict_data)\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Predictions: \" + str(len(predictions)))\n",
        "  # np.savetxt(\"predictions.csv\", predictions, delimiter=\",\")\n",
        "\n",
        "\n",
        "  # ========= FALI USTE DA SE ISPARSIRA PREDICTIONOT... NE E SREDEN OVOJ KOD DOLE =======\n",
        "\n",
        "  int_pred = np.argmax(predictions, axis=1)\n",
        "  int_ytest = np.argmax(y_test, axis=1)\n",
        "\n",
        "  session_start = 0\n",
        "  start_prediction_index = 0\n",
        "  end_prediction_index = 0\n",
        "  for session in range(0, 3):\n",
        "    print(f\"============== Сесија ({session}) ==============\")\n",
        "    for block in range(0, 50):    \n",
        "      events_per_block = test_runs_per_block[i-1][session]\n",
        "\n",
        "      start_prediction_index = session_start + (block*events_per_block)*8\n",
        "      end_prediction_index = session_start + ((block+1)*events_per_block)*8\n",
        "\n",
        "      block_prediction = int_pred[start_prediction_index:end_prediction_index]\n",
        "      prediction = np.bincount(block_prediction).argmax()\n",
        "      df.iat[session+39,block+2] = prediction+1\n",
        "      # UNCOMMENT ZA PODOBAR PRIKAZ :)\n",
        "      # print(f\"Session {session} | Block: {block} | Prediction: {prediction} | Address: {end_prediction_index}\")\n",
        "\n",
        "      print(str(prediction+1) + \",\", end=\"\")\n",
        "    session_start = end_prediction_index\n",
        "    print(\"\")\n",
        "  print(\"Stigna li do kraj: \" + str(session_start == len(predictions)))\n",
        "  print(f\"====================== Примерок ({i}) ======================\\n\\n\")"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Примерок (14) ======================\n",
            "Вчитување тест податоци од испитниот примерок 14...\n",
            "\t - Тест податоците од сесија 1 се вчитани.\n",
            "\t - Тест податоците од сесија 2 се вчитани.\n",
            "\t - Тест податоците од сесија 3 се вчитани.\n",
            "Тест податоците од испитниот примерок 14 се вчитани.\n",
            "\n",
            "SBJ13| Test_data: (8, 350, 9200)\n",
            "SBJ13| Test_events: 9200\n",
            "SBJ13 / S00| Runs per block: 10\n",
            "SBJ13 / S01| Runs per block: 6\n",
            "SBJ13 / S02| Runs per block: 7\n",
            "SBJ13| Predictions: 9200\n",
            "============== Сесија (0) ==============\n",
            "4,4,3,4,3,8,2,4,3,1,4,4,4,4,4,4,4,4,4,7,4,4,4,4,4,4,3,4,4,4,4,4,3,4,4,4,6,4,4,4,3,6,4,6,3,6,4,3,4,1,\n",
            "============== Сесија (1) ==============\n",
            "2,3,4,3,3,3,3,3,3,3,3,3,2,3,2,3,3,7,3,3,2,3,7,8,4,5,2,4,3,3,2,3,3,2,3,2,2,3,3,2,2,2,2,3,3,2,3,3,3,2,\n",
            "============== Сесија (2) ==============\n",
            "7,4,6,4,6,3,7,6,4,6,4,6,4,4,6,4,7,4,4,4,4,4,4,5,4,4,4,4,4,4,4,4,4,4,4,4,4,7,3,4,7,4,4,4,4,4,4,4,4,7,\n",
            "Stigna li do kraj: True\n",
            "====================== Примерок (14) ======================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctjiRjFiyuul",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7a4ea5e-150c-4539-fea6-806e86da45bb"
      },
      "source": [
        "df"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUBJECT</th>\n",
              "      <th>SESSION</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>Unnamed: 52</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    SUBJECT  SESSION  1  2  3  4  5  6  7  ... 43 44 45 46 47 48 49 50 Unnamed: 52\n",
              "0         1        1  6  6  3  6  6  3  6  ...  6  4  8  3  4  5  5  7         NaN\n",
              "1         1        2  6  3  2  1  2  1  3  ...  6  2  2  2  3  6  6  2         NaN\n",
              "2         1        3  3  3  3  3  3  3  3  ...  3  3  6  3  6  7  1  6         NaN\n",
              "3         2        1  8  8  8  8  8  8  8  ...  8  4  8  4  8  7  8  8         NaN\n",
              "4         2        2  2  7  6  6  6  6  7  ...  2  6  6  2  6  6  2  2         NaN\n",
              "5         2        3  2  7  2  6  7  4  2  ...  2  6  2  2  7  2  2  2         NaN\n",
              "6         3        1  3  3  4  4  4  3  6  ...  4  6  3  3  4  3  4  4         NaN\n",
              "7         3        2  6  1  7  7  7  7  7  ...  6  7  1  6  6  6  6  6         NaN\n",
              "8         3        3  6  4  8  8  5  7  8  ...  4  2  8  2  2  2  2  8         NaN\n",
              "9         4        1  1  1  2  1  5  5  6  ...  1  1  7  1  6  6  6  6         NaN\n",
              "10        4        2  7  7  7  7  6  6  7  ...  1  5  5  5  5  5  2  6         NaN\n",
              "11        4        3  7  3  7  3  6  6  7  ...  6  1  4  4  4  6  6  6         NaN\n",
              "12        5        1  5  4  4  4  6  4  5  ...  3  1  4  4  1  3  3  5         NaN\n",
              "13        5        2  3  6  3  5  2  6  7  ...  6  6  6  6  6  6  6  4         NaN\n",
              "14        5        3  4  3  3  6  5  7  6  ...  7  2  2  7  5  6  4  2         NaN\n",
              "15        6        1  1  3  1  8  3  3  3  ...  4  3  7  2  7  2  5  8         NaN\n",
              "16        6        2  3  4  1  7  1  1  1  ...  5  5  5  5  5  5  5  5         NaN\n",
              "17        6        3  2  3  2  5  3  3  3  ...  2  3  3  3  1  1  3  3         NaN\n",
              "18        7        1  4  7  5  7  4  5  4  ...  5  1  1  4  4  4  1  4         NaN\n",
              "19        7        2  2  2  2  8  2  5  2  ...  5  5  5  2  2  2  2  2         NaN\n",
              "20        7        3  2  7  7  5  7  5  4  ...  1  3  5  5  3  5  5  1         NaN\n",
              "21        8        1  5  8  2  8  2  2  1  ...  1  8  2  2  5  5  2  8         NaN\n",
              "22        8        2  2  7  1  5  1  2  1  ...  7  1  7  8  2  1  7  7         NaN\n",
              "23        8        3  4  1  6  6  1  2  1  ...  1  8  1  7  1  1  7  1         NaN\n",
              "24        9        1  6  5  6  6  7  6  8  ...  6  6  6  5  5  5  3  5         NaN\n",
              "25        9        2  3  4  8  4  1  6  1  ...  1  1  1  1  1  3  6  6         NaN\n",
              "26        9        3  5  2  2  2  2  3  2  ...  2  2  2  3  2  2  4  2         NaN\n",
              "27       10        1  1  1  8  7  3  1  1  ...  5  1  1  1  1  5  1  5         NaN\n",
              "28       10        2  1  5  5  6  1  1  1  ...  5  5  6  6  2  1  2  1         NaN\n",
              "29       10        3  5  8  5  5  5  6  5  ...  5  5  2  5  5  5  5  5         NaN\n",
              "30       11        1  3  4  4  4  3  4  3  ...  5  4  3  1  3  4  4  4         NaN\n",
              "31       11        2  3  1  1  4  8  4  4  ...  1  7  3  8  2  3  8  3         NaN\n",
              "32       11        3  4  2  4  4  4  4  4  ...  8  4  4  7  8  4  8  4         NaN\n",
              "33       12        1  1  1  1  1  1  1  6  ...  1  6  7  3  1  7  7  3         NaN\n",
              "34       12        2  3  2  6  2  2  3  2  ...  5  1  2  1  5  5  2  5         NaN\n",
              "35       12        3  1  6  2  2  1  2  2  ...  5  6  2  1  2  2  2  2         NaN\n",
              "36       13        1  6  6  1  5  3  1  1  ...  1  1  1  5  6  1  1  1         NaN\n",
              "37       13        2  6  1  5  4  1  1  6  ...  7  6  8  6  1  6  6  1         NaN\n",
              "38       13        3  1  6  5  1  5  2  6  ...  7  1  1  7  2  2  6  1         NaN\n",
              "39       14        1  4  4  3  4  3  8  2  ...  4  6  3  6  4  3  4  1         NaN\n",
              "40       14        2  2  3  4  3  3  3  3  ...  2  3  3  2  3  3  3  2         NaN\n",
              "41       14        3  7  4  6  4  6  3  7  ...  4  4  4  4  4  4  4  7         NaN\n",
              "42       15        1  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "43       15        2  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "44       15        3  X  X  X  X  X  X  X  ...  X  X  X  X  X  X  X  X         NaN\n",
              "\n",
              "[45 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vms5i95y3_z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "739c439a-9706-4173-9537-4adb5528c698"
      },
      "source": [
        "for i in range(15, 16): # Итерација низ секој испитен примерок\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_data = np.empty(0)\n",
        "  temp_labels = np.empty(0)\n",
        "  temp_events = np.empty(0)\n",
        "  temp_targets = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainData.mat\"\n",
        "    temp = loadmat(full_path)['trainData']\n",
        "    if temp_data.size != 0:\n",
        "      temp_data = np.concatenate((temp_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на label-ите\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainLabels.txt\"\n",
        "    with open(full_path, \"r\") as file_labels:\n",
        "      temp = file_labels.read().splitlines()\n",
        "      temp = np.repeat(temp, 8*10)\n",
        "      if temp_labels.size != 0:\n",
        "        temp_labels = np.concatenate((temp_labels, temp))\n",
        "      else:\n",
        "        temp_labels = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_events.size != 0:\n",
        "        temp_events = np.append(temp_events, temp)\n",
        "      else:\n",
        "        temp_events = np.array(temp)\n",
        "      \n",
        "\n",
        "    # Вчитување на редоследот на објекти кои се target\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/Train/trainTargets.txt\"\n",
        "    with open(full_path, \"r\") as file_targets:\n",
        "      temp = file_targets.read().splitlines()\n",
        "      if temp_targets.size != 0:\n",
        "        temp_targets = np.concatenate((temp_targets, temp))\n",
        "      else:\n",
        "        temp_targets = np.array(temp)\n",
        "    print(\"\\t - Податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "\n",
        "  for j in range(4, 8): # Итерација низ секоја сесија\n",
        "    file_train_set = 'S' + format(j, '02') \n",
        "\n",
        "      \n",
        "  # Зачувај ги податоците вчитани од испитниот примерок во низа\n",
        "  data.append(temp_data)\n",
        "  labels.append(temp_labels)\n",
        "  events.append(temp_events)\n",
        "  targets.append(temp_targets)\n",
        "\n",
        "  \n",
        "  print(\"Податоците од испитниот примерок \" + str(i) + \" се вчитани.\")\n",
        "\n",
        "\n",
        "  #data = target_events_data_scaled\n",
        "  mne_array = np.swapaxes(data[i-1], 0, 2) # (епохa, канал, настан). \n",
        "  mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "  mne_array = mne_array.reshape(mne_array.shape[0],1, 8,350)\n",
        "  print(mne_array.shape)\n",
        "\n",
        "  events_arr = events[i-1].astype(np.int)\n",
        "  labels_arr = labels[i-1].astype(np.int)\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(mne_array, labels_arr-1, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "  model15 = DeepConvNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "  model15.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = Adam(0.0009))\n",
        "  checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                                verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "  #clf = RandomForestClassifier(max_depth=5)\n",
        "  #clf.fit(X_train, y_train)\n",
        "  #score = clf.score(X_test, y_test)\n",
        "  # print(score)\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  num_batch_size=100\n",
        "  num_epochs=400\n",
        "  model15.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "            validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "  score = model15.evaluate(X_test, y_test, verbose=1)\n",
        "  print(score)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t - Податоците од сесија 1 се вчитани.\n",
            "\t - Податоците од сесија 2 се вчитани.\n",
            "\t - Податоците од сесија 3 се вчитани.\n",
            "Податоците од испитниот примерок 15 се вчитани.\n",
            "(4800, 1, 8, 350)\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Version 10\n",
            "Train on 3600 samples, validate on 1200 samples\n",
            "Epoch 1/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.3338 - acc: 0.1532\n",
            "Epoch 00001: val_loss improved from inf to 2.31488, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 10s 3ms/sample - loss: 2.3257 - acc: 0.1550 - val_loss: 2.3149 - val_acc: 0.1467\n",
            "Epoch 2/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.2481 - acc: 0.1667\n",
            "Epoch 00002: val_loss improved from 2.31488 to 2.23936, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 2.2556 - acc: 0.1644 - val_loss: 2.2394 - val_acc: 0.1733\n",
            "Epoch 3/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.1828 - acc: 0.1891\n",
            "Epoch 00003: val_loss did not improve from 2.23936\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 2.1800 - acc: 0.1911 - val_loss: 2.3212 - val_acc: 0.1933\n",
            "Epoch 4/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.1810 - acc: 0.2049\n",
            "Epoch 00004: val_loss improved from 2.23936 to 2.22292, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 2.1842 - acc: 0.2028 - val_loss: 2.2229 - val_acc: 0.1867\n",
            "Epoch 5/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.0772 - acc: 0.2263\n",
            "Epoch 00005: val_loss did not improve from 2.22292\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 2.0769 - acc: 0.2264 - val_loss: 2.5235 - val_acc: 0.1767\n",
            "Epoch 6/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.0455 - acc: 0.2462\n",
            "Epoch 00006: val_loss did not improve from 2.22292\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 2.0532 - acc: 0.2436 - val_loss: 2.4286 - val_acc: 0.1850\n",
            "Epoch 7/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.9922 - acc: 0.2582\n",
            "Epoch 00007: val_loss did not improve from 2.22292\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 1.9858 - acc: 0.2628 - val_loss: 2.3102 - val_acc: 0.2050\n",
            "Epoch 8/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.8767 - acc: 0.2950\n",
            "Epoch 00008: val_loss did not improve from 2.22292\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 1.8751 - acc: 0.2950 - val_loss: 2.3268 - val_acc: 0.2100\n",
            "Epoch 9/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.7937 - acc: 0.3186\n",
            "Epoch 00009: val_loss did not improve from 2.22292\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 1.7907 - acc: 0.3186 - val_loss: 2.4180 - val_acc: 0.2133\n",
            "Epoch 10/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.7144 - acc: 0.3406\n",
            "Epoch 00010: val_loss improved from 2.22292 to 2.16775, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 1.7183 - acc: 0.3358 - val_loss: 2.1678 - val_acc: 0.2417\n",
            "Epoch 11/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.6501 - acc: 0.3535\n",
            "Epoch 00011: val_loss did not improve from 2.16775\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 1.6522 - acc: 0.3517 - val_loss: 2.2153 - val_acc: 0.2342\n",
            "Epoch 12/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.6071 - acc: 0.3703\n",
            "Epoch 00012: val_loss improved from 2.16775 to 2.04559, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 1.6117 - acc: 0.3697 - val_loss: 2.0456 - val_acc: 0.2675\n",
            "Epoch 13/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.5581 - acc: 0.4024\n",
            "Epoch 00013: val_loss improved from 2.04559 to 1.90630, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 1.5549 - acc: 0.4025 - val_loss: 1.9063 - val_acc: 0.2733\n",
            "Epoch 14/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.4936 - acc: 0.4162\n",
            "Epoch 00014: val_loss improved from 1.90630 to 1.84923, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 224us/sample - loss: 1.4937 - acc: 0.4156 - val_loss: 1.8492 - val_acc: 0.3192\n",
            "Epoch 15/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.4581 - acc: 0.4351\n",
            "Epoch 00015: val_loss did not improve from 1.84923\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 1.4572 - acc: 0.4367 - val_loss: 1.9089 - val_acc: 0.2867\n",
            "Epoch 16/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.3988 - acc: 0.4574\n",
            "Epoch 00016: val_loss improved from 1.84923 to 1.80819, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 1.4011 - acc: 0.4564 - val_loss: 1.8082 - val_acc: 0.3250\n",
            "Epoch 17/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.3729 - acc: 0.4676\n",
            "Epoch 00017: val_loss did not improve from 1.80819\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 1.3765 - acc: 0.4681 - val_loss: 1.8335 - val_acc: 0.3642\n",
            "Epoch 18/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.3286 - acc: 0.4809\n",
            "Epoch 00018: val_loss improved from 1.80819 to 1.67530, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 1.3259 - acc: 0.4839 - val_loss: 1.6753 - val_acc: 0.3858\n",
            "Epoch 19/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2667 - acc: 0.5179\n",
            "Epoch 00019: val_loss did not improve from 1.67530\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 1.2704 - acc: 0.5153 - val_loss: 1.8615 - val_acc: 0.3425\n",
            "Epoch 20/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2603 - acc: 0.5176\n",
            "Epoch 00020: val_loss improved from 1.67530 to 1.55110, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 1.2622 - acc: 0.5161 - val_loss: 1.5511 - val_acc: 0.4067\n",
            "Epoch 21/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.2023 - acc: 0.5444\n",
            "Epoch 00021: val_loss improved from 1.55110 to 1.43681, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 1.1992 - acc: 0.5458 - val_loss: 1.4368 - val_acc: 0.4517\n",
            "Epoch 22/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.1504 - acc: 0.5657\n",
            "Epoch 00022: val_loss did not improve from 1.43681\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 1.1489 - acc: 0.5678 - val_loss: 1.4925 - val_acc: 0.4283\n",
            "Epoch 23/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.1155 - acc: 0.5844\n",
            "Epoch 00023: val_loss improved from 1.43681 to 1.32698, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 1.1182 - acc: 0.5811 - val_loss: 1.3270 - val_acc: 0.4950\n",
            "Epoch 24/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.0572 - acc: 0.6083\n",
            "Epoch 00024: val_loss improved from 1.32698 to 1.32196, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 1.0567 - acc: 0.6086 - val_loss: 1.3220 - val_acc: 0.5000\n",
            "Epoch 25/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 1.0685 - acc: 0.5894\n",
            "Epoch 00025: val_loss did not improve from 1.32196\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 1.0658 - acc: 0.5922 - val_loss: 1.4070 - val_acc: 0.4500\n",
            "Epoch 26/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 1.0015 - acc: 0.6269\n",
            "Epoch 00026: val_loss improved from 1.32196 to 1.27737, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 1.0045 - acc: 0.6256 - val_loss: 1.2774 - val_acc: 0.5242\n",
            "Epoch 27/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.9764 - acc: 0.6474\n",
            "Epoch 00027: val_loss improved from 1.27737 to 1.18995, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.9801 - acc: 0.6458 - val_loss: 1.1899 - val_acc: 0.5392\n",
            "Epoch 28/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9439 - acc: 0.6582\n",
            "Epoch 00028: val_loss did not improve from 1.18995\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.9358 - acc: 0.6608 - val_loss: 1.2626 - val_acc: 0.5267\n",
            "Epoch 29/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8840 - acc: 0.6756\n",
            "Epoch 00029: val_loss improved from 1.18995 to 1.14829, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.8971 - acc: 0.6700 - val_loss: 1.1483 - val_acc: 0.5758\n",
            "Epoch 30/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.9076 - acc: 0.6621\n",
            "Epoch 00030: val_loss improved from 1.14829 to 1.12636, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.9071 - acc: 0.6628 - val_loss: 1.1264 - val_acc: 0.5792\n",
            "Epoch 31/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.8582 - acc: 0.6906\n",
            "Epoch 00031: val_loss did not improve from 1.12636\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.8571 - acc: 0.6917 - val_loss: 1.2396 - val_acc: 0.5342\n",
            "Epoch 32/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.8218 - acc: 0.7076\n",
            "Epoch 00032: val_loss improved from 1.12636 to 1.11097, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.8225 - acc: 0.7069 - val_loss: 1.1110 - val_acc: 0.5858\n",
            "Epoch 33/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7546 - acc: 0.7366\n",
            "Epoch 00033: val_loss improved from 1.11097 to 1.04375, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.7540 - acc: 0.7369 - val_loss: 1.0437 - val_acc: 0.6108\n",
            "Epoch 34/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7881 - acc: 0.7109\n",
            "Epoch 00034: val_loss improved from 1.04375 to 0.95888, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.7851 - acc: 0.7139 - val_loss: 0.9589 - val_acc: 0.6425\n",
            "Epoch 35/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7512 - acc: 0.7366\n",
            "Epoch 00035: val_loss did not improve from 0.95888\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.7510 - acc: 0.7356 - val_loss: 1.0308 - val_acc: 0.6158\n",
            "Epoch 36/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.7199 - acc: 0.7488\n",
            "Epoch 00036: val_loss did not improve from 0.95888\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.7200 - acc: 0.7494 - val_loss: 1.2204 - val_acc: 0.5550\n",
            "Epoch 37/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.6999 - acc: 0.7582\n",
            "Epoch 00037: val_loss did not improve from 0.95888\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.7102 - acc: 0.7550 - val_loss: 1.0106 - val_acc: 0.6183\n",
            "Epoch 38/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6818 - acc: 0.7562\n",
            "Epoch 00038: val_loss did not improve from 0.95888\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.6815 - acc: 0.7567 - val_loss: 0.9653 - val_acc: 0.6517\n",
            "Epoch 39/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6758 - acc: 0.7624\n",
            "Epoch 00039: val_loss improved from 0.95888 to 0.84744, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.6737 - acc: 0.7642 - val_loss: 0.8474 - val_acc: 0.6967\n",
            "Epoch 40/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6210 - acc: 0.7891\n",
            "Epoch 00040: val_loss did not improve from 0.84744\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.6212 - acc: 0.7883 - val_loss: 0.8528 - val_acc: 0.6750\n",
            "Epoch 41/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6068 - acc: 0.7897\n",
            "Epoch 00041: val_loss improved from 0.84744 to 0.73811, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.6139 - acc: 0.7869 - val_loss: 0.7381 - val_acc: 0.7392\n",
            "Epoch 42/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.6063 - acc: 0.7926\n",
            "Epoch 00042: val_loss did not improve from 0.73811\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.6042 - acc: 0.7939 - val_loss: 0.8150 - val_acc: 0.7000\n",
            "Epoch 43/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5774 - acc: 0.8026\n",
            "Epoch 00043: val_loss improved from 0.73811 to 0.68333, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.5796 - acc: 0.8017 - val_loss: 0.6833 - val_acc: 0.7633\n",
            "Epoch 44/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.5539 - acc: 0.8121\n",
            "Epoch 00044: val_loss did not improve from 0.68333\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.5480 - acc: 0.8158 - val_loss: 0.7132 - val_acc: 0.7642\n",
            "Epoch 45/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5245 - acc: 0.8226\n",
            "Epoch 00045: val_loss did not improve from 0.68333\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.5235 - acc: 0.8236 - val_loss: 0.7040 - val_acc: 0.7375\n",
            "Epoch 46/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.5123 - acc: 0.8300\n",
            "Epoch 00046: val_loss improved from 0.68333 to 0.68020, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.5099 - acc: 0.8311 - val_loss: 0.6802 - val_acc: 0.7567\n",
            "Epoch 47/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.5101 - acc: 0.8324\n",
            "Epoch 00047: val_loss improved from 0.68020 to 0.65730, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.5099 - acc: 0.8317 - val_loss: 0.6573 - val_acc: 0.7708\n",
            "Epoch 48/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.4980 - acc: 0.8397\n",
            "Epoch 00048: val_loss did not improve from 0.65730\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.4982 - acc: 0.8389 - val_loss: 0.6931 - val_acc: 0.7442\n",
            "Epoch 49/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4704 - acc: 0.8462\n",
            "Epoch 00049: val_loss did not improve from 0.65730\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.4682 - acc: 0.8464 - val_loss: 0.6965 - val_acc: 0.7342\n",
            "Epoch 50/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4397 - acc: 0.8621\n",
            "Epoch 00050: val_loss improved from 0.65730 to 0.61684, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.4399 - acc: 0.8611 - val_loss: 0.6168 - val_acc: 0.7775\n",
            "Epoch 51/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4363 - acc: 0.8629\n",
            "Epoch 00051: val_loss improved from 0.61684 to 0.54261, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 0.4338 - acc: 0.8633 - val_loss: 0.5426 - val_acc: 0.8000\n",
            "Epoch 52/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4247 - acc: 0.8603\n",
            "Epoch 00052: val_loss did not improve from 0.54261\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.4247 - acc: 0.8611 - val_loss: 0.5624 - val_acc: 0.7958\n",
            "Epoch 53/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.4070 - acc: 0.8694\n",
            "Epoch 00053: val_loss did not improve from 0.54261\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.4079 - acc: 0.8689 - val_loss: 0.5485 - val_acc: 0.8000\n",
            "Epoch 54/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.3789 - acc: 0.8816\n",
            "Epoch 00054: val_loss did not improve from 0.54261\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.3837 - acc: 0.8797 - val_loss: 0.6685 - val_acc: 0.7400\n",
            "Epoch 55/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3799 - acc: 0.8817\n",
            "Epoch 00055: val_loss did not improve from 0.54261\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.3779 - acc: 0.8817 - val_loss: 0.5561 - val_acc: 0.7958\n",
            "Epoch 56/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.3803 - acc: 0.8797\n",
            "Epoch 00056: val_loss improved from 0.54261 to 0.48200, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.3802 - acc: 0.8811 - val_loss: 0.4820 - val_acc: 0.8292\n",
            "Epoch 57/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3656 - acc: 0.8835\n",
            "Epoch 00057: val_loss did not improve from 0.48200\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.3682 - acc: 0.8828 - val_loss: 0.5144 - val_acc: 0.8167\n",
            "Epoch 58/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.8923\n",
            "Epoch 00058: val_loss improved from 0.48200 to 0.43494, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.3544 - acc: 0.8892 - val_loss: 0.4349 - val_acc: 0.8517\n",
            "Epoch 59/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3407 - acc: 0.8897\n",
            "Epoch 00059: val_loss did not improve from 0.43494\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.3407 - acc: 0.8900 - val_loss: 0.4689 - val_acc: 0.8375\n",
            "Epoch 60/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3213 - acc: 0.9059\n",
            "Epoch 00060: val_loss improved from 0.43494 to 0.43424, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.3247 - acc: 0.9036 - val_loss: 0.4342 - val_acc: 0.8542\n",
            "Epoch 61/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.3115 - acc: 0.9106\n",
            "Epoch 00061: val_loss improved from 0.43424 to 0.38590, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.3114 - acc: 0.9103 - val_loss: 0.3859 - val_acc: 0.8792\n",
            "Epoch 62/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3234 - acc: 0.9000\n",
            "Epoch 00062: val_loss did not improve from 0.38590\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.3210 - acc: 0.9003 - val_loss: 0.4587 - val_acc: 0.8442\n",
            "Epoch 63/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3211 - acc: 0.9029\n",
            "Epoch 00063: val_loss did not improve from 0.38590\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.3212 - acc: 0.9025 - val_loss: 0.4071 - val_acc: 0.8717\n",
            "Epoch 64/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.3068 - acc: 0.9032\n",
            "Epoch 00064: val_loss did not improve from 0.38590\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.3070 - acc: 0.9033 - val_loss: 0.4226 - val_acc: 0.8608\n",
            "Epoch 65/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2907 - acc: 0.9156\n",
            "Epoch 00065: val_loss improved from 0.38590 to 0.37570, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 0.2943 - acc: 0.9139 - val_loss: 0.3757 - val_acc: 0.8733\n",
            "Epoch 66/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2780 - acc: 0.9147\n",
            "Epoch 00066: val_loss did not improve from 0.37570\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.2849 - acc: 0.9114 - val_loss: 0.3999 - val_acc: 0.8617\n",
            "Epoch 67/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2803 - acc: 0.9191\n",
            "Epoch 00067: val_loss improved from 0.37570 to 0.33486, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.2837 - acc: 0.9186 - val_loss: 0.3349 - val_acc: 0.8942\n",
            "Epoch 68/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2542 - acc: 0.9282\n",
            "Epoch 00068: val_loss did not improve from 0.33486\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.2593 - acc: 0.9256 - val_loss: 0.3432 - val_acc: 0.8883\n",
            "Epoch 69/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2810 - acc: 0.9194\n",
            "Epoch 00069: val_loss improved from 0.33486 to 0.32765, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.2792 - acc: 0.9194 - val_loss: 0.3277 - val_acc: 0.8867\n",
            "Epoch 70/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2516 - acc: 0.9271\n",
            "Epoch 00070: val_loss did not improve from 0.32765\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.2525 - acc: 0.9272 - val_loss: 0.3285 - val_acc: 0.8958\n",
            "Epoch 71/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2426 - acc: 0.9331\n",
            "Epoch 00071: val_loss improved from 0.32765 to 0.32255, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 221us/sample - loss: 0.2434 - acc: 0.9322 - val_loss: 0.3226 - val_acc: 0.8917\n",
            "Epoch 72/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2593 - acc: 0.9200\n",
            "Epoch 00072: val_loss improved from 0.32255 to 0.30236, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.2563 - acc: 0.9217 - val_loss: 0.3024 - val_acc: 0.9067\n",
            "Epoch 73/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2478 - acc: 0.9282\n",
            "Epoch 00073: val_loss did not improve from 0.30236\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.2467 - acc: 0.9297 - val_loss: 0.3055 - val_acc: 0.9075\n",
            "Epoch 74/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.9306\n",
            "Epoch 00074: val_loss did not improve from 0.30236\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.2341 - acc: 0.9314 - val_loss: 0.3046 - val_acc: 0.8983\n",
            "Epoch 75/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2404 - acc: 0.9318\n",
            "Epoch 00075: val_loss did not improve from 0.30236\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.2428 - acc: 0.9308 - val_loss: 0.3388 - val_acc: 0.8875\n",
            "Epoch 76/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 0.9383\n",
            "Epoch 00076: val_loss improved from 0.30236 to 0.30100, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.2224 - acc: 0.9378 - val_loss: 0.3010 - val_acc: 0.9050\n",
            "Epoch 77/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2266 - acc: 0.9365\n",
            "Epoch 00077: val_loss improved from 0.30100 to 0.28596, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.2245 - acc: 0.9364 - val_loss: 0.2860 - val_acc: 0.9183\n",
            "Epoch 78/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2385 - acc: 0.9318\n",
            "Epoch 00078: val_loss improved from 0.28596 to 0.27133, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.2378 - acc: 0.9319 - val_loss: 0.2713 - val_acc: 0.9217\n",
            "Epoch 79/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2065 - acc: 0.9415\n",
            "Epoch 00079: val_loss improved from 0.27133 to 0.27032, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.2094 - acc: 0.9400 - val_loss: 0.2703 - val_acc: 0.9208\n",
            "Epoch 80/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2133 - acc: 0.9376\n",
            "Epoch 00080: val_loss did not improve from 0.27032\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.2177 - acc: 0.9356 - val_loss: 0.2742 - val_acc: 0.9133\n",
            "Epoch 81/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.2113 - acc: 0.9415\n",
            "Epoch 00081: val_loss improved from 0.27032 to 0.26091, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.2109 - acc: 0.9419 - val_loss: 0.2609 - val_acc: 0.9200\n",
            "Epoch 82/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.2042 - acc: 0.9403\n",
            "Epoch 00082: val_loss did not improve from 0.26091\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.2037 - acc: 0.9411 - val_loss: 0.2834 - val_acc: 0.9150\n",
            "Epoch 83/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1920 - acc: 0.9515\n",
            "Epoch 00083: val_loss improved from 0.26091 to 0.25289, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.1934 - acc: 0.9497 - val_loss: 0.2529 - val_acc: 0.9267\n",
            "Epoch 84/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1876 - acc: 0.9458\n",
            "Epoch 00084: val_loss did not improve from 0.25289\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1866 - acc: 0.9458 - val_loss: 0.2717 - val_acc: 0.9217\n",
            "Epoch 85/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1923 - acc: 0.9482\n",
            "Epoch 00085: val_loss did not improve from 0.25289\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1929 - acc: 0.9475 - val_loss: 0.3266 - val_acc: 0.8875\n",
            "Epoch 86/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1779 - acc: 0.9529\n",
            "Epoch 00086: val_loss improved from 0.25289 to 0.22427, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.1790 - acc: 0.9517 - val_loss: 0.2243 - val_acc: 0.9375\n",
            "Epoch 87/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1754 - acc: 0.9485\n",
            "Epoch 00087: val_loss did not improve from 0.22427\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1777 - acc: 0.9483 - val_loss: 0.2584 - val_acc: 0.9208\n",
            "Epoch 88/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1874 - acc: 0.9453\n",
            "Epoch 00088: val_loss improved from 0.22427 to 0.22355, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.1891 - acc: 0.9436 - val_loss: 0.2236 - val_acc: 0.9408\n",
            "Epoch 89/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1799 - acc: 0.9471\n",
            "Epoch 00089: val_loss improved from 0.22355 to 0.22082, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.1842 - acc: 0.9456 - val_loss: 0.2208 - val_acc: 0.9408\n",
            "Epoch 90/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1853 - acc: 0.9479\n",
            "Epoch 00090: val_loss did not improve from 0.22082\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1870 - acc: 0.9481 - val_loss: 0.2329 - val_acc: 0.9325\n",
            "Epoch 91/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1773 - acc: 0.9479\n",
            "Epoch 00091: val_loss did not improve from 0.22082\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1791 - acc: 0.9475 - val_loss: 0.2225 - val_acc: 0.9392\n",
            "Epoch 92/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1817 - acc: 0.9509\n",
            "Epoch 00092: val_loss did not improve from 0.22082\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1797 - acc: 0.9511 - val_loss: 0.2622 - val_acc: 0.9233\n",
            "Epoch 93/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9526\n",
            "Epoch 00093: val_loss did not improve from 0.22082\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1654 - acc: 0.9531 - val_loss: 0.2687 - val_acc: 0.9225\n",
            "Epoch 94/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1657 - acc: 0.9547\n",
            "Epoch 00094: val_loss improved from 0.22082 to 0.19485, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 0.1647 - acc: 0.9553 - val_loss: 0.1948 - val_acc: 0.9425\n",
            "Epoch 95/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1713 - acc: 0.9491\n",
            "Epoch 00095: val_loss did not improve from 0.19485\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1719 - acc: 0.9494 - val_loss: 0.2581 - val_acc: 0.9258\n",
            "Epoch 96/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1588 - acc: 0.9600\n",
            "Epoch 00096: val_loss did not improve from 0.19485\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1592 - acc: 0.9603 - val_loss: 0.2272 - val_acc: 0.9283\n",
            "Epoch 97/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1681 - acc: 0.9482\n",
            "Epoch 00097: val_loss did not improve from 0.19485\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1715 - acc: 0.9444 - val_loss: 0.2052 - val_acc: 0.9433\n",
            "Epoch 98/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1751 - acc: 0.9482\n",
            "Epoch 00098: val_loss did not improve from 0.19485\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1771 - acc: 0.9472 - val_loss: 0.2536 - val_acc: 0.9192\n",
            "Epoch 99/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1678 - acc: 0.9547\n",
            "Epoch 00099: val_loss improved from 0.19485 to 0.18128, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 0.1678 - acc: 0.9547 - val_loss: 0.1813 - val_acc: 0.9492\n",
            "Epoch 100/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1519 - acc: 0.9597\n",
            "Epoch 00100: val_loss did not improve from 0.18128\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1516 - acc: 0.9597 - val_loss: 0.1996 - val_acc: 0.9467\n",
            "Epoch 101/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1541 - acc: 0.9553\n",
            "Epoch 00101: val_loss did not improve from 0.18128\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1536 - acc: 0.9556 - val_loss: 0.2044 - val_acc: 0.9417\n",
            "Epoch 102/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1482 - acc: 0.9597\n",
            "Epoch 00102: val_loss did not improve from 0.18128\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1503 - acc: 0.9586 - val_loss: 0.2246 - val_acc: 0.9325\n",
            "Epoch 103/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1455 - acc: 0.9638\n",
            "Epoch 00103: val_loss did not improve from 0.18128\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1454 - acc: 0.9642 - val_loss: 0.1911 - val_acc: 0.9442\n",
            "Epoch 104/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1521 - acc: 0.9530\n",
            "Epoch 00104: val_loss improved from 0.18128 to 0.17366, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 212us/sample - loss: 0.1507 - acc: 0.9531 - val_loss: 0.1737 - val_acc: 0.9583\n",
            "Epoch 105/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1500 - acc: 0.9562\n",
            "Epoch 00105: val_loss improved from 0.17366 to 0.16667, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.1549 - acc: 0.9542 - val_loss: 0.1667 - val_acc: 0.9600\n",
            "Epoch 106/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9606\n",
            "Epoch 00106: val_loss improved from 0.16667 to 0.16281, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.1441 - acc: 0.9608 - val_loss: 0.1628 - val_acc: 0.9608\n",
            "Epoch 107/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1424 - acc: 0.9618\n",
            "Epoch 00107: val_loss did not improve from 0.16281\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1438 - acc: 0.9611 - val_loss: 0.1905 - val_acc: 0.9517\n",
            "Epoch 108/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1510 - acc: 0.9569\n",
            "Epoch 00108: val_loss did not improve from 0.16281\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1506 - acc: 0.9572 - val_loss: 0.1642 - val_acc: 0.9583\n",
            "Epoch 109/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.9577\n",
            "Epoch 00109: val_loss did not improve from 0.16281\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1476 - acc: 0.9586 - val_loss: 0.1756 - val_acc: 0.9450\n",
            "Epoch 110/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1326 - acc: 0.9656\n",
            "Epoch 00110: val_loss did not improve from 0.16281\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1340 - acc: 0.9647 - val_loss: 0.1660 - val_acc: 0.9550\n",
            "Epoch 111/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1491 - acc: 0.9568\n",
            "Epoch 00111: val_loss did not improve from 0.16281\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1505 - acc: 0.9558 - val_loss: 0.1963 - val_acc: 0.9358\n",
            "Epoch 112/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1481 - acc: 0.9600\n",
            "Epoch 00112: val_loss improved from 0.16281 to 0.16011, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.1461 - acc: 0.9608 - val_loss: 0.1601 - val_acc: 0.9608\n",
            "Epoch 113/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1405 - acc: 0.9624\n",
            "Epoch 00113: val_loss did not improve from 0.16011\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1415 - acc: 0.9628 - val_loss: 0.1813 - val_acc: 0.9525\n",
            "Epoch 114/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1232 - acc: 0.9721\n",
            "Epoch 00114: val_loss improved from 0.16011 to 0.15495, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.1224 - acc: 0.9725 - val_loss: 0.1550 - val_acc: 0.9575\n",
            "Epoch 115/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1364 - acc: 0.9629\n",
            "Epoch 00115: val_loss did not improve from 0.15495\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1372 - acc: 0.9628 - val_loss: 0.1923 - val_acc: 0.9417\n",
            "Epoch 116/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1373 - acc: 0.9624\n",
            "Epoch 00116: val_loss improved from 0.15495 to 0.15295, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.1377 - acc: 0.9631 - val_loss: 0.1529 - val_acc: 0.9608\n",
            "Epoch 117/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1361 - acc: 0.9585\n",
            "Epoch 00117: val_loss did not improve from 0.15295\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1366 - acc: 0.9589 - val_loss: 0.2337 - val_acc: 0.9217\n",
            "Epoch 118/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1262 - acc: 0.9679\n",
            "Epoch 00118: val_loss did not improve from 0.15295\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1271 - acc: 0.9678 - val_loss: 0.1822 - val_acc: 0.9467\n",
            "Epoch 119/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1263 - acc: 0.9644\n",
            "Epoch 00119: val_loss did not improve from 0.15295\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1277 - acc: 0.9636 - val_loss: 0.1672 - val_acc: 0.9500\n",
            "Epoch 120/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1265 - acc: 0.9671\n",
            "Epoch 00120: val_loss improved from 0.15295 to 0.14292, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 216us/sample - loss: 0.1268 - acc: 0.9669 - val_loss: 0.1429 - val_acc: 0.9600\n",
            "Epoch 121/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1389 - acc: 0.9582\n",
            "Epoch 00121: val_loss did not improve from 0.14292\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1408 - acc: 0.9575 - val_loss: 0.2037 - val_acc: 0.9433\n",
            "Epoch 122/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1310 - acc: 0.9647\n",
            "Epoch 00122: val_loss improved from 0.14292 to 0.13419, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.1314 - acc: 0.9636 - val_loss: 0.1342 - val_acc: 0.9667\n",
            "Epoch 123/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1431 - acc: 0.9588\n",
            "Epoch 00123: val_loss did not improve from 0.13419\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1447 - acc: 0.9581 - val_loss: 0.1889 - val_acc: 0.9450\n",
            "Epoch 124/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1130 - acc: 0.9724\n",
            "Epoch 00124: val_loss did not improve from 0.13419\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1140 - acc: 0.9714 - val_loss: 0.1472 - val_acc: 0.9617\n",
            "Epoch 125/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1183 - acc: 0.9674\n",
            "Epoch 00125: val_loss did not improve from 0.13419\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1177 - acc: 0.9672 - val_loss: 0.1367 - val_acc: 0.9608\n",
            "Epoch 126/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1363 - acc: 0.9624\n",
            "Epoch 00126: val_loss did not improve from 0.13419\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1366 - acc: 0.9614 - val_loss: 0.1641 - val_acc: 0.9542\n",
            "Epoch 127/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1306 - acc: 0.9647\n",
            "Epoch 00127: val_loss did not improve from 0.13419\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1300 - acc: 0.9650 - val_loss: 0.1882 - val_acc: 0.9358\n",
            "Epoch 128/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1250 - acc: 0.9679\n",
            "Epoch 00128: val_loss improved from 0.13419 to 0.13111, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.1249 - acc: 0.9678 - val_loss: 0.1311 - val_acc: 0.9642\n",
            "Epoch 129/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9657\n",
            "Epoch 00129: val_loss improved from 0.13111 to 0.13079, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.1215 - acc: 0.9664 - val_loss: 0.1308 - val_acc: 0.9675\n",
            "Epoch 130/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1134 - acc: 0.9709\n",
            "Epoch 00130: val_loss did not improve from 0.13079\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1133 - acc: 0.9703 - val_loss: 0.1345 - val_acc: 0.9700\n",
            "Epoch 131/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1242 - acc: 0.9685\n",
            "Epoch 00131: val_loss did not improve from 0.13079\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1233 - acc: 0.9683 - val_loss: 0.1526 - val_acc: 0.9592\n",
            "Epoch 132/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1170 - acc: 0.9688\n",
            "Epoch 00132: val_loss did not improve from 0.13079\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1186 - acc: 0.9683 - val_loss: 0.1330 - val_acc: 0.9642\n",
            "Epoch 133/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9657\n",
            "Epoch 00133: val_loss did not improve from 0.13079\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1209 - acc: 0.9656 - val_loss: 0.1395 - val_acc: 0.9667\n",
            "Epoch 134/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1111 - acc: 0.9697\n",
            "Epoch 00134: val_loss did not improve from 0.13079\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1107 - acc: 0.9706 - val_loss: 0.1422 - val_acc: 0.9625\n",
            "Epoch 135/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9683\n",
            "Epoch 00135: val_loss improved from 0.13079 to 0.12805, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.1163 - acc: 0.9686 - val_loss: 0.1280 - val_acc: 0.9633\n",
            "Epoch 136/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1152 - acc: 0.9694\n",
            "Epoch 00136: val_loss did not improve from 0.12805\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1148 - acc: 0.9697 - val_loss: 0.1667 - val_acc: 0.9417\n",
            "Epoch 137/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9669\n",
            "Epoch 00137: val_loss did not improve from 0.12805\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1162 - acc: 0.9672 - val_loss: 0.1344 - val_acc: 0.9692\n",
            "Epoch 138/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1252 - acc: 0.9644\n",
            "Epoch 00138: val_loss did not improve from 0.12805\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1261 - acc: 0.9642 - val_loss: 0.1519 - val_acc: 0.9617\n",
            "Epoch 139/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1290 - acc: 0.9642\n",
            "Epoch 00139: val_loss did not improve from 0.12805\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1276 - acc: 0.9639 - val_loss: 0.1427 - val_acc: 0.9567\n",
            "Epoch 140/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1101 - acc: 0.9733\n",
            "Epoch 00140: val_loss did not improve from 0.12805\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1138 - acc: 0.9714 - val_loss: 0.1348 - val_acc: 0.9675\n",
            "Epoch 141/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1262 - acc: 0.9653\n",
            "Epoch 00141: val_loss did not improve from 0.12805\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1241 - acc: 0.9661 - val_loss: 0.1490 - val_acc: 0.9583\n",
            "Epoch 142/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9594\n",
            "Epoch 00142: val_loss did not improve from 0.12805\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1388 - acc: 0.9583 - val_loss: 0.1411 - val_acc: 0.9625\n",
            "Epoch 143/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9549\n",
            "Epoch 00143: val_loss did not improve from 0.12805\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1473 - acc: 0.9547 - val_loss: 0.1518 - val_acc: 0.9550\n",
            "Epoch 144/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1281 - acc: 0.9585\n",
            "Epoch 00144: val_loss did not improve from 0.12805\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1303 - acc: 0.9581 - val_loss: 0.1349 - val_acc: 0.9592\n",
            "Epoch 145/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9611\n",
            "Epoch 00145: val_loss did not improve from 0.12805\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1335 - acc: 0.9619 - val_loss: 0.1434 - val_acc: 0.9608\n",
            "Epoch 146/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1256 - acc: 0.9648\n",
            "Epoch 00146: val_loss did not improve from 0.12805\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1221 - acc: 0.9661 - val_loss: 0.1444 - val_acc: 0.9575\n",
            "Epoch 147/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1228 - acc: 0.9659\n",
            "Epoch 00147: val_loss did not improve from 0.12805\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1229 - acc: 0.9653 - val_loss: 0.1781 - val_acc: 0.9383\n",
            "Epoch 148/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.1231 - acc: 0.9678\n",
            "Epoch 00148: val_loss improved from 0.12805 to 0.11890, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.1278 - acc: 0.9661 - val_loss: 0.1189 - val_acc: 0.9733\n",
            "Epoch 149/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1018 - acc: 0.9726\n",
            "Epoch 00149: val_loss did not improve from 0.11890\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1037 - acc: 0.9725 - val_loss: 0.1685 - val_acc: 0.9500\n",
            "Epoch 150/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1072 - acc: 0.9688\n",
            "Epoch 00150: val_loss did not improve from 0.11890\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1094 - acc: 0.9678 - val_loss: 0.1415 - val_acc: 0.9592\n",
            "Epoch 151/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1189 - acc: 0.9682\n",
            "Epoch 00151: val_loss did not improve from 0.11890\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1186 - acc: 0.9689 - val_loss: 0.1313 - val_acc: 0.9600\n",
            "Epoch 152/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1152 - acc: 0.9676\n",
            "Epoch 00152: val_loss did not improve from 0.11890\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1182 - acc: 0.9661 - val_loss: 0.1319 - val_acc: 0.9675\n",
            "Epoch 153/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1070 - acc: 0.9700\n",
            "Epoch 00153: val_loss did not improve from 0.11890\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1053 - acc: 0.9708 - val_loss: 0.1337 - val_acc: 0.9608\n",
            "Epoch 154/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9726\n",
            "Epoch 00154: val_loss did not improve from 0.11890\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1048 - acc: 0.9733 - val_loss: 0.1378 - val_acc: 0.9592\n",
            "Epoch 155/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1145 - acc: 0.9667\n",
            "Epoch 00155: val_loss did not improve from 0.11890\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1117 - acc: 0.9675 - val_loss: 0.1217 - val_acc: 0.9683\n",
            "Epoch 156/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9743\n",
            "Epoch 00156: val_loss improved from 0.11890 to 0.10352, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.1016 - acc: 0.9742 - val_loss: 0.1035 - val_acc: 0.9775\n",
            "Epoch 157/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1155 - acc: 0.9655\n",
            "Epoch 00157: val_loss did not improve from 0.10352\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1153 - acc: 0.9661 - val_loss: 0.1190 - val_acc: 0.9692\n",
            "Epoch 158/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1055 - acc: 0.9724\n",
            "Epoch 00158: val_loss did not improve from 0.10352\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1050 - acc: 0.9722 - val_loss: 0.1201 - val_acc: 0.9683\n",
            "Epoch 159/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9646\n",
            "Epoch 00159: val_loss did not improve from 0.10352\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1137 - acc: 0.9639 - val_loss: 0.1421 - val_acc: 0.9575\n",
            "Epoch 160/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9691\n",
            "Epoch 00160: val_loss did not improve from 0.10352\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1109 - acc: 0.9692 - val_loss: 0.1178 - val_acc: 0.9658\n",
            "Epoch 161/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1103 - acc: 0.9676\n",
            "Epoch 00161: val_loss did not improve from 0.10352\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1093 - acc: 0.9681 - val_loss: 0.1082 - val_acc: 0.9683\n",
            "Epoch 162/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1027 - acc: 0.9715\n",
            "Epoch 00162: val_loss did not improve from 0.10352\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1045 - acc: 0.9706 - val_loss: 0.1322 - val_acc: 0.9617\n",
            "Epoch 163/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1005 - acc: 0.9712\n",
            "Epoch 00163: val_loss improved from 0.10352 to 0.10255, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.1012 - acc: 0.9711 - val_loss: 0.1026 - val_acc: 0.9708\n",
            "Epoch 164/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1092 - acc: 0.9694\n",
            "Epoch 00164: val_loss did not improve from 0.10255\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.1095 - acc: 0.9689 - val_loss: 0.1368 - val_acc: 0.9633\n",
            "Epoch 165/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1104 - acc: 0.9688\n",
            "Epoch 00165: val_loss did not improve from 0.10255\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1110 - acc: 0.9692 - val_loss: 0.1176 - val_acc: 0.9633\n",
            "Epoch 166/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1088 - acc: 0.9688\n",
            "Epoch 00166: val_loss did not improve from 0.10255\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1105 - acc: 0.9683 - val_loss: 0.1164 - val_acc: 0.9708\n",
            "Epoch 167/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.1079 - acc: 0.9712\n",
            "Epoch 00167: val_loss did not improve from 0.10255\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.1075 - acc: 0.9711 - val_loss: 0.1130 - val_acc: 0.9692\n",
            "Epoch 168/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1015 - acc: 0.9732\n",
            "Epoch 00168: val_loss did not improve from 0.10255\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.1021 - acc: 0.9733 - val_loss: 0.1347 - val_acc: 0.9583\n",
            "Epoch 169/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0948 - acc: 0.9729\n",
            "Epoch 00169: val_loss did not improve from 0.10255\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0966 - acc: 0.9722 - val_loss: 0.1217 - val_acc: 0.9633\n",
            "Epoch 170/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0966 - acc: 0.9759\n",
            "Epoch 00170: val_loss did not improve from 0.10255\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0965 - acc: 0.9764 - val_loss: 0.1167 - val_acc: 0.9650\n",
            "Epoch 171/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1022 - acc: 0.9732\n",
            "Epoch 00171: val_loss did not improve from 0.10255\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1031 - acc: 0.9722 - val_loss: 0.1082 - val_acc: 0.9725\n",
            "Epoch 172/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1022 - acc: 0.9718\n",
            "Epoch 00172: val_loss did not improve from 0.10255\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1023 - acc: 0.9719 - val_loss: 0.1406 - val_acc: 0.9558\n",
            "Epoch 173/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1146 - acc: 0.9656\n",
            "Epoch 00173: val_loss did not improve from 0.10255\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1152 - acc: 0.9650 - val_loss: 0.1442 - val_acc: 0.9558\n",
            "Epoch 174/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1074 - acc: 0.9676\n",
            "Epoch 00174: val_loss improved from 0.10255 to 0.10119, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 217us/sample - loss: 0.1073 - acc: 0.9681 - val_loss: 0.1012 - val_acc: 0.9800\n",
            "Epoch 175/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1125 - acc: 0.9694\n",
            "Epoch 00175: val_loss did not improve from 0.10119\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1116 - acc: 0.9697 - val_loss: 0.1047 - val_acc: 0.9733\n",
            "Epoch 176/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1053 - acc: 0.9674\n",
            "Epoch 00176: val_loss did not improve from 0.10119\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1055 - acc: 0.9675 - val_loss: 0.1328 - val_acc: 0.9625\n",
            "Epoch 177/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9700\n",
            "Epoch 00177: val_loss did not improve from 0.10119\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1031 - acc: 0.9703 - val_loss: 0.1026 - val_acc: 0.9742\n",
            "Epoch 178/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9751\n",
            "Epoch 00178: val_loss did not improve from 0.10119\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0982 - acc: 0.9753 - val_loss: 0.1099 - val_acc: 0.9683\n",
            "Epoch 179/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0989 - acc: 0.9691\n",
            "Epoch 00179: val_loss did not improve from 0.10119\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1026 - acc: 0.9681 - val_loss: 0.1368 - val_acc: 0.9600\n",
            "Epoch 180/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9786\n",
            "Epoch 00180: val_loss improved from 0.10119 to 0.09381, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.0932 - acc: 0.9789 - val_loss: 0.0938 - val_acc: 0.9800\n",
            "Epoch 181/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0875 - acc: 0.9771\n",
            "Epoch 00181: val_loss did not improve from 0.09381\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0884 - acc: 0.9764 - val_loss: 0.1006 - val_acc: 0.9708\n",
            "Epoch 182/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0878 - acc: 0.9785\n",
            "Epoch 00182: val_loss did not improve from 0.09381\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0896 - acc: 0.9767 - val_loss: 0.1100 - val_acc: 0.9683\n",
            "Epoch 183/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1000 - acc: 0.9706\n",
            "Epoch 00183: val_loss did not improve from 0.09381\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0996 - acc: 0.9708 - val_loss: 0.1082 - val_acc: 0.9667\n",
            "Epoch 184/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0934 - acc: 0.9768\n",
            "Epoch 00184: val_loss did not improve from 0.09381\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 0.0945 - acc: 0.9764 - val_loss: 0.1227 - val_acc: 0.9667\n",
            "Epoch 185/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1026 - acc: 0.9700\n",
            "Epoch 00185: val_loss did not improve from 0.09381\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1018 - acc: 0.9708 - val_loss: 0.1058 - val_acc: 0.9767\n",
            "Epoch 186/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9740\n",
            "Epoch 00186: val_loss did not improve from 0.09381\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1009 - acc: 0.9742 - val_loss: 0.1087 - val_acc: 0.9683\n",
            "Epoch 187/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0870 - acc: 0.9766\n",
            "Epoch 00187: val_loss did not improve from 0.09381\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0858 - acc: 0.9778 - val_loss: 0.1239 - val_acc: 0.9658\n",
            "Epoch 188/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0922 - acc: 0.9703\n",
            "Epoch 00188: val_loss improved from 0.09381 to 0.09128, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.0913 - acc: 0.9708 - val_loss: 0.0913 - val_acc: 0.9817\n",
            "Epoch 189/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1089 - acc: 0.9682\n",
            "Epoch 00189: val_loss did not improve from 0.09128\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.1084 - acc: 0.9678 - val_loss: 0.1158 - val_acc: 0.9650\n",
            "Epoch 190/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1031 - acc: 0.9721\n",
            "Epoch 00190: val_loss improved from 0.09128 to 0.09097, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.1036 - acc: 0.9717 - val_loss: 0.0910 - val_acc: 0.9750\n",
            "Epoch 191/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9640\n",
            "Epoch 00191: val_loss did not improve from 0.09097\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1179 - acc: 0.9642 - val_loss: 0.1330 - val_acc: 0.9633\n",
            "Epoch 192/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0928 - acc: 0.9744\n",
            "Epoch 00192: val_loss did not improve from 0.09097\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0934 - acc: 0.9747 - val_loss: 0.1129 - val_acc: 0.9667\n",
            "Epoch 193/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0912 - acc: 0.9765\n",
            "Epoch 00193: val_loss did not improve from 0.09097\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.0920 - acc: 0.9753 - val_loss: 0.1012 - val_acc: 0.9758\n",
            "Epoch 194/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0938 - acc: 0.9724\n",
            "Epoch 00194: val_loss did not improve from 0.09097\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0930 - acc: 0.9733 - val_loss: 0.1515 - val_acc: 0.9442\n",
            "Epoch 195/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0955 - acc: 0.9735\n",
            "Epoch 00195: val_loss did not improve from 0.09097\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.0979 - acc: 0.9722 - val_loss: 0.0911 - val_acc: 0.9750\n",
            "Epoch 196/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0848 - acc: 0.9782\n",
            "Epoch 00196: val_loss did not improve from 0.09097\n",
            "3600/3600 [==============================] - 1s 208us/sample - loss: 0.0851 - acc: 0.9778 - val_loss: 0.0999 - val_acc: 0.9758\n",
            "Epoch 197/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0800 - acc: 0.9809\n",
            "Epoch 00197: val_loss did not improve from 0.09097\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.0817 - acc: 0.9800 - val_loss: 0.1005 - val_acc: 0.9717\n",
            "Epoch 198/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1021 - acc: 0.9691\n",
            "Epoch 00198: val_loss did not improve from 0.09097\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.1029 - acc: 0.9692 - val_loss: 0.1302 - val_acc: 0.9633\n",
            "Epoch 199/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0939 - acc: 0.9732\n",
            "Epoch 00199: val_loss did not improve from 0.09097\n",
            "3600/3600 [==============================] - 1s 207us/sample - loss: 0.0959 - acc: 0.9722 - val_loss: 0.1121 - val_acc: 0.9650\n",
            "Epoch 200/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0898 - acc: 0.9762\n",
            "Epoch 00200: val_loss did not improve from 0.09097\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.0895 - acc: 0.9764 - val_loss: 0.1200 - val_acc: 0.9600\n",
            "Epoch 201/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9666\n",
            "Epoch 00201: val_loss did not improve from 0.09097\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.1086 - acc: 0.9669 - val_loss: 0.1266 - val_acc: 0.9642\n",
            "Epoch 202/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0982 - acc: 0.9706\n",
            "Epoch 00202: val_loss did not improve from 0.09097\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0966 - acc: 0.9714 - val_loss: 0.1148 - val_acc: 0.9725\n",
            "Epoch 203/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1006 - acc: 0.9729\n",
            "Epoch 00203: val_loss improved from 0.09097 to 0.08777, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 218us/sample - loss: 0.1004 - acc: 0.9725 - val_loss: 0.0878 - val_acc: 0.9775\n",
            "Epoch 204/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0906 - acc: 0.9750\n",
            "Epoch 00204: val_loss improved from 0.08777 to 0.07799, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 222us/sample - loss: 0.0888 - acc: 0.9758 - val_loss: 0.0780 - val_acc: 0.9800\n",
            "Epoch 205/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1019 - acc: 0.9691\n",
            "Epoch 00205: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1010 - acc: 0.9694 - val_loss: 0.0934 - val_acc: 0.9758\n",
            "Epoch 206/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1059 - acc: 0.9674\n",
            "Epoch 00206: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.1030 - acc: 0.9683 - val_loss: 0.1202 - val_acc: 0.9575\n",
            "Epoch 207/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1147 - acc: 0.9676\n",
            "Epoch 00207: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1149 - acc: 0.9669 - val_loss: 0.1156 - val_acc: 0.9642\n",
            "Epoch 208/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0956 - acc: 0.9738\n",
            "Epoch 00208: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0945 - acc: 0.9747 - val_loss: 0.0997 - val_acc: 0.9717\n",
            "Epoch 209/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0786 - acc: 0.9791\n",
            "Epoch 00209: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0807 - acc: 0.9783 - val_loss: 0.0828 - val_acc: 0.9775\n",
            "Epoch 210/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0986 - acc: 0.9697\n",
            "Epoch 00210: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.1022 - acc: 0.9689 - val_loss: 0.1062 - val_acc: 0.9733\n",
            "Epoch 211/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9726\n",
            "Epoch 00211: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0962 - acc: 0.9731 - val_loss: 0.0948 - val_acc: 0.9742\n",
            "Epoch 212/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0860 - acc: 0.9779\n",
            "Epoch 00212: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0869 - acc: 0.9775 - val_loss: 0.1126 - val_acc: 0.9750\n",
            "Epoch 213/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.9783\n",
            "Epoch 00213: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0851 - acc: 0.9775 - val_loss: 0.1223 - val_acc: 0.9642\n",
            "Epoch 214/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9689\n",
            "Epoch 00214: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0961 - acc: 0.9694 - val_loss: 0.1156 - val_acc: 0.9683\n",
            "Epoch 215/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1129 - acc: 0.9638\n",
            "Epoch 00215: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.1153 - acc: 0.9636 - val_loss: 0.1041 - val_acc: 0.9683\n",
            "Epoch 216/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9743\n",
            "Epoch 00216: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0901 - acc: 0.9742 - val_loss: 0.0852 - val_acc: 0.9792\n",
            "Epoch 217/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9771\n",
            "Epoch 00217: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0806 - acc: 0.9767 - val_loss: 0.1060 - val_acc: 0.9708\n",
            "Epoch 218/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9746\n",
            "Epoch 00218: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0850 - acc: 0.9750 - val_loss: 0.1014 - val_acc: 0.9750\n",
            "Epoch 219/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0868 - acc: 0.9779\n",
            "Epoch 00219: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0874 - acc: 0.9769 - val_loss: 0.0916 - val_acc: 0.9758\n",
            "Epoch 220/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0951 - acc: 0.9732\n",
            "Epoch 00220: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0956 - acc: 0.9728 - val_loss: 0.1135 - val_acc: 0.9650\n",
            "Epoch 221/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1080 - acc: 0.9659\n",
            "Epoch 00221: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1074 - acc: 0.9658 - val_loss: 0.1148 - val_acc: 0.9650\n",
            "Epoch 222/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0827 - acc: 0.9762\n",
            "Epoch 00222: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0835 - acc: 0.9758 - val_loss: 0.0908 - val_acc: 0.9750\n",
            "Epoch 223/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9746\n",
            "Epoch 00223: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0866 - acc: 0.9747 - val_loss: 0.1013 - val_acc: 0.9750\n",
            "Epoch 224/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0906 - acc: 0.9741\n",
            "Epoch 00224: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0917 - acc: 0.9739 - val_loss: 0.1013 - val_acc: 0.9733\n",
            "Epoch 225/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0942 - acc: 0.9709\n",
            "Epoch 00225: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0941 - acc: 0.9711 - val_loss: 0.0823 - val_acc: 0.9800\n",
            "Epoch 226/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0855 - acc: 0.9794\n",
            "Epoch 00226: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0899 - acc: 0.9775 - val_loss: 0.1122 - val_acc: 0.9617\n",
            "Epoch 227/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0861 - acc: 0.9762\n",
            "Epoch 00227: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0885 - acc: 0.9758 - val_loss: 0.1015 - val_acc: 0.9750\n",
            "Epoch 228/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0786 - acc: 0.9774\n",
            "Epoch 00228: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0792 - acc: 0.9772 - val_loss: 0.0998 - val_acc: 0.9675\n",
            "Epoch 229/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0809 - acc: 0.9779\n",
            "Epoch 00229: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0847 - acc: 0.9764 - val_loss: 0.1188 - val_acc: 0.9658\n",
            "Epoch 230/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9760\n",
            "Epoch 00230: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0835 - acc: 0.9761 - val_loss: 0.0986 - val_acc: 0.9717\n",
            "Epoch 231/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9694\n",
            "Epoch 00231: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0952 - acc: 0.9694 - val_loss: 0.0937 - val_acc: 0.9758\n",
            "Epoch 232/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1027 - acc: 0.9685\n",
            "Epoch 00232: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.1014 - acc: 0.9686 - val_loss: 0.0905 - val_acc: 0.9783\n",
            "Epoch 233/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0891 - acc: 0.9770\n",
            "Epoch 00233: val_loss did not improve from 0.07799\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0895 - acc: 0.9769 - val_loss: 0.1004 - val_acc: 0.9750\n",
            "Epoch 234/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0859 - acc: 0.9752\n",
            "Epoch 00234: val_loss improved from 0.07799 to 0.07718, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.0862 - acc: 0.9750 - val_loss: 0.0772 - val_acc: 0.9800\n",
            "Epoch 235/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0845 - acc: 0.9779\n",
            "Epoch 00235: val_loss did not improve from 0.07718\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0834 - acc: 0.9783 - val_loss: 0.0847 - val_acc: 0.9783\n",
            "Epoch 236/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0762 - acc: 0.9800\n",
            "Epoch 00236: val_loss did not improve from 0.07718\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0769 - acc: 0.9797 - val_loss: 0.1112 - val_acc: 0.9650\n",
            "Epoch 237/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0808 - acc: 0.9774\n",
            "Epoch 00237: val_loss did not improve from 0.07718\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0822 - acc: 0.9761 - val_loss: 0.0872 - val_acc: 0.9725\n",
            "Epoch 238/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0899 - acc: 0.9726\n",
            "Epoch 00238: val_loss did not improve from 0.07718\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0930 - acc: 0.9719 - val_loss: 0.0819 - val_acc: 0.9808\n",
            "Epoch 239/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0888 - acc: 0.9744\n",
            "Epoch 00239: val_loss did not improve from 0.07718\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0904 - acc: 0.9742 - val_loss: 0.0896 - val_acc: 0.9758\n",
            "Epoch 240/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0804 - acc: 0.9797\n",
            "Epoch 00240: val_loss did not improve from 0.07718\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.0790 - acc: 0.9797 - val_loss: 0.0777 - val_acc: 0.9825\n",
            "Epoch 241/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0816 - acc: 0.9756\n",
            "Epoch 00241: val_loss did not improve from 0.07718\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0823 - acc: 0.9753 - val_loss: 0.1052 - val_acc: 0.9708\n",
            "Epoch 242/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0919 - acc: 0.9735\n",
            "Epoch 00242: val_loss improved from 0.07718 to 0.07464, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 219us/sample - loss: 0.0923 - acc: 0.9736 - val_loss: 0.0746 - val_acc: 0.9842\n",
            "Epoch 243/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0977 - acc: 0.9709\n",
            "Epoch 00243: val_loss did not improve from 0.07464\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0980 - acc: 0.9711 - val_loss: 0.0960 - val_acc: 0.9717\n",
            "Epoch 244/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0934 - acc: 0.9700\n",
            "Epoch 00244: val_loss did not improve from 0.07464\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0969 - acc: 0.9689 - val_loss: 0.0809 - val_acc: 0.9808\n",
            "Epoch 245/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0913 - acc: 0.9744\n",
            "Epoch 00245: val_loss improved from 0.07464 to 0.07098, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 223us/sample - loss: 0.0921 - acc: 0.9742 - val_loss: 0.0710 - val_acc: 0.9833\n",
            "Epoch 246/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0901 - acc: 0.9732\n",
            "Epoch 00246: val_loss did not improve from 0.07098\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.0947 - acc: 0.9717 - val_loss: 0.0984 - val_acc: 0.9733\n",
            "Epoch 247/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1029 - acc: 0.9656\n",
            "Epoch 00247: val_loss did not improve from 0.07098\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.1021 - acc: 0.9664 - val_loss: 0.0806 - val_acc: 0.9758\n",
            "Epoch 248/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0818 - acc: 0.9773\n",
            "Epoch 00248: val_loss did not improve from 0.07098\n",
            "3600/3600 [==============================] - 1s 206us/sample - loss: 0.0835 - acc: 0.9764 - val_loss: 0.0818 - val_acc: 0.9767\n",
            "Epoch 249/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0775 - acc: 0.9788\n",
            "Epoch 00249: val_loss did not improve from 0.07098\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.0775 - acc: 0.9789 - val_loss: 0.0901 - val_acc: 0.9725\n",
            "Epoch 250/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0816 - acc: 0.9794\n",
            "Epoch 00250: val_loss did not improve from 0.07098\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.0806 - acc: 0.9792 - val_loss: 0.0788 - val_acc: 0.9825\n",
            "Epoch 251/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0785 - acc: 0.9774\n",
            "Epoch 00251: val_loss did not improve from 0.07098\n",
            "3600/3600 [==============================] - 1s 204us/sample - loss: 0.0795 - acc: 0.9764 - val_loss: 0.0831 - val_acc: 0.9800\n",
            "Epoch 252/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0937 - acc: 0.9706\n",
            "Epoch 00252: val_loss did not improve from 0.07098\n",
            "3600/3600 [==============================] - 1s 205us/sample - loss: 0.0944 - acc: 0.9706 - val_loss: 0.0971 - val_acc: 0.9750\n",
            "Epoch 253/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0882 - acc: 0.9774\n",
            "Epoch 00253: val_loss did not improve from 0.07098\n",
            "3600/3600 [==============================] - 1s 202us/sample - loss: 0.0886 - acc: 0.9764 - val_loss: 0.0845 - val_acc: 0.9742\n",
            "Epoch 254/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0749 - acc: 0.9800\n",
            "Epoch 00254: val_loss did not improve from 0.07098\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0755 - acc: 0.9792 - val_loss: 0.1152 - val_acc: 0.9650\n",
            "Epoch 255/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0844 - acc: 0.9724\n",
            "Epoch 00255: val_loss did not improve from 0.07098\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0842 - acc: 0.9722 - val_loss: 0.0715 - val_acc: 0.9842\n",
            "Epoch 256/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0755 - acc: 0.9794\n",
            "Epoch 00256: val_loss did not improve from 0.07098\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0765 - acc: 0.9789 - val_loss: 0.0745 - val_acc: 0.9792\n",
            "Epoch 257/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0908 - acc: 0.9703\n",
            "Epoch 00257: val_loss did not improve from 0.07098\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0901 - acc: 0.9703 - val_loss: 0.0856 - val_acc: 0.9742\n",
            "Epoch 258/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0885 - acc: 0.9744\n",
            "Epoch 00258: val_loss did not improve from 0.07098\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.0884 - acc: 0.9744 - val_loss: 0.0858 - val_acc: 0.9750\n",
            "Epoch 259/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0921 - acc: 0.9729\n",
            "Epoch 00259: val_loss did not improve from 0.07098\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0910 - acc: 0.9733 - val_loss: 0.0846 - val_acc: 0.9742\n",
            "Epoch 260/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0973 - acc: 0.9703\n",
            "Epoch 00260: val_loss did not improve from 0.07098\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0981 - acc: 0.9697 - val_loss: 0.0926 - val_acc: 0.9725\n",
            "Epoch 261/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0979 - acc: 0.9688\n",
            "Epoch 00261: val_loss did not improve from 0.07098\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0975 - acc: 0.9697 - val_loss: 0.1014 - val_acc: 0.9725\n",
            "Epoch 262/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0904 - acc: 0.9744\n",
            "Epoch 00262: val_loss did not improve from 0.07098\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0902 - acc: 0.9742 - val_loss: 0.0991 - val_acc: 0.9725\n",
            "Epoch 263/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0797 - acc: 0.9782\n",
            "Epoch 00263: val_loss did not improve from 0.07098\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0790 - acc: 0.9789 - val_loss: 0.0722 - val_acc: 0.9833\n",
            "Epoch 264/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0775 - acc: 0.9806\n",
            "Epoch 00264: val_loss improved from 0.07098 to 0.06989, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.0785 - acc: 0.9803 - val_loss: 0.0699 - val_acc: 0.9850\n",
            "Epoch 265/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0859 - acc: 0.9753\n",
            "Epoch 00265: val_loss did not improve from 0.06989\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0863 - acc: 0.9750 - val_loss: 0.0788 - val_acc: 0.9750\n",
            "Epoch 266/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9720\n",
            "Epoch 00266: val_loss did not improve from 0.06989\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0870 - acc: 0.9722 - val_loss: 0.0951 - val_acc: 0.9725\n",
            "Epoch 267/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0802 - acc: 0.9747\n",
            "Epoch 00267: val_loss did not improve from 0.06989\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0794 - acc: 0.9747 - val_loss: 0.0904 - val_acc: 0.9750\n",
            "Epoch 268/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0842 - acc: 0.9774\n",
            "Epoch 00268: val_loss did not improve from 0.06989\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0833 - acc: 0.9781 - val_loss: 0.0827 - val_acc: 0.9733\n",
            "Epoch 269/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0912 - acc: 0.9726\n",
            "Epoch 00269: val_loss did not improve from 0.06989\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0917 - acc: 0.9725 - val_loss: 0.0740 - val_acc: 0.9800\n",
            "Epoch 270/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0892 - acc: 0.9732\n",
            "Epoch 00270: val_loss did not improve from 0.06989\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0892 - acc: 0.9731 - val_loss: 0.0899 - val_acc: 0.9708\n",
            "Epoch 271/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0944 - acc: 0.9726\n",
            "Epoch 00271: val_loss did not improve from 0.06989\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0956 - acc: 0.9717 - val_loss: 0.0731 - val_acc: 0.9825\n",
            "Epoch 272/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9760\n",
            "Epoch 00272: val_loss did not improve from 0.06989\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0826 - acc: 0.9767 - val_loss: 0.0743 - val_acc: 0.9758\n",
            "Epoch 273/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0813 - acc: 0.9779\n",
            "Epoch 00273: val_loss did not improve from 0.06989\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0802 - acc: 0.9781 - val_loss: 0.1082 - val_acc: 0.9650\n",
            "Epoch 274/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0861 - acc: 0.9726\n",
            "Epoch 00274: val_loss did not improve from 0.06989\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0853 - acc: 0.9731 - val_loss: 0.0728 - val_acc: 0.9783\n",
            "Epoch 275/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0798 - acc: 0.9759\n",
            "Epoch 00275: val_loss did not improve from 0.06989\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0816 - acc: 0.9756 - val_loss: 0.1021 - val_acc: 0.9742\n",
            "Epoch 276/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0902 - acc: 0.9718\n",
            "Epoch 00276: val_loss did not improve from 0.06989\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0889 - acc: 0.9722 - val_loss: 0.0778 - val_acc: 0.9833\n",
            "Epoch 277/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0698 - acc: 0.9794\n",
            "Epoch 00277: val_loss did not improve from 0.06989\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0697 - acc: 0.9794 - val_loss: 0.0701 - val_acc: 0.9808\n",
            "Epoch 278/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0731 - acc: 0.9812\n",
            "Epoch 00278: val_loss did not improve from 0.06989\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0743 - acc: 0.9808 - val_loss: 0.0805 - val_acc: 0.9775\n",
            "Epoch 279/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.1000 - acc: 0.9694\n",
            "Epoch 00279: val_loss did not improve from 0.06989\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0979 - acc: 0.9706 - val_loss: 0.0823 - val_acc: 0.9792\n",
            "Epoch 280/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0666 - acc: 0.9832\n",
            "Epoch 00280: val_loss improved from 0.06989 to 0.05549, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 220us/sample - loss: 0.0670 - acc: 0.9833 - val_loss: 0.0555 - val_acc: 0.9858\n",
            "Epoch 281/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0692 - acc: 0.9812\n",
            "Epoch 00281: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0703 - acc: 0.9806 - val_loss: 0.0692 - val_acc: 0.9808\n",
            "Epoch 282/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0764 - acc: 0.9779\n",
            "Epoch 00282: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0766 - acc: 0.9775 - val_loss: 0.0670 - val_acc: 0.9833\n",
            "Epoch 283/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0797 - acc: 0.9759\n",
            "Epoch 00283: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0789 - acc: 0.9764 - val_loss: 0.0727 - val_acc: 0.9842\n",
            "Epoch 284/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0923 - acc: 0.9694\n",
            "Epoch 00284: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0927 - acc: 0.9697 - val_loss: 0.0941 - val_acc: 0.9725\n",
            "Epoch 285/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0745 - acc: 0.9785\n",
            "Epoch 00285: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0735 - acc: 0.9792 - val_loss: 0.0651 - val_acc: 0.9833\n",
            "Epoch 286/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9849\n",
            "Epoch 00286: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0622 - acc: 0.9842 - val_loss: 0.0735 - val_acc: 0.9792\n",
            "Epoch 287/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0775 - acc: 0.9756\n",
            "Epoch 00287: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0779 - acc: 0.9753 - val_loss: 0.0940 - val_acc: 0.9750\n",
            "Epoch 288/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9737\n",
            "Epoch 00288: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0840 - acc: 0.9736 - val_loss: 0.0853 - val_acc: 0.9750\n",
            "Epoch 289/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0949 - acc: 0.9709\n",
            "Epoch 00289: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0926 - acc: 0.9719 - val_loss: 0.0900 - val_acc: 0.9675\n",
            "Epoch 290/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0850 - acc: 0.9718\n",
            "Epoch 00290: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0857 - acc: 0.9714 - val_loss: 0.0616 - val_acc: 0.9833\n",
            "Epoch 291/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9666\n",
            "Epoch 00291: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.1064 - acc: 0.9664 - val_loss: 0.1059 - val_acc: 0.9658\n",
            "Epoch 292/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0857 - acc: 0.9759\n",
            "Epoch 00292: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 200us/sample - loss: 0.0882 - acc: 0.9750 - val_loss: 0.1052 - val_acc: 0.9717\n",
            "Epoch 293/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9717\n",
            "Epoch 00293: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0895 - acc: 0.9719 - val_loss: 0.0625 - val_acc: 0.9850\n",
            "Epoch 294/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0833 - acc: 0.9774\n",
            "Epoch 00294: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0832 - acc: 0.9775 - val_loss: 0.1130 - val_acc: 0.9700\n",
            "Epoch 295/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0752 - acc: 0.9788\n",
            "Epoch 00295: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0747 - acc: 0.9792 - val_loss: 0.0698 - val_acc: 0.9808\n",
            "Epoch 296/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0683 - acc: 0.9791\n",
            "Epoch 00296: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.0682 - acc: 0.9794 - val_loss: 0.0643 - val_acc: 0.9808\n",
            "Epoch 297/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0791 - acc: 0.9776\n",
            "Epoch 00297: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0817 - acc: 0.9769 - val_loss: 0.0712 - val_acc: 0.9775\n",
            "Epoch 298/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0703 - acc: 0.9806\n",
            "Epoch 00298: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0708 - acc: 0.9806 - val_loss: 0.0586 - val_acc: 0.9817\n",
            "Epoch 299/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0694 - acc: 0.9797\n",
            "Epoch 00299: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0714 - acc: 0.9792 - val_loss: 0.0731 - val_acc: 0.9783\n",
            "Epoch 300/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0710 - acc: 0.9821\n",
            "Epoch 00300: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0704 - acc: 0.9822 - val_loss: 0.0783 - val_acc: 0.9767\n",
            "Epoch 301/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0878 - acc: 0.9729\n",
            "Epoch 00301: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0879 - acc: 0.9725 - val_loss: 0.0937 - val_acc: 0.9767\n",
            "Epoch 302/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0887 - acc: 0.9703\n",
            "Epoch 00302: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0876 - acc: 0.9711 - val_loss: 0.0779 - val_acc: 0.9817\n",
            "Epoch 303/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0823 - acc: 0.9744\n",
            "Epoch 00303: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0823 - acc: 0.9742 - val_loss: 0.0651 - val_acc: 0.9825\n",
            "Epoch 304/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0786 - acc: 0.9776\n",
            "Epoch 00304: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0781 - acc: 0.9778 - val_loss: 0.0640 - val_acc: 0.9842\n",
            "Epoch 305/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0856 - acc: 0.9759\n",
            "Epoch 00305: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0839 - acc: 0.9767 - val_loss: 0.0598 - val_acc: 0.9842\n",
            "Epoch 306/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0686 - acc: 0.9809\n",
            "Epoch 00306: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0691 - acc: 0.9800 - val_loss: 0.0803 - val_acc: 0.9733\n",
            "Epoch 307/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0816 - acc: 0.9768\n",
            "Epoch 00307: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0804 - acc: 0.9775 - val_loss: 0.0775 - val_acc: 0.9792\n",
            "Epoch 308/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9820\n",
            "Epoch 00308: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0692 - acc: 0.9819 - val_loss: 0.0754 - val_acc: 0.9792\n",
            "Epoch 309/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0650 - acc: 0.9824\n",
            "Epoch 00309: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0651 - acc: 0.9825 - val_loss: 0.0773 - val_acc: 0.9792\n",
            "Epoch 310/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0621 - acc: 0.9832\n",
            "Epoch 00310: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0621 - acc: 0.9833 - val_loss: 0.0738 - val_acc: 0.9775\n",
            "Epoch 311/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0725 - acc: 0.9791\n",
            "Epoch 00311: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0729 - acc: 0.9792 - val_loss: 0.0772 - val_acc: 0.9783\n",
            "Epoch 312/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0738 - acc: 0.9774\n",
            "Epoch 00312: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0739 - acc: 0.9778 - val_loss: 0.0623 - val_acc: 0.9842\n",
            "Epoch 313/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9837\n",
            "Epoch 00313: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0686 - acc: 0.9836 - val_loss: 0.0708 - val_acc: 0.9800\n",
            "Epoch 314/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0844 - acc: 0.9735\n",
            "Epoch 00314: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0829 - acc: 0.9739 - val_loss: 0.0769 - val_acc: 0.9792\n",
            "Epoch 315/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0704 - acc: 0.9797\n",
            "Epoch 00315: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0696 - acc: 0.9806 - val_loss: 0.0785 - val_acc: 0.9792\n",
            "Epoch 316/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0686 - acc: 0.9800\n",
            "Epoch 00316: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0733 - acc: 0.9781 - val_loss: 0.0677 - val_acc: 0.9783\n",
            "Epoch 317/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0873 - acc: 0.9715\n",
            "Epoch 00317: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0888 - acc: 0.9711 - val_loss: 0.0693 - val_acc: 0.9817\n",
            "Epoch 318/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0818 - acc: 0.9788\n",
            "Epoch 00318: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0862 - acc: 0.9769 - val_loss: 0.0661 - val_acc: 0.9825\n",
            "Epoch 319/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0724 - acc: 0.9821\n",
            "Epoch 00319: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0723 - acc: 0.9814 - val_loss: 0.0692 - val_acc: 0.9775\n",
            "Epoch 320/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0696 - acc: 0.9791\n",
            "Epoch 00320: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0691 - acc: 0.9794 - val_loss: 0.0881 - val_acc: 0.9733\n",
            "Epoch 321/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0771 - acc: 0.9741\n",
            "Epoch 00321: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0793 - acc: 0.9728 - val_loss: 0.0613 - val_acc: 0.9825\n",
            "Epoch 322/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0860 - acc: 0.9744\n",
            "Epoch 00322: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0837 - acc: 0.9758 - val_loss: 0.0757 - val_acc: 0.9775\n",
            "Epoch 323/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0821 - acc: 0.9750\n",
            "Epoch 00323: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0817 - acc: 0.9756 - val_loss: 0.0665 - val_acc: 0.9783\n",
            "Epoch 324/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0737 - acc: 0.9818\n",
            "Epoch 00324: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0746 - acc: 0.9811 - val_loss: 0.0658 - val_acc: 0.9842\n",
            "Epoch 325/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0718 - acc: 0.9806\n",
            "Epoch 00325: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0712 - acc: 0.9806 - val_loss: 0.0710 - val_acc: 0.9800\n",
            "Epoch 326/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9783\n",
            "Epoch 00326: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0739 - acc: 0.9786 - val_loss: 0.0659 - val_acc: 0.9825\n",
            "Epoch 327/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0704 - acc: 0.9809\n",
            "Epoch 00327: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0688 - acc: 0.9814 - val_loss: 0.0680 - val_acc: 0.9792\n",
            "Epoch 328/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0751 - acc: 0.9800\n",
            "Epoch 00328: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0742 - acc: 0.9808 - val_loss: 0.0823 - val_acc: 0.9758\n",
            "Epoch 329/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0801 - acc: 0.9762\n",
            "Epoch 00329: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0803 - acc: 0.9756 - val_loss: 0.0677 - val_acc: 0.9817\n",
            "Epoch 330/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0848 - acc: 0.9738\n",
            "Epoch 00330: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0897 - acc: 0.9722 - val_loss: 0.0983 - val_acc: 0.9692\n",
            "Epoch 331/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9686\n",
            "Epoch 00331: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0980 - acc: 0.9692 - val_loss: 0.0859 - val_acc: 0.9742\n",
            "Epoch 332/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0730 - acc: 0.9776\n",
            "Epoch 00332: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 201us/sample - loss: 0.0736 - acc: 0.9772 - val_loss: 0.0935 - val_acc: 0.9725\n",
            "Epoch 333/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0774 - acc: 0.9779\n",
            "Epoch 00333: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0774 - acc: 0.9778 - val_loss: 0.0616 - val_acc: 0.9850\n",
            "Epoch 334/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0793 - acc: 0.9747\n",
            "Epoch 00334: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0792 - acc: 0.9756 - val_loss: 0.0571 - val_acc: 0.9858\n",
            "Epoch 335/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9794\n",
            "Epoch 00335: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0734 - acc: 0.9794 - val_loss: 0.0679 - val_acc: 0.9792\n",
            "Epoch 336/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9780\n",
            "Epoch 00336: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0709 - acc: 0.9783 - val_loss: 0.0926 - val_acc: 0.9675\n",
            "Epoch 337/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0800 - acc: 0.9773\n",
            "Epoch 00337: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0804 - acc: 0.9772 - val_loss: 0.0574 - val_acc: 0.9850\n",
            "Epoch 338/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9780\n",
            "Epoch 00338: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0751 - acc: 0.9783 - val_loss: 0.0612 - val_acc: 0.9833\n",
            "Epoch 339/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0885 - acc: 0.9721\n",
            "Epoch 00339: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0918 - acc: 0.9697 - val_loss: 0.0651 - val_acc: 0.9817\n",
            "Epoch 340/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0929 - acc: 0.9712\n",
            "Epoch 00340: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0944 - acc: 0.9708 - val_loss: 0.0939 - val_acc: 0.9733\n",
            "Epoch 341/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9794\n",
            "Epoch 00341: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0741 - acc: 0.9797 - val_loss: 0.0720 - val_acc: 0.9775\n",
            "Epoch 342/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9763\n",
            "Epoch 00342: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0816 - acc: 0.9767 - val_loss: 0.0736 - val_acc: 0.9783\n",
            "Epoch 343/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0753 - acc: 0.9800\n",
            "Epoch 00343: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0744 - acc: 0.9803 - val_loss: 0.0737 - val_acc: 0.9825\n",
            "Epoch 344/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0942 - acc: 0.9721\n",
            "Epoch 00344: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0930 - acc: 0.9722 - val_loss: 0.0867 - val_acc: 0.9750\n",
            "Epoch 345/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0788 - acc: 0.9753\n",
            "Epoch 00345: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0786 - acc: 0.9750 - val_loss: 0.0601 - val_acc: 0.9867\n",
            "Epoch 346/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0845 - acc: 0.9739\n",
            "Epoch 00346: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 190us/sample - loss: 0.0834 - acc: 0.9747 - val_loss: 0.0730 - val_acc: 0.9800\n",
            "Epoch 347/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0885 - acc: 0.9715\n",
            "Epoch 00347: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0886 - acc: 0.9722 - val_loss: 0.0881 - val_acc: 0.9692\n",
            "Epoch 348/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9729\n",
            "Epoch 00348: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0835 - acc: 0.9728 - val_loss: 0.0789 - val_acc: 0.9708\n",
            "Epoch 349/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0782 - acc: 0.9791\n",
            "Epoch 00349: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0770 - acc: 0.9792 - val_loss: 0.0659 - val_acc: 0.9792\n",
            "Epoch 350/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9780\n",
            "Epoch 00350: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0772 - acc: 0.9775 - val_loss: 0.0569 - val_acc: 0.9850\n",
            "Epoch 351/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9740\n",
            "Epoch 00351: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0800 - acc: 0.9742 - val_loss: 0.0910 - val_acc: 0.9725\n",
            "Epoch 352/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0696 - acc: 0.9794\n",
            "Epoch 00352: val_loss did not improve from 0.05549\n",
            "3600/3600 [==============================] - 1s 203us/sample - loss: 0.0724 - acc: 0.9781 - val_loss: 0.0634 - val_acc: 0.9825\n",
            "Epoch 353/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0701 - acc: 0.9800\n",
            "Epoch 00353: val_loss improved from 0.05549 to 0.05385, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.0707 - acc: 0.9794 - val_loss: 0.0538 - val_acc: 0.9858\n",
            "Epoch 354/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0732 - acc: 0.9771\n",
            "Epoch 00354: val_loss did not improve from 0.05385\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0722 - acc: 0.9775 - val_loss: 0.0646 - val_acc: 0.9808\n",
            "Epoch 355/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0707 - acc: 0.9756\n",
            "Epoch 00355: val_loss did not improve from 0.05385\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0701 - acc: 0.9761 - val_loss: 0.0584 - val_acc: 0.9842\n",
            "Epoch 356/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9843\n",
            "Epoch 00356: val_loss did not improve from 0.05385\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0663 - acc: 0.9836 - val_loss: 0.0733 - val_acc: 0.9783\n",
            "Epoch 357/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0855 - acc: 0.9741\n",
            "Epoch 00357: val_loss improved from 0.05385 to 0.05338, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 215us/sample - loss: 0.0863 - acc: 0.9739 - val_loss: 0.0534 - val_acc: 0.9850\n",
            "Epoch 358/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.0765 - acc: 0.9762\n",
            "Epoch 00358: val_loss did not improve from 0.05338\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0757 - acc: 0.9772 - val_loss: 0.0804 - val_acc: 0.9742\n",
            "Epoch 359/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0695 - acc: 0.9809\n",
            "Epoch 00359: val_loss did not improve from 0.05338\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0699 - acc: 0.9806 - val_loss: 0.0604 - val_acc: 0.9825\n",
            "Epoch 360/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9803\n",
            "Epoch 00360: val_loss did not improve from 0.05338\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0722 - acc: 0.9800 - val_loss: 0.0632 - val_acc: 0.9808\n",
            "Epoch 361/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9771\n",
            "Epoch 00361: val_loss did not improve from 0.05338\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0764 - acc: 0.9769 - val_loss: 0.0679 - val_acc: 0.9825\n",
            "Epoch 362/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9737\n",
            "Epoch 00362: val_loss did not improve from 0.05338\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0941 - acc: 0.9736 - val_loss: 0.1062 - val_acc: 0.9692\n",
            "Epoch 363/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0789 - acc: 0.9744\n",
            "Epoch 00363: val_loss improved from 0.05338 to 0.04816, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 213us/sample - loss: 0.0793 - acc: 0.9747 - val_loss: 0.0482 - val_acc: 0.9883\n",
            "Epoch 364/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9829\n",
            "Epoch 00364: val_loss did not improve from 0.04816\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0664 - acc: 0.9825 - val_loss: 0.0575 - val_acc: 0.9792\n",
            "Epoch 365/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0741 - acc: 0.9809\n",
            "Epoch 00365: val_loss did not improve from 0.04816\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0737 - acc: 0.9814 - val_loss: 0.0525 - val_acc: 0.9867\n",
            "Epoch 366/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0691 - acc: 0.9821\n",
            "Epoch 00366: val_loss did not improve from 0.04816\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0699 - acc: 0.9814 - val_loss: 0.0563 - val_acc: 0.9825\n",
            "Epoch 367/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0662 - acc: 0.9836\n",
            "Epoch 00367: val_loss improved from 0.04816 to 0.04226, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 214us/sample - loss: 0.0673 - acc: 0.9833 - val_loss: 0.0423 - val_acc: 0.9917\n",
            "Epoch 368/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9797\n",
            "Epoch 00368: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0718 - acc: 0.9786 - val_loss: 0.0601 - val_acc: 0.9817\n",
            "Epoch 369/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0719 - acc: 0.9794\n",
            "Epoch 00369: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 198us/sample - loss: 0.0725 - acc: 0.9789 - val_loss: 0.0578 - val_acc: 0.9825\n",
            "Epoch 370/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9797\n",
            "Epoch 00370: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0705 - acc: 0.9794 - val_loss: 0.0579 - val_acc: 0.9833\n",
            "Epoch 371/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9791\n",
            "Epoch 00371: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0676 - acc: 0.9794 - val_loss: 0.0584 - val_acc: 0.9817\n",
            "Epoch 372/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0628 - acc: 0.9841\n",
            "Epoch 00372: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0641 - acc: 0.9836 - val_loss: 0.0820 - val_acc: 0.9725\n",
            "Epoch 373/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0700 - acc: 0.9809\n",
            "Epoch 00373: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0704 - acc: 0.9808 - val_loss: 0.0590 - val_acc: 0.9825\n",
            "Epoch 374/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0821 - acc: 0.9735\n",
            "Epoch 00374: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0821 - acc: 0.9739 - val_loss: 0.0538 - val_acc: 0.9833\n",
            "Epoch 375/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9800\n",
            "Epoch 00375: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0736 - acc: 0.9797 - val_loss: 0.0557 - val_acc: 0.9825\n",
            "Epoch 376/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0711 - acc: 0.9788\n",
            "Epoch 00376: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0732 - acc: 0.9778 - val_loss: 0.0631 - val_acc: 0.9817\n",
            "Epoch 377/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0760 - acc: 0.9768\n",
            "Epoch 00377: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0784 - acc: 0.9753 - val_loss: 0.0850 - val_acc: 0.9708\n",
            "Epoch 378/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0835 - acc: 0.9724\n",
            "Epoch 00378: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0819 - acc: 0.9731 - val_loss: 0.0583 - val_acc: 0.9858\n",
            "Epoch 379/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0714 - acc: 0.9774\n",
            "Epoch 00379: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0716 - acc: 0.9769 - val_loss: 0.0637 - val_acc: 0.9783\n",
            "Epoch 380/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0621 - acc: 0.9835\n",
            "Epoch 00380: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0635 - acc: 0.9831 - val_loss: 0.0531 - val_acc: 0.9858\n",
            "Epoch 381/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0825 - acc: 0.9735\n",
            "Epoch 00381: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0847 - acc: 0.9728 - val_loss: 0.0575 - val_acc: 0.9833\n",
            "Epoch 382/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0746 - acc: 0.9785\n",
            "Epoch 00382: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0737 - acc: 0.9789 - val_loss: 0.0676 - val_acc: 0.9800\n",
            "Epoch 383/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0780 - acc: 0.9756\n",
            "Epoch 00383: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0791 - acc: 0.9753 - val_loss: 0.0771 - val_acc: 0.9767\n",
            "Epoch 384/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0782 - acc: 0.9765\n",
            "Epoch 00384: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 199us/sample - loss: 0.0759 - acc: 0.9767 - val_loss: 0.0678 - val_acc: 0.9817\n",
            "Epoch 385/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0756 - acc: 0.9776\n",
            "Epoch 00385: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0763 - acc: 0.9772 - val_loss: 0.0772 - val_acc: 0.9817\n",
            "Epoch 386/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0689 - acc: 0.9821\n",
            "Epoch 00386: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0716 - acc: 0.9811 - val_loss: 0.0698 - val_acc: 0.9800\n",
            "Epoch 387/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0778 - acc: 0.9782\n",
            "Epoch 00387: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0767 - acc: 0.9789 - val_loss: 0.0664 - val_acc: 0.9817\n",
            "Epoch 388/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0719 - acc: 0.9782\n",
            "Epoch 00388: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0707 - acc: 0.9783 - val_loss: 0.0864 - val_acc: 0.9717\n",
            "Epoch 389/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0634 - acc: 0.9844\n",
            "Epoch 00389: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0636 - acc: 0.9839 - val_loss: 0.0549 - val_acc: 0.9833\n",
            "Epoch 390/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0734 - acc: 0.9788\n",
            "Epoch 00390: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0739 - acc: 0.9781 - val_loss: 0.0688 - val_acc: 0.9767\n",
            "Epoch 391/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0648 - acc: 0.9803\n",
            "Epoch 00391: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 197us/sample - loss: 0.0667 - acc: 0.9794 - val_loss: 0.0756 - val_acc: 0.9767\n",
            "Epoch 392/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0726 - acc: 0.9770\n",
            "Epoch 00392: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0720 - acc: 0.9775 - val_loss: 0.0642 - val_acc: 0.9825\n",
            "Epoch 393/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0818 - acc: 0.9741\n",
            "Epoch 00393: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0810 - acc: 0.9744 - val_loss: 0.0593 - val_acc: 0.9825\n",
            "Epoch 394/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0865 - acc: 0.9739\n",
            "Epoch 00394: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 193us/sample - loss: 0.0841 - acc: 0.9747 - val_loss: 0.0537 - val_acc: 0.9833\n",
            "Epoch 395/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9763\n",
            "Epoch 00395: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0795 - acc: 0.9767 - val_loss: 0.0511 - val_acc: 0.9858\n",
            "Epoch 396/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.0783 - acc: 0.9761\n",
            "Epoch 00396: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 194us/sample - loss: 0.0783 - acc: 0.9764 - val_loss: 0.0646 - val_acc: 0.9775\n",
            "Epoch 397/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0703 - acc: 0.9797\n",
            "Epoch 00397: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0697 - acc: 0.9797 - val_loss: 0.0570 - val_acc: 0.9858\n",
            "Epoch 398/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0681 - acc: 0.9803\n",
            "Epoch 00398: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 196us/sample - loss: 0.0678 - acc: 0.9806 - val_loss: 0.0646 - val_acc: 0.9792\n",
            "Epoch 399/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 0.0731 - acc: 0.9794\n",
            "Epoch 00399: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 195us/sample - loss: 0.0729 - acc: 0.9800 - val_loss: 0.0862 - val_acc: 0.9750\n",
            "Epoch 400/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9809\n",
            "Epoch 00400: val_loss did not improve from 0.04226\n",
            "3600/3600 [==============================] - 1s 192us/sample - loss: 0.0685 - acc: 0.9814 - val_loss: 0.0656 - val_acc: 0.9800\n",
            "1200/1200 [==============================] - 0s 136us/sample - loss: 0.0656 - acc: 0.9800\n",
            "[0.06558359483256936, 0.98]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl95BH9AzKL4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "0dbad18c-8cfa-485d-91ab-9cd8b16f56bf"
      },
      "source": [
        "for i in range(15, 16): # Итерација низ секој испитен примерок\n",
        "  print(f\"====================== Примерок ({i}) ======================\")\n",
        "  print(\"Вчитување тест податоци од испитниот примерок \" + str(i) + \"...\")\n",
        "  \n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  \n",
        "  # Иницијализација на помошни променливи\n",
        "  temp_test_data = np.empty(0)\n",
        "  temp_test_events = np.empty(0)\n",
        "  \n",
        "  for j in range(1, 4): # Итерација низ секоја сесија\n",
        "    file_test_set = 'S' + format(j, '02') + '/Test'\n",
        "\n",
        "    # Вчитување на податоците\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testData.mat\"\n",
        "    temp = loadmat(full_path)['testData']\n",
        "    if temp_test_data.size != 0:\n",
        "      temp_test_data = np.concatenate((temp_test_data, temp), axis=2)\n",
        "    else: \n",
        "      temp_test_data = np.array(temp)\n",
        "\n",
        "    # Вчитување на редоследот на светкање\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/testEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      if temp_test_events.size != 0:\n",
        "        temp_test_events = np.append(temp_test_events, temp)\n",
        "      else:\n",
        "        temp_test_events = np.array(temp)\n",
        "\n",
        "    # Вчитување на бројот на runs \n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_test_set + \"/runs_per_block.txt\"\n",
        "    with open(full_path, \"r\") as runs_per_block:\n",
        "      test_runs_per_block[i-1][j-1] = int(runs_per_block.read())\n",
        "\n",
        "    print(\"\\t - Тест податоците од сесија \" + str(j) + \" се вчитани.\")\n",
        "  # Зачувај ги тест податоците вчитани од испитниот примерок во низа\n",
        "  test_data.append(temp_test_data)\n",
        "  test_events.append(temp_test_events)\n",
        "  print(\"Тест податоците од испитниот примерок \" + str(i) + \" се вчитани.\\n\")\n",
        "\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_data: \" + str(test_data[i-1].shape)) # test_data to predict\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Test_events: \" + str(len(test_events[i-1]))) # test_events\n",
        "  for j in range (1,4):\n",
        "    print(\"SBJ\" + str(format(i-1, '02')) + \" / S\" + str(format(j-1, '02')) + \"| Runs per block: \" + str(test_runs_per_block[i-1][j-1])) # runs per block in SJB01, SJ00 \n",
        "\n",
        "  to_predict_data = reshape_data_to_mne_format(test_data[i-1])\n",
        "  predictions = model15.predict(to_predict_data)\n",
        "  print(\"SBJ\" + str(format(i-1, '02')) + \"| Predictions: \" + str(len(predictions)))\n",
        "  # np.savetxt(\"predictions.csv\", predictions, delimiter=\",\")\n",
        "\n",
        "\n",
        "  # ========= FALI USTE DA SE ISPARSIRA PREDICTIONOT... NE E SREDEN OVOJ KOD DOLE =======\n",
        "\n",
        "  int_pred = np.argmax(predictions, axis=1)\n",
        "  int_ytest = np.argmax(y_test, axis=1)\n",
        "\n",
        "  session_start = 0\n",
        "  start_prediction_index = 0\n",
        "  end_prediction_index = 0\n",
        "  for session in range(0, 3):\n",
        "    print(f\"============== Сесија ({session}) ==============\")\n",
        "    for block in range(0, 50):    \n",
        "      events_per_block = test_runs_per_block[i-1][session]\n",
        "\n",
        "      start_prediction_index = session_start + (block*events_per_block)*8\n",
        "      end_prediction_index = session_start + ((block+1)*events_per_block)*8\n",
        "\n",
        "      block_prediction = int_pred[start_prediction_index:end_prediction_index]\n",
        "      prediction = np.bincount(block_prediction).argmax()\n",
        "      df.iat[session+42,block+2] = prediction+1\n",
        "      # UNCOMMENT ZA PODOBAR PRIKAZ :)\n",
        "      # print(f\"Session {session} | Block: {block} | Prediction: {prediction} | Address: {end_prediction_index}\")\n",
        "\n",
        "      print(str(prediction+1) + \",\", end=\"\")\n",
        "    session_start = end_prediction_index\n",
        "    print(\"\")\n",
        "  print(\"Stigna li do kraj: \" + str(session_start == len(predictions)))\n",
        "  print(f\"====================== Примерок ({i}) ======================\\n\\n\")"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Примерок (15) ======================\n",
            "Вчитување тест податоци од испитниот примерок 15...\n",
            "\t - Тест податоците од сесија 1 се вчитани.\n",
            "\t - Тест податоците од сесија 2 се вчитани.\n",
            "\t - Тест податоците од сесија 3 се вчитани.\n",
            "Тест податоците од испитниот примерок 15 се вчитани.\n",
            "\n",
            "SBJ14| Test_data: (8, 350, 8800)\n",
            "SBJ14| Test_events: 8800\n",
            "SBJ14 / S00| Runs per block: 6\n",
            "SBJ14 / S01| Runs per block: 6\n",
            "SBJ14 / S02| Runs per block: 10\n",
            "SBJ14| Predictions: 8800\n",
            "============== Сесија (0) ==============\n",
            "4,7,4,3,1,1,5,1,5,5,5,2,8,1,5,4,1,5,1,5,1,1,3,5,5,5,5,5,5,5,6,5,1,3,5,1,1,1,5,5,1,1,1,5,5,5,5,1,1,1,\n",
            "============== Сесија (1) ==============\n",
            "7,6,4,5,5,5,5,5,5,5,5,4,5,7,5,5,5,5,5,1,5,5,7,7,7,7,7,5,7,5,5,5,7,5,7,3,4,7,7,7,7,5,2,5,4,5,7,5,5,8,\n",
            "============== Сесија (2) ==============\n",
            "5,4,8,4,2,4,1,4,5,5,1,4,1,1,4,5,2,5,5,4,2,1,1,2,5,5,5,2,1,4,4,4,1,4,4,5,1,7,1,4,4,7,3,2,1,5,4,1,5,5,\n",
            "Stigna li do kraj: True\n",
            "====================== Примерок (15) ======================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n7pxlztzT4v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "221e6a47-4997-4923-f986-570240aa67d0"
      },
      "source": [
        "df"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUBJECT</th>\n",
              "      <th>SESSION</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>Unnamed: 52</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    SUBJECT  SESSION  1  2  3  4  5  6  7  ... 43 44 45 46 47 48 49 50 Unnamed: 52\n",
              "0         1        1  6  6  3  6  6  3  6  ...  6  4  8  3  4  5  5  7         NaN\n",
              "1         1        2  6  3  2  1  2  1  3  ...  6  2  2  2  3  6  6  2         NaN\n",
              "2         1        3  3  3  3  3  3  3  3  ...  3  3  6  3  6  7  1  6         NaN\n",
              "3         2        1  8  8  8  8  8  8  8  ...  8  4  8  4  8  7  8  8         NaN\n",
              "4         2        2  2  7  6  6  6  6  7  ...  2  6  6  2  6  6  2  2         NaN\n",
              "5         2        3  2  7  2  6  7  4  2  ...  2  6  2  2  7  2  2  2         NaN\n",
              "6         3        1  3  3  4  4  4  3  6  ...  4  6  3  3  4  3  4  4         NaN\n",
              "7         3        2  6  1  7  7  7  7  7  ...  6  7  1  6  6  6  6  6         NaN\n",
              "8         3        3  6  4  8  8  5  7  8  ...  4  2  8  2  2  2  2  8         NaN\n",
              "9         4        1  1  1  2  1  5  5  6  ...  1  1  7  1  6  6  6  6         NaN\n",
              "10        4        2  7  7  7  7  6  6  7  ...  1  5  5  5  5  5  2  6         NaN\n",
              "11        4        3  7  3  7  3  6  6  7  ...  6  1  4  4  4  6  6  6         NaN\n",
              "12        5        1  5  4  4  4  6  4  5  ...  3  1  4  4  1  3  3  5         NaN\n",
              "13        5        2  3  6  3  5  2  6  7  ...  6  6  6  6  6  6  6  4         NaN\n",
              "14        5        3  4  3  3  6  5  7  6  ...  7  2  2  7  5  6  4  2         NaN\n",
              "15        6        1  1  3  1  8  3  3  3  ...  4  3  7  2  7  2  5  8         NaN\n",
              "16        6        2  3  4  1  7  1  1  1  ...  5  5  5  5  5  5  5  5         NaN\n",
              "17        6        3  2  3  2  5  3  3  3  ...  2  3  3  3  1  1  3  3         NaN\n",
              "18        7        1  4  7  5  7  4  5  4  ...  5  1  1  4  4  4  1  4         NaN\n",
              "19        7        2  2  2  2  8  2  5  2  ...  5  5  5  2  2  2  2  2         NaN\n",
              "20        7        3  2  7  7  5  7  5  4  ...  1  3  5  5  3  5  5  1         NaN\n",
              "21        8        1  5  8  2  8  2  2  1  ...  1  8  2  2  5  5  2  8         NaN\n",
              "22        8        2  2  7  1  5  1  2  1  ...  7  1  7  8  2  1  7  7         NaN\n",
              "23        8        3  4  1  6  6  1  2  1  ...  1  8  1  7  1  1  7  1         NaN\n",
              "24        9        1  6  5  6  6  7  6  8  ...  6  6  6  5  5  5  3  5         NaN\n",
              "25        9        2  3  4  8  4  1  6  1  ...  1  1  1  1  1  3  6  6         NaN\n",
              "26        9        3  5  2  2  2  2  3  2  ...  2  2  2  3  2  2  4  2         NaN\n",
              "27       10        1  1  1  8  7  3  1  1  ...  5  1  1  1  1  5  1  5         NaN\n",
              "28       10        2  1  5  5  6  1  1  1  ...  5  5  6  6  2  1  2  1         NaN\n",
              "29       10        3  5  8  5  5  5  6  5  ...  5  5  2  5  5  5  5  5         NaN\n",
              "30       11        1  3  4  4  4  3  4  3  ...  5  4  3  1  3  4  4  4         NaN\n",
              "31       11        2  3  1  1  4  8  4  4  ...  1  7  3  8  2  3  8  3         NaN\n",
              "32       11        3  4  2  4  4  4  4  4  ...  8  4  4  7  8  4  8  4         NaN\n",
              "33       12        1  1  1  1  1  1  1  6  ...  1  6  7  3  1  7  7  3         NaN\n",
              "34       12        2  3  2  6  2  2  3  2  ...  5  1  2  1  5  5  2  5         NaN\n",
              "35       12        3  1  6  2  2  1  2  2  ...  5  6  2  1  2  2  2  2         NaN\n",
              "36       13        1  6  6  1  5  3  1  1  ...  1  1  1  5  6  1  1  1         NaN\n",
              "37       13        2  6  1  5  4  1  1  6  ...  7  6  8  6  1  6  6  1         NaN\n",
              "38       13        3  1  6  5  1  5  2  6  ...  7  1  1  7  2  2  6  1         NaN\n",
              "39       14        1  4  4  3  4  3  8  2  ...  4  6  3  6  4  3  4  1         NaN\n",
              "40       14        2  2  3  4  3  3  3  3  ...  2  3  3  2  3  3  3  2         NaN\n",
              "41       14        3  7  4  6  4  6  3  7  ...  4  4  4  4  4  4  4  7         NaN\n",
              "42       15        1  4  7  4  3  1  1  5  ...  1  5  5  5  5  1  1  1         NaN\n",
              "43       15        2  7  6  4  5  5  5  5  ...  2  5  4  5  7  5  5  8         NaN\n",
              "44       15        3  5  4  8  4  2  4  1  ...  3  2  1  5  4  1  5  5         NaN\n",
              "\n",
              "[45 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAHddY4_0GVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('drive/My Drive/Интелигентни Системи/output.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}