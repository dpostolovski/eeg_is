{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IS_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dpostolovski/eeg_is/blob/train_compare_full_data/chisti_ltsm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBqW8_NNlAgz",
        "colab_type": "text"
      },
      "source": [
        "<h1>\n",
        "  <img alt=\"FINKI **LOGO**\" height=\"30px\" src=\"https://www.finki.ukim.mk/Content/dataImages/downloads/logo-large-500x500_2.png\" hspace=\"10px\" vspace=\"0px\">\n",
        "  Интелигентни системи - Лабораториска вежба 2 (Претпроцесирање)\n",
        "</h1>\n",
        "<center><h3><i>Група 5<i><h3></center>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTx3XQgUOhCX",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "0a1b384f-ac48-444b-d64c-739303221ea4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#@title Монтирање на Google Drive податочниот систем\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLrrPj8clKjP",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "cf883e1d-3a14-4ef5-b65a-92f7a44ab911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "#@title Инсталирање и вчитување на потребните библиотеки\n",
        "\n",
        "# Библиотека за истражување, визуелизација и анализирање на човечки \n",
        "# неврофизиолошки податоци (EEG, sEEG и др)\n",
        "!pip install mne \n",
        "!pip install termcolor\n",
        "\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, date, time\n",
        "import pandas as pd\n",
        "from termcolor import colored\n",
        "import mne\n",
        "from sklearn.decomposition import PCA, FastICA\n",
        "import mne"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mne\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/f5/524020ce32785c632c84686aa5441fda59ccc78337ff0d45376d35d9c37a/mne-0.20.5-py3-none-any.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 670kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from mne) (1.18.4)\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from mne) (1.4.1)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-0.20.5\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (1.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NeWVBhf1VxlH",
        "outputId": "402f1b56-8445-47f7-d78a-c35b0df02e88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "%tensorflow_version 1.12.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.12.0`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBEiowYKtnPJ",
        "colab_type": "code",
        "outputId": "66498bd6-e1bc-4c94-9858-d5f16fc779d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "!wget \"https://raw.githubusercontent.com/vlawhern/arl-eegmodels/master/EEGModels.py\"\n",
        "!mkdir saved_models"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-19 16:13:38--  https://raw.githubusercontent.com/vlawhern/arl-eegmodels/master/EEGModels.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18283 (18K) [text/plain]\n",
            "Saving to: ‘EEGModels.py’\n",
            "\n",
            "\rEEGModels.py          0%[                    ]       0  --.-KB/s               \rEEGModels.py        100%[===================>]  17.85K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-05-19 16:13:38 (82.5 MB/s) - ‘EEGModels.py’ saved [18283/18283]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWGg4jBTlCR9",
        "colab_type": "code",
        "outputId": "90897956-750d-473d-ceb9-82e403d0861d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!readlink -f ~/.keras/keras.json"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.keras/keras.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c8w9tgTiV53",
        "colab_type": "code",
        "outputId": "f25757d4-59c1-468a-bc20-4c817a27fe1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "#@title Fourier analysis\n",
        "# Вчитување на податоците\n",
        "\n",
        "from sklearn import metrics \n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint \n",
        "from sklearn.model_selection import train_test_split\n",
        "from EEGModels import EEGNet,ShallowConvNet\n",
        "import scipy.io as sio\n",
        "from scipy.fft import fft\n",
        "from scipy import signal\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import os\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow import keras\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "print(open(\"/root/.keras/keras.json\").read())\n",
        "\n",
        "# Вчитување на податоците\n",
        "data = loadmat('drive/My Drive/Интелигентни Системи/Data/SBJ01/S01-Train/trainData.mat')['trainData'] \n",
        "for i in range(1, 2):\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  for j in range(1, 4):\n",
        "    if i == 1 and j == 1: continue\n",
        "    file_train_set = 'S' + format(j, '02') + '-Train'\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/trainData.mat\"\n",
        "    temp = loadmat(full_path)['trainData'] \n",
        "    data = np.concatenate((data, temp), axis=2)\n",
        "\n",
        "print(data.size)\n",
        "print(data.shape)\n",
        "\n",
        "# Вчитување на label-ите\n",
        "labels_arr = np.empty(0)\n",
        "for i in range(1, 2):\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  for j in range(1, 4):\n",
        "    file_train_set = 'S' + format(j, '02') + '-Train'\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/trainLabels.txt\"\n",
        "    with open(full_path, \"r\") as file_labels:\n",
        "      temp = file_labels.read().splitlines()\n",
        "      temp = np.repeat(temp, 8*10)\n",
        "      labels_arr = np.concatenate((labels_arr, temp))\n",
        "\n",
        "print(labels_arr)\n",
        "print(len(labels_arr))\n",
        "\n",
        "# Вчитување на редоследот на светкање\n",
        "events_arr = np.empty(0)\n",
        "for i in range(1, 2):\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  for j in range(1, 4):\n",
        "    file_train_set = 'S' + format(j, '02') + '-Train'\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/trainEvents.txt\"\n",
        "    with open(full_path, \"r\") as file_events:\n",
        "      temp = file_events.read().splitlines()\n",
        "      events_arr = np.concatenate((events_arr, temp))\n",
        "\n",
        "# Вчитување на редоследот на објекти кои се target\n",
        "targets_arr = np.empty(0)\n",
        "for i in range(1, 2):\n",
        "  file_name = 'SBJ' + format(i, '02')\n",
        "  for j in range(1, 4):\n",
        "    file_train_set = 'S' + format(j, '02') + '-Train'\n",
        "    full_path = 'drive/My Drive/Интелигентни Системи/Data/' + file_name + \"/\" + file_train_set + \"/trainTargets.txt\"\n",
        "    with open(full_path, \"r\") as file_targets:\n",
        "      temp = file_targets.read().splitlines()\n",
        "      targets_arr = np.concatenate((targets_arr, temp))\n"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"epsilon\": 1e-07, \n",
            "    \"floatx\": \"float32\", \n",
            "    \"image_data_format\": \"channels_last\", \n",
            "    \"backend\": \"tensorflow\"\n",
            "}\n",
            "13440000\n",
            "(8, 350, 4800)\n",
            "['6' '6' '6' ... '3' '3' '3']\n",
            "4800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwFUCIcUnKgS",
        "colab_type": "code",
        "outputId": "87fc1605-2b15-41aa-9bb8-74838b55d0a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 350, 4800)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_6RGx7K4xkv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82e1dc1e-2c06-492e-8243-1442d9e9c4be"
      },
      "source": [
        "#data = target_events_data_scaled\n",
        "mne_array = np.swapaxes(data, 0, 2) # (епохa, канал, настан). \n",
        "mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан).\n",
        "mne_array = mne_array.reshape(mne_array.shape[0],1, 8,350)\n",
        "print(mne_array.shape)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4800, 1, 8, 350)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZpAD8zt4zrd",
        "colab_type": "code",
        "outputId": "93cce925-8756-4237-992e-7edbef7e0db2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from EEGModels import EEGNet,ShallowConvNet,DeepConvNet\n",
        "events_arr = events_arr.astype(np.int)\n",
        "labels_arr = labels_arr.astype(np.int)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "#mne_array = data.reshape(2800, data.shape[-1])\n",
        "#mne_array = np.swapaxes(mne_array,0,1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(mne_array, labels_arr-1, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "model = DeepConvNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "model.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = Adam(0.003))\n",
        "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "#clf = RandomForestClassifier(max_depth=5)\n",
        "#clf.fit(X_train, y_train)\n",
        "#score = clf.score(X_test, y_test)\n",
        "#print(score)\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "num_batch_size=100\n",
        "num_epochs=400\n",
        "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "          validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(score)\n",
        "\n",
        "#model = Sequential()\n",
        "#model.add(Dense(2800, input_dim=2800), activation='relu')\n",
        "#model.add(Dense(350, activation='sigmoid'))\n",
        "#model.add(Dense(350, activation='sigmoid'))\n",
        "#model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#clf = svm.SVC()\n",
        "#clf = RandomForestClassifier(random_state=0, n_estimators=350)\n",
        "#clf = LinearDiscriminantAnalysis()\n",
        "#clf.fit(X_train, y_train)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3600 samples, validate on 1200 samples\n",
            "Epoch 1/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.3496 - acc: 0.1786\n",
            "Epoch 00001: val_loss improved from inf to 2.14333, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 4s 1ms/sample - loss: 2.3426 - acc: 0.1786 - val_loss: 2.1433 - val_acc: 0.1175\n",
            "Epoch 2/400\n",
            "3400/3600 [===========================>..] - ETA: 0s - loss: 2.1659 - acc: 0.1832\n",
            "Epoch 00002: val_loss did not improve from 2.14333\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 2.1665 - acc: 0.1808 - val_loss: 2.2743 - val_acc: 0.1000\n",
            "Epoch 3/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.1431 - acc: 0.1852\n",
            "Epoch 00003: val_loss did not improve from 2.14333\n",
            "3600/3600 [==============================] - 1s 174us/sample - loss: 2.1398 - acc: 0.1864 - val_loss: 2.1487 - val_acc: 0.1308\n",
            "Epoch 4/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 2.0876 - acc: 0.1969\n",
            "Epoch 00004: val_loss improved from 2.14333 to 2.10147, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 189us/sample - loss: 2.0863 - acc: 0.1989 - val_loss: 2.1015 - val_acc: 0.2058\n",
            "Epoch 5/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.0862 - acc: 0.1888\n",
            "Epoch 00005: val_loss did not improve from 2.10147\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 2.0847 - acc: 0.1900 - val_loss: 2.1878 - val_acc: 0.1458\n",
            "Epoch 6/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.0873 - acc: 0.2000\n",
            "Epoch 00006: val_loss did not improve from 2.10147\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 2.0915 - acc: 0.2017 - val_loss: 2.1892 - val_acc: 0.2125\n",
            "Epoch 7/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.0053 - acc: 0.2291\n",
            "Epoch 00007: val_loss improved from 2.10147 to 1.90837, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 191us/sample - loss: 1.9958 - acc: 0.2303 - val_loss: 1.9084 - val_acc: 0.2225\n",
            "Epoch 8/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.9262 - acc: 0.2394\n",
            "Epoch 00008: val_loss did not improve from 1.90837\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 1.9235 - acc: 0.2411 - val_loss: 2.0043 - val_acc: 0.1983\n",
            "Epoch 9/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.8584 - acc: 0.2655\n",
            "Epoch 00009: val_loss did not improve from 1.90837\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 1.8690 - acc: 0.2622 - val_loss: 1.9651 - val_acc: 0.1983\n",
            "Epoch 10/400\n",
            " 500/3600 [===>..........................] - ETA: 0s - loss: 1.9299 - acc: 0.2660"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-9fd522a88de2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mnum_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaXHzm6wPMEv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "b5d79991-8caf-4b89-8616-7a4dd969cfce"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow import convert_to_tensor\n",
        "np.random.seed(7)\n",
        "embedding_vecor_length = 8\n",
        "model = Sequential()\n",
        "data_ltsm = np.swapaxes(data,0,1)\n",
        "data_ltsm = convert_to_tensor(data_ltsm)\n",
        "print(data_ltsm.shape)\n",
        "model.add(LSTM(return_sequences=False,units= 8))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "X_trainl, X_testl, y_trainl, y_testl = train_test_split(mne_array, labels_arr-1, test_size=0.25, random_state=42)\n",
        "y_trainl = to_categorical(y_trainl)\n",
        "y_testl = to_categorical(y_testl)\n",
        "X_trainl=np.swapaxes(X_trainl,1,2)\n",
        "X_testl=np.swapaxes(X_testl,1,2)\n",
        "model.fit(X_trainl[:,:,:,0], y_trainl,epochs=400, batch_size=350)\n",
        "#Final evaluation of the model\n",
        "scores = model.evaluate(X_testl, y_testl, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(350, 8, 4800)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-162-36db4f49c86f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mX_trainl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trainl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mX_testl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_testl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trainl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trainl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m350\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m#Final evaluation of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_testl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_testl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_37 to have shape (1,) but got array with shape (8,)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVrfvU9u-DHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(y_train);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmSE4LNT5Dhv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = clf.score(X_test, y_test)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-JYssfepBMvl",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "print(data.shape)\n",
        "#mne_array = data.reshape([target_events_data.shape[2],1,8, 350])\n",
        "print(mne_array.shape)\n",
        "#stft = mne.time_frequency.stft(mne_array[0], 24)\n",
        "\n",
        "# Number of sample points\n",
        "\n",
        "FS = 250\n",
        "N = 350\n",
        "# sample spacing\n",
        "T = 1.0 / FS\n",
        "x = np.linspace(0.0, N*T, N)\n",
        "num_labels= 8\n",
        "#plt.plot(mne_array[2][2], np.linspace(0.0,N,N))\n",
        "\n",
        "#fig, axs = plt.subplots(8,8, figsize=(4*8,4*8))\n",
        "#freq_data = list()\n",
        "#for i in range(8):\n",
        "#  for x in range(8):\n",
        "#    f, t, Sxx = signal.spectrogram(mne_array[x][i], fs=FS, nperseg=50, window=('hamming'), noverlap=35)\n",
        "#    freq_data.append(Sxx)\n",
        "#    axs[i,x].pcolormesh(t, f, Sxx)\n",
        "#plt.show()\n",
        "#print(open(\"~/keras/keras.json\").read())\n",
        "\n",
        "#events_arr = list(map(int, events_arr))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#y_train = np.array(y_train)\n",
        "#y_train = to_categorical(y_train)\n",
        "#y_test = np.array(y_test)\n",
        "#y_test = to_categorical(y_test)\n",
        "# Construct model \n",
        "\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "num_epochs = 50\n",
        "num_batch_size = 100\n",
        "\n",
        "clf = LinearDiscriminantAnalysis()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "score = clf.score(X_test, y_test)\n",
        "print(score)\n",
        "\n",
        "#preds = model.predict(X_test, verbose=0)\n",
        "#acc = accuracy_score(y_test, preds)\n",
        "\n",
        "print(\"accuracy_score: {f}\".format(f=score))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ovrlCYuqKIT4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6abb0307-9c47-4a59-a514-e5a9bb1c7faa"
      },
      "source": [
        "#@title 1st objest, target vs non target\n",
        "# Вчитување на податоците\n",
        "data = loadmat('drive/My Drive/Интелигентни Системи/Data/SBJ01/S01-Train/trainData.mat')['trainData'] \n",
        "\n",
        "# Вчитување на label-ите\n",
        "labels_arr = []\n",
        "with open(\"drive/My Drive/Интелигентни Системи/Data/SBJ01/S01-Train/trainLabels.txt\", \"r\") as file_labels:\n",
        "    labels_arr = file_labels.read().splitlines()\n",
        "\n",
        "# Вчитување на редоследот на светкање\n",
        "events_arr = []\n",
        "with open(\"drive/My Drive/Интелигентни Системи/Data/SBJ01/S01-Train/trainEvents.txt\", \"r\") as file_events:\n",
        "    events_arr = file_events.read().splitlines()\n",
        "\n",
        "# Вчитување на редоследот на објекти кои се target\n",
        "targets_arr = []\n",
        "with open(\"drive/My Drive/Интелигентни Системи/Data/SBJ01/S01-Train/trainTargets.txt\", \"r\") as file_targets:\n",
        "    targets_arr = file_targets.read().splitlines()\n",
        "\n",
        "# Прилагодување на податоците за користење со mne библиотеката\n",
        "ch_names = [\"C3\", \"Cz\", \"C4\", \"CPz\", \"P3\", \"Pz\", \"P4\", \"POz\"]\n",
        "ch_types = ['eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg']\n",
        "mne_info = mne.create_info(ch_names=ch_names, sfreq=250, ch_types=ch_types)\n",
        "#mne_array = np.swapaxes(data, 0, 2) # (епохa, канал, настан). \n",
        "#mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан). \n",
        "#raw_data = mne.epochs.EpochsArray(mne_array, mne_info)\n",
        "\n",
        "# Извлекување на настаните каде светнал првиот објект и бил target.\n",
        "first_object_events_target = [index for index, value in enumerate(events_arr) if value == '1']\n",
        "first_object_non_target = [index for index, value in enumerate(events_arr) if value == '1']\n",
        "\n",
        "for event_pos in first_object_events_target:\n",
        "  if targets_arr[event_pos] == 1:\n",
        "    first_object_non_target.remove(event_pos)\n",
        "    continue # Продолжи\n",
        "  else:\n",
        "    first_object_events_target.remove(event_pos) # Избриши -> Објектот не е target\n",
        "first_object_target_eeg_data = np.zeros((8,350, len(first_object_events_target)))\n",
        "first_object_non_target_eeg_data = np.zeros((8,350, len(first_object_non_target)))\n",
        "for channel in range(0, 8): # Секој канал\n",
        "  for epoch in range(0, 350): # Секоја епоха\n",
        "    i = 0\n",
        "    for event in first_object_events_target: # Настан\n",
        "      first_object_target_eeg_data[channel][epoch][i] = data[channel][epoch][event]\n",
        "      i = i+1\n",
        "    \n",
        "    k=0\n",
        "    for event in first_object_non_target: # Настан\n",
        "      first_object_non_target_eeg_data[channel][epoch][i] = data[channel][epoch][event]\n",
        "      k = k+1\n",
        "\n",
        "print(first_object_target_eeg_data.shape)\n",
        "print(first_object_non_target_eeg_data.shape)\n",
        "\n",
        "first_object_target_eeg_data = np.mean(first_object_target_eeg_data, axis=2)\n",
        "first_object_non_target_eeg_data = np.mean(first_object_non_target_eeg_data, axis=2)\n",
        "\n",
        "fig, axs = plt.subplots(8,2,figsize=(16,16*4))\n",
        "for i in range(8):\n",
        "  f, t, Sxx = signal.spectrogram(first_object_target_eeg_data[i], fs=FS, nperseg=50, window=('hamming'), noverlap=35)\n",
        "  fnon, tnon, Sxxnon = signal.spectrogram(first_object_non_target_eeg_data[i], fs=FS, nperseg=50, window=('hamming'), noverlap=35)\n",
        "\n",
        "  axs[i,0].pcolormesh(t, f, Sxx)\n",
        "  axs[i,1].pcolormesh(tnon, fnon, Sxxnon)\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8, 350, 100)\n",
            "(8, 350, 200)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-87ab52a20596>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m   \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_object_target_eeg_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnperseg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hamming'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoverlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0mfnon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtnon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSxxnon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_object_non_target_eeg_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnperseg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hamming'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoverlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'FS' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x4608 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kitlqn__dEi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "outputId": "4c02869d-25b1-488d-b37c-ea213b029d7a"
      },
      "source": [
        "ch_names = [\"C3\", \"Cz\", \"C4\", \"CPz\", \"P3\", \"Pz\", \"P4\", \"POz\"]\n",
        "ch_types = ['eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg']\n",
        "mne_info = mne.create_info(ch_names=ch_names, sfreq=250, ch_types=ch_types)\n",
        "mne_array_clean = np.swapaxes(data, 0, 1)\n",
        "raw_data_clean = mne.epochs.EpochsArray(mne_array_clean, mne_info)\n",
        "Min, Max = round(mne_array_clean.min()),round(mne_array_clean.max())*1000000\n",
        "picks = mne.pick_types(raw_data_clean.info, meg=False, eeg=True, stim=False, eog=False)\n",
        "xd = mne.preprocessing.Xdawn(n_components=2, signal_cov=None)\n",
        "xd.fit(raw_data_clean)\n",
        "epochs_denoised = xd.apply(raw_data_clean)\n",
        "epochs_denoised.keys()\n",
        "mne.viz.plot_epochs_image(epochs_denoised['1'], picks='eeg', vmin=Min, vmax=Max)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "350 matching events found\n",
            "No baseline correction applied\n",
            "Not setting metadata\n",
            "0 projection items activated\n",
            "0 bad epochs dropped\n",
            "Computing rank from data with rank='full'\n",
            "    EEG: rank 8 from info\n",
            "Reducing data rank from 8 -> 8\n",
            "Estimating covariance using EMPIRICAL\n",
            "Done.\n",
            "Computing rank from data with rank='full'\n",
            "    EEG: rank 8 from info\n",
            "Reducing data rank from 8 -> 8\n",
            "Estimating covariance using EMPIRICAL\n",
            "Done.\n",
            "Transforming to Xdawn space\n",
            "Zeroing out 6 Xdawn components\n",
            "Inverse transforming to sensor space\n",
            "350 matching events found\n",
            "No baseline correction applied\n",
            "Not setting metadata\n",
            "0 projection items activated\n",
            "0 bad epochs dropped\n",
            "combining channels using \"gfp\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<Figure size 432x288 with 3 Axes>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT2HF8aKDN81",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "02396692-ce03-4039-b528-23aeaef91574"
      },
      "source": [
        "array_test=np.swapaxes(epochs_denoised['1'],0,0)\n",
        "array_test=np.swapaxes(array_test,0,2)\n",
        "array_test=array_test.reshape(array_test.shape[0],1,8,350)\n",
        "print(array_test.shape)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4800, 1, 8, 350)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nJEQxG5C3id",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9de6c1f4-31bb-4b6e-c9b5-f34e89ccd384"
      },
      "source": [
        "print(np.swapaxes(epochs_denoised['1'],0,0).shape)\n",
        "standardizer = mne.decoding.Scaler(scalings='mean')\n",
        "standardizer.fit(np.swapaxes(epochs_denoised['1'],0,0))\n",
        "standardized_data = standardizer.transform(np.swapaxes(epochs_denoised['1'],0,0))\n",
        "print(standardized_data.shape)\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(350, 8, 4800)\n",
            "(350, 8, 4800)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e4d727ed-402b-424f-99bb-fab6ac217934",
        "id": "0sxmE4OcFlIe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "standardizer = mne.decoding.Scaler(scalings='mean')\n",
        "#print(np.swapaxes(data,0,1).shape)\n",
        "standardizer.fit(data)\n",
        "standardized_data = standardizer.transform(data)\n",
        "print(standardized_data.shape)\n",
        "mne_array = np.swapaxes(standardized_data, 0, 2) # (епохa, канал, настан). \n",
        "mne_array = np.swapaxes(mne_array, 1, 2) # (епохa, канал, настан).\n",
        "mne_array = mne_array.reshape(mne_array.shape[0],1, 8,350)\n",
        "print(mne_array.shape)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8, 350, 4800)\n",
            "(4800, 1, 8, 350)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6ILVN_zGHRe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6881e89-cf70-4739-ff26-b08c807dbd21"
      },
      "source": [
        "standardized_data.shape\n",
        "standardized_data=np.swapaxes(standardized_data,0,2)\n",
        "standardized_data=standardized_data.reshape(standardized_data.shape[0],1,8,350)\n",
        "print(standardized_data.shape)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4800, 1, 8, 350)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRiY4zWo8zab",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2ce8f9d9-9f7e-43a2-c0a0-a69c36141fe2"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(mne_array, labels_arr-1, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "model = DeepConvNet(nb_classes = 8, Chans = 8, Samples = 350)\n",
        "model.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = Adam(0.003))\n",
        "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "#clf = RandomForestClassifier(max_depth=5)\n",
        "#clf.fit(X_train, y_train)\n",
        "#score = clf.score(X_test, y_test)\n",
        "#print(score)\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "num_batch_size=100\n",
        "num_epochs=400\n",
        "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "          validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(score)\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3600 samples, validate on 1200 samples\n",
            "Epoch 1/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 2.4846 - acc: 0.1717\n",
            "Epoch 00001: val_loss improved from inf to 2.58751, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 3s 726us/sample - loss: 2.4772 - acc: 0.1733 - val_loss: 2.5875 - val_acc: 0.1058\n",
            "Epoch 2/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.2323 - acc: 0.1764\n",
            "Epoch 00002: val_loss improved from 2.58751 to 2.12061, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 2.2270 - acc: 0.1767 - val_loss: 2.1206 - val_acc: 0.1750\n",
            "Epoch 3/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.2768 - acc: 0.1639\n",
            "Epoch 00003: val_loss did not improve from 2.12061\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 2.2611 - acc: 0.1672 - val_loss: 2.1614 - val_acc: 0.2067\n",
            "Epoch 4/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.2078 - acc: 0.1924\n",
            "Epoch 00004: val_loss improved from 2.12061 to 2.06396, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 2.2039 - acc: 0.1911 - val_loss: 2.0640 - val_acc: 0.1675\n",
            "Epoch 5/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.1327 - acc: 0.1876\n",
            "Epoch 00005: val_loss improved from 2.06396 to 2.05707, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 181us/sample - loss: 2.1295 - acc: 0.1847 - val_loss: 2.0571 - val_acc: 0.1758\n",
            "Epoch 6/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.0644 - acc: 0.1973\n",
            "Epoch 00006: val_loss did not improve from 2.05707\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 2.0703 - acc: 0.1964 - val_loss: 2.0797 - val_acc: 0.1667\n",
            "Epoch 7/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.0580 - acc: 0.2152\n",
            "Epoch 00007: val_loss did not improve from 2.05707\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 2.0547 - acc: 0.2153 - val_loss: 2.2017 - val_acc: 0.1725\n",
            "Epoch 8/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 2.0160 - acc: 0.2206\n",
            "Epoch 00008: val_loss did not improve from 2.05707\n",
            "3600/3600 [==============================] - 1s 166us/sample - loss: 2.0133 - acc: 0.2164 - val_loss: 2.1224 - val_acc: 0.1300\n",
            "Epoch 9/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.9598 - acc: 0.2336\n",
            "Epoch 00009: val_loss improved from 2.05707 to 1.93903, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 1.9553 - acc: 0.2353 - val_loss: 1.9390 - val_acc: 0.2508\n",
            "Epoch 10/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.8767 - acc: 0.2552\n",
            "Epoch 00010: val_loss improved from 1.93903 to 1.80548, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 1.8705 - acc: 0.2553 - val_loss: 1.8055 - val_acc: 0.2733\n",
            "Epoch 11/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.7912 - acc: 0.2785\n",
            "Epoch 00011: val_loss improved from 1.80548 to 1.78531, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 180us/sample - loss: 1.7931 - acc: 0.2789 - val_loss: 1.7853 - val_acc: 0.2908\n",
            "Epoch 12/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.7932 - acc: 0.2936\n",
            "Epoch 00012: val_loss did not improve from 1.78531\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 1.7882 - acc: 0.2944 - val_loss: 1.7877 - val_acc: 0.2758\n",
            "Epoch 13/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.7477 - acc: 0.3048\n",
            "Epoch 00013: val_loss improved from 1.78531 to 1.64885, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 181us/sample - loss: 1.7428 - acc: 0.3083 - val_loss: 1.6489 - val_acc: 0.3192\n",
            "Epoch 14/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.6472 - acc: 0.3306\n",
            "Epoch 00014: val_loss improved from 1.64885 to 1.64336, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 1.6454 - acc: 0.3333 - val_loss: 1.6434 - val_acc: 0.3700\n",
            "Epoch 15/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.6208 - acc: 0.3472\n",
            "Epoch 00015: val_loss improved from 1.64336 to 1.59172, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 1.6303 - acc: 0.3450 - val_loss: 1.5917 - val_acc: 0.3458\n",
            "Epoch 16/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.5902 - acc: 0.3591\n",
            "Epoch 00016: val_loss did not improve from 1.59172\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 1.5896 - acc: 0.3572 - val_loss: 1.6001 - val_acc: 0.3292\n",
            "Epoch 17/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.5836 - acc: 0.3676\n",
            "Epoch 00017: val_loss improved from 1.59172 to 1.58988, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 180us/sample - loss: 1.5807 - acc: 0.3683 - val_loss: 1.5899 - val_acc: 0.3925\n",
            "Epoch 18/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.5275 - acc: 0.3706\n",
            "Epoch 00018: val_loss improved from 1.58988 to 1.51129, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 181us/sample - loss: 1.5287 - acc: 0.3742 - val_loss: 1.5113 - val_acc: 0.3942\n",
            "Epoch 19/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.5031 - acc: 0.3912\n",
            "Epoch 00019: val_loss improved from 1.51129 to 1.45962, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 1.5038 - acc: 0.3900 - val_loss: 1.4596 - val_acc: 0.3983\n",
            "Epoch 20/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.4553 - acc: 0.4182\n",
            "Epoch 00020: val_loss did not improve from 1.45962\n",
            "3600/3600 [==============================] - 1s 166us/sample - loss: 1.4661 - acc: 0.4136 - val_loss: 1.5134 - val_acc: 0.4017\n",
            "Epoch 21/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.4826 - acc: 0.4042\n",
            "Epoch 00021: val_loss improved from 1.45962 to 1.43879, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 1.4824 - acc: 0.4050 - val_loss: 1.4388 - val_acc: 0.4233\n",
            "Epoch 22/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.4632 - acc: 0.4136\n",
            "Epoch 00022: val_loss did not improve from 1.43879\n",
            "3600/3600 [==============================] - 1s 166us/sample - loss: 1.4589 - acc: 0.4161 - val_loss: 1.4857 - val_acc: 0.3850\n",
            "Epoch 23/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.4546 - acc: 0.4152\n",
            "Epoch 00023: val_loss improved from 1.43879 to 1.42756, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 1.4450 - acc: 0.4197 - val_loss: 1.4276 - val_acc: 0.4317\n",
            "Epoch 24/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.4562 - acc: 0.4103\n",
            "Epoch 00024: val_loss did not improve from 1.42756\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 1.4575 - acc: 0.4094 - val_loss: 1.4335 - val_acc: 0.4283\n",
            "Epoch 25/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.4515 - acc: 0.4112\n",
            "Epoch 00025: val_loss improved from 1.42756 to 1.41665, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.4513 - acc: 0.4114 - val_loss: 1.4166 - val_acc: 0.4192\n",
            "Epoch 26/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.4104 - acc: 0.4321\n",
            "Epoch 00026: val_loss improved from 1.41665 to 1.33427, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 181us/sample - loss: 1.4142 - acc: 0.4319 - val_loss: 1.3343 - val_acc: 0.4667\n",
            "Epoch 27/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.3923 - acc: 0.4397\n",
            "Epoch 00027: val_loss did not improve from 1.33427\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 1.4002 - acc: 0.4375 - val_loss: 1.4258 - val_acc: 0.4208\n",
            "Epoch 28/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.3772 - acc: 0.4439\n",
            "Epoch 00028: val_loss did not improve from 1.33427\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 1.3825 - acc: 0.4408 - val_loss: 1.3756 - val_acc: 0.4375\n",
            "Epoch 29/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.3543 - acc: 0.4645\n",
            "Epoch 00029: val_loss did not improve from 1.33427\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 1.3614 - acc: 0.4617 - val_loss: 1.3518 - val_acc: 0.4408\n",
            "Epoch 30/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.3836 - acc: 0.4406\n",
            "Epoch 00030: val_loss improved from 1.33427 to 1.33267, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 1.3742 - acc: 0.4456 - val_loss: 1.3327 - val_acc: 0.4550\n",
            "Epoch 31/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.3405 - acc: 0.4606\n",
            "Epoch 00031: val_loss improved from 1.33267 to 1.30380, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 1.3463 - acc: 0.4581 - val_loss: 1.3038 - val_acc: 0.4917\n",
            "Epoch 32/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.3708 - acc: 0.4582\n",
            "Epoch 00032: val_loss did not improve from 1.30380\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 1.3765 - acc: 0.4558 - val_loss: 1.3055 - val_acc: 0.4800\n",
            "Epoch 33/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.3246 - acc: 0.4688\n",
            "Epoch 00033: val_loss improved from 1.30380 to 1.28894, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 1.3201 - acc: 0.4717 - val_loss: 1.2889 - val_acc: 0.4642\n",
            "Epoch 34/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.3310 - acc: 0.4627\n",
            "Epoch 00034: val_loss improved from 1.28894 to 1.25873, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 1.3308 - acc: 0.4656 - val_loss: 1.2587 - val_acc: 0.5000\n",
            "Epoch 35/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.2980 - acc: 0.4836\n",
            "Epoch 00035: val_loss improved from 1.25873 to 1.22135, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 1.2997 - acc: 0.4856 - val_loss: 1.2214 - val_acc: 0.5067\n",
            "Epoch 36/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.2836 - acc: 0.4818\n",
            "Epoch 00036: val_loss did not improve from 1.22135\n",
            "3600/3600 [==============================] - 1s 166us/sample - loss: 1.2942 - acc: 0.4772 - val_loss: 1.3616 - val_acc: 0.4442\n",
            "Epoch 37/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.2699 - acc: 0.4827\n",
            "Epoch 00037: val_loss improved from 1.22135 to 1.20357, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.2725 - acc: 0.4831 - val_loss: 1.2036 - val_acc: 0.5217\n",
            "Epoch 38/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.2453 - acc: 0.5142\n",
            "Epoch 00038: val_loss did not improve from 1.20357\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 1.2518 - acc: 0.5108 - val_loss: 1.2725 - val_acc: 0.4933\n",
            "Epoch 39/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.2696 - acc: 0.5073\n",
            "Epoch 00039: val_loss did not improve from 1.20357\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 1.2678 - acc: 0.5056 - val_loss: 1.2529 - val_acc: 0.4992\n",
            "Epoch 40/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.2441 - acc: 0.5061\n",
            "Epoch 00040: val_loss did not improve from 1.20357\n",
            "3600/3600 [==============================] - 1s 166us/sample - loss: 1.2500 - acc: 0.5042 - val_loss: 1.2331 - val_acc: 0.5075\n",
            "Epoch 41/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.2506 - acc: 0.5091\n",
            "Epoch 00041: val_loss improved from 1.20357 to 1.17349, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 1.2485 - acc: 0.5072 - val_loss: 1.1735 - val_acc: 0.5417\n",
            "Epoch 42/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.2201 - acc: 0.5145\n",
            "Epoch 00042: val_loss improved from 1.17349 to 1.15635, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 1.2199 - acc: 0.5156 - val_loss: 1.1564 - val_acc: 0.5575\n",
            "Epoch 43/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.2177 - acc: 0.5091\n",
            "Epoch 00043: val_loss did not improve from 1.15635\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 1.2204 - acc: 0.5092 - val_loss: 1.1977 - val_acc: 0.5567\n",
            "Epoch 44/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.2068 - acc: 0.5188\n",
            "Epoch 00044: val_loss improved from 1.15635 to 1.14555, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 1.2038 - acc: 0.5217 - val_loss: 1.1455 - val_acc: 0.5433\n",
            "Epoch 45/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.2105 - acc: 0.5221\n",
            "Epoch 00045: val_loss did not improve from 1.14555\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 1.2101 - acc: 0.5244 - val_loss: 1.2709 - val_acc: 0.4758\n",
            "Epoch 46/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1810 - acc: 0.5379\n",
            "Epoch 00046: val_loss improved from 1.14555 to 1.13460, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 1.1792 - acc: 0.5367 - val_loss: 1.1346 - val_acc: 0.5650\n",
            "Epoch 47/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1789 - acc: 0.5400\n",
            "Epoch 00047: val_loss improved from 1.13460 to 1.11709, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 1.1836 - acc: 0.5389 - val_loss: 1.1171 - val_acc: 0.5575\n",
            "Epoch 48/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1737 - acc: 0.5515\n",
            "Epoch 00048: val_loss improved from 1.11709 to 1.04864, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 1.1757 - acc: 0.5511 - val_loss: 1.0486 - val_acc: 0.6000\n",
            "Epoch 49/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1631 - acc: 0.5497\n",
            "Epoch 00049: val_loss did not improve from 1.04864\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 1.1705 - acc: 0.5461 - val_loss: 1.1205 - val_acc: 0.5475\n",
            "Epoch 50/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1641 - acc: 0.5370\n",
            "Epoch 00050: val_loss did not improve from 1.04864\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 1.1695 - acc: 0.5342 - val_loss: 1.1613 - val_acc: 0.5292\n",
            "Epoch 51/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1501 - acc: 0.5436\n",
            "Epoch 00051: val_loss did not improve from 1.04864\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 1.1494 - acc: 0.5456 - val_loss: 1.0944 - val_acc: 0.5567\n",
            "Epoch 52/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1387 - acc: 0.5476\n",
            "Epoch 00052: val_loss did not improve from 1.04864\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 1.1472 - acc: 0.5464 - val_loss: 1.0632 - val_acc: 0.5817\n",
            "Epoch 53/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1226 - acc: 0.5588\n",
            "Epoch 00053: val_loss did not improve from 1.04864\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 1.1255 - acc: 0.5569 - val_loss: 1.0511 - val_acc: 0.5742\n",
            "Epoch 54/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.1281 - acc: 0.5663\n",
            "Epoch 00054: val_loss did not improve from 1.04864\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 1.1316 - acc: 0.5603 - val_loss: 1.1049 - val_acc: 0.5567\n",
            "Epoch 55/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1256 - acc: 0.5676\n",
            "Epoch 00055: val_loss improved from 1.04864 to 1.04415, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 1.1332 - acc: 0.5642 - val_loss: 1.0442 - val_acc: 0.5767\n",
            "Epoch 56/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1461 - acc: 0.5548\n",
            "Epoch 00056: val_loss did not improve from 1.04415\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 1.1464 - acc: 0.5539 - val_loss: 1.1061 - val_acc: 0.5900\n",
            "Epoch 57/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1187 - acc: 0.5700\n",
            "Epoch 00057: val_loss improved from 1.04415 to 1.02048, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 1.1199 - acc: 0.5672 - val_loss: 1.0205 - val_acc: 0.5992\n",
            "Epoch 58/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1244 - acc: 0.5636\n",
            "Epoch 00058: val_loss did not improve from 1.02048\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 1.1284 - acc: 0.5644 - val_loss: 1.0466 - val_acc: 0.6100\n",
            "Epoch 59/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0782 - acc: 0.5830\n",
            "Epoch 00059: val_loss did not improve from 1.02048\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 1.0834 - acc: 0.5822 - val_loss: 1.0329 - val_acc: 0.6108\n",
            "Epoch 60/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1021 - acc: 0.5624\n",
            "Epoch 00060: val_loss did not improve from 1.02048\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 1.1108 - acc: 0.5614 - val_loss: 1.0442 - val_acc: 0.6033\n",
            "Epoch 61/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0949 - acc: 0.5727\n",
            "Epoch 00061: val_loss improved from 1.02048 to 1.00381, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 1.0890 - acc: 0.5767 - val_loss: 1.0038 - val_acc: 0.6283\n",
            "Epoch 62/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1009 - acc: 0.5679\n",
            "Epoch 00062: val_loss did not improve from 1.00381\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 1.0985 - acc: 0.5700 - val_loss: 1.0923 - val_acc: 0.5675\n",
            "Epoch 63/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0689 - acc: 0.5788\n",
            "Epoch 00063: val_loss did not improve from 1.00381\n",
            "3600/3600 [==============================] - 1s 174us/sample - loss: 1.0777 - acc: 0.5786 - val_loss: 1.1199 - val_acc: 0.5742\n",
            "Epoch 64/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0847 - acc: 0.5845\n",
            "Epoch 00064: val_loss improved from 1.00381 to 1.00321, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 181us/sample - loss: 1.0915 - acc: 0.5817 - val_loss: 1.0032 - val_acc: 0.6233\n",
            "Epoch 65/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.1030 - acc: 0.5776\n",
            "Epoch 00065: val_loss improved from 1.00321 to 0.97921, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 181us/sample - loss: 1.1027 - acc: 0.5808 - val_loss: 0.9792 - val_acc: 0.6250\n",
            "Epoch 66/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0673 - acc: 0.5736\n",
            "Epoch 00066: val_loss did not improve from 0.97921\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 1.0672 - acc: 0.5767 - val_loss: 1.0269 - val_acc: 0.5975\n",
            "Epoch 67/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.0455 - acc: 0.5956\n",
            "Epoch 00067: val_loss improved from 0.97921 to 0.92396, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 1.0460 - acc: 0.5972 - val_loss: 0.9240 - val_acc: 0.6592\n",
            "Epoch 68/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0872 - acc: 0.5812\n",
            "Epoch 00068: val_loss did not improve from 0.92396\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 1.0929 - acc: 0.5783 - val_loss: 0.9309 - val_acc: 0.6550\n",
            "Epoch 69/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0551 - acc: 0.5909\n",
            "Epoch 00069: val_loss did not improve from 0.92396\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 1.0608 - acc: 0.5894 - val_loss: 0.9765 - val_acc: 0.6325\n",
            "Epoch 70/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0613 - acc: 0.5979\n",
            "Epoch 00070: val_loss did not improve from 0.92396\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 1.0629 - acc: 0.5975 - val_loss: 1.0153 - val_acc: 0.6183\n",
            "Epoch 71/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0886 - acc: 0.5803\n",
            "Epoch 00071: val_loss did not improve from 0.92396\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 1.0801 - acc: 0.5836 - val_loss: 1.0141 - val_acc: 0.5950\n",
            "Epoch 72/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0611 - acc: 0.5864\n",
            "Epoch 00072: val_loss did not improve from 0.92396\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 1.0650 - acc: 0.5886 - val_loss: 1.0246 - val_acc: 0.6017\n",
            "Epoch 73/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0423 - acc: 0.5945\n",
            "Epoch 00073: val_loss did not improve from 0.92396\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 1.0499 - acc: 0.5911 - val_loss: 0.9878 - val_acc: 0.6292\n",
            "Epoch 74/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0256 - acc: 0.5964\n",
            "Epoch 00074: val_loss did not improve from 0.92396\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 1.0279 - acc: 0.5958 - val_loss: 1.0085 - val_acc: 0.5933\n",
            "Epoch 75/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0683 - acc: 0.5894\n",
            "Epoch 00075: val_loss did not improve from 0.92396\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 1.0646 - acc: 0.5925 - val_loss: 0.9687 - val_acc: 0.6350\n",
            "Epoch 76/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0020 - acc: 0.6100\n",
            "Epoch 00076: val_loss improved from 0.92396 to 0.90415, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 1.0016 - acc: 0.6114 - val_loss: 0.9041 - val_acc: 0.6567\n",
            "Epoch 77/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0363 - acc: 0.5988\n",
            "Epoch 00077: val_loss did not improve from 0.90415\n",
            "3600/3600 [==============================] - 1s 166us/sample - loss: 1.0433 - acc: 0.5956 - val_loss: 0.9521 - val_acc: 0.6425\n",
            "Epoch 78/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0167 - acc: 0.6145\n",
            "Epoch 00078: val_loss did not improve from 0.90415\n",
            "3600/3600 [==============================] - 1s 166us/sample - loss: 1.0139 - acc: 0.6153 - val_loss: 1.0329 - val_acc: 0.5800\n",
            "Epoch 79/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0426 - acc: 0.5991\n",
            "Epoch 00079: val_loss did not improve from 0.90415\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 1.0439 - acc: 0.5978 - val_loss: 0.9721 - val_acc: 0.6242\n",
            "Epoch 80/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.9921 - acc: 0.6162\n",
            "Epoch 00080: val_loss did not improve from 0.90415\n",
            "3600/3600 [==============================] - 1s 174us/sample - loss: 1.0015 - acc: 0.6106 - val_loss: 1.0643 - val_acc: 0.5642\n",
            "Epoch 81/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0365 - acc: 0.6082\n",
            "Epoch 00081: val_loss did not improve from 0.90415\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 1.0352 - acc: 0.6100 - val_loss: 0.9581 - val_acc: 0.6317\n",
            "Epoch 82/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0255 - acc: 0.6085\n",
            "Epoch 00082: val_loss did not improve from 0.90415\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 1.0280 - acc: 0.6067 - val_loss: 1.0115 - val_acc: 0.6067\n",
            "Epoch 83/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.0123 - acc: 0.6097\n",
            "Epoch 00083: val_loss did not improve from 0.90415\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 1.0117 - acc: 0.6097 - val_loss: 0.9506 - val_acc: 0.6392\n",
            "Epoch 84/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9880 - acc: 0.6303\n",
            "Epoch 00084: val_loss improved from 0.90415 to 0.86828, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.9920 - acc: 0.6278 - val_loss: 0.8683 - val_acc: 0.6833\n",
            "Epoch 85/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0250 - acc: 0.6142\n",
            "Epoch 00085: val_loss did not improve from 0.86828\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 1.0307 - acc: 0.6081 - val_loss: 0.9705 - val_acc: 0.6083\n",
            "Epoch 86/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0212 - acc: 0.6058\n",
            "Epoch 00086: val_loss did not improve from 0.86828\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 1.0227 - acc: 0.6075 - val_loss: 0.9620 - val_acc: 0.6375\n",
            "Epoch 87/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.0188 - acc: 0.6069\n",
            "Epoch 00087: val_loss did not improve from 0.86828\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 1.0211 - acc: 0.6014 - val_loss: 0.9986 - val_acc: 0.6292\n",
            "Epoch 88/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.9980 - acc: 0.6075\n",
            "Epoch 00088: val_loss did not improve from 0.86828\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.9971 - acc: 0.6083 - val_loss: 0.9081 - val_acc: 0.6375\n",
            "Epoch 89/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.0061 - acc: 0.6034\n",
            "Epoch 00089: val_loss did not improve from 0.86828\n",
            "3600/3600 [==============================] - 1s 173us/sample - loss: 1.0071 - acc: 0.6022 - val_loss: 1.0016 - val_acc: 0.6167\n",
            "Epoch 90/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.9954 - acc: 0.6228\n",
            "Epoch 00090: val_loss did not improve from 0.86828\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 1.0080 - acc: 0.6178 - val_loss: 0.9169 - val_acc: 0.6608\n",
            "Epoch 91/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9743 - acc: 0.6273\n",
            "Epoch 00091: val_loss did not improve from 0.86828\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.9823 - acc: 0.6242 - val_loss: 0.9362 - val_acc: 0.6508\n",
            "Epoch 92/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9990 - acc: 0.6185\n",
            "Epoch 00092: val_loss did not improve from 0.86828\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 1.0006 - acc: 0.6161 - val_loss: 0.9076 - val_acc: 0.6675\n",
            "Epoch 93/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 1.0113 - acc: 0.6141\n",
            "Epoch 00093: val_loss did not improve from 0.86828\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 1.0108 - acc: 0.6144 - val_loss: 0.9085 - val_acc: 0.6517\n",
            "Epoch 94/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9663 - acc: 0.6327\n",
            "Epoch 00094: val_loss did not improve from 0.86828\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 0.9705 - acc: 0.6317 - val_loss: 1.0233 - val_acc: 0.6083\n",
            "Epoch 95/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 1.0006 - acc: 0.6039\n",
            "Epoch 00095: val_loss did not improve from 0.86828\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.9955 - acc: 0.6056 - val_loss: 0.8980 - val_acc: 0.6508\n",
            "Epoch 96/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9837 - acc: 0.6233\n",
            "Epoch 00096: val_loss did not improve from 0.86828\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.9811 - acc: 0.6267 - val_loss: 0.9339 - val_acc: 0.6458\n",
            "Epoch 97/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9775 - acc: 0.6212\n",
            "Epoch 00097: val_loss did not improve from 0.86828\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 0.9761 - acc: 0.6217 - val_loss: 0.9801 - val_acc: 0.6175\n",
            "Epoch 98/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9818 - acc: 0.6224\n",
            "Epoch 00098: val_loss did not improve from 0.86828\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.9823 - acc: 0.6233 - val_loss: 0.9820 - val_acc: 0.6058\n",
            "Epoch 99/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9741 - acc: 0.6288\n",
            "Epoch 00099: val_loss did not improve from 0.86828\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.9749 - acc: 0.6286 - val_loss: 0.9579 - val_acc: 0.6158\n",
            "Epoch 100/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9743 - acc: 0.6215\n",
            "Epoch 00100: val_loss did not improve from 0.86828\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.9719 - acc: 0.6239 - val_loss: 0.9243 - val_acc: 0.6508\n",
            "Epoch 101/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.9467 - acc: 0.6463\n",
            "Epoch 00101: val_loss did not improve from 0.86828\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.9515 - acc: 0.6436 - val_loss: 0.8689 - val_acc: 0.6783\n",
            "Epoch 102/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9555 - acc: 0.6364\n",
            "Epoch 00102: val_loss improved from 0.86828 to 0.85809, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.9549 - acc: 0.6375 - val_loss: 0.8581 - val_acc: 0.6892\n",
            "Epoch 103/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.9703 - acc: 0.6325\n",
            "Epoch 00103: val_loss did not improve from 0.85809\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.9750 - acc: 0.6281 - val_loss: 0.8764 - val_acc: 0.6775\n",
            "Epoch 104/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9268 - acc: 0.6509\n",
            "Epoch 00104: val_loss did not improve from 0.85809\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.9286 - acc: 0.6503 - val_loss: 0.8779 - val_acc: 0.6650\n",
            "Epoch 105/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9767 - acc: 0.6212\n",
            "Epoch 00105: val_loss did not improve from 0.85809\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.9712 - acc: 0.6233 - val_loss: 0.8831 - val_acc: 0.6733\n",
            "Epoch 106/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9457 - acc: 0.6345\n",
            "Epoch 00106: val_loss improved from 0.85809 to 0.83587, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.9529 - acc: 0.6328 - val_loss: 0.8359 - val_acc: 0.6950\n",
            "Epoch 107/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9443 - acc: 0.6188\n",
            "Epoch 00107: val_loss did not improve from 0.83587\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.9440 - acc: 0.6194 - val_loss: 0.8412 - val_acc: 0.6817\n",
            "Epoch 108/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9626 - acc: 0.6264\n",
            "Epoch 00108: val_loss did not improve from 0.83587\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.9654 - acc: 0.6233 - val_loss: 0.8736 - val_acc: 0.6575\n",
            "Epoch 109/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9449 - acc: 0.6376\n",
            "Epoch 00109: val_loss did not improve from 0.83587\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.9493 - acc: 0.6367 - val_loss: 0.8572 - val_acc: 0.6875\n",
            "Epoch 110/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9643 - acc: 0.6421\n",
            "Epoch 00110: val_loss did not improve from 0.83587\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.9574 - acc: 0.6419 - val_loss: 0.9144 - val_acc: 0.6383\n",
            "Epoch 111/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9359 - acc: 0.6461\n",
            "Epoch 00111: val_loss did not improve from 0.83587\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.9445 - acc: 0.6433 - val_loss: 0.9131 - val_acc: 0.6583\n",
            "Epoch 112/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9151 - acc: 0.6527\n",
            "Epoch 00112: val_loss did not improve from 0.83587\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.9201 - acc: 0.6494 - val_loss: 0.8683 - val_acc: 0.6592\n",
            "Epoch 113/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.9108 - acc: 0.6525\n",
            "Epoch 00113: val_loss did not improve from 0.83587\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.9189 - acc: 0.6486 - val_loss: 0.9402 - val_acc: 0.6092\n",
            "Epoch 114/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9165 - acc: 0.6533\n",
            "Epoch 00114: val_loss improved from 0.83587 to 0.83223, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.9156 - acc: 0.6539 - val_loss: 0.8322 - val_acc: 0.6783\n",
            "Epoch 115/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9719 - acc: 0.6297\n",
            "Epoch 00115: val_loss improved from 0.83223 to 0.82123, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.9785 - acc: 0.6283 - val_loss: 0.8212 - val_acc: 0.7100\n",
            "Epoch 116/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9433 - acc: 0.6482\n",
            "Epoch 00116: val_loss improved from 0.82123 to 0.81589, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.9427 - acc: 0.6475 - val_loss: 0.8159 - val_acc: 0.6917\n",
            "Epoch 117/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9381 - acc: 0.6430\n",
            "Epoch 00117: val_loss did not improve from 0.81589\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.9442 - acc: 0.6414 - val_loss: 0.8648 - val_acc: 0.6792\n",
            "Epoch 118/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9556 - acc: 0.6373\n",
            "Epoch 00118: val_loss did not improve from 0.81589\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.9592 - acc: 0.6339 - val_loss: 0.9308 - val_acc: 0.6650\n",
            "Epoch 119/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9200 - acc: 0.6585\n",
            "Epoch 00119: val_loss improved from 0.81589 to 0.80421, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.9176 - acc: 0.6578 - val_loss: 0.8042 - val_acc: 0.7192\n",
            "Epoch 120/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8929 - acc: 0.6712\n",
            "Epoch 00120: val_loss did not improve from 0.80421\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.9058 - acc: 0.6642 - val_loss: 0.8976 - val_acc: 0.6525\n",
            "Epoch 121/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9495 - acc: 0.6300\n",
            "Epoch 00121: val_loss improved from 0.80421 to 0.79169, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 181us/sample - loss: 0.9527 - acc: 0.6300 - val_loss: 0.7917 - val_acc: 0.6917\n",
            "Epoch 122/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.9214 - acc: 0.6497\n",
            "Epoch 00122: val_loss did not improve from 0.79169\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.9217 - acc: 0.6489 - val_loss: 0.8473 - val_acc: 0.6750\n",
            "Epoch 123/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9156 - acc: 0.6545\n",
            "Epoch 00123: val_loss did not improve from 0.79169\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.9160 - acc: 0.6539 - val_loss: 0.8108 - val_acc: 0.7067\n",
            "Epoch 124/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8967 - acc: 0.6630\n",
            "Epoch 00124: val_loss did not improve from 0.79169\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.9086 - acc: 0.6606 - val_loss: 0.9097 - val_acc: 0.6542\n",
            "Epoch 125/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9111 - acc: 0.6591\n",
            "Epoch 00125: val_loss did not improve from 0.79169\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.9102 - acc: 0.6592 - val_loss: 0.8802 - val_acc: 0.6758\n",
            "Epoch 126/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8902 - acc: 0.6652\n",
            "Epoch 00126: val_loss did not improve from 0.79169\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8935 - acc: 0.6633 - val_loss: 0.7991 - val_acc: 0.7050\n",
            "Epoch 127/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9270 - acc: 0.6455\n",
            "Epoch 00127: val_loss did not improve from 0.79169\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.9185 - acc: 0.6500 - val_loss: 0.8531 - val_acc: 0.6875\n",
            "Epoch 128/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9108 - acc: 0.6591\n",
            "Epoch 00128: val_loss did not improve from 0.79169\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 0.9131 - acc: 0.6572 - val_loss: 0.8187 - val_acc: 0.7050\n",
            "Epoch 129/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9023 - acc: 0.6648\n",
            "Epoch 00129: val_loss did not improve from 0.79169\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.9057 - acc: 0.6631 - val_loss: 0.8056 - val_acc: 0.6950\n",
            "Epoch 130/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9091 - acc: 0.6470\n",
            "Epoch 00130: val_loss did not improve from 0.79169\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.9151 - acc: 0.6469 - val_loss: 0.8465 - val_acc: 0.6975\n",
            "Epoch 131/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.9075 - acc: 0.6622\n",
            "Epoch 00131: val_loss improved from 0.79169 to 0.72926, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.9078 - acc: 0.6606 - val_loss: 0.7293 - val_acc: 0.7442\n",
            "Epoch 132/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.9113 - acc: 0.6631\n",
            "Epoch 00132: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.9117 - acc: 0.6606 - val_loss: 0.8400 - val_acc: 0.6917\n",
            "Epoch 133/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9163 - acc: 0.6552\n",
            "Epoch 00133: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 166us/sample - loss: 0.9192 - acc: 0.6539 - val_loss: 0.8416 - val_acc: 0.7017\n",
            "Epoch 134/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9158 - acc: 0.6542\n",
            "Epoch 00134: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.9173 - acc: 0.6517 - val_loss: 0.8343 - val_acc: 0.6792\n",
            "Epoch 135/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8922 - acc: 0.6527\n",
            "Epoch 00135: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8997 - acc: 0.6483 - val_loss: 0.8649 - val_acc: 0.6975\n",
            "Epoch 136/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9210 - acc: 0.6436\n",
            "Epoch 00136: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 0.9205 - acc: 0.6458 - val_loss: 0.7830 - val_acc: 0.7258\n",
            "Epoch 137/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8956 - acc: 0.6627\n",
            "Epoch 00137: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8910 - acc: 0.6650 - val_loss: 0.9296 - val_acc: 0.6408\n",
            "Epoch 138/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9093 - acc: 0.6503\n",
            "Epoch 00138: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.9116 - acc: 0.6489 - val_loss: 0.8899 - val_acc: 0.6933\n",
            "Epoch 139/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8577 - acc: 0.6842\n",
            "Epoch 00139: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8649 - acc: 0.6819 - val_loss: 0.7781 - val_acc: 0.7042\n",
            "Epoch 140/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9173 - acc: 0.6588\n",
            "Epoch 00140: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.9171 - acc: 0.6578 - val_loss: 0.9136 - val_acc: 0.6517\n",
            "Epoch 141/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9347 - acc: 0.6412\n",
            "Epoch 00141: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.9352 - acc: 0.6411 - val_loss: 0.7976 - val_acc: 0.7167\n",
            "Epoch 142/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8938 - acc: 0.6606\n",
            "Epoch 00142: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8993 - acc: 0.6586 - val_loss: 0.8545 - val_acc: 0.6817\n",
            "Epoch 143/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8934 - acc: 0.6615\n",
            "Epoch 00143: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8903 - acc: 0.6617 - val_loss: 0.7640 - val_acc: 0.7167\n",
            "Epoch 144/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8689 - acc: 0.6742\n",
            "Epoch 00144: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8791 - acc: 0.6708 - val_loss: 0.7750 - val_acc: 0.7267\n",
            "Epoch 145/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9052 - acc: 0.6539\n",
            "Epoch 00145: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 173us/sample - loss: 0.9040 - acc: 0.6544 - val_loss: 0.7849 - val_acc: 0.7233\n",
            "Epoch 146/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9008 - acc: 0.6679\n",
            "Epoch 00146: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.9044 - acc: 0.6642 - val_loss: 0.7514 - val_acc: 0.7292\n",
            "Epoch 147/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8868 - acc: 0.6594\n",
            "Epoch 00147: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8920 - acc: 0.6606 - val_loss: 0.8010 - val_acc: 0.7100\n",
            "Epoch 148/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8931 - acc: 0.6618\n",
            "Epoch 00148: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 0.8969 - acc: 0.6608 - val_loss: 0.7588 - val_acc: 0.7333\n",
            "Epoch 149/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8673 - acc: 0.6833\n",
            "Epoch 00149: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.8689 - acc: 0.6811 - val_loss: 0.8544 - val_acc: 0.6692\n",
            "Epoch 150/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.9002 - acc: 0.6539\n",
            "Epoch 00150: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8973 - acc: 0.6531 - val_loss: 0.7812 - val_acc: 0.7133\n",
            "Epoch 151/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8603 - acc: 0.6839\n",
            "Epoch 00151: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.8593 - acc: 0.6850 - val_loss: 0.7391 - val_acc: 0.7258\n",
            "Epoch 152/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8586 - acc: 0.6821\n",
            "Epoch 00152: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8578 - acc: 0.6817 - val_loss: 0.7826 - val_acc: 0.7325\n",
            "Epoch 153/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8965 - acc: 0.6588\n",
            "Epoch 00153: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 166us/sample - loss: 0.9051 - acc: 0.6586 - val_loss: 0.8261 - val_acc: 0.7083\n",
            "Epoch 154/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8843 - acc: 0.6667\n",
            "Epoch 00154: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8824 - acc: 0.6672 - val_loss: 0.7886 - val_acc: 0.7217\n",
            "Epoch 155/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8776 - acc: 0.6752\n",
            "Epoch 00155: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 174us/sample - loss: 0.8732 - acc: 0.6756 - val_loss: 0.8236 - val_acc: 0.6908\n",
            "Epoch 156/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8724 - acc: 0.6827\n",
            "Epoch 00156: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 173us/sample - loss: 0.8684 - acc: 0.6828 - val_loss: 0.7848 - val_acc: 0.7158\n",
            "Epoch 157/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8644 - acc: 0.6762\n",
            "Epoch 00157: val_loss did not improve from 0.72926\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8690 - acc: 0.6728 - val_loss: 0.8319 - val_acc: 0.6867\n",
            "Epoch 158/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8361 - acc: 0.6921\n",
            "Epoch 00158: val_loss improved from 0.72926 to 0.72607, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.8414 - acc: 0.6883 - val_loss: 0.7261 - val_acc: 0.7192\n",
            "Epoch 159/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8691 - acc: 0.6733\n",
            "Epoch 00159: val_loss did not improve from 0.72607\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.8713 - acc: 0.6717 - val_loss: 0.8142 - val_acc: 0.6933\n",
            "Epoch 160/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8823 - acc: 0.6697\n",
            "Epoch 00160: val_loss did not improve from 0.72607\n",
            "3600/3600 [==============================] - 1s 173us/sample - loss: 0.8950 - acc: 0.6644 - val_loss: 0.7677 - val_acc: 0.7258\n",
            "Epoch 161/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8587 - acc: 0.6762\n",
            "Epoch 00161: val_loss did not improve from 0.72607\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8642 - acc: 0.6728 - val_loss: 0.7390 - val_acc: 0.7258\n",
            "Epoch 162/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8609 - acc: 0.6739\n",
            "Epoch 00162: val_loss did not improve from 0.72607\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 0.8657 - acc: 0.6733 - val_loss: 0.7708 - val_acc: 0.7300\n",
            "Epoch 163/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8566 - acc: 0.6788\n",
            "Epoch 00163: val_loss did not improve from 0.72607\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8651 - acc: 0.6725 - val_loss: 0.7670 - val_acc: 0.7317\n",
            "Epoch 164/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8654 - acc: 0.6782\n",
            "Epoch 00164: val_loss improved from 0.72607 to 0.71566, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.8678 - acc: 0.6758 - val_loss: 0.7157 - val_acc: 0.7383\n",
            "Epoch 165/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8584 - acc: 0.6824\n",
            "Epoch 00165: val_loss improved from 0.71566 to 0.70006, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.8666 - acc: 0.6811 - val_loss: 0.7001 - val_acc: 0.7442\n",
            "Epoch 166/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8494 - acc: 0.6782\n",
            "Epoch 00166: val_loss did not improve from 0.70006\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8512 - acc: 0.6772 - val_loss: 0.7397 - val_acc: 0.7383\n",
            "Epoch 167/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8619 - acc: 0.6794\n",
            "Epoch 00167: val_loss did not improve from 0.70006\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.8674 - acc: 0.6767 - val_loss: 0.7098 - val_acc: 0.7567\n",
            "Epoch 168/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8638 - acc: 0.6772\n",
            "Epoch 00168: val_loss did not improve from 0.70006\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8685 - acc: 0.6753 - val_loss: 0.7492 - val_acc: 0.7367\n",
            "Epoch 169/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8480 - acc: 0.6800\n",
            "Epoch 00169: val_loss improved from 0.70006 to 0.67144, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.8466 - acc: 0.6808 - val_loss: 0.6714 - val_acc: 0.7733\n",
            "Epoch 170/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.8615 - acc: 0.6689\n",
            "Epoch 00170: val_loss did not improve from 0.67144\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.8674 - acc: 0.6650 - val_loss: 0.6972 - val_acc: 0.7575\n",
            "Epoch 171/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8959 - acc: 0.6588\n",
            "Epoch 00171: val_loss did not improve from 0.67144\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8925 - acc: 0.6608 - val_loss: 0.7438 - val_acc: 0.7417\n",
            "Epoch 172/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8437 - acc: 0.6876\n",
            "Epoch 00172: val_loss did not improve from 0.67144\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8433 - acc: 0.6886 - val_loss: 0.7183 - val_acc: 0.7575\n",
            "Epoch 173/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8268 - acc: 0.6961\n",
            "Epoch 00173: val_loss improved from 0.67144 to 0.64745, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 184us/sample - loss: 0.8252 - acc: 0.6958 - val_loss: 0.6474 - val_acc: 0.7758\n",
            "Epoch 174/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8318 - acc: 0.6873\n",
            "Epoch 00174: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.8369 - acc: 0.6872 - val_loss: 0.7479 - val_acc: 0.7250\n",
            "Epoch 175/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8612 - acc: 0.6818\n",
            "Epoch 00175: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8550 - acc: 0.6831 - val_loss: 0.7935 - val_acc: 0.7075\n",
            "Epoch 176/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8540 - acc: 0.6797\n",
            "Epoch 00176: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.8582 - acc: 0.6769 - val_loss: 0.7716 - val_acc: 0.7175\n",
            "Epoch 177/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8334 - acc: 0.6967\n",
            "Epoch 00177: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8354 - acc: 0.6950 - val_loss: 0.7207 - val_acc: 0.7392\n",
            "Epoch 178/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8528 - acc: 0.6839\n",
            "Epoch 00178: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8555 - acc: 0.6819 - val_loss: 0.6750 - val_acc: 0.7617\n",
            "Epoch 179/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8518 - acc: 0.6779\n",
            "Epoch 00179: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8557 - acc: 0.6747 - val_loss: 0.6881 - val_acc: 0.7567\n",
            "Epoch 180/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8542 - acc: 0.6797\n",
            "Epoch 00180: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 175us/sample - loss: 0.8634 - acc: 0.6767 - val_loss: 0.8660 - val_acc: 0.6933\n",
            "Epoch 181/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8564 - acc: 0.6861\n",
            "Epoch 00181: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 175us/sample - loss: 0.8565 - acc: 0.6856 - val_loss: 0.7965 - val_acc: 0.7233\n",
            "Epoch 182/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8696 - acc: 0.6709\n",
            "Epoch 00182: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8767 - acc: 0.6667 - val_loss: 0.7911 - val_acc: 0.7258\n",
            "Epoch 183/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8396 - acc: 0.6866\n",
            "Epoch 00183: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8426 - acc: 0.6861 - val_loss: 0.7173 - val_acc: 0.7450\n",
            "Epoch 184/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8440 - acc: 0.6930\n",
            "Epoch 00184: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 0.8486 - acc: 0.6897 - val_loss: 0.7888 - val_acc: 0.7108\n",
            "Epoch 185/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8399 - acc: 0.6794\n",
            "Epoch 00185: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.8428 - acc: 0.6781 - val_loss: 0.6959 - val_acc: 0.7508\n",
            "Epoch 186/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8333 - acc: 0.6959\n",
            "Epoch 00186: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 174us/sample - loss: 0.8445 - acc: 0.6903 - val_loss: 0.7618 - val_acc: 0.7092\n",
            "Epoch 187/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8140 - acc: 0.6942\n",
            "Epoch 00187: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8269 - acc: 0.6900 - val_loss: 0.7365 - val_acc: 0.7283\n",
            "Epoch 188/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8768 - acc: 0.6694\n",
            "Epoch 00188: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 173us/sample - loss: 0.8763 - acc: 0.6708 - val_loss: 0.7498 - val_acc: 0.7442\n",
            "Epoch 189/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8356 - acc: 0.6827\n",
            "Epoch 00189: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 173us/sample - loss: 0.8393 - acc: 0.6808 - val_loss: 0.7011 - val_acc: 0.7642\n",
            "Epoch 190/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8553 - acc: 0.6936\n",
            "Epoch 00190: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8595 - acc: 0.6903 - val_loss: 0.6975 - val_acc: 0.7675\n",
            "Epoch 191/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8177 - acc: 0.7033\n",
            "Epoch 00191: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.8199 - acc: 0.6994 - val_loss: 0.6621 - val_acc: 0.7700\n",
            "Epoch 192/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8348 - acc: 0.6830\n",
            "Epoch 00192: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8278 - acc: 0.6881 - val_loss: 0.7243 - val_acc: 0.7267\n",
            "Epoch 193/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8451 - acc: 0.6847\n",
            "Epoch 00193: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8561 - acc: 0.6831 - val_loss: 0.7313 - val_acc: 0.7308\n",
            "Epoch 194/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8454 - acc: 0.6767\n",
            "Epoch 00194: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8485 - acc: 0.6753 - val_loss: 0.8027 - val_acc: 0.6875\n",
            "Epoch 195/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8318 - acc: 0.6909\n",
            "Epoch 00195: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 173us/sample - loss: 0.8309 - acc: 0.6925 - val_loss: 0.7436 - val_acc: 0.7258\n",
            "Epoch 196/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8235 - acc: 0.6879\n",
            "Epoch 00196: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 174us/sample - loss: 0.8268 - acc: 0.6858 - val_loss: 0.6978 - val_acc: 0.7617\n",
            "Epoch 197/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8493 - acc: 0.6839\n",
            "Epoch 00197: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.8578 - acc: 0.6811 - val_loss: 0.6967 - val_acc: 0.7825\n",
            "Epoch 198/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8120 - acc: 0.6972\n",
            "Epoch 00198: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 176us/sample - loss: 0.8303 - acc: 0.6886 - val_loss: 0.7476 - val_acc: 0.7308\n",
            "Epoch 199/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8857 - acc: 0.6812\n",
            "Epoch 00199: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.8848 - acc: 0.6811 - val_loss: 0.7506 - val_acc: 0.7375\n",
            "Epoch 200/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8200 - acc: 0.6955\n",
            "Epoch 00200: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8219 - acc: 0.6933 - val_loss: 0.6897 - val_acc: 0.7467\n",
            "Epoch 201/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8284 - acc: 0.7013\n",
            "Epoch 00201: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.8363 - acc: 0.6969 - val_loss: 0.7257 - val_acc: 0.7450\n",
            "Epoch 202/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8359 - acc: 0.6773\n",
            "Epoch 00202: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8413 - acc: 0.6775 - val_loss: 0.7501 - val_acc: 0.7283\n",
            "Epoch 203/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8353 - acc: 0.6909\n",
            "Epoch 00203: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 173us/sample - loss: 0.8302 - acc: 0.6919 - val_loss: 0.6519 - val_acc: 0.7733\n",
            "Epoch 204/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8447 - acc: 0.6842\n",
            "Epoch 00204: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 173us/sample - loss: 0.8390 - acc: 0.6875 - val_loss: 0.8179 - val_acc: 0.6958\n",
            "Epoch 205/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8327 - acc: 0.6876\n",
            "Epoch 00205: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8333 - acc: 0.6900 - val_loss: 0.7313 - val_acc: 0.7433\n",
            "Epoch 206/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8121 - acc: 0.6912\n",
            "Epoch 00206: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8229 - acc: 0.6872 - val_loss: 0.7325 - val_acc: 0.7308\n",
            "Epoch 207/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8075 - acc: 0.6955\n",
            "Epoch 00207: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8169 - acc: 0.6953 - val_loss: 0.8074 - val_acc: 0.7050\n",
            "Epoch 208/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8365 - acc: 0.6894\n",
            "Epoch 00208: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8348 - acc: 0.6906 - val_loss: 0.6768 - val_acc: 0.7600\n",
            "Epoch 209/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8318 - acc: 0.6842\n",
            "Epoch 00209: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8335 - acc: 0.6831 - val_loss: 0.6565 - val_acc: 0.7700\n",
            "Epoch 210/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8411 - acc: 0.6848\n",
            "Epoch 00210: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.8450 - acc: 0.6825 - val_loss: 0.8243 - val_acc: 0.7025\n",
            "Epoch 211/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8299 - acc: 0.6867\n",
            "Epoch 00211: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8328 - acc: 0.6872 - val_loss: 0.6481 - val_acc: 0.7825\n",
            "Epoch 212/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7888 - acc: 0.7073\n",
            "Epoch 00212: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 0.7842 - acc: 0.7108 - val_loss: 0.7144 - val_acc: 0.7517\n",
            "Epoch 213/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8088 - acc: 0.6973\n",
            "Epoch 00213: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8148 - acc: 0.6947 - val_loss: 0.7541 - val_acc: 0.7250\n",
            "Epoch 214/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8221 - acc: 0.6942\n",
            "Epoch 00214: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8189 - acc: 0.6931 - val_loss: 0.7685 - val_acc: 0.7208\n",
            "Epoch 215/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8110 - acc: 0.6973\n",
            "Epoch 00215: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8191 - acc: 0.6942 - val_loss: 0.7789 - val_acc: 0.7217\n",
            "Epoch 216/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8214 - acc: 0.7006\n",
            "Epoch 00216: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.8232 - acc: 0.6992 - val_loss: 0.7870 - val_acc: 0.7125\n",
            "Epoch 217/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8025 - acc: 0.7018\n",
            "Epoch 00217: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 0.8124 - acc: 0.6961 - val_loss: 0.6522 - val_acc: 0.7675\n",
            "Epoch 218/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8320 - acc: 0.6919\n",
            "Epoch 00218: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.8333 - acc: 0.6906 - val_loss: 0.6792 - val_acc: 0.7625\n",
            "Epoch 219/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8115 - acc: 0.6979\n",
            "Epoch 00219: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 0.8122 - acc: 0.6975 - val_loss: 0.6997 - val_acc: 0.7458\n",
            "Epoch 220/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8354 - acc: 0.6885\n",
            "Epoch 00220: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 173us/sample - loss: 0.8374 - acc: 0.6861 - val_loss: 0.6808 - val_acc: 0.7500\n",
            "Epoch 221/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8236 - acc: 0.6897\n",
            "Epoch 00221: val_loss did not improve from 0.64745\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8286 - acc: 0.6889 - val_loss: 0.7292 - val_acc: 0.7442\n",
            "Epoch 222/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7817 - acc: 0.7119\n",
            "Epoch 00222: val_loss improved from 0.64745 to 0.60628, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.7777 - acc: 0.7150 - val_loss: 0.6063 - val_acc: 0.7942\n",
            "Epoch 223/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8255 - acc: 0.6912\n",
            "Epoch 00223: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8303 - acc: 0.6897 - val_loss: 0.6702 - val_acc: 0.7600\n",
            "Epoch 224/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8058 - acc: 0.7036\n",
            "Epoch 00224: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8073 - acc: 0.7022 - val_loss: 0.6743 - val_acc: 0.7658\n",
            "Epoch 225/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7929 - acc: 0.7091\n",
            "Epoch 00225: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7990 - acc: 0.7047 - val_loss: 0.6605 - val_acc: 0.7658\n",
            "Epoch 226/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7924 - acc: 0.7024\n",
            "Epoch 00226: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7999 - acc: 0.6986 - val_loss: 0.7483 - val_acc: 0.7192\n",
            "Epoch 227/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8290 - acc: 0.6852\n",
            "Epoch 00227: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8337 - acc: 0.6819 - val_loss: 0.6718 - val_acc: 0.7725\n",
            "Epoch 228/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8077 - acc: 0.6994\n",
            "Epoch 00228: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.8120 - acc: 0.6994 - val_loss: 0.6172 - val_acc: 0.7867\n",
            "Epoch 229/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8261 - acc: 0.6955\n",
            "Epoch 00229: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8301 - acc: 0.6939 - val_loss: 0.7030 - val_acc: 0.7367\n",
            "Epoch 230/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8259 - acc: 0.6953\n",
            "Epoch 00230: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8263 - acc: 0.6944 - val_loss: 0.6763 - val_acc: 0.7683\n",
            "Epoch 231/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7937 - acc: 0.7064\n",
            "Epoch 00231: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.7985 - acc: 0.7008 - val_loss: 0.7368 - val_acc: 0.7367\n",
            "Epoch 232/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8239 - acc: 0.6870\n",
            "Epoch 00232: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8225 - acc: 0.6897 - val_loss: 0.6594 - val_acc: 0.7600\n",
            "Epoch 233/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8099 - acc: 0.6994\n",
            "Epoch 00233: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.8115 - acc: 0.7003 - val_loss: 0.7357 - val_acc: 0.7283\n",
            "Epoch 234/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7908 - acc: 0.7085\n",
            "Epoch 00234: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8038 - acc: 0.7033 - val_loss: 0.7326 - val_acc: 0.7275\n",
            "Epoch 235/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8047 - acc: 0.7021\n",
            "Epoch 00235: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.8091 - acc: 0.7014 - val_loss: 0.9438 - val_acc: 0.6817\n",
            "Epoch 236/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8803 - acc: 0.6758\n",
            "Epoch 00236: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.8830 - acc: 0.6733 - val_loss: 0.7070 - val_acc: 0.7533\n",
            "Epoch 237/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8251 - acc: 0.6994\n",
            "Epoch 00237: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8284 - acc: 0.6978 - val_loss: 0.6306 - val_acc: 0.7867\n",
            "Epoch 238/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8059 - acc: 0.7091\n",
            "Epoch 00238: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.8126 - acc: 0.7050 - val_loss: 0.6080 - val_acc: 0.7867\n",
            "Epoch 239/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8350 - acc: 0.6927\n",
            "Epoch 00239: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8290 - acc: 0.6967 - val_loss: 0.6454 - val_acc: 0.7675\n",
            "Epoch 240/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7742 - acc: 0.7144\n",
            "Epoch 00240: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7729 - acc: 0.7142 - val_loss: 0.6471 - val_acc: 0.7608\n",
            "Epoch 241/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8015 - acc: 0.7018\n",
            "Epoch 00241: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.7977 - acc: 0.7033 - val_loss: 0.6385 - val_acc: 0.7692\n",
            "Epoch 242/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7932 - acc: 0.6997\n",
            "Epoch 00242: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.8119 - acc: 0.6933 - val_loss: 0.7255 - val_acc: 0.7358\n",
            "Epoch 243/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8160 - acc: 0.6948\n",
            "Epoch 00243: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8210 - acc: 0.6914 - val_loss: 0.7437 - val_acc: 0.7442\n",
            "Epoch 244/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8012 - acc: 0.7055\n",
            "Epoch 00244: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7971 - acc: 0.7056 - val_loss: 0.6718 - val_acc: 0.7708\n",
            "Epoch 245/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8116 - acc: 0.6979\n",
            "Epoch 00245: val_loss did not improve from 0.60628\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.8109 - acc: 0.6972 - val_loss: 0.7332 - val_acc: 0.7367\n",
            "Epoch 246/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7898 - acc: 0.6939\n",
            "Epoch 00246: val_loss improved from 0.60628 to 0.59671, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 186us/sample - loss: 0.7876 - acc: 0.6956 - val_loss: 0.5967 - val_acc: 0.7942\n",
            "Epoch 247/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7944 - acc: 0.7018\n",
            "Epoch 00247: val_loss did not improve from 0.59671\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8008 - acc: 0.6994 - val_loss: 0.6389 - val_acc: 0.7800\n",
            "Epoch 248/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8020 - acc: 0.7022\n",
            "Epoch 00248: val_loss did not improve from 0.59671\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.7976 - acc: 0.7042 - val_loss: 0.5995 - val_acc: 0.7925\n",
            "Epoch 249/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7950 - acc: 0.7045\n",
            "Epoch 00249: val_loss improved from 0.59671 to 0.58649, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 183us/sample - loss: 0.8033 - acc: 0.7011 - val_loss: 0.5865 - val_acc: 0.8025\n",
            "Epoch 250/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8060 - acc: 0.7041\n",
            "Epoch 00250: val_loss did not improve from 0.58649\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8093 - acc: 0.7033 - val_loss: 0.7453 - val_acc: 0.7100\n",
            "Epoch 251/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8180 - acc: 0.6979\n",
            "Epoch 00251: val_loss did not improve from 0.58649\n",
            "3600/3600 [==============================] - 1s 173us/sample - loss: 0.8161 - acc: 0.7006 - val_loss: 0.6089 - val_acc: 0.7900\n",
            "Epoch 252/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7998 - acc: 0.7009\n",
            "Epoch 00252: val_loss did not improve from 0.58649\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.8082 - acc: 0.6992 - val_loss: 0.6774 - val_acc: 0.7658\n",
            "Epoch 253/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7984 - acc: 0.7030\n",
            "Epoch 00253: val_loss did not improve from 0.58649\n",
            "3600/3600 [==============================] - 1s 166us/sample - loss: 0.7980 - acc: 0.7056 - val_loss: 0.6184 - val_acc: 0.7875\n",
            "Epoch 254/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.8081 - acc: 0.7025\n",
            "Epoch 00254: val_loss did not improve from 0.58649\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8155 - acc: 0.6992 - val_loss: 0.6165 - val_acc: 0.7858\n",
            "Epoch 255/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7992 - acc: 0.7042\n",
            "Epoch 00255: val_loss did not improve from 0.58649\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.7972 - acc: 0.7022 - val_loss: 0.7915 - val_acc: 0.7250\n",
            "Epoch 256/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7951 - acc: 0.7103\n",
            "Epoch 00256: val_loss did not improve from 0.58649\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8027 - acc: 0.7036 - val_loss: 0.6672 - val_acc: 0.7608\n",
            "Epoch 257/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7971 - acc: 0.7064\n",
            "Epoch 00257: val_loss improved from 0.58649 to 0.57700, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.7956 - acc: 0.7081 - val_loss: 0.5770 - val_acc: 0.8033\n",
            "Epoch 258/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7555 - acc: 0.7244\n",
            "Epoch 00258: val_loss did not improve from 0.57700\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7712 - acc: 0.7169 - val_loss: 0.6747 - val_acc: 0.7550\n",
            "Epoch 259/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7892 - acc: 0.7115\n",
            "Epoch 00259: val_loss did not improve from 0.57700\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7866 - acc: 0.7136 - val_loss: 0.6899 - val_acc: 0.7408\n",
            "Epoch 260/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8086 - acc: 0.7003\n",
            "Epoch 00260: val_loss did not improve from 0.57700\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.8144 - acc: 0.6989 - val_loss: 0.5997 - val_acc: 0.7933\n",
            "Epoch 261/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7859 - acc: 0.7100\n",
            "Epoch 00261: val_loss did not improve from 0.57700\n",
            "3600/3600 [==============================] - 1s 174us/sample - loss: 0.7891 - acc: 0.7086 - val_loss: 0.6415 - val_acc: 0.7825\n",
            "Epoch 262/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7921 - acc: 0.7136\n",
            "Epoch 00262: val_loss did not improve from 0.57700\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8027 - acc: 0.7072 - val_loss: 0.6750 - val_acc: 0.7650\n",
            "Epoch 263/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8215 - acc: 0.6961\n",
            "Epoch 00263: val_loss did not improve from 0.57700\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.8254 - acc: 0.6933 - val_loss: 0.5959 - val_acc: 0.8000\n",
            "Epoch 264/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7922 - acc: 0.7144\n",
            "Epoch 00264: val_loss did not improve from 0.57700\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.8078 - acc: 0.7044 - val_loss: 0.6416 - val_acc: 0.7817\n",
            "Epoch 265/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7745 - acc: 0.7115\n",
            "Epoch 00265: val_loss did not improve from 0.57700\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.7758 - acc: 0.7125 - val_loss: 0.6267 - val_acc: 0.7833\n",
            "Epoch 266/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7716 - acc: 0.7130\n",
            "Epoch 00266: val_loss did not improve from 0.57700\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7819 - acc: 0.7100 - val_loss: 0.6568 - val_acc: 0.7650\n",
            "Epoch 267/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7965 - acc: 0.7091\n",
            "Epoch 00267: val_loss did not improve from 0.57700\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7863 - acc: 0.7122 - val_loss: 0.6590 - val_acc: 0.7742\n",
            "Epoch 268/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7963 - acc: 0.7100\n",
            "Epoch 00268: val_loss did not improve from 0.57700\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8089 - acc: 0.7042 - val_loss: 0.5903 - val_acc: 0.7975\n",
            "Epoch 269/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7830 - acc: 0.7097\n",
            "Epoch 00269: val_loss did not improve from 0.57700\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7888 - acc: 0.7094 - val_loss: 0.6434 - val_acc: 0.7708\n",
            "Epoch 270/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7977 - acc: 0.6985\n",
            "Epoch 00270: val_loss improved from 0.57700 to 0.56477, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 182us/sample - loss: 0.7970 - acc: 0.6992 - val_loss: 0.5648 - val_acc: 0.8075\n",
            "Epoch 271/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7770 - acc: 0.7106\n",
            "Epoch 00271: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.7821 - acc: 0.7086 - val_loss: 0.6747 - val_acc: 0.7467\n",
            "Epoch 272/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7875 - acc: 0.7079\n",
            "Epoch 00272: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7854 - acc: 0.7106 - val_loss: 0.6954 - val_acc: 0.7625\n",
            "Epoch 273/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8196 - acc: 0.6997\n",
            "Epoch 00273: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8209 - acc: 0.6975 - val_loss: 0.6028 - val_acc: 0.7917\n",
            "Epoch 274/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8020 - acc: 0.7015\n",
            "Epoch 00274: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.8070 - acc: 0.6981 - val_loss: 0.6503 - val_acc: 0.7567\n",
            "Epoch 275/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8098 - acc: 0.7024\n",
            "Epoch 00275: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8182 - acc: 0.7011 - val_loss: 0.7864 - val_acc: 0.7133\n",
            "Epoch 276/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7892 - acc: 0.7122\n",
            "Epoch 00276: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7862 - acc: 0.7131 - val_loss: 0.7120 - val_acc: 0.7417\n",
            "Epoch 277/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7956 - acc: 0.7033\n",
            "Epoch 00277: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 173us/sample - loss: 0.8019 - acc: 0.6992 - val_loss: 0.5857 - val_acc: 0.7867\n",
            "Epoch 278/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7598 - acc: 0.7258\n",
            "Epoch 00278: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7639 - acc: 0.7233 - val_loss: 0.8197 - val_acc: 0.6967\n",
            "Epoch 279/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8050 - acc: 0.6982\n",
            "Epoch 00279: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.8164 - acc: 0.6917 - val_loss: 0.5783 - val_acc: 0.7992\n",
            "Epoch 280/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7845 - acc: 0.7079\n",
            "Epoch 00280: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.7909 - acc: 0.7042 - val_loss: 0.7693 - val_acc: 0.7167\n",
            "Epoch 281/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7965 - acc: 0.7033\n",
            "Epoch 00281: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7985 - acc: 0.7017 - val_loss: 0.6057 - val_acc: 0.7892\n",
            "Epoch 282/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7950 - acc: 0.7063\n",
            "Epoch 00282: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 173us/sample - loss: 0.8012 - acc: 0.7033 - val_loss: 0.7655 - val_acc: 0.7108\n",
            "Epoch 283/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7954 - acc: 0.7133\n",
            "Epoch 00283: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 0.7998 - acc: 0.7119 - val_loss: 0.6312 - val_acc: 0.7850\n",
            "Epoch 284/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7903 - acc: 0.7042\n",
            "Epoch 00284: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 173us/sample - loss: 0.7944 - acc: 0.7031 - val_loss: 0.6012 - val_acc: 0.7925\n",
            "Epoch 285/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7906 - acc: 0.7036\n",
            "Epoch 00285: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 173us/sample - loss: 0.7872 - acc: 0.7050 - val_loss: 0.6109 - val_acc: 0.7800\n",
            "Epoch 286/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7748 - acc: 0.7147\n",
            "Epoch 00286: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.7767 - acc: 0.7147 - val_loss: 0.5976 - val_acc: 0.7867\n",
            "Epoch 287/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8226 - acc: 0.6979\n",
            "Epoch 00287: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.8210 - acc: 0.6981 - val_loss: 0.5882 - val_acc: 0.8033\n",
            "Epoch 288/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7901 - acc: 0.7100\n",
            "Epoch 00288: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7945 - acc: 0.7067 - val_loss: 0.6042 - val_acc: 0.7842\n",
            "Epoch 289/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7770 - acc: 0.7179\n",
            "Epoch 00289: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7763 - acc: 0.7186 - val_loss: 0.6113 - val_acc: 0.7967\n",
            "Epoch 290/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7935 - acc: 0.7000\n",
            "Epoch 00290: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7954 - acc: 0.7008 - val_loss: 0.6607 - val_acc: 0.7692\n",
            "Epoch 291/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7612 - acc: 0.7161\n",
            "Epoch 00291: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.7575 - acc: 0.7175 - val_loss: 0.5693 - val_acc: 0.8108\n",
            "Epoch 292/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7983 - acc: 0.6976\n",
            "Epoch 00292: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.8007 - acc: 0.6986 - val_loss: 0.5661 - val_acc: 0.8133\n",
            "Epoch 293/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7471 - acc: 0.7230\n",
            "Epoch 00293: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.7503 - acc: 0.7225 - val_loss: 0.5956 - val_acc: 0.7917\n",
            "Epoch 294/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8072 - acc: 0.6982\n",
            "Epoch 00294: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.8198 - acc: 0.6933 - val_loss: 0.6976 - val_acc: 0.7350\n",
            "Epoch 295/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7828 - acc: 0.7212\n",
            "Epoch 00295: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7804 - acc: 0.7217 - val_loss: 0.6118 - val_acc: 0.7892\n",
            "Epoch 296/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7633 - acc: 0.7127\n",
            "Epoch 00296: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7628 - acc: 0.7133 - val_loss: 0.6294 - val_acc: 0.7842\n",
            "Epoch 297/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7581 - acc: 0.7161\n",
            "Epoch 00297: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7646 - acc: 0.7125 - val_loss: 0.6304 - val_acc: 0.7833\n",
            "Epoch 298/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8096 - acc: 0.6958\n",
            "Epoch 00298: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.8068 - acc: 0.6969 - val_loss: 0.7049 - val_acc: 0.7308\n",
            "Epoch 299/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7938 - acc: 0.7019\n",
            "Epoch 00299: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.7995 - acc: 0.6986 - val_loss: 0.5957 - val_acc: 0.7967\n",
            "Epoch 300/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.8029 - acc: 0.6991\n",
            "Epoch 00300: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.8033 - acc: 0.7011 - val_loss: 0.6778 - val_acc: 0.7400\n",
            "Epoch 301/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7792 - acc: 0.7028\n",
            "Epoch 00301: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7847 - acc: 0.7008 - val_loss: 0.5886 - val_acc: 0.8108\n",
            "Epoch 302/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7767 - acc: 0.7145\n",
            "Epoch 00302: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7833 - acc: 0.7114 - val_loss: 0.6934 - val_acc: 0.7433\n",
            "Epoch 303/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7835 - acc: 0.7145\n",
            "Epoch 00303: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7917 - acc: 0.7114 - val_loss: 0.6630 - val_acc: 0.7792\n",
            "Epoch 304/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7676 - acc: 0.7119\n",
            "Epoch 00304: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.7717 - acc: 0.7083 - val_loss: 0.7592 - val_acc: 0.7242\n",
            "Epoch 305/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7502 - acc: 0.7278\n",
            "Epoch 00305: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.7592 - acc: 0.7242 - val_loss: 0.6767 - val_acc: 0.7500\n",
            "Epoch 306/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7895 - acc: 0.7179\n",
            "Epoch 00306: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7843 - acc: 0.7200 - val_loss: 0.7070 - val_acc: 0.7408\n",
            "Epoch 307/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7612 - acc: 0.7273\n",
            "Epoch 00307: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7668 - acc: 0.7236 - val_loss: 0.5654 - val_acc: 0.8175\n",
            "Epoch 308/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7927 - acc: 0.7076\n",
            "Epoch 00308: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 0.7996 - acc: 0.7022 - val_loss: 0.6048 - val_acc: 0.7892\n",
            "Epoch 309/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7675 - acc: 0.7175\n",
            "Epoch 00309: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7666 - acc: 0.7172 - val_loss: 0.6219 - val_acc: 0.7742\n",
            "Epoch 310/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7445 - acc: 0.7221\n",
            "Epoch 00310: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7565 - acc: 0.7183 - val_loss: 0.5961 - val_acc: 0.7892\n",
            "Epoch 311/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7871 - acc: 0.7091\n",
            "Epoch 00311: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7863 - acc: 0.7072 - val_loss: 0.6015 - val_acc: 0.7842\n",
            "Epoch 312/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7675 - acc: 0.7191\n",
            "Epoch 00312: val_loss did not improve from 0.56477\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7682 - acc: 0.7194 - val_loss: 0.6331 - val_acc: 0.7683\n",
            "Epoch 313/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7902 - acc: 0.7106\n",
            "Epoch 00313: val_loss improved from 0.56477 to 0.55862, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.7847 - acc: 0.7136 - val_loss: 0.5586 - val_acc: 0.8150\n",
            "Epoch 314/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7578 - acc: 0.7148\n",
            "Epoch 00314: val_loss did not improve from 0.55862\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7628 - acc: 0.7125 - val_loss: 0.5951 - val_acc: 0.7867\n",
            "Epoch 315/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7456 - acc: 0.7176\n",
            "Epoch 00315: val_loss did not improve from 0.55862\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.7472 - acc: 0.7172 - val_loss: 0.6237 - val_acc: 0.7800\n",
            "Epoch 316/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7850 - acc: 0.7018\n",
            "Epoch 00316: val_loss did not improve from 0.55862\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7756 - acc: 0.7050 - val_loss: 0.7499 - val_acc: 0.7125\n",
            "Epoch 317/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7834 - acc: 0.7156\n",
            "Epoch 00317: val_loss did not improve from 0.55862\n",
            "3600/3600 [==============================] - 1s 176us/sample - loss: 0.7739 - acc: 0.7211 - val_loss: 0.6596 - val_acc: 0.7475\n",
            "Epoch 318/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7785 - acc: 0.7103\n",
            "Epoch 00318: val_loss improved from 0.55862 to 0.53136, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 185us/sample - loss: 0.7800 - acc: 0.7122 - val_loss: 0.5314 - val_acc: 0.8267\n",
            "Epoch 319/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7638 - acc: 0.7236\n",
            "Epoch 00319: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 0.7814 - acc: 0.7178 - val_loss: 0.6017 - val_acc: 0.7958\n",
            "Epoch 320/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7988 - acc: 0.7073\n",
            "Epoch 00320: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.7990 - acc: 0.7053 - val_loss: 0.5906 - val_acc: 0.8033\n",
            "Epoch 321/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7474 - acc: 0.7294\n",
            "Epoch 00321: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7590 - acc: 0.7239 - val_loss: 0.5843 - val_acc: 0.8083\n",
            "Epoch 322/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7807 - acc: 0.7142\n",
            "Epoch 00322: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7839 - acc: 0.7125 - val_loss: 0.6706 - val_acc: 0.7567\n",
            "Epoch 323/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7437 - acc: 0.7245\n",
            "Epoch 00323: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7436 - acc: 0.7253 - val_loss: 0.5551 - val_acc: 0.8267\n",
            "Epoch 324/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7749 - acc: 0.7194\n",
            "Epoch 00324: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7849 - acc: 0.7164 - val_loss: 0.6043 - val_acc: 0.7850\n",
            "Epoch 325/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7774 - acc: 0.7128\n",
            "Epoch 00325: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7830 - acc: 0.7092 - val_loss: 0.6718 - val_acc: 0.7625\n",
            "Epoch 326/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7663 - acc: 0.7279\n",
            "Epoch 00326: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7778 - acc: 0.7225 - val_loss: 0.6737 - val_acc: 0.7525\n",
            "Epoch 327/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7714 - acc: 0.7138\n",
            "Epoch 00327: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 173us/sample - loss: 0.7793 - acc: 0.7114 - val_loss: 0.6201 - val_acc: 0.7650\n",
            "Epoch 328/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7549 - acc: 0.7182\n",
            "Epoch 00328: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7637 - acc: 0.7169 - val_loss: 0.7073 - val_acc: 0.7700\n",
            "Epoch 329/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7627 - acc: 0.7142\n",
            "Epoch 00329: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 0.7682 - acc: 0.7147 - val_loss: 0.6003 - val_acc: 0.7875\n",
            "Epoch 330/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7463 - acc: 0.7239\n",
            "Epoch 00330: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7553 - acc: 0.7236 - val_loss: 0.6740 - val_acc: 0.7417\n",
            "Epoch 331/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7760 - acc: 0.7091\n",
            "Epoch 00331: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7816 - acc: 0.7056 - val_loss: 0.7023 - val_acc: 0.7467\n",
            "Epoch 332/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7875 - acc: 0.7100\n",
            "Epoch 00332: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7861 - acc: 0.7078 - val_loss: 0.6883 - val_acc: 0.7575\n",
            "Epoch 333/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7437 - acc: 0.7294\n",
            "Epoch 00333: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7502 - acc: 0.7281 - val_loss: 0.6265 - val_acc: 0.7808\n",
            "Epoch 334/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7849 - acc: 0.7133\n",
            "Epoch 00334: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7858 - acc: 0.7128 - val_loss: 0.6542 - val_acc: 0.7675\n",
            "Epoch 335/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7613 - acc: 0.7259\n",
            "Epoch 00335: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 175us/sample - loss: 0.7634 - acc: 0.7239 - val_loss: 0.5904 - val_acc: 0.7975\n",
            "Epoch 336/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7463 - acc: 0.7288\n",
            "Epoch 00336: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7560 - acc: 0.7269 - val_loss: 0.6265 - val_acc: 0.7892\n",
            "Epoch 337/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7339 - acc: 0.7319\n",
            "Epoch 00337: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7440 - acc: 0.7247 - val_loss: 0.5834 - val_acc: 0.7933\n",
            "Epoch 338/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7644 - acc: 0.7200\n",
            "Epoch 00338: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7647 - acc: 0.7197 - val_loss: 0.5915 - val_acc: 0.7975\n",
            "Epoch 339/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7715 - acc: 0.7209\n",
            "Epoch 00339: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 0.7745 - acc: 0.7211 - val_loss: 0.6937 - val_acc: 0.7558\n",
            "Epoch 340/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7357 - acc: 0.7333\n",
            "Epoch 00340: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7417 - acc: 0.7317 - val_loss: 0.6861 - val_acc: 0.7408\n",
            "Epoch 341/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7768 - acc: 0.7170\n",
            "Epoch 00341: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7798 - acc: 0.7144 - val_loss: 0.7075 - val_acc: 0.7475\n",
            "Epoch 342/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7831 - acc: 0.7055\n",
            "Epoch 00342: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7930 - acc: 0.7022 - val_loss: 0.6408 - val_acc: 0.7742\n",
            "Epoch 343/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7860 - acc: 0.7103\n",
            "Epoch 00343: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 174us/sample - loss: 0.7827 - acc: 0.7125 - val_loss: 0.5403 - val_acc: 0.8108\n",
            "Epoch 344/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7434 - acc: 0.7194\n",
            "Epoch 00344: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7453 - acc: 0.7214 - val_loss: 0.6847 - val_acc: 0.7550\n",
            "Epoch 345/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7669 - acc: 0.7197\n",
            "Epoch 00345: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.7670 - acc: 0.7161 - val_loss: 0.5458 - val_acc: 0.8267\n",
            "Epoch 346/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7406 - acc: 0.7303\n",
            "Epoch 00346: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.7532 - acc: 0.7244 - val_loss: 0.6491 - val_acc: 0.7717\n",
            "Epoch 347/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7535 - acc: 0.7222\n",
            "Epoch 00347: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7512 - acc: 0.7231 - val_loss: 0.6227 - val_acc: 0.7817\n",
            "Epoch 348/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7348 - acc: 0.7287\n",
            "Epoch 00348: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7445 - acc: 0.7269 - val_loss: 0.5963 - val_acc: 0.8033\n",
            "Epoch 349/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7664 - acc: 0.7136\n",
            "Epoch 00349: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7594 - acc: 0.7175 - val_loss: 0.5785 - val_acc: 0.7975\n",
            "Epoch 350/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7532 - acc: 0.7221\n",
            "Epoch 00350: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7600 - acc: 0.7211 - val_loss: 0.6283 - val_acc: 0.7817\n",
            "Epoch 351/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7146 - acc: 0.7450\n",
            "Epoch 00351: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 173us/sample - loss: 0.7203 - acc: 0.7414 - val_loss: 0.6338 - val_acc: 0.7842\n",
            "Epoch 352/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7845 - acc: 0.7109\n",
            "Epoch 00352: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 174us/sample - loss: 0.7946 - acc: 0.7053 - val_loss: 0.6614 - val_acc: 0.7733\n",
            "Epoch 353/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7796 - acc: 0.7122\n",
            "Epoch 00353: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 174us/sample - loss: 0.7807 - acc: 0.7131 - val_loss: 0.6440 - val_acc: 0.7642\n",
            "Epoch 354/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7824 - acc: 0.7085\n",
            "Epoch 00354: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.7817 - acc: 0.7094 - val_loss: 0.5552 - val_acc: 0.8083\n",
            "Epoch 355/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7277 - acc: 0.7361\n",
            "Epoch 00355: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7268 - acc: 0.7372 - val_loss: 0.6051 - val_acc: 0.7875\n",
            "Epoch 356/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7648 - acc: 0.7179\n",
            "Epoch 00356: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7709 - acc: 0.7144 - val_loss: 0.6687 - val_acc: 0.7650\n",
            "Epoch 357/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7453 - acc: 0.7221\n",
            "Epoch 00357: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7462 - acc: 0.7233 - val_loss: 0.5499 - val_acc: 0.8092\n",
            "Epoch 358/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7489 - acc: 0.7148\n",
            "Epoch 00358: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7507 - acc: 0.7156 - val_loss: 0.6845 - val_acc: 0.7467\n",
            "Epoch 359/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7451 - acc: 0.7339\n",
            "Epoch 00359: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.7555 - acc: 0.7300 - val_loss: 0.6176 - val_acc: 0.7850\n",
            "Epoch 360/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7640 - acc: 0.7185\n",
            "Epoch 00360: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7668 - acc: 0.7181 - val_loss: 0.5314 - val_acc: 0.8150\n",
            "Epoch 361/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7509 - acc: 0.7236\n",
            "Epoch 00361: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7536 - acc: 0.7225 - val_loss: 0.5591 - val_acc: 0.8117\n",
            "Epoch 362/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7293 - acc: 0.7264\n",
            "Epoch 00362: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 166us/sample - loss: 0.7405 - acc: 0.7231 - val_loss: 0.6073 - val_acc: 0.7858\n",
            "Epoch 363/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7732 - acc: 0.7115\n",
            "Epoch 00363: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7736 - acc: 0.7108 - val_loss: 0.6083 - val_acc: 0.7875\n",
            "Epoch 364/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7670 - acc: 0.7170\n",
            "Epoch 00364: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7631 - acc: 0.7194 - val_loss: 0.5479 - val_acc: 0.8075\n",
            "Epoch 365/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7730 - acc: 0.7148\n",
            "Epoch 00365: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7834 - acc: 0.7075 - val_loss: 0.5815 - val_acc: 0.8092\n",
            "Epoch 366/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7961 - acc: 0.7097\n",
            "Epoch 00366: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 0.7941 - acc: 0.7111 - val_loss: 0.6694 - val_acc: 0.7617\n",
            "Epoch 367/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7417 - acc: 0.7215\n",
            "Epoch 00367: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.7498 - acc: 0.7178 - val_loss: 0.5904 - val_acc: 0.8008\n",
            "Epoch 368/400\n",
            "3500/3600 [============================>.] - ETA: 0s - loss: 0.7673 - acc: 0.7071\n",
            "Epoch 00368: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 175us/sample - loss: 0.7664 - acc: 0.7092 - val_loss: 0.5684 - val_acc: 0.8017\n",
            "Epoch 369/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7862 - acc: 0.7042\n",
            "Epoch 00369: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.7876 - acc: 0.7036 - val_loss: 0.5510 - val_acc: 0.8125\n",
            "Epoch 370/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7558 - acc: 0.7109\n",
            "Epoch 00370: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 0.7615 - acc: 0.7064 - val_loss: 0.5959 - val_acc: 0.7917\n",
            "Epoch 371/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7810 - acc: 0.7045\n",
            "Epoch 00371: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.7809 - acc: 0.7050 - val_loss: 0.5744 - val_acc: 0.8017\n",
            "Epoch 372/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7473 - acc: 0.7239\n",
            "Epoch 00372: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7419 - acc: 0.7297 - val_loss: 0.5456 - val_acc: 0.8192\n",
            "Epoch 373/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7474 - acc: 0.7239\n",
            "Epoch 00373: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7508 - acc: 0.7239 - val_loss: 0.6265 - val_acc: 0.7675\n",
            "Epoch 374/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7727 - acc: 0.7073\n",
            "Epoch 00374: val_loss did not improve from 0.53136\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7728 - acc: 0.7058 - val_loss: 0.5721 - val_acc: 0.7975\n",
            "Epoch 375/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7795 - acc: 0.7091\n",
            "Epoch 00375: val_loss improved from 0.53136 to 0.52870, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 188us/sample - loss: 0.7850 - acc: 0.7056 - val_loss: 0.5287 - val_acc: 0.8342\n",
            "Epoch 376/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7714 - acc: 0.7181\n",
            "Epoch 00376: val_loss did not improve from 0.52870\n",
            "3600/3600 [==============================] - 1s 174us/sample - loss: 0.7823 - acc: 0.7114 - val_loss: 0.6046 - val_acc: 0.7933\n",
            "Epoch 377/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7740 - acc: 0.7109\n",
            "Epoch 00377: val_loss did not improve from 0.52870\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7685 - acc: 0.7111 - val_loss: 0.6148 - val_acc: 0.7917\n",
            "Epoch 378/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7456 - acc: 0.7275\n",
            "Epoch 00378: val_loss did not improve from 0.52870\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7485 - acc: 0.7286 - val_loss: 0.5589 - val_acc: 0.8133\n",
            "Epoch 379/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7336 - acc: 0.7294\n",
            "Epoch 00379: val_loss did not improve from 0.52870\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.7384 - acc: 0.7289 - val_loss: 0.6516 - val_acc: 0.7725\n",
            "Epoch 380/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7295 - acc: 0.7321\n",
            "Epoch 00380: val_loss did not improve from 0.52870\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.7315 - acc: 0.7303 - val_loss: 0.5954 - val_acc: 0.8142\n",
            "Epoch 381/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7338 - acc: 0.7287\n",
            "Epoch 00381: val_loss did not improve from 0.52870\n",
            "3600/3600 [==============================] - 1s 174us/sample - loss: 0.7380 - acc: 0.7286 - val_loss: 0.5583 - val_acc: 0.7942\n",
            "Epoch 382/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7444 - acc: 0.7276\n",
            "Epoch 00382: val_loss did not improve from 0.52870\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.7493 - acc: 0.7281 - val_loss: 0.6326 - val_acc: 0.7800\n",
            "Epoch 383/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7298 - acc: 0.7336\n",
            "Epoch 00383: val_loss did not improve from 0.52870\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7458 - acc: 0.7278 - val_loss: 0.6531 - val_acc: 0.7692\n",
            "Epoch 384/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7311 - acc: 0.7255\n",
            "Epoch 00384: val_loss did not improve from 0.52870\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7386 - acc: 0.7219 - val_loss: 0.5359 - val_acc: 0.8175\n",
            "Epoch 385/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7616 - acc: 0.7215\n",
            "Epoch 00385: val_loss did not improve from 0.52870\n",
            "3600/3600 [==============================] - 1s 173us/sample - loss: 0.7709 - acc: 0.7189 - val_loss: 0.5443 - val_acc: 0.8050\n",
            "Epoch 386/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7433 - acc: 0.7227\n",
            "Epoch 00386: val_loss did not improve from 0.52870\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7453 - acc: 0.7228 - val_loss: 0.5808 - val_acc: 0.7975\n",
            "Epoch 387/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7458 - acc: 0.7258\n",
            "Epoch 00387: val_loss did not improve from 0.52870\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7533 - acc: 0.7214 - val_loss: 0.5574 - val_acc: 0.7942\n",
            "Epoch 388/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7576 - acc: 0.7094\n",
            "Epoch 00388: val_loss did not improve from 0.52870\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7617 - acc: 0.7097 - val_loss: 0.6147 - val_acc: 0.7842\n",
            "Epoch 389/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7506 - acc: 0.7224\n",
            "Epoch 00389: val_loss improved from 0.52870 to 0.51811, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
            "3600/3600 [==============================] - 1s 187us/sample - loss: 0.7480 - acc: 0.7242 - val_loss: 0.5181 - val_acc: 0.8308\n",
            "Epoch 390/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7698 - acc: 0.7124\n",
            "Epoch 00390: val_loss did not improve from 0.51811\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7750 - acc: 0.7106 - val_loss: 0.5854 - val_acc: 0.7867\n",
            "Epoch 391/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7327 - acc: 0.7348\n",
            "Epoch 00391: val_loss did not improve from 0.51811\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.7367 - acc: 0.7308 - val_loss: 0.6419 - val_acc: 0.7683\n",
            "Epoch 392/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7565 - acc: 0.7185\n",
            "Epoch 00392: val_loss did not improve from 0.51811\n",
            "3600/3600 [==============================] - 1s 168us/sample - loss: 0.7553 - acc: 0.7186 - val_loss: 0.6682 - val_acc: 0.7525\n",
            "Epoch 393/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7859 - acc: 0.7066\n",
            "Epoch 00393: val_loss did not improve from 0.51811\n",
            "3600/3600 [==============================] - 1s 173us/sample - loss: 0.7843 - acc: 0.7081 - val_loss: 0.5987 - val_acc: 0.7992\n",
            "Epoch 394/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7270 - acc: 0.7300\n",
            "Epoch 00394: val_loss did not improve from 0.51811\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7335 - acc: 0.7247 - val_loss: 0.5706 - val_acc: 0.7967\n",
            "Epoch 395/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7295 - acc: 0.7339\n",
            "Epoch 00395: val_loss did not improve from 0.51811\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7326 - acc: 0.7331 - val_loss: 0.5638 - val_acc: 0.8167\n",
            "Epoch 396/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7553 - acc: 0.7239\n",
            "Epoch 00396: val_loss did not improve from 0.51811\n",
            "3600/3600 [==============================] - 1s 169us/sample - loss: 0.7527 - acc: 0.7242 - val_loss: 0.5818 - val_acc: 0.7925\n",
            "Epoch 397/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7712 - acc: 0.7224\n",
            "Epoch 00397: val_loss did not improve from 0.51811\n",
            "3600/3600 [==============================] - 1s 167us/sample - loss: 0.7745 - acc: 0.7208 - val_loss: 0.5536 - val_acc: 0.8133\n",
            "Epoch 398/400\n",
            "3300/3600 [==========================>...] - ETA: 0s - loss: 0.7824 - acc: 0.7061\n",
            "Epoch 00398: val_loss did not improve from 0.51811\n",
            "3600/3600 [==============================] - 1s 170us/sample - loss: 0.7798 - acc: 0.7064 - val_loss: 0.5243 - val_acc: 0.8383\n",
            "Epoch 399/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7224 - acc: 0.7334\n",
            "Epoch 00399: val_loss did not improve from 0.51811\n",
            "3600/3600 [==============================] - 1s 171us/sample - loss: 0.7312 - acc: 0.7286 - val_loss: 0.5412 - val_acc: 0.8125\n",
            "Epoch 400/400\n",
            "3200/3600 [=========================>....] - ETA: 0s - loss: 0.7512 - acc: 0.7259\n",
            "Epoch 00400: val_loss did not improve from 0.51811\n",
            "3600/3600 [==============================] - 1s 172us/sample - loss: 0.7601 - acc: 0.7228 - val_loss: 0.5873 - val_acc: 0.7925\n",
            "1200/1200 [==============================] - 0s 103us/sample - loss: 0.5873 - acc: 0.7925\n",
            "[0.5872603336970011, 0.7925]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGywLqcAFgdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FS = 250\n",
        "N = 350\n",
        "# sample spacing\n",
        "T = 1.0 / FS\n",
        "x = np.linspace(0.0, N*T, N)\n",
        "num_labels= 8\n",
        "#plt.plot(mne_array[2][2], np.linspace(0.0,N,N))\n",
        "\n",
        "#fig, axs = plt.subplots(8,8, figsize=(4*8,4*8))\n",
        "#freq_data = list()\n",
        "#for i in range(8):\n",
        "#  for x in range(8):\n",
        "#    f, t, Sxx = signal.spectrogram(mne_array[x][i], fs=FS, nperseg=50, window=('hamming'), noverlap=35)\n",
        "#    freq_data.append(Sxx)\n",
        "#    axs[i,x].pcolormesh(t, f, Sxx)\n",
        "#plt.show()\n",
        "#print(open(\"~/keras/keras.json\").read())\n",
        "\n",
        "#events_arr = list(map(int, events_arr))\n",
        "standardized_data_eeg = np.swapaxes(standardized_data,0,2)\n",
        "standardized_data_eeg=standardized_data_eeg.reshape([1600,1,8, 350])\n",
        "X_train, X_test, y_train, y_test = train_test_split(standardized_data_eeg, events_arr, test_size=0.33, random_state=42)\n",
        "#y_train = np.array(y_train)\n",
        "y_train = to_categorical(y_train)\n",
        "#y_test = np.array(y_test)\n",
        "y_test = to_categorical(y_test)\n",
        "# Construct model \n",
        "\n",
        "\n",
        "num_epochs = 50\n",
        "num_batch_size = 10\n",
        "\n",
        "model = EEGNet(nb_classes = 9, Chans = 8, Samples = 350)\n",
        "model.compile(loss = 'categorical_crossentropy', metrics=['accuracy'],optimizer = 'adam')\n",
        "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "start = datetime.now()\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, \\\n",
        "          validation_data=(X_test, y_test),callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "#preds = model.predict(X_test, verbose=0)\n",
        "#acc = accuracy_score(y_test, preds)\n",
        "\n",
        "print(\"accuracy_score: {f}\".format(f=score))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}